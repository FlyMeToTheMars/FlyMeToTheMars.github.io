<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/xingqiushangcheng.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/xingqiushangcheng.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark,">





  <link rel="alternate" href="/atom.xml" title="Mars" type="application/atom+xml">



  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "bfb40164"
    });
  daovoice('update');
  </script>





<meta name="description" content="Spark架构，运行原理，任务调度和资源调度分析，内存管理分析，SparkSQL，SparkSreaming与kafaka，数据倾斜的解决，调优。">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark知识整合">
<meta property="og:url" content="http://yoursite.com/2020/03/09/Spark 知识整合/index.html">
<meta property="og:site_name" content="Mars">
<meta property="og:description" content="Spark架构，运行原理，任务调度和资源调度分析，内存管理分析，SparkSQL，SparkSreaming与kafaka，数据倾斜的解决，调优。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://chant00.com/media/15050347352078.jpg">
<meta property="og:image" content="http://chant00.com/media/15050348791707.jpg">
<meta property="og:image" content="http://chant00.com/media/15050356690153.jpg">
<meta property="og:image" content="http://chant00.com/media/15050362138086.jpg">
<meta property="og:image" content="http://chant00.com/media/15050363083877.jpg">
<meta property="og:image" content="http://chant00.com/media/15050364198942.jpg">
<meta property="og:image" content="http://chant00.com/media/15050364665674.jpg">
<meta property="og:image" content="http://chant00.com/media/15050365718731.jpg">
<meta property="og:image" content="http://chant00.com/media/15050366172339.jpg">
<meta property="og:image" content="http://chant00.com/media/15050368012564.jpg">
<meta property="og:image" content="http://chant00.com/media/15050368218982.jpg">
<meta property="og:image" content="http://chant00.com/media/15050366896862.jpg">
<meta property="og:image" content="http://chant00.com/media/15050367305609.jpg">
<meta property="og:image" content="http://chant00.com/media/15050369109617.jpg">
<meta property="og:image" content="http://chant00.com/media/15050369658168.jpg">
<meta property="og:image" content="http://chant00.com/media/15050414638959.jpg">
<meta property="og:image" content="http://chant00.com/media/15051361379636.jpg">
<meta property="og:image" content="http://chant00.com/media/15051371637582.jpg">
<meta property="og:image" content="http://chant00.com/media/15051424923899.jpg">
<meta property="og:image" content="http://chant00.com/media/15051439297624.jpg">
<meta property="og:image" content="http://chant00.com/media/15051439652988.jpg">
<meta property="og:image" content="http://chant00.com/media/15051440398327.jpg">
<meta property="og:image" content="http://chant00.com/media/15051442295868.jpg">
<meta property="og:image" content="http://chant00.com/media/15051449619762.jpg">
<meta property="og:image" content="http://chant00.com/media/15052093011051.jpg">
<meta property="og:image" content="http://chant00.com/media/15052093251093.jpg">
<meta property="og:image" content="http://chant00.com/media/15052093599400.jpg">
<meta property="og:image" content="http://chant00.com/media/15052093791909.jpg">
<meta property="og:image" content="http://chant00.com/media/15052094179523.jpg">
<meta property="og:image" content="http://chant00.com/media/15052092504222.jpg">
<meta property="og:image" content="http://chant00.com/media/15052153748327.jpg">
<meta property="og:image" content="http://chant00.com/media/15052101744162.jpg">
<meta property="og:image" content="http://chant00.com/media/15052102502535.jpg">
<meta property="og:image" content="http://chant00.com/media/spark_%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90_Driver%E8%BF%9B%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA.png">
<meta property="og:image" content="http://chant00.com/media/spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90_%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6_Executor%E7%9A%84%E5%90%AF%E5%8A%A8.png">
<meta property="og:image" content="http://chant00.com/media/15052168177467.jpg">
<meta property="og:image" content="http://chant00.com/media/15052168728002.jpg">
<meta property="og:image" content="http://chant00.com/media/15052168987010.jpg">
<meta property="og:image" content="http://chant00.com/media/15052169289580.jpg">
<meta property="og:image" content="http://chant00.com/media/15052169381307.jpg">
<meta property="og:image" content="http://chant00.com/media/15052170871109.jpg">
<meta property="og:image" content="http://chant00.com/media/15052171146792.jpg">
<meta property="og:image" content="http://chant00.com/media/15052171461798.jpg">
<meta property="og:image" content="http://chant00.com/media/15052171555883.jpg">
<meta property="og:image" content="http://chant00.com/media/15052171893498.jpg">
<meta property="og:image" content="http://chant00.com/media/15052171986395.jpg">
<meta property="og:image" content="http://chant00.com/media/15052172249932.jpg">
<meta property="og:image" content="http://chant00.com/media/spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6.png">
<meta property="og:image" content="http://chant00.com/media/15052175596102.jpg">
<meta property="og:image" content="http://chant00.com/media/15052177677901.jpg">
<meta property="og:image" content="http://chant00.com/media/15052183410200.jpg">
<meta property="og:image" content="http://chant00.com/media/15052186244693.jpg">
<meta property="og:image" content="http://chant00.com/media/15052189861132.jpg">
<meta property="og:image" content="http://chant00.com/media/15052194149122.jpg">
<meta property="og:image" content="http://chant00.com/media/15052195976097.jpg">
<meta property="og:image" content="http://chant00.com/media/15052199735375.jpg">
<meta property="og:image" content="http://chant00.com/media/15052199860032.jpg">
<meta property="og:image" content="http://chant00.com/media/15052200135780.jpg">
<meta property="og:image" content="http://chant00.com/media/15052200295354.jpg">
<meta property="og:image" content="http://chant00.com/media/15052200673813.jpg">
<meta property="og:image" content="http://chant00.com/media/15052209033503.jpg">
<meta property="og:image" content="http://chant00.com/media/15052209140931.jpg">
<meta property="og:image" content="http://chant00.com/media/15052209386045.jpg">
<meta property="og:image" content="http://chant00.com/media/15052210085405.jpg">
<meta property="og:image" content="http://chant00.com/media/15052210190366.jpg">
<meta property="og:image" content="http://chant00.com/media/15052210621811.jpg">
<meta property="og:image" content="http://chant00.com/media/15052213964855.jpg">
<meta property="og:image" content="http://chant00.com/media/15052299928793.jpg">
<meta property="og:image" content="http://chant00.com/media/15052328013148.jpg">
<meta property="og:image" content="http://chant00.com/media/15052329433749.jpg">
<meta property="og:image" content="http://chant00.com/media/15052834285291.jpg">
<meta property="og:image" content="http://chant00.com/media/15053969470635.jpg">
<meta property="og:image" content="http://chant00.com/media/15052969860133.jpg">
<meta property="og:image" content="http://chant00.com/media/15053921543814.jpg">
<meta property="og:image" content="http://chant00.com/media/15052983161313.jpg">
<meta property="og:image" content="http://chant00.com/media/15052983913084.jpg">
<meta property="og:image" content="http://chant00.com/media/15052992976209.jpg">
<meta property="og:image" content="http://chant00.com/media/15053155944357.jpg">
<meta property="og:image" content="http://chant00.com/media/15053158002878.jpg">
<meta property="og:image" content="http://chant00.com/media/15053179353512.jpg">
<meta property="og:image" content="http://chant00.com/media/15053180739268.jpg">
<meta property="og:image" content="http://chant00.com/media/15053191033367.jpg">
<meta property="og:image" content="http://chant00.com/media/15053668820845.jpg">
<meta property="og:image" content="http://chant00.com/media/15053673731094.jpg">
<meta property="og:image" content="http://chant00.com/media/15053704805816.jpg">
<meta property="og:image" content="http://chant00.com/media/15053705012302.jpg">
<meta property="og:image" content="http://chant00.com/media/SparkStreaming%E5%92%8CKafka%E6%95%B4%E5%90%88%E5%8E%9F%E7%90%86.png">
<meta property="og:updated_time" content="2020-01-20T06:56:54.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark知识整合">
<meta name="twitter:description" content="Spark架构，运行原理，任务调度和资源调度分析，内存管理分析，SparkSQL，SparkSreaming与kafaka，数据倾斜的解决，调优。">
<meta name="twitter:image" content="http://chant00.com/media/15050347352078.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/03/09/Spark 知识整合/">





  <title>Spark知识整合 | Mars</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'f6a8160e9467bb9adc80f36030e1c37b', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Mars</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/09/Spark 知识整合/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Fly Hugh">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mars">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark知识整合</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-03-09T17:06:50+08:00">
                2020-03-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Apache/" itemprop="url" rel="index">
                    <span itemprop="name">Apache</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Apache/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2020/03/09/Spark 知识整合/" class="leancloud_visitors" data-flag-title="Spark知识整合">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  26k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  114
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>Spark架构，运行原理，任务调度和资源调度分析，内存管理分析，SparkSQL，SparkSreaming与kafaka，数据倾斜的解决，调优。</p>
</blockquote>
<a id="more"></a> 
<h3 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h3><p>Spark是美国加州大学伯克利分校的AMP实验室（主要创始人lester和Matei）开发的通用的大数据处理框架。</p>
<blockquote>
<p>Apache Spark™ is a fast and general engine for large-scale data processing.<br>Apache Spark is an open source cluster computing system that aims to make data analytics fast,both fast to run and fast to wrtie</p>
</blockquote>
<p>Spark应用程序可以使用R语言、Java、Scala和Python进行编写，极少使用R语言编写Spark程序，Java和Scala语言编写的Spark程序的执行效率是相同的，但Java语言写的代码量多，Scala简洁优雅，但可读性不如Java，Python语言编写的Spark程序的执行效率不如Java和Scala。</p>
<p>Spark有4中运行模式：</p>
<ol>
<li>local模式，适用于测试</li>
<li>standalone，并非是单节点，而是使用spark自带的资源调度框架</li>
<li>yarn，最流行的方式，使用yarn集群调度资源</li>
<li>mesos，国外使用的多</li>
</ol>
<h4 id="Spark比MapReduce快的原因"><a href="#Spark比MapReduce快的原因" class="headerlink" title="Spark比MapReduce快的原因"></a>Spark比MapReduce快的原因</h4><ol>
<li><p>Spark基于内存迭代，而MapReduce基于磁盘迭代<br>MapReduce的设计：中间结果保存到文件，可以提高可靠性，减少内存占用，但是牺牲了性能。<br>Spark的设计：数据在内存中进行交换，要快一些，但是内存这个东西，可靠性比不过MapReduce。</p>
</li>
<li><p>DAG计算模型在迭代计算上还是比MR的更有效率。<br>在图论中，如果一个有向图无法从某个顶点出发经过若干条边回到该点，则这个图是一个有向无环图（DAG）</p>
<p>DAG计算模型在Spark任务调度中详解！<br>Spark计算比MapReduce快的根本原因在于DAG计算模型。一般而言，DAG相比MapReduce在大多数情况下可以减少shuffle次数。Spark的DAGScheduler相当于一个改进版的MapReduce，如果计算不涉及与其他节点进行数据交换，Spark可以在内存中一次性完成这些操作，也就是中间结果无须落盘，减少了磁盘IO的操作。但是，如果计算过程中涉及数据交换，Spark也是会把shuffle的数据写磁盘的！有一个误区，Spark是基于内存的计算，所以快，这不是主要原因，要对数据做计算，必然得加载到内存，Hadoop也是如此，只不过Spark支持将需要反复用到的数据给Cache到内存中，减少数据加载耗时，所以Spark跑机器学习算法比较在行（需要对数据进行反复迭代）。Spark基于磁盘的计算也是比Hadoop快。刚刚提到了Spark的DAGScheduler是个改进版的MapReduce，所以Spark天生适合做批处理的任务。Hadoop的MapReduce虽然不如spark性能好，但是HDFS仍然是业界的大数据存储标准。</p>
</li>
<li><p>Spark是粗粒度的资源调度，而MR是细粒度的资源调度。<br>粗细粒度的资源调度，在Spark资源调度中详解！</p>
</li>
</ol>
<h4 id="RDD（Resilient-Distributed-Dataset-弹性分布式数据集"><a href="#RDD（Resilient-Distributed-Dataset-弹性分布式数据集" class="headerlink" title="RDD（Resilient Distributed Dataset )-弹性分布式数据集"></a>RDD（Resilient Distributed Dataset )-弹性分布式数据集</h4><blockquote>
<p>A list of partitions<br>A function for computing each partition<br>A list of dependencies on other RDDs<br>Optionally, a Partitioner for key-value RDDs<br>Optionally, a list of preferred locations to compute each split on</p>
</blockquote>
<p><a href="http://chant00.com/media/15050347352078.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050347352078.jpg" alt="img"></a><br>RDD之间的依赖关系称作为Lineage——血统</p>
<h4 id="Spark任务执行流程"><a href="#Spark任务执行流程" class="headerlink" title="Spark任务执行流程"></a>Spark任务执行流程</h4><p><a href="http://chant00.com/media/15050348791707.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050348791707.jpg" alt="img"></a></p>
<h4 id="写一个Spark应用程序的流程"><a href="#写一个Spark应用程序的流程" class="headerlink" title="写一个Spark应用程序的流程"></a>写一个Spark应用程序的流程</h4><h5 id="1-加载数据集（获得RDD）"><a href="#1-加载数据集（获得RDD）" class="headerlink" title="1.加载数据集（获得RDD）"></a>1.加载数据集（获得RDD）</h5><p>可以从HDFS，NoSQL数据库中加载数据集</p>
<h5 id="2-使用transformations算子对RDD进行操作"><a href="#2-使用transformations算子对RDD进行操作" class="headerlink" title="2.使用transformations算子对RDD进行操作"></a>2.使用transformations算子对RDD进行操作</h5><p>transformations算子是一系列懒执行的函数</p>
<h5 id="3-使用actions算子触发执行"><a href="#3-使用actions算子触发执行" class="headerlink" title="3.使用actions算子触发执行"></a>3.使用actions算子触发执行</h5><p>transformations算子对RDD的操作会被先记录，当actions算子触发后才会真正执行</p>
<p>伪代码示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lines = sc.textFile(“hdfs://...”) //加载数据集</span><br><span class="line">errors = lines.filter(_.startsWith(“ERROR”)) //transformations算子</span><br><span class="line">lines.filter(x=&gt;&#123;x.startsWith(“ERROR”)&#125;) //transformations算子</span><br><span class="line">Mysql_errors = errors.filter(_.contain(“MySQL”)).count //count是actions算子</span><br><span class="line">http_errors = errors.filter(_.contain(“Http”)).count</span><br></pre></td></tr></table></figure>
<h3 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h3><p><a href="http://chant00.com/media/15050356690153.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050356690153.jpg" alt="img"></a></p>
<h4 id="Actions"><a href="#Actions" class="headerlink" title="Actions"></a>Actions</h4><h5 id="count：统计RDD中元素的个数"><a href="#count：统计RDD中元素的个数" class="headerlink" title="count：统计RDD中元素的个数"></a>count：统计RDD中元素的个数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(&quot;hello&quot;,&quot;hello&quot;,&quot;hello&quot;,&quot;world&quot;))</span><br><span class="line">val num = rdd.count()</span><br><span class="line">println(num)</span><br><span class="line">结果：</span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<h5 id="foreach：遍历RDD中的元素"><a href="#foreach：遍历RDD中的元素" class="headerlink" title="foreach：遍历RDD中的元素"></a>foreach：遍历RDD中的元素</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(&quot;hello&quot;,&quot;hello&quot;,&quot;hello&quot;,&quot;world&quot;))</span><br><span class="line">rdd.foreach(println)</span><br><span class="line">结果：</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">world</span><br></pre></td></tr></table></figure>
<h5 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h5><p>foreach以一条记录为单位来遍历RDD<br>foreachPartition以分区为单位遍历RDD<br>foreach和foreachPartition都是actions算子<br>map和mapPartition可以与它们做类比，但map和mapPartitions是transformations算子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//设置rdd的分区数为2</span><br><span class="line">val rdd = sc.parallelize(1 to 6, 2)</span><br><span class="line">rdd.foreachPartition(x =&gt; &#123;</span><br><span class="line">  println(&quot;data from a partition:&quot;)</span><br><span class="line">  while(x.hasNext) &#123;</span><br><span class="line">    println(x.next())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line">结果：</span><br><span class="line">data from a partition:</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">data from a partition:</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td></tr></table></figure>
<h5 id="collect：把运行结果拉回到Driver端"><a href="#collect：把运行结果拉回到Driver端" class="headerlink" title="collect：把运行结果拉回到Driver端"></a>collect：把运行结果拉回到Driver端</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(</span><br><span class="line">  (5,&quot;Tom&quot;),(10,&quot;Jed&quot;),(3,&quot;Tony&quot;),(2,&quot;Jack&quot;)</span><br><span class="line">))</span><br><span class="line">val resultRDD = rdd.sortByKey()</span><br><span class="line">val list = resultRDD.collect()</span><br><span class="line">list.foreach(println)</span><br><span class="line">结果：</span><br><span class="line">(2,Jack)</span><br><span class="line">(3,Tony)</span><br><span class="line">(5,Tom)</span><br><span class="line">(10,Jed)</span><br></pre></td></tr></table></figure>
<h5 id="take-n-：取RDD中的前n个元素"><a href="#take-n-：取RDD中的前n个元素" class="headerlink" title="take(n)：取RDD中的前n个元素"></a>take(n)：取RDD中的前n个元素</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(&quot;hello&quot;,&quot;hello&quot;,&quot;hello&quot;,&quot;world&quot;))</span><br><span class="line">rdd.take(2).foreach(println)</span><br><span class="line">结果：</span><br><span class="line">hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure>
<h5 id="first-：相当于take-1"><a href="#first-：相当于take-1" class="headerlink" title="first ：相当于take(1)"></a>first ：相当于take(1)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(&quot;hello&quot;,&quot;hello&quot;,&quot;hello&quot;,&quot;world&quot;))</span><br><span class="line">println(rdd.first)</span><br><span class="line">结果：</span><br><span class="line">Hello</span><br></pre></td></tr></table></figure>
<h5 id="reduce：按照指定规则聚合RDD中的元素"><a href="#reduce：按照指定规则聚合RDD中的元素" class="headerlink" title="reduce：按照指定规则聚合RDD中的元素"></a>reduce：按照指定规则聚合RDD中的元素</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val numArr = Array(1,2,3,4,5)</span><br><span class="line">val rdd = sc.parallelize(numArr)</span><br><span class="line">val sum = rdd.reduce(_+_)</span><br><span class="line">println(sum)</span><br><span class="line">结果：</span><br><span class="line">15</span><br></pre></td></tr></table></figure>
<h5 id="countByKey：统计出KV格式的RDD中相同的K的个数"><a href="#countByKey：统计出KV格式的RDD中相同的K的个数" class="headerlink" title="countByKey：统计出KV格式的RDD中相同的K的个数"></a>countByKey：统计出KV格式的RDD中相同的K的个数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Array(</span><br><span class="line">  (&quot;销售部&quot;,&quot;Tom&quot;), (&quot;销售部&quot;,&quot;Jack&quot;),(&quot;销售部&quot;,&quot;Bob&quot;),(&quot;销售部&quot;,&quot;Terry&quot;),</span><br><span class="line">  (&quot;后勤部&quot;,&quot;Jack&quot;),(&quot;后勤部&quot;,&quot;Selina&quot;),(&quot;后勤部&quot;,&quot;Hebe&quot;),</span><br><span class="line">  (&quot;人力部&quot;,&quot;Ella&quot;),(&quot;人力部&quot;,&quot;Harry&quot;),</span><br><span class="line">  (&quot;开发部&quot;,&quot;Allen&quot;)</span><br><span class="line">))</span><br><span class="line">val result = rdd.countByKey();</span><br><span class="line">result.foreach(println)</span><br><span class="line">结果：</span><br><span class="line">(后勤部,3)</span><br><span class="line">(开发部,1)</span><br><span class="line">(销售部,4)</span><br><span class="line">(人力部,2)</span><br></pre></td></tr></table></figure>
<h5 id="countByValue：统计出RDD中每个元素的个数"><a href="#countByValue：统计出RDD中每个元素的个数" class="headerlink" title="countByValue：统计出RDD中每个元素的个数"></a>countByValue：统计出RDD中每个元素的个数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Array(</span><br><span class="line">  &quot;Tom&quot;,&quot;Jed&quot;,&quot;Tom&quot;,</span><br><span class="line">  &quot;Tom&quot;,&quot;Jed&quot;,&quot;Jed&quot;,</span><br><span class="line">  &quot;Tom&quot;,&quot;Tony&quot;,&quot;Jed&quot;</span><br><span class="line">))</span><br><span class="line">val result = rdd.countByValue();</span><br><span class="line">result.foreach(println)</span><br><span class="line">结果：</span><br><span class="line">(Tom,4)</span><br><span class="line">(Tony,1)</span><br><span class="line">(Jed,4)</span><br></pre></td></tr></table></figure>
<h4 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h4><h5 id="filter：过滤"><a href="#filter：过滤" class="headerlink" title="filter：过滤"></a>filter：过滤</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(&quot;hello&quot;,&quot;hello&quot;,&quot;hello&quot;,&quot;world&quot;))</span><br><span class="line">rdd.filter(!_.contains(&quot;hello&quot;)).foreach(println)</span><br><span class="line">结果：</span><br><span class="line">world</span><br></pre></td></tr></table></figure>
<h5 id="map-和flatMap"><a href="#map-和flatMap" class="headerlink" title="map 和flatMap"></a>map 和flatMap</h5><p><a href="http://chant00.com/media/15050362138086.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050362138086.jpg" alt="img"></a></p>
<h5 id="sample-：随机抽样"><a href="#sample-：随机抽样" class="headerlink" title="sample ：随机抽样"></a>sample ：随机抽样</h5><p>sample(withReplacement: Boolean, fraction: Double, seed: Long)<br>withReplacement : 是否是放回式抽样<br>true代表如果抽中A元素，之后还可以抽取A元素<br>false代表如果抽住了A元素，之后都不在抽取A元素<br>fraction : 抽样的比例<br>seed : 抽样算法的初始值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(</span><br><span class="line">  &quot;hello1&quot;,&quot;hello2&quot;,&quot;hello3&quot;,&quot;hello4&quot;,&quot;hello5&quot;,&quot;hello6&quot;,</span><br><span class="line">  &quot;world1&quot;,&quot;world2&quot;,&quot;world3&quot;,&quot;world4&quot;</span><br><span class="line">))</span><br><span class="line">rdd.sample(false, 0.3).foreach(println)</span><br><span class="line">结果：</span><br><span class="line">hello4</span><br><span class="line">world1</span><br><span class="line">在数据量不大的时候，不会很准确</span><br></pre></td></tr></table></figure>
<h5 id="groupByKey和reduceByKey"><a href="#groupByKey和reduceByKey" class="headerlink" title="groupByKey和reduceByKey"></a>groupByKey和reduceByKey</h5><p><a href="http://chant00.com/media/15050363083877.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050363083877.jpg" alt="img"></a></p>
<h5 id="sortByKey：按key进行排序"><a href="#sortByKey：按key进行排序" class="headerlink" title="sortByKey：按key进行排序"></a>sortByKey：按key进行排序</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(</span><br><span class="line">  (5,&quot;Tom&quot;),(10,&quot;Jed&quot;),(3,&quot;Tony&quot;),(2,&quot;Jack&quot;)</span><br><span class="line">))</span><br><span class="line">rdd.sortByKey().foreach(println)</span><br><span class="line">结果：</span><br><span class="line">(2,Jack)</span><br><span class="line">(3,Tony)</span><br><span class="line">(5,Tom)</span><br><span class="line">(10,Jed)</span><br><span class="line">说明：</span><br><span class="line">sortByKey(fasle)：倒序</span><br></pre></td></tr></table></figure>
<h5 id="sortBy：自定义排序规则"><a href="#sortBy：自定义排序规则" class="headerlink" title="sortBy：自定义排序规则"></a>sortBy：自定义排序规则</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">object SortByOperator &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">   val conf = new SparkConf().setAppName(&quot;TestSortBy&quot;).setMaster(&quot;local&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val arr = Array(</span><br><span class="line">        Tuple3(190,100,&quot;Jed&quot;),</span><br><span class="line">        Tuple3(100,202,&quot;Tom&quot;),</span><br><span class="line">        Tuple3(90,111,&quot;Tony&quot;)</span><br><span class="line">    )</span><br><span class="line">    val rdd = sc.parallelize(arr)</span><br><span class="line">    rdd.sortBy(_._1).foreach(println)</span><br><span class="line">    /* (90,111,Tony)</span><br><span class="line">       (100,202,Tom)</span><br><span class="line">       (190,100,Jed)</span><br><span class="line">     */</span><br><span class="line">    rdd.sortBy(_._2).foreach(println)</span><br><span class="line">    /*(190,100,Jed)</span><br><span class="line">       (90,111,Tony)</span><br><span class="line">       (100,202,Tom)</span><br><span class="line">     */</span><br><span class="line">    rdd.sortBy(_._3).foreach(println)</span><br><span class="line">    /*</span><br><span class="line">       (190,100,Jed)</span><br><span class="line">       (100,202,Tom)</span><br><span class="line">       (90,111,Tony)</span><br><span class="line">     */</span><br><span class="line">    sc.stop();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="distinct：去掉重复数据"><a href="#distinct：去掉重复数据" class="headerlink" title="distinct：去掉重复数据"></a>distinct：去掉重复数据</h5><p>distinct算子实际上经过了以下步骤：<br><a href="http://chant00.com/media/15050364198942.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050364198942.jpg" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.makeRDD(Array(</span><br><span class="line">      &quot;hello&quot;,</span><br><span class="line">      &quot;hello&quot;,</span><br><span class="line">      &quot;hello&quot;,</span><br><span class="line">      &quot;world&quot;</span><br><span class="line">))</span><br><span class="line">val distinctRDD = rdd</span><br><span class="line">      .map &#123;(_,1)&#125;</span><br><span class="line">      .reduceByKey(_+_)</span><br><span class="line">      .map(_._1)</span><br><span class="line">distinctRDD.foreach &#123;println&#125;</span><br><span class="line">等价于：</span><br><span class="line">rdd.distinct().foreach &#123;println&#125;</span><br></pre></td></tr></table></figure>
<h5 id="join"><a href="#join" class="headerlink" title="join"></a>join</h5><p>先看看SQL中的join<br>假设有如下两张表：table A是左表，table B是右表<br><a href="http://chant00.com/media/15050364665674.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050364665674.jpg" alt="img"></a></p>
<p>不同join方式会有不同的结果</p>
<h6 id="1-Inner-join"><a href="#1-Inner-join" class="headerlink" title="1.Inner join"></a>1.Inner join</h6><p>产生的结果集是A和B的交集<br><a href="http://chant00.com/media/15050365718731.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050365718731.jpg" alt="img"></a><br>执行SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM TableA</span><br><span class="line">INNER JOIN TableB</span><br><span class="line">ON TableA.name = TableB.name</span><br></pre></td></tr></table></figure>
<p>结果：<br><a href="http://chant00.com/media/15050366172339.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050366172339.jpg" alt="img"></a></p>
<h6 id="2-Left-outer-join"><a href="#2-Left-outer-join" class="headerlink" title="2.Left outer join"></a>2.Left outer join</h6><p>产生表A的完全集，而B表中匹配的则有值，没有匹配的则以null值取代<br><a href="http://chant00.com/media/15050368012564.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050368012564.jpg" alt="img"></a></p>
<p>执行SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM TableA</span><br><span class="line">LEFT OUTER JOIN TableB</span><br><span class="line">ON TableA.name = TableB.name</span><br></pre></td></tr></table></figure>
<p>结果：<br><a href="http://chant00.com/media/15050368218982.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050368218982.jpg" alt="img"></a></p>
<h6 id="3-Right-outer-join"><a href="#3-Right-outer-join" class="headerlink" title="3.Right outer join"></a>3.Right outer join</h6><p>产生表B的完全集，而A表中匹配的则有值，没有匹配的则以null值取代<br><a href="http://chant00.com/media/15050366896862.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050366896862.jpg" alt="img"></a><br>执行SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM TableA</span><br><span class="line">RIGHT OUTER JOIN TableB</span><br><span class="line">ON TableA.name = TableB.name</span><br></pre></td></tr></table></figure>
<p>结果：<br><a href="http://chant00.com/media/15050367305609.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050367305609.jpg" alt="img"></a></p>
<h6 id="4-Full-outer-join（MySQL不支持）"><a href="#4-Full-outer-join（MySQL不支持）" class="headerlink" title="4.Full outer join（MySQL不支持）"></a>4.Full outer join（MySQL不支持）</h6><p>产生A和B的并集，但是需要注意的是，对于没有匹配的记录，则会以null做为值<br><a href="http://chant00.com/media/15050369109617.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050369109617.jpg" alt="img"></a></p>
<p>执行SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM TableA</span><br><span class="line">FULL OUTER JOIN TableB</span><br><span class="line">ON TableA.name = TableB.name</span><br></pre></td></tr></table></figure>
<p>结果：<br><a href="http://chant00.com/media/15050369658168.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050369658168.jpg" alt="img"></a></p>
<p>在Spark的算子中，对两个RDD进行join有着类似的作用<br>假设有两个RDD：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">val nameList = List(</span><br><span class="line">      (1,&quot;Jed&quot;),</span><br><span class="line">      (2,&quot;Tom&quot;),</span><br><span class="line">      (3,&quot;Bob&quot;),</span><br><span class="line">      (4,&quot;Tony&quot;)</span><br><span class="line">)</span><br><span class="line">   </span><br><span class="line">val salaryArr = Array(</span><br><span class="line">      (1,8000),</span><br><span class="line">      (2,6000),</span><br><span class="line">      (3,5000)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * parallelize(Seq[T],Int num)</span><br><span class="line"> * 使用指定的集合(可以是List、Array等)来创建RDD</span><br><span class="line"> * num 指定RDD的分区数，默认为1</span><br><span class="line"> * 这个方法经常用于测试环境</span><br><span class="line"> * join产生的RDD的分区数由分区数最多的父RDD决定</span><br><span class="line"> */</span><br><span class="line">val nameRDD = sc.parallelize(nameList,2)</span><br><span class="line">val salaryRDD = sc.parallelize(salaryArr,3)</span><br></pre></td></tr></table></figure>
<p>分别对4种join做测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val joinRDD = nameRDD.join(salaryRDD)</span><br><span class="line">joinRDD.foreach( x =&gt; &#123;</span><br><span class="line">    val id = x._1</span><br><span class="line">    val name = x._2._1</span><br><span class="line">    val salary = x._2._2</span><br><span class="line">    println(id + &quot;\t&quot; + name + &quot;\t&quot; + salary)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>结果：<br>1 Jed 8000<br>2 Tom 6000<br>3 Bob 5000</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val leftOuterJoinRDD = nameRDD.leftOuterJoin(salaryRDD)</span><br><span class="line">leftOuterJoinRDD.foreach( x =&gt; &#123;</span><br><span class="line">      val id = x._1</span><br><span class="line">      val name = x._2._1</span><br><span class="line">      val salary = x._2._2</span><br><span class="line">      println(id + &quot;\t&quot; + name + &quot;\t&quot; + salary)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>结果：<br>1 Jed Some(8000)<br>2 Tom Some(6000)<br>3 Bob Some(5000)<br>4 Tony None</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rightOuterJoinRDD = nameRDD.rightOuterJoin(salaryRDD)</span><br><span class="line">rightOuterJoinRDD.foreach( x =&gt; &#123;</span><br><span class="line">      val id = x._1</span><br><span class="line">      val name = x._2._1</span><br><span class="line">      val salary = x._2._2</span><br><span class="line">      println(id + &quot;\t&quot; + name + &quot;\t&quot; + salary)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>结果：<br>1 Some(Jed) 8000<br>2 Some(Tom) 6000<br>3 Some(Bob) 5000</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val fullOuterJoinRDD = nameRDD.fullOuterJoin(salaryRDD)</span><br><span class="line">fullOuterJoinRDD.foreach( x =&gt; &#123;</span><br><span class="line">      val id = x._1</span><br><span class="line">      val name = x._2._1</span><br><span class="line">      val salary = x._2._2</span><br><span class="line">      println(id + &quot;\t&quot; + name + &quot;\t&quot; + salary)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>结果：<br>1 Some(Jed) Some(8000)<br>2 Some(Tom) Some(6000)<br>3 Some(Bob) Some(5000)<br>4 Some(Tony) None</p>
<h5 id="union：把两个RDD进行逻辑上的合并"><a href="#union：把两个RDD进行逻辑上的合并" class="headerlink" title="union：把两个RDD进行逻辑上的合并"></a>union：把两个RDD进行逻辑上的合并</h5><p>union这个算子关联的两个RDD必须类型一致</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 =sc.makeRDD(1 to 10)</span><br><span class="line">val rdd2 = sc.parallelize(11 until 20)</span><br><span class="line">rdd1.union(rdd2).foreach &#123;println&#125;</span><br></pre></td></tr></table></figure>
<h5 id="map和mapPartitions"><a href="#map和mapPartitions" class="headerlink" title="map和mapPartitions"></a>map和mapPartitions</h5><p>map()会一条记录为单位进行操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">val arr = Array(&quot;Tom&quot;,&quot;Bob&quot;,&quot;Tony&quot;,&quot;Jerry&quot;)</span><br><span class="line">//把4条数据分到两个分区中</span><br><span class="line">val rdd = sc.parallelize(arr,2)</span><br><span class="line">   </span><br><span class="line">/*</span><br><span class="line"> * 模拟把RDD中的元素写入数据库的过程</span><br><span class="line"> */</span><br><span class="line">rdd.map(x =&gt; &#123;</span><br><span class="line">  println(&quot;创建数据库连接...&quot;)</span><br><span class="line">  println(&quot;写入数据库...&quot;)</span><br><span class="line">  println(&quot;关闭数据库连接...&quot;)</span><br><span class="line">  println()</span><br><span class="line">&#125;).count()</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">创建数据库连接...</span><br><span class="line">写入数据库...</span><br><span class="line">关闭数据库连接...</span><br><span class="line"></span><br><span class="line">创建数据库连接...</span><br><span class="line">写入数据库...</span><br><span class="line">关闭数据库连接...</span><br><span class="line"></span><br><span class="line">创建数据库连接...</span><br><span class="line">写入数据库...</span><br><span class="line">关闭数据库连接...</span><br><span class="line"></span><br><span class="line">创建数据库连接...</span><br><span class="line">写入数据库...</span><br><span class="line">关闭数据库连接...</span><br></pre></td></tr></table></figure>
<p>mapPartitions以分区为单位进行操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * 将RDD中的数据写入到数据库中，绝大部分使用mapPartitions算子来实现</span><br><span class="line"> */</span><br><span class="line">rdd.mapPartitions(x =&gt; &#123;</span><br><span class="line">  println(&quot;创建数据库&quot;)</span><br><span class="line">  val list = new ListBuffer[String]()</span><br><span class="line">  while(x.hasNext)&#123;</span><br><span class="line">    //写入数据库</span><br><span class="line">    list += x.next()+&quot;:写入数据库&quot;</span><br><span class="line">  &#125;</span><br><span class="line">  //执行SQL语句  批量插入</span><br><span class="line">  list.iterator</span><br><span class="line">&#125;)foreach(println)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">创建数据库</span><br><span class="line">Tom:写入数据库</span><br><span class="line">Bob:写入数据库 </span><br><span class="line">创建数据库</span><br><span class="line">Tony:写入数据库</span><br><span class="line">Jerry:写入数据库</span><br></pre></td></tr></table></figure>
<h5 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">val dataArr = Array(&quot;Tom01&quot;,&quot;Tom02&quot;,&quot;Tom03&quot;</span><br><span class="line">                  ,&quot;Tom04&quot;,&quot;Tom05&quot;,&quot;Tom06&quot;</span><br><span class="line">                  ,&quot;Tom07&quot;,&quot;Tom08&quot;,&quot;Tom09&quot;</span><br><span class="line">                  ,&quot;Tom10&quot;,&quot;Tom11&quot;,&quot;Tom12&quot;)</span><br><span class="line">val rdd = sc.parallelize(dataArr, 3);</span><br><span class="line">val result = rdd.mapPartitionsWithIndex((index,x) =&gt; &#123;</span><br><span class="line">    val list = ListBuffer[String]()</span><br><span class="line">    while (x.hasNext) &#123;</span><br><span class="line">      list += &quot;partition:&quot;+ index + &quot; content:&quot; + x.next</span><br><span class="line">    &#125;</span><br><span class="line">    list.iterator</span><br><span class="line">&#125;)</span><br><span class="line">println(&quot;分区数量:&quot; + result.partitions.size)</span><br><span class="line">val resultArr = result.collect()</span><br><span class="line">for(x &lt;- resultArr)&#123;</span><br><span class="line">  println(x)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">分区数量:3</span><br><span class="line">partition:0 content:Tom01</span><br><span class="line">partition:0 content:Tom02</span><br><span class="line">partition:0 content:Tom03</span><br><span class="line">partition:0 content:Tom04</span><br><span class="line">partition:1 content:Tom05</span><br><span class="line">partition:1 content:Tom06</span><br><span class="line">partition:1 content:Tom07</span><br><span class="line">partition:1 content:Tom08</span><br><span class="line">partition:2 content:Tom09</span><br><span class="line">partition:2 content:Tom10</span><br><span class="line">partition:2 content:Tom11</span><br><span class="line">partition:2 content:Tom12</span><br></pre></td></tr></table></figure>
<h5 id="coalesce：改变RDD的分区数"><a href="#coalesce：改变RDD的分区数" class="headerlink" title="coalesce：改变RDD的分区数"></a>coalesce：改变RDD的分区数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * false:不产生shuffle</span><br><span class="line"> * true:产生shuffle</span><br><span class="line"> * 如果重分区的数量大于原来的分区数量,必须设置为true,否则分区数不变</span><br><span class="line"> * 增加分区会把原来的分区中的数据随机分配给设置的分区个数</span><br><span class="line"> */</span><br><span class="line">val coalesceRdd = result.coalesce(6,true)</span><br><span class="line">   </span><br><span class="line">val results = coalesceRdd.mapPartitionsWithIndex((index,x) =&gt; &#123;</span><br><span class="line">  val list = ListBuffer[String]()</span><br><span class="line">  while (x.hasNext) &#123;</span><br><span class="line">      list += &quot;partition:&quot;+ index + &quot; content:[&quot; + x.next + &quot;]&quot;</span><br><span class="line">  &#125;</span><br><span class="line">  list.iterator</span><br><span class="line">&#125;)</span><br><span class="line">   </span><br><span class="line">println(&quot;分区数量:&quot; + results.partitions.size)</span><br><span class="line">val resultArr = results.collect()</span><br><span class="line">for(x &lt;- resultArr)&#123;</span><br><span class="line">  println(x)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">分区数量:6</span><br><span class="line">partition:0 content:[partition:1 content:Tom07]</span><br><span class="line">partition:0 content:[partition:2 content:Tom10]</span><br><span class="line">partition:1 content:[partition:0 content:Tom01]</span><br><span class="line">partition:1 content:[partition:1 content:Tom08]</span><br><span class="line">partition:1 content:[partition:2 content:Tom11]</span><br><span class="line">partition:2 content:[partition:0 content:Tom02]</span><br><span class="line">partition:2 content:[partition:2 content:Tom12]</span><br><span class="line">partition:3 content:[partition:0 content:Tom03]</span><br><span class="line">partition:4 content:[partition:0 content:Tom04]</span><br><span class="line">partition:4 content:[partition:1 content:Tom05]</span><br><span class="line">partition:5 content:[partition:1 content:Tom06]</span><br><span class="line">partition:5 content:[partition:2 content:Tom09]</span><br><span class="line"></span><br><span class="line">val coalesceRdd = result.coalesce(6,fasle)的结果是：</span><br><span class="line">分区数量:3</span><br><span class="line">partition:0 content:[partition:0 content:Tom01]</span><br><span class="line">partition:0 content:[partition:0 content:Tom02]</span><br><span class="line">partition:0 content:[partition:0 content:Tom03]</span><br><span class="line">partition:0 content:[partition:0 content:Tom04]</span><br><span class="line">partition:1 content:[partition:1 content:Tom05]</span><br><span class="line">partition:1 content:[partition:1 content:Tom06]</span><br><span class="line">partition:1 content:[partition:1 content:Tom07]</span><br><span class="line">partition:1 content:[partition:1 content:Tom08]</span><br><span class="line">partition:2 content:[partition:2 content:Tom09]</span><br><span class="line">partition:2 content:[partition:2 content:Tom10]</span><br><span class="line">partition:2 content:[partition:2 content:Tom11]</span><br><span class="line">partition:2 content:[partition:2 content:Tom12]</span><br><span class="line"></span><br><span class="line">val coalesceRdd = result.coalesce(2,fasle)的结果是：</span><br><span class="line">分区数量:2</span><br><span class="line">partition:0 content:[partition:0 content:Tom01]</span><br><span class="line">partition:0 content:[partition:0 content:Tom02]</span><br><span class="line">partition:0 content:[partition:0 content:Tom03]</span><br><span class="line">partition:0 content:[partition:0 content:Tom04]</span><br><span class="line">partition:1 content:[partition:1 content:Tom05]</span><br><span class="line">partition:1 content:[partition:1 content:Tom06]</span><br><span class="line">partition:1 content:[partition:1 content:Tom07]</span><br><span class="line">partition:1 content:[partition:1 content:Tom08]</span><br><span class="line">partition:1 content:[partition:2 content:Tom09]</span><br><span class="line">partition:1 content:[partition:2 content:Tom10]</span><br><span class="line">partition:1 content:[partition:2 content:Tom11]</span><br><span class="line">partition:1 content:[partition:2 content:Tom12]</span><br><span class="line"></span><br><span class="line">val coalesceRdd = result.coalesce(2,true)的结果是：</span><br><span class="line">分区数量:2</span><br><span class="line">partition:0 content:[partition:0 content:Tom01]</span><br><span class="line">partition:0 content:[partition:0 content:Tom03]</span><br><span class="line">partition:0 content:[partition:1 content:Tom05]</span><br><span class="line">partition:0 content:[partition:1 content:Tom07]</span><br><span class="line">partition:0 content:[partition:2 content:Tom09]</span><br><span class="line">partition:0 content:[partition:2 content:Tom11]</span><br><span class="line">partition:1 content:[partition:0 content:Tom02]</span><br><span class="line">partition:1 content:[partition:0 content:Tom04]</span><br><span class="line">partition:1 content:[partition:1 content:Tom06]</span><br><span class="line">partition:1 content:[partition:1 content:Tom08]</span><br><span class="line">partition:1 content:[partition:2 content:Tom10]</span><br><span class="line">partition:1 content:[partition:2 content:Tom12]</span><br></pre></td></tr></table></figure>
<p>下图说明了三种coalesce的情况：<br><a href="http://chant00.com/media/15050414638959.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15050414638959.jpg" alt="img"></a></p>
<h5 id="repartition：改变RDD分区数"><a href="#repartition：改变RDD分区数" class="headerlink" title="repartition：改变RDD分区数"></a>repartition：改变RDD分区数</h5><p>repartition(int n) = coalesce(int n, true)</p>
<h5 id="partitionBy：通过自定义分区器改变RDD分区数"><a href="#partitionBy：通过自定义分区器改变RDD分区数" class="headerlink" title="partitionBy：通过自定义分区器改变RDD分区数"></a>partitionBy：通过自定义分区器改变RDD分区数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;Integer, String&gt; partitionByRDD = nameRDD.partitionBy(new Partitioner() &#123;</span><br><span class="line">              </span><br><span class="line">    private static final long serialVersionUID = 1L;</span><br><span class="line">    </span><br><span class="line">    //分区数2</span><br><span class="line">    @Override</span><br><span class="line">    public int numPartitions() &#123;</span><br><span class="line">        return 2;</span><br><span class="line">    &#125;</span><br><span class="line">    //分区逻辑</span><br><span class="line">    @Override</span><br><span class="line">    public int getPartition(Object obj) &#123;</span><br><span class="line">        int i = (int)obj;</span><br><span class="line">        if(i % 2 == 0)&#123;</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            return 1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">​```  </span><br><span class="line">  </span><br><span class="line">##### glom：把分区中的元素封装到数组中</span><br><span class="line">​```scala</span><br><span class="line">val rdd = sc.parallelize(1 to 10,2) </span><br><span class="line">/**</span><br><span class="line"> *  rdd有两个分区</span><br><span class="line"> *   partition0分区里面的所有元素封装到一个数组</span><br><span class="line"> *   partition1分区里面的所有元素封装到一个数组</span><br><span class="line"> */</span><br><span class="line">val glomRDD = rdd.glom()</span><br><span class="line">glomRDD.foreach(x =&gt; &#123;</span><br><span class="line">  println(&quot;============&quot;)</span><br><span class="line">  x.foreach(println)</span><br><span class="line">  println(&quot;============&quot;)</span><br><span class="line">&#125;)</span><br><span class="line">println(glomRDD.count())</span><br><span class="line">结果：</span><br><span class="line">============</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">============</span><br><span class="line">============</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">============</span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<h5 id="randomSplit：拆分RDD"><a href="#randomSplit：拆分RDD" class="headerlink" title="randomSplit：拆分RDD"></a>randomSplit：拆分RDD</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * randomSplit:</span><br><span class="line"> *   根据传入的 Array中每个元素的权重将rdd拆分成Array.size个RDD</span><br><span class="line"> *  拆分后的RDD中元素数量由权重来决定，数据量不大时不一定准确</span><br><span class="line"> */</span><br><span class="line">val rdd = sc.parallelize(1 to 10)</span><br><span class="line">rdd.randomSplit(Array(0.1,0.2,0.3,0.4)).foreach(x =&gt; &#123;println(x.count)&#125;)</span><br><span class="line">理论结果：</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">实际结果不一定准确</span><br></pre></td></tr></table></figure>
<h5 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h5><p>与zip有关的3个算子如下图所示：<br><a href="http://chant00.com/media/15051361379636.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051361379636.jpg" alt="img"></a></p>
<h3 id="算子案例"><a href="#算子案例" class="headerlink" title="算子案例"></a>算子案例</h3><h4 id="WordCount-Java版"><a href="#WordCount-Java版" class="headerlink" title="WordCount-Java版"></a>WordCount-Java版</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 文件中的数据 :</span><br><span class="line"> * Spark Core</span><br><span class="line"> * Spark Streaming</span><br><span class="line"> * Spark SQL</span><br><span class="line"> * @author root</span><br><span class="line"> */</span><br><span class="line">public class WordCount &#123;</span><br><span class="line">     </span><br><span class="line">     public static void main(String[] args) &#123;</span><br><span class="line">         /*</span><br><span class="line">          * SparkConf对象主要用于设置Spark运行时的环境参数 :</span><br><span class="line">          * 1.运行模式</span><br><span class="line">          * 2.Application Name</span><br><span class="line">          * 3.运行时的资源需求</span><br><span class="line">          */</span><br><span class="line">         SparkConf conf = new SparkConf();</span><br><span class="line">          conf.setMaster(&quot;local&quot;).setAppName(&quot;WordCount&quot;);</span><br><span class="line">         /*</span><br><span class="line">          * SparkContext是Spark运行的上下文，是通往Spark集群的唯一通道</span><br><span class="line">          */</span><br><span class="line">         JavaSparkContext jsc = new JavaSparkContext(conf);</span><br><span class="line">         </span><br><span class="line">         String path = &quot;cs&quot;;</span><br><span class="line">         JavaRDD&lt;String&gt; rdd = jsc.textFile(path);</span><br><span class="line">         </span><br><span class="line">         //================== wordcount start =================</span><br><span class="line">         flatMapRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public Tuple2&lt;String, Integer&gt; call(String word) </span><br><span class="line">throws Exception &#123;</span><br><span class="line">                  return new Tuple2&lt;String, Integer&gt;(word, 1);</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;).reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">              </span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public Integer call(Integer v1, Integer v2) </span><br><span class="line">throws Exception &#123;</span><br><span class="line">                  return v1 + v2;</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;).mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, </span><br><span class="line">Integer, String&gt;() &#123;</span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public Tuple2&lt;Integer, String&gt; call(</span><br><span class="line">Tuple2&lt;String, Integer&gt; tuple) throws Exception &#123;</span><br><span class="line">                  return new Tuple2&lt;Integer, String&gt;(tuple._2, tuple._1);</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;).sortByKey(false) //fasle : 降序</span><br><span class="line">         .mapToPair(new PairFunction&lt;Tuple2&lt;Integer,String&gt;, </span><br><span class="line">String, Integer&gt;() &#123;</span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public Tuple2&lt;String, Integer&gt; call(</span><br><span class="line">Tuple2&lt;Integer, String&gt; tuple) throws Exception &#123;</span><br><span class="line">                  return new Tuple2&lt;String, Integer&gt;(tuple._2, tuple._1);</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;).foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">              </span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public void call(Tuple2&lt;String, Integer&gt; tuple) </span><br><span class="line">throws Exception &#123;</span><br><span class="line">                  System.out.println(tuple);</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;);;</span><br><span class="line">         //================= wordcount end ==================</span><br><span class="line">         jsc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br><span class="line">(Spark,3)</span><br><span class="line">(SQL,1)</span><br><span class="line">(Streaming,1)</span><br><span class="line">(Core,1)</span><br></pre></td></tr></table></figure>
<h4 id="过滤掉出现次数最多的数据-Scala版"><a href="#过滤掉出现次数最多的数据-Scala版" class="headerlink" title="过滤掉出现次数最多的数据-Scala版"></a>过滤掉出现次数最多的数据-Scala版</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">/** </span><br><span class="line"> * 文件中的数据 :</span><br><span class="line"> * hello java</span><br><span class="line"> * hello java</span><br><span class="line"> * hello java</span><br><span class="line"> * hello java</span><br><span class="line"> * hello java</span><br><span class="line"> * hello hadoop</span><br><span class="line"> * hello hadoop</span><br><span class="line"> * hello hadoop</span><br><span class="line"> * hello hive</span><br><span class="line"> * hello hive</span><br><span class="line"> * hello world</span><br><span class="line"> * hello spark</span><br><span class="line"> * @author root</span><br><span class="line"> */</span><br><span class="line">object FilterMost &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setMaster(&quot;local&quot;)</span><br><span class="line">        .setAppName(&quot;FilterMost&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">   </span><br><span class="line">    val rdd : RDD[String] = sc.textFile(&quot;test&quot;)</span><br><span class="line">    val sampleRDD : RDD[String] = rdd.sample(false, 0.9)</span><br><span class="line">    val result = sampleRDD</span><br><span class="line">      .map &#123; x =&gt; (x.split(&quot; &quot;)(1),1) &#125;</span><br><span class="line">      .reduceByKey(_+_)</span><br><span class="line">      .map &#123; x =&gt; &#123;(x._2,x._1)&#125;&#125;</span><br><span class="line">      .sortByKey(false)</span><br><span class="line">      .first()</span><br><span class="line">      ._2</span><br><span class="line">    rdd</span><br><span class="line">      .filter &#123;(!_.contains(result))&#125;</span><br><span class="line">      .foreach(println)</span><br><span class="line">     </span><br><span class="line">    sc.stop();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br><span class="line">hello hadoop</span><br><span class="line">hello hadoop</span><br><span class="line">hello hadoop</span><br><span class="line">hello hive</span><br><span class="line">hello hive</span><br><span class="line">hello world</span><br><span class="line">hello spark</span><br></pre></td></tr></table></figure>
<h4 id="统计每个页面的UV"><a href="#统计每个页面的UV" class="headerlink" title="统计每个页面的UV"></a>统计每个页面的UV</h4><p>部分数据如下：<br>日期 时间戳 用户ID pageID 模块 用户事件<br>2017-05-13 1494643577030 null 54 Kafka View<br>2017-05-13 1494643577031 8 70 Kafka Register<br>2017-05-13 1494643577031 9 12 Storm View<br>2017-05-13 1494643577031 9 1 Scala View<br>2017-05-13 1494643577032 7 73 Scala Register<br>2017-05-13 1494643577032 16 23 Storm Register</p>
<p>scala代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">object CountUV &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">    conf.setMaster(&quot;local&quot;)</span><br><span class="line">    conf.setAppName(&quot;CountUV&quot;)</span><br><span class="line">   </span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">   </span><br><span class="line">    val rdd = sc.textFile(&quot;userLog&quot;)</span><br><span class="line">    val result = rdd.filter(!_.split(&quot;\t&quot;)(2).contains(&quot;null&quot;))</span><br><span class="line">    .map(x =&gt; &#123;</span><br><span class="line">      (x.split(&quot;\t&quot;)(3), x.split(&quot;\t&quot;)(2))</span><br><span class="line">    &#125;)</span><br><span class="line">    .distinct().countByKey()</span><br><span class="line">   </span><br><span class="line">    result.foreach(x =&gt; &#123;</span><br><span class="line">      println(&quot;PageId: &quot; + x._1 + &quot;\tUV: &quot; + x._2)</span><br><span class="line">    &#125;)</span><br><span class="line">   </span><br><span class="line">    sc.stop();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Java代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public class CountUV &#123;</span><br><span class="line">     </span><br><span class="line">     public static void main(String[] args) &#123;</span><br><span class="line">         SparkConf sparkConf = new SparkConf()</span><br><span class="line">.setMaster(&quot;local&quot;)</span><br><span class="line">.setAppName(&quot;CountUV&quot;);</span><br><span class="line">         final JavaSparkContext jsc = new JavaSparkContext(sparkConf);</span><br><span class="line">         </span><br><span class="line">         JavaRDD&lt;String&gt; rdd = jsc.textFile(&quot;userLog&quot;);</span><br><span class="line">         </span><br><span class="line">         JavaRDD&lt;String&gt; filteredRDD = </span><br><span class="line">rdd.filter(new Function&lt;String, Boolean&gt;() &#123;</span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public Boolean call(String v1) throws Exception &#123;</span><br><span class="line">                  return !&quot;null&quot;.equals(v1.split(&quot;\t&quot;)[2]);</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         </span><br><span class="line">         JavaPairRDD&lt;String, String&gt; pairRDD = </span><br><span class="line">filteredRDD.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public Tuple2&lt;String, String&gt; call(String t) </span><br><span class="line">throws Exception &#123;</span><br><span class="line">                  String[] splits = t.split(&quot;\t&quot;);</span><br><span class="line">                 return new Tuple2&lt;String, String&gt;(splits[3], splits[2]);</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         </span><br><span class="line">         JavaPairRDD&lt;String, String&gt; distinctRDD = pairRDD.distinct();</span><br><span class="line">         </span><br><span class="line">         Map&lt;String, Object&gt; resultMap = distinctRDD.countByKey();</span><br><span class="line">         </span><br><span class="line">         for(Entry&lt;String, Object&gt; entry : resultMap.entrySet()) &#123;</span><br><span class="line">              System.out.println(&quot;pageId:&quot;+ entry.getKey() + </span><br><span class="line">&quot; UV:&quot; + entry.getValue());</span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">         jsc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>部分结果：<br>pageId:45 UV:20<br>pageId:98 UV:20<br>pageId:34 UV:18<br>pageId:67 UV:20<br>pageId:93 UV:20</p>
<h4 id="二次排序-scala版"><a href="#二次排序-scala版" class="headerlink" title="二次排序-scala版"></a>二次排序-scala版</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">object SecondSort &#123;</span><br><span class="line"> </span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;SecondSort&quot;)</span><br><span class="line">    val sc = new SparkContext(sparkConf)</span><br><span class="line">   </span><br><span class="line">    val rdd = sc.textFile(&quot;secondSort.txt&quot;)</span><br><span class="line">   </span><br><span class="line">    val mapRDD = rdd.map(x =&gt; &#123;</span><br><span class="line">  (new SecondSortKey(x.split(&quot; &quot;)(0).toInt, x.split(&quot; &quot;)(1).toInt), null)</span><br><span class="line">    &#125;)</span><br><span class="line">   </span><br><span class="line">    val sortedRDD = mapRDD.sortByKey(false)</span><br><span class="line">    //val sortedRDD = mapRDD.sortBy(_._1, false)</span><br><span class="line">   </span><br><span class="line">    sortedRDD.map(_._1).foreach(println)</span><br><span class="line">   </span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">class SecondSortKey(val first:Int, val second:Int) extends Ordered[SecondSortKey] with Serializable&#123;</span><br><span class="line">  def compare(ssk:SecondSortKey): Int = &#123;</span><br><span class="line">    if(this.first - ssk.first == 0) &#123;</span><br><span class="line">      this.second - ssk.second</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">      this.first - ssk.first</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  override</span><br><span class="line">  def toString(): String = &#123;</span><br><span class="line">    this.first + &quot; &quot; + this.second</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="分组取TopN问题：找出每个班级中排名前三的分数-Java版"><a href="#分组取TopN问题：找出每个班级中排名前三的分数-Java版" class="headerlink" title="分组取TopN问题：找出每个班级中排名前三的分数-Java版"></a>分组取TopN问题：找出每个班级中排名前三的分数-Java版</h4><p>部分数据：<br>class1 100<br>class2 85<br>class3 70<br>class1 102<br>class2 65<br>class1 45</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">思路：</span><br><span class="line">     java:mapToPair/scala:map</span><br><span class="line">         (class1,100),(class2,80)...</span><br><span class="line">     groupByKey </span><br><span class="line">         class1 [100,101,88,99...]</span><br><span class="line">     如果把[100,101,88,99....]封装成List,然后进行Collections.sort(list)</span><br><span class="line">     会有问题：</span><br><span class="line">         大数据级别的value放到list里排序可能会造成OOM</span><br><span class="line">     解决办法：</span><br><span class="line">         定义一个定长的数组，通过一个简单的算法解决</span><br><span class="line"> * @author root</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public class GroupTopN &#123;</span><br><span class="line">     </span><br><span class="line">     private final static Integer N = 3;</span><br><span class="line">     </span><br><span class="line">     public static void main(String[] args) &#123;</span><br><span class="line">         SparkConf sparkConf = new SparkConf()</span><br><span class="line">                  .setMaster(&quot;local&quot;)</span><br><span class="line">                  .setAppName(&quot;GroupTopN&quot;);</span><br><span class="line">         JavaSparkContext jsc = new JavaSparkContext(sparkConf);</span><br><span class="line">         JavaRDD&lt;String&gt; rdd = jsc.textFile(&quot;scores.txt&quot;);</span><br><span class="line">         </span><br><span class="line">         JavaPairRDD&lt;String, Integer&gt; pairRDD =</span><br><span class="line">              rdd.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">                       /**</span><br><span class="line">                        *</span><br><span class="line">                        */</span><br><span class="line">                       private static final long serialVersionUID = 1L;</span><br><span class="line">                       @Override</span><br><span class="line">                       public Tuple2&lt;String, Integer&gt; call(String t) </span><br><span class="line">throws Exception &#123;</span><br><span class="line">                            String className = t.split(&quot;\t&quot;)[0];</span><br><span class="line">                       Integer score = Integer.valueOf(t.split(&quot;\t&quot;)[1]);</span><br><span class="line">                    return new Tuple2&lt;String, Integer&gt;(className, score);</span><br><span class="line">                       &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         </span><br><span class="line">         pairRDD.groupByKey().foreach(</span><br><span class="line">new VoidFunction&lt;Tuple2&lt;String,Iterable&lt;Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">              </span><br><span class="line">              /**</span><br><span class="line">               *</span><br><span class="line">               */</span><br><span class="line">              private static final long serialVersionUID = 1L;</span><br><span class="line">              @Override</span><br><span class="line">              public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; t) </span><br><span class="line">throws Exception &#123;</span><br><span class="line">                  String className = t._1;</span><br><span class="line">                  Iterator&lt;Integer&gt; iter = t._2.iterator();</span><br><span class="line">                  </span><br><span class="line">                  Integer[] nums = new Integer[N];</span><br><span class="line">                  </span><br><span class="line">                  while(iter.hasNext()) &#123;</span><br><span class="line">                       Integer score = iter.next();</span><br><span class="line">                       for(int i=0; i&lt;nums.length; i++) &#123;</span><br><span class="line">                            if(nums[i] == null) &#123;</span><br><span class="line">                                nums[i] = score;//给数组的前三个元素赋值</span><br><span class="line">                                break;</span><br><span class="line">                            &#125;else if(score &gt; nums[i]) &#123;</span><br><span class="line">                                for (int j = 2; j &gt; i; j--) &#123;</span><br><span class="line">                                     nums[j] = nums[j-1];</span><br><span class="line">                                &#125;</span><br><span class="line">                                nums[i] = score;</span><br><span class="line">                                break;</span><br><span class="line">                            &#125;</span><br><span class="line">                       &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">                  </span><br><span class="line">                  System.out.println(className);</span><br><span class="line">                  for(Integer i : nums) &#123;</span><br><span class="line">                       System.out.println(i);</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">    </span><br><span class="line">         jsc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br><span class="line">class1</span><br><span class="line">102</span><br><span class="line">100</span><br><span class="line">99</span><br><span class="line">class2</span><br><span class="line">88</span><br><span class="line">85</span><br><span class="line">85</span><br><span class="line">class3</span><br><span class="line">98</span><br><span class="line">70</span><br><span class="line">70</span><br></pre></td></tr></table></figure>
<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>有如下伪代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">var rdd = sc.textFile(path)</span><br><span class="line">val blackName = “Tom”</span><br><span class="line">val fliterRDD = rdd.fliter(_.equals(blackName))</span><br><span class="line">filterRDD.count()</span><br></pre></td></tr></table></figure>
<p>blackName是RDD外部的变量，当把task发送到其他节点执行，需要使用这个变量时，必须给每个task发送这个变量，假如这个变量占用的内存很大，而且task数量也有很多，那么导致集群资源紧张。</p>
<p>广播变量可以解决这个问题：<br>把这个变量定义为广播变量，发送到每个executor中，每个在executor中执行的task都可以使用这个广播变量，而一个executor可以包含多个task，task数一般是executor数的好几倍，这样就减少了集群的资源负荷。</p>
<p>注意：</p>
<ol>
<li>广播变量只能在Driver端定义</li>
<li>广播变量在Executor端无法修改</li>
<li>只能在Driver端改变广播变量的值</li>
</ol>
<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>有如下伪代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//统计RDD中元素的个数</span><br><span class="line">var rdd = sc.textFile(path)</span><br><span class="line">var count = 0 //这是定义在Driver端的变量</span><br><span class="line">rdd.map(x =&gt; &#123;</span><br><span class="line">    count += 1 //这个计算在Executor端执行</span><br><span class="line">&#125;)</span><br><span class="line">println(count)</span><br></pre></td></tr></table></figure>
<p>这个代码执行完毕是得不到rdd中元素的个数的，原因：<br><a href="http://chant00.com/media/15051371637582.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051371637582.jpg" alt="img"></a></p>
<p>在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的知识原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。</p>
<p>注意：</p>
<ol>
<li>累加器定义在Driver端</li>
<li>Executor端只能对累加器进行操作，也就是只能累加</li>
<li>Driver端可以读取累加器的值，Executor端不能读取累加器的值</li>
</ol>
<h4 id="累加器-Accumulator-陷阱及解决办法"><a href="#累加器-Accumulator-陷阱及解决办法" class="headerlink" title="累加器(Accumulator)陷阱及解决办法"></a><a href="http://blog.csdn.net/lsshlsw/article/details/50979579" target="_blank" rel="noopener">累加器(Accumulator)陷阱及解决办法</a></h4><h5 id="计数器的测试代码"><a href="#计数器的测试代码" class="headerlink" title="计数器的测试代码"></a>计数器的测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//在driver中定义</span><br><span class="line">val accum = sc.accumulator(0, &quot;Example Accumulator&quot;)</span><br><span class="line">//在task中进行累加</span><br><span class="line">sc.parallelize(1 to 10).foreach(x=&gt; accum += 1)</span><br><span class="line"></span><br><span class="line">//在driver中输出</span><br><span class="line">accum.value</span><br><span class="line">//结果将返回10</span><br><span class="line">res: 10</span><br></pre></td></tr></table></figure>
<h5 id="累加器的错误用法"><a href="#累加器的错误用法" class="headerlink" title="累加器的错误用法"></a>累加器的错误用法</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val accum= sc.accumulator(0, &quot;Error Accumulator&quot;)</span><br><span class="line">val data = sc.parallelize(1 to 10)</span><br><span class="line">//用accumulator统计偶数出现的次数，同时偶数返回0，奇数返回1</span><br><span class="line">val newData = data.map&#123;x =&gt; &#123;</span><br><span class="line">  if(x%2 == 0)&#123;</span><br><span class="line">    accum += 1</span><br><span class="line">      0</span><br><span class="line">    &#125;else 1</span><br><span class="line">&#125;&#125;</span><br><span class="line">//使用action操作触发执行</span><br><span class="line">newData.count</span><br><span class="line">//此时accum的值为5，是我们要的结果</span><br><span class="line">accum.value</span><br><span class="line"></span><br><span class="line">//继续操作，查看刚才变动的数据,foreach也是action操作</span><br><span class="line">newData.foreach(println)</span><br><span class="line">//上个步骤没有进行累计器操作，可是累加器此时的结果已经是10了</span><br><span class="line">//这并不是我们想要的结果</span><br><span class="line">accum.value</span><br></pre></td></tr></table></figure>
<h5 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h5><p>我们都知道，spark中的一系列transform操作会构成一串长的任务链，此时需要通过一个action操作来触发，accumulator也是一样。因此在一个action操作之前，你调用value方法查看其数值，肯定是没有任何变化的。</p>
<p>所以在第一次count(action操作)之后，我们发现累加器的数值变成了5，是我们要的答案。</p>
<p>之后又对新产生的的newData进行了一次foreach(action操作)，其实这个时候又执行了一次map(transform)操作，所以累加器又增加了5。最终获得的结果变成了10。</p>
<h5 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h5><blockquote>
<p>For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.</p>
</blockquote>
<p>看了上面的分析，大家都有这种印象了，那就是使用累加器的过程中只能使用一次action的操作才能保证结果的准确性。</p>
<p>事实上，还是有解决方案的，只要将任务之间的依赖关系切断就可以了。什么方法有这种功能呢？你们肯定都想到了，cache，persist。调用这个方法的时候会将之前的依赖切除，后续的累加器就不会再被之前的transfrom操作影响到了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object AccumulatorTest &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = new SparkContext(new SparkConf().setAppName(&quot;MapPartitionsOperator&quot;).setMaster(&quot;local&quot;))</span><br><span class="line"></span><br><span class="line">    val accum= sc.accumulator(0, &quot;Error Accumulator&quot;)</span><br><span class="line">    val data = sc.parallelize(1 to 10)</span><br><span class="line">    //用accumulator统计偶数出现的次数，同时偶数返回0，奇数返回1</span><br><span class="line">    val newData = data.map&#123;x =&gt; &#123;</span><br><span class="line">      if(x%2 == 0)&#123;</span><br><span class="line">        accum += 1</span><br><span class="line">        0</span><br><span class="line">      &#125;else 1</span><br><span class="line">    &#125;&#125;</span><br><span class="line">    newData.cache.count</span><br><span class="line">    //使用action操作触发执行</span><br><span class="line">//    newData.count</span><br><span class="line">    //此时accum的值为5，是我们要的结果</span><br><span class="line">    println(accum.value)</span><br><span class="line">    println(&quot;test&quot;)</span><br><span class="line"></span><br><span class="line">    //继续操作，查看刚才变动的数据,foreach也是action操作</span><br><span class="line">    newData.foreach(println)</span><br><span class="line">    //上个步骤没有进行累计器操作，可是累加器此时的结果已经是10了</span><br><span class="line">    //这并不是我们想要的结果</span><br><span class="line">    println(&quot;test&quot;)</span><br><span class="line">    println(accum.value)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="http://blog.csdn.net/lxhandlbb/article/details/52146211" target="_blank" rel="noopener">自定义累加器</a></p>
<p>所以在第一次count(action操作)之后，我们发现累加器的数值变成了5，是我们要的答案。<br>之后又对新产生的的newData进行了一次foreach(action操作)，其实这个时候又执行了一次map(transform)操作，所以累加器又增加了5。最终获得的结果变成了10。</p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>使用Accumulator时，为了保证准确性，只使用一次action操作。如果需要使用多次则使用cache或persist操作切断依赖。<br>RDD持久化<br>这段伪代码的瑕疵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lines = sc.textFile(“hdfs://...”)</span><br><span class="line">errors = lines.filter(_.startsWith(“ERROR”))</span><br><span class="line">mysql_errors = errors.filter(_.contain(“MySQL”)).count</span><br><span class="line">http_errors = errors.filter(_.contain(“Http”)).count</span><br></pre></td></tr></table></figure>
<p>errors是一个RDD，mysql_errors这个RDD执行时，会先读文件，然后获取数据，通过计算errors，把数据传给mysql_errors，再进行计算，因为RDD中是不存储数据的，所以http_errors计算的时候会重新读数据，计算errors后把数据传给http_errors进行计算，重复使用errors这个RDD很有必须，这就需要把errors这个RDD持久化，以便其他RDD使用。<br>RDD持久化有三个算子：cache、persist、checkpoint</p>
<h4 id="cache：把RDD持久化到内存"><a href="#cache：把RDD持久化到内存" class="headerlink" title="cache：把RDD持久化到内存"></a>cache：把RDD持久化到内存</h4><p>使用方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var rdd = sc.textFile(&quot;test&quot;)</span><br><span class="line">rdd = rdd.cache()</span><br><span class="line">val count = rdd.count() //或者其他操作</span><br></pre></td></tr></table></figure>
<p>查看源码，可以发现其实<code>cahce</code>就是<code>persist(StorageLevel.MEMORY_ONLY)</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span><br><span class="line">def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line"></span><br><span class="line">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span><br><span class="line">def cache(): this.type = persist()</span><br></pre></td></tr></table></figure>
<h4 id="persist：可以选择多种持久化方式"><a href="#persist：可以选择多种持久化方式" class="headerlink" title="persist：可以选择多种持久化方式"></a>persist：可以选择多种持久化方式</h4><p>使用方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var rdd = sc.textFile(&quot;test&quot;)</span><br><span class="line">rdd = rdd.persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line">val count = rdd.count() //或者其他操作</span><br></pre></td></tr></table></figure>
<p>Persist StorageLevel说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class StorageLevel private(</span><br><span class="line">    private var _useDisk: Boolean,</span><br><span class="line">    private var _useMemory: Boolean,</span><br><span class="line">    private var _useOffHeap: Boolean,</span><br><span class="line">    private var _deserialized: Boolean,</span><br><span class="line">    private var _replication: Int = 1)</span><br></pre></td></tr></table></figure>
<p>初始化StorageLevel可以传入5个参数，分别对应是否存入磁盘、是否存入内存、是否使用堆外内存、是否不进行序列化，副本数（默认为1）<br><a href="http://chant00.com/media/15051424923899.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051424923899.jpg" alt="img"></a></p>
<p>使用不同参数的组合构造的实例被预先定义为一些值，比如MEMORY_ONLY代表着不存入磁盘，存入内存，不使用堆外内存，不进行序列化，副本数为1，使用persisit()方法时把这些持久化的级别作为参数传入即可，cache()与persist(StorageLevel.MEMORY_ONLY)是等价的。<br>cache和persist的注意事项</p>
<ol>
<li><p>cache 和persist是懒执行算子，需要有一个action类的算子触发执行</p>
</li>
<li><p>cache 和 persist算子的返回执行必须赋值给一个变量，在接下来的job中直接使用这个变量，那么就是使用了持久化的数据了，如果application中只有一个job，没有必要使用RDD持久化</p>
</li>
<li><p>cache 和 persist算子后不能立即紧跟action类算子，比如count算子，但是在下一行可以有action类算子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">`error : rdd = cache().count()`</span><br><span class="line">`right : rdd = rdd.cache()`</span><br><span class="line">`rdd.count()****`</span><br></pre></td></tr></table></figure>
</li>
<li><p>cache() = persist(StorageLevel.MEMORY_ONLY)</p>
</li>
</ol>
<h5 id="checkpoint-可以把RDD持久化到HDFS，同时切断RDD之间的依赖"><a href="#checkpoint-可以把RDD持久化到HDFS，同时切断RDD之间的依赖" class="headerlink" title="checkpoint : 可以把RDD持久化到HDFS，同时切断RDD之间的依赖"></a>checkpoint : 可以把RDD持久化到HDFS，同时切断RDD之间的依赖</h5><p>使用方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.setCheckpointDir(&quot;hdfs://...&quot;)</span><br><span class="line">var rdd = sc.textFile(&quot;test&quot;)</span><br><span class="line">rdd.checkpoint()</span><br><span class="line">val count = rdd.count() //或者其他操作</span><br></pre></td></tr></table></figure>
<p>对于切断RDD之间的依赖的说明：<br>当业务逻辑很复杂时，RDD之间频繁转换，RDD的血统很长，如果中间某个RDD的数据丢失，还需要重新从头计算，如果对中间某个RDD调用了checkpoint()方法，把这个RDD上传到HDFS，同时让后面的RDD不再依赖于这个RDD，而是依赖于HDFS上的数据，那么下次计算会方便很多。<br>checkpoint()执行原理：</p>
<ol>
<li>当RDD的job执行完毕后，会从finalRDD从后往前回溯</li>
<li>当回溯到调用了checkpoint()方法的RDD后，会给这个RDD做一个标记</li>
<li>Spark框架自动启动一个新的job，计算这个RDD的数据，然后把数据持久化到HDFS上</li>
<li>优化：对某个RDD执行checkpoint()之前，对该RDD执行cache()，这样的话，新启动的job只需要把内存中的数据上传到HDFS中即可，不需要重新计算。</li>
</ol>
<h3 id="Spark任务调度和资源调度"><a href="#Spark任务调度和资源调度" class="headerlink" title="Spark任务调度和资源调度"></a>Spark任务调度和资源调度</h3><h4 id="一些术语"><a href="#一些术语" class="headerlink" title="一些术语"></a>一些术语</h4><p>Master(standalone)：资源管理的主节点(进程)<br>Cluster Manager：在集群上获取资源的外部服务(例如standalone,Mesos,Yarn)<br>Worker Node(standalone)：资源管理的从节点(进程)或者说管理本机资源的进程<br>Application：基于Spark的用户程序，包含了driver程序和运行在集群上的executor程序<br>Driver Program：用来连接工作进程（Worker）的程序<br>Executor：是在一个worker进程所管理的节点上为某Application启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上，每个应用都有各自独立的executors<br>Task：被送到某个executor上的工作单元<br>Job：包含很多任务(Task)的并行计算，和action是对应的<br>Stage：一个Job会被拆分很多组任务，每组任务被称为Stage(就像Mapreduce分map task和reduce task一样)</p>
<h4 id="宽窄依赖"><a href="#宽窄依赖" class="headerlink" title="宽窄依赖"></a>宽窄依赖</h4><p><a href="http://chant00.com/media/15051439297624.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051439297624.jpg" alt="img"></a><br><a href="http://chant00.com/media/15051439652988.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051439652988.jpg" alt="img"></a><br>join既可能是窄依赖，又是宽依赖。</p>
<h4 id="宽窄依赖的作用"><a href="#宽窄依赖的作用" class="headerlink" title="宽窄依赖的作用"></a>宽窄依赖的作用</h4><p><a href="http://chant00.com/media/15051440398327.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051440398327.jpg" alt="img"></a></p>
<p>在Spark里每一个操作生成一个RDD，RDD之间连一条边，最后这些RDD和他们之间的边组成一个有向无环图，这个就是DAG，Spark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。</p>
<p>有了计算的DAG图，Spark内核下一步的任务就是根据DAG图将计算划分成Stage，如上图，G与F之间是宽依赖，所以把G和F分为两个Stage，而CD到F，E到F都是窄依赖，所以CDEF最终划分为一个Stage2，A与B之间是宽依赖，B与G之间是窄依赖，所以最终，A被划分为一个Stage1，因为BG的stage依赖于stage1和stage2，所以最终把整个DAG划分为一个stage3，所以说，<strong>宽窄依赖的作用就是切割job，划分stage。</strong></p>
<p>Stage：由一组可以并行计算的task组成。<br>Stage的并行度：就是其中的task的数量</p>
<p>与互联网业界的概念有些差异：在互联网的概念中，并行度是指可同时开辟的线程数，并发数是指每个线程中可处理的最大数据量，比如4个线程，每个线程可处理的数据为100万条，那么并行度就是4，并发量是100万，而对于stage而言，即使其中的task是分批进行执行的，也都算在并行度中，比如，stage中有100个task，而这100个task分4次才能执行完，那么该stage的并行度也为100。</p>
<p><strong>Stage的并行度是由最后一个RDD的分区决定的。</strong></p>
<h4 id="RDD中为什么不存储数据以及stage的计算模式"><a href="#RDD中为什么不存储数据以及stage的计算模式" class="headerlink" title="RDD中为什么不存储数据以及stage的计算模式"></a>RDD中为什么不存储数据以及stage的计算模式</h4><p>有伪代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var lineRDD = sc.textFile(“hdfs://…”) 从HDFS中读数据</span><br><span class="line">var fliterRDD = rdd.fliter(x =&gt; &#123;</span><br><span class="line">	println(“fliter” + x)</span><br><span class="line">true</span><br><span class="line">&#125;)</span><br><span class="line">var mapRDD = fliterRDD.map(x =&gt; &#123;</span><br><span class="line">	println(“map” + x)</span><br><span class="line">	x</span><br><span class="line">&#125;)</span><br><span class="line">mapRDD.count()</span><br></pre></td></tr></table></figure>
<p>执行流程：<br><a href="http://chant00.com/media/15051442295868.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051442295868.jpg" alt="img"></a></p>
<p>在每一个task执行之前，它会把所有的RDD的处理逻辑进行整合，以递归函数的展开式整合，即map(fliter(readFromB()))，而spark没有读取文件的方法，用的是MR的读文件的方法，所以readFromB()实际上是一行一行的读数据，所以以上task执行时会输出：<br>fliter<br>map<br>fliter<br>map<br>……<br>stage的计算模式就是：pipeline模式，即计算过程中数据不会落地，也就是不会存到磁盘，而是放在内存中直接给下一个函数使用，stage的计算模式类似于 1+1+1 = 3，而MapReduce的计算模式类似于 1+1=2、2+1=3，就是说MR的中间结果都会写到磁盘上</p>
<p>管道中的数据在以下情况会落地：</p>
<ol>
<li>对某一个RDD执行控制算子(比如对mapRDD执行了foreach()操作)</li>
<li>在每一个task执行完毕后，数据会写入到磁盘上，这就是shuffle write阶段</li>
</ol>
<h4 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h4><p><a href="http://chant00.com/media/15051449619762.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15051449619762.jpg" alt="img"></a></p>
<ol>
<li>N（N&gt;=1）个RDD Object组成了一个DAG，它用代码实现后就是一个application</li>
<li>DAGScheduler是任务调度的高层调度器的对象，它依据RDD之间的宽窄依赖把DAG切割成一个个Stage，然后把这些stage以TaskSet的形式提交给TaskScheduler（调用了TaskScheduler的某个方法，然后把TaskSet作为参数传进去）</li>
<li>TaskScheduler是任务调度的底层调度器的对象</li>
<li>Stage是一组task的组合，TaskSet是task的集合，所以两者并没有本质的区别，只是在不同层次的两个概念而已</li>
<li>TaskScheduler遍历TaskSet，把每个task发送到Executor中的线程池中进行计算</li>
<li>当某个task执行失败后，会由TaskScheduler进行重新提交给Executor，默认重试3次，如果重试3次仍然失败，那么该task所在的stage就执行失败，由DAGScheduler进行重新发送，默认重试4次，如果重试4次后，stage仍然执行失败，那么该stage所在的job宣布执行失败，且不会再重试</li>
<li>TaskScheduler还可以重试straggling tasks，就是那些运行缓慢的task，当TaskScheduler认为某task0是straggling task，那么TaskScheduler会发送一条相同的task1，task0与task1中选择先执行完的task的计算结果为最终结果，这种机制被称为<strong>推测执行</strong>。</li>
<li>推测执行建议关闭而且默认就是关闭的，原因如下：<br><strong>推测执行可能导致数据重复</strong><br>比如做数据清洗时，某个task正在往关系型数据库中写数据，而当它执行的一定阶段但还没有执行完的时候，此时如果TaskScheduler认为它是straggling task，那么TaskScheduler会新开启一个一模一样的task进行数据写入，会造成数据重复。<br>推测执行可能会大量占用资源导致集群崩溃<br>比如某条task执行时发生了数据倾斜，该task需要计算大量的数据而造成它执行缓慢，那么当它被认为是straggling task后，TaskScheduler会新开启一个一模一样的task进行计算，新的task计算的还是大量的数据，且分配得到与之前的task相同的资源，两条task执行比之前一条task执行还慢，TaskScheduler有可能再分配一条task来计算这些数据，这样下去，资源越来越少，task越加越多，形成死循环后，程序可能永远都跑不完。</li>
</ol>
<h4 id="资源调度"><a href="#资源调度" class="headerlink" title="资源调度"></a>资源调度</h4><p>1）<br><a href="http://chant00.com/media/15052093011051.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052093011051.jpg" alt="img"></a><br>2）<br><a href="http://chant00.com/media/15052093251093.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052093251093.jpg" alt="img"></a><br>3）<br><a href="http://chant00.com/media/15052093599400.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052093599400.jpg" alt="img"></a><br>4）<br><a href="http://chant00.com/media/15052093791909.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052093791909.jpg" alt="img"></a><br>5&amp;6）<br><a href="http://chant00.com/media/15052094179523.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052094179523.jpg" alt="img"></a><br>7&amp;8）<br><a href="http://chant00.com/media/15052092504222.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052092504222.jpg" alt="img"></a></p>
<p>注意：<br>application执行之前申请的这批executor可以被这个application中的所有job共享。</p>
<h4 id="粗粒度和细粒度的资源申请"><a href="#粗粒度和细粒度的资源申请" class="headerlink" title="粗粒度和细粒度的资源申请"></a>粗粒度和细粒度的资源申请</h4><h5 id="粗粒度的资源申请：Spark"><a href="#粗粒度的资源申请：Spark" class="headerlink" title="粗粒度的资源申请：Spark"></a>粗粒度的资源申请：Spark</h5><p>在Application执行之前，将所有的资源申请完毕，然后再进行任务调度，直到最后一个task执行完毕，才会释放资源<br>优点：每一个task执行之前不需要自己去申请资源，直接使用资源就可以，每一个task的启动时间就变短了，task执行时间缩短，使得整个Application执行的速度较快<br>缺点：无法充分利用集群的资源，比如总共有10万的task，就要申请10万个task的资源，即使只剩下一个task要执行，也得等它执行完才释放资源，在这期间99999个task的资源没有执行任何task，但也不能被其他需要的进程或线程使用</p>
<h5 id="细粒度的资源申请：MapReduce"><a href="#细粒度的资源申请：MapReduce" class="headerlink" title="细粒度的资源申请：MapReduce"></a>细粒度的资源申请：MapReduce</h5><p>在Application执行之前，不需要申请好资源，直接进行任务的调度，在每一个task执行之前，自己去申请资源，申请到就执行，申请不到就等待，每一个task执行完毕后就立马释放资源。<br>优点：可以充分的利用集群的资源<br>缺点：每一个task的执行时间变长了，导致整个Application的执行的速度较慢</p>
<h5 id="yarn如何同时调度粗细两种方式"><a href="#yarn如何同时调度粗细两种方式" class="headerlink" title="yarn如何同时调度粗细两种方式"></a>yarn如何同时调度粗细两种方式</h5><p>Spark和MapReduce都可以跑在yarn上，那怎么做到一个组粒度一个细粒度呢？原因是他们各自实现<strong>ApplicationMaster</strong>的方式不同。<br><a href="http://chant00.com/media/15052153748327.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052153748327.jpg" alt="img"></a></p>
<h3 id="资源调度源码分析"><a href="#资源调度源码分析" class="headerlink" title="资源调度源码分析"></a>资源调度源码分析</h3><h4 id="分析以集群方式提交命令后的资源调度源码"><a href="#分析以集群方式提交命令后的资源调度源码" class="headerlink" title="分析以集群方式提交命令后的资源调度源码"></a>分析以集群方式提交命令后的资源调度源码</h4><p>资源调度的源码（Master.scala）位置：<br><a href="http://chant00.com/media/15052101744162.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052101744162.jpg" alt="img"></a><br>1.Worker启动后向Master注册<br>2.client向Master发送一条消息，为当前的Application启动一个Driver进程</p>
<p><a href="http://chant00.com/media/15052102502535.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052102502535.jpg" alt="img"></a><br>schedule()方法是对Driver和Executor进行调度的方法，看看启动Driver进程的过程：</p>
<p><a href="http://chant00.com/media/spark_资源调度源码分析_Driver进程的创建.png" target="_blank" rel="noopener"><img src="http://chant00.com/media/spark_%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90_Driver%E8%BF%9B%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA.png" alt="spark_资源调度源码分析_Driver进程的创建"></a></p>
<p>schedule()方法有一些问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private def schedule(): Unit = &#123;</span><br><span class="line">  if (state != RecoveryState.ALIVE) &#123; return &#125;</span><br><span class="line">  // Drivers take strict precedence over executors</span><br><span class="line">  val shuffledWorkers = Random.shuffle(workers) // Randomization helps balance drivers  for (worker &lt;- shuffledWorkers if worker.state == WorkerState.ALIVE) &#123;</span><br><span class="line">    for (driver &lt;- waitingDrivers) &#123;</span><br><span class="line">      if (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123;</span><br><span class="line">        launchDriver(worker, driver)</span><br><span class="line">        waitingDrivers -= driver</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  startExecutorsOnWorkers()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1) 如果是以客户端方式命令执行程序，那么不需要Master来调度Worker创建Driver进程，那么waitingDrivers这个集合中就没有元素，所以也就不需要遍历shuffledWorkers，源码并没有考虑这种情况。应该是有if语句进行非空判定。<br>2) 如果waitingDrivers中只有一个元素，那么也会一直遍历shuffledWorkers这个集合，实际上是不需要的。</p>
<p>3.Driver进程向Master发送消息：为当前的Application申请一批Executor。</p>
<p>下面看看Executor的创建过程：<br><a href="http://chant00.com/media/spark源码分析_资源调度_Executor的启动.png" target="_blank" rel="noopener"><img src="http://chant00.com/media/spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90_%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6_Executor%E7%9A%84%E5%90%AF%E5%8A%A8.png" alt="spark源码分析_资源调度_Executor的启动"></a></p>
<p>通过以上过程，Executor进程就被启动了</p>
<h4 id="资源调度的三个结论"><a href="#资源调度的三个结论" class="headerlink" title="资源调度的三个结论"></a>资源调度的三个结论</h4><ol>
<li>在默认情况下（没有使用<code>--executor --cores</code>这个选项）时，每一个Worker节点为当前的Application只启动一个Executor，这个Executor会使用这个Worker管理的所有的core(原因：<code>assignedCores(pos) += minCoresPerExecutor</code>)</li>
<li>默认情况下，每个Executor使用1G内存</li>
<li>如果想要在一个Worker节点启动多个Executor，需要使<code>--executor --cores</code>这个选项</li>
<li>spreadOutApps这个参数可以决定Executor的启动方式，默认轮询方式启动，这样有利于数据的本地化。</li>
</ol>
<h4 id="验证资源调度的三个结论"><a href="#验证资源调度的三个结论" class="headerlink" title="验证资源调度的三个结论"></a>验证资源调度的三个结论</h4><p>集群中总共有6个core和4G内存可用，每个Worker管理3个core和2G内存<br><a href="http://chant00.com/media/15052168177467.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052168177467.jpg" alt="img"></a></p>
<p>SPARK_HOME/bin下有一个spark-shell脚本文件，执行这个脚本文件就是提交一个application</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function main() &#123;</span><br><span class="line">……</span><br><span class="line">&quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line">……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>默认情况（不指定任何参数）下启动spark-shell：<br><code>[root@node04 bin]# ./spark-shell --master spark://node01:7077</code><br><a href="http://chant00.com/media/15052168728002.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052168728002.jpg" alt="img"></a><br><a href="http://chant00.com/media/15052168987010.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052168987010.jpg" alt="img"></a></p>
<p>这个application启动了2个Executor进程，每个Worker节点上启动一个，总共使用了6个core和2G内存，每个Work提供3个core和1G内存。</p>
<p>设置每个executor使用1个core<br><code>[root@node04 bin]# ./spark-shell --master spark://node01:7077 --executor-cores 1</code><br><a href="http://chant00.com/media/15052169289580.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052169289580.jpg" alt="img"></a><br><a href="http://chant00.com/media/15052169381307.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052169381307.jpg" alt="img"></a></p>
<p>那么每个worker为application启动两个executor，每个executor使用1个core，这是因为启动两个executor后，内存已经用完了，所以即使还有剩余的core可用，也无法再启动executor了</p>
<p>设置每个executor使用2个core<br><code>[root@node04 bin]# ./spark-shell --master spark://node01:7077 --executor-cores 2</code><br><a href="http://chant00.com/media/15052170871109.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052170871109.jpg" alt="img"></a></p>
<p>那么每个worker为application启动1个executor，每个executor使用2个core，这是因为启动两个executor后，每个executor剩余的core为1，已经不够再启动一个exexutor了</p>
<p>设置每个executor使用3G内存<br><code>[root@node04 bin]# ./spark-shell --master spark://node01:7077 --executor-memory 3G</code><br><a href="http://chant00.com/media/15052171146792.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052171146792.jpg" alt="img"></a><br>提交任务显示为waiting状态，而不是running状态，也不会启动executor</p>
<p>设置每个executor使用1个core，500M内存<br><code>[root@node04 bin]# ./spark-shell --master spark://node01:7077 --executor-cores 1 --executor-memory 500M</code><br><a href="http://chant00.com/media/15052171461798.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052171461798.jpg" alt="img"></a><br><a href="http://chant00.com/media/15052171555883.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052171555883.jpg" alt="img"></a></p>
<p>设置每个设置每个executor使用1个core，500M内存，集群总共可以使用3个core，集群总共启动3个executor，其中有一个Worker启动了两个executor<br><code>[root@node04 bin]# ./spark-shell --master spark://node01:7077 --executor-cores 1 --executor-memory 500M --total-executor-cores 3</code><br><a href="http://chant00.com/media/15052171893498.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052171893498.jpg" alt="img"></a><br><a href="http://chant00.com/media/15052171986395.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052171986395.jpg" alt="img"></a></p>
<p>设置每个设置每个executor使用1个core，1.2内存，集群总共可以使用3个core，集群总共启动2个executor，每个Worker启动了1个executor，表面上看起来，两个worker加起来的内存（1.6G）和剩余的core数（1），还够启动一个exexutor，但是这里需要注意的是，两个Worker的内存并不能共用，每个Worker剩余的内存（800M）并不足以启动一个executor<br><code>[root@node04 bin]# ./spark-shell --master spark://node01:7077 --executor-cores 1 --executor-memory 1200M --total-executor-cores 3</code><br><a href="http://chant00.com/media/15052172249932.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052172249932.jpg" alt="img"></a></p>
<h3 id="任务调度源码分析"><a href="#任务调度源码分析" class="headerlink" title="任务调度源码分析"></a>任务调度源码分析</h3><p>源码位置：core/src/main/scala/rdd/RDD.scala<br><a href="http://chant00.com/media/spark任务调度.png" target="_blank" rel="noopener"><img src="http://chant00.com/media/spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6.png" alt="spark任务调度"></a></p>
<h3 id="Spark-Standalone集群搭建"><a href="#Spark-Standalone集群搭建" class="headerlink" title="Spark Standalone集群搭建"></a>Spark Standalone集群搭建</h3><h4 id="角色划分"><a href="#角色划分" class="headerlink" title="角色划分"></a>角色划分</h4><p><a href="http://chant00.com/media/15052175596102.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052175596102.jpg" alt="img"></a></p>
<h5 id="1-解压安装包"><a href="#1-解压安装包" class="headerlink" title="1.解压安装包"></a>1.解压安装包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 chant]# tar zxf spark-1.6.0-bin-hadoop2.6.tgz</span><br></pre></td></tr></table></figure>
<h5 id="2-编辑spark-env-sh文件"><a href="#2-编辑spark-env-sh文件" class="headerlink" title="2.编辑spark-env.sh文件"></a>2.编辑spark-env.sh文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 chant]# mv spark-1.6.0-bin-hadoop2.6 spark-1.6.0</span><br><span class="line">[root@node01 chant]# cd spark-1.6.0/conf/</span><br><span class="line">[root@node01 conf]# cp spark-env.sh.template spark-env.sh</span><br><span class="line">[root@node01 conf]# vi spark-env.sh</span><br><span class="line"># 绑定Master的IP</span><br><span class="line">export SPARK_MASTER_IP=node01</span><br><span class="line"># 提交Application的端口</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"># 每一个Worker最多可以支配core的个数，注意core是否支持超线程</span><br><span class="line">export SPARK_WORKER_CORES=3</span><br><span class="line"># 每一个Worker最多可以支配的内存</span><br><span class="line">export SPARK_WORKER_MEMORY=2g</span><br></pre></td></tr></table></figure>
<h5 id="3-编辑slaves文件"><a href="#3-编辑slaves文件" class="headerlink" title="3.编辑slaves文件"></a>3.编辑slaves文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 conf]# cp slaves.template slaves</span><br><span class="line">[root@node01 conf]# vi slaves</span><br><span class="line">node02</span><br><span class="line">node03</span><br></pre></td></tr></table></figure>
<h5 id="4-Spark的web端口默认为8080，与Tomcat冲突，进行修改"><a href="#4-Spark的web端口默认为8080，与Tomcat冲突，进行修改" class="headerlink" title="4.Spark的web端口默认为8080，与Tomcat冲突，进行修改"></a>4.Spark的web端口默认为8080，与Tomcat冲突，进行修改</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 spark-1.6.0]# cd sbin/</span><br><span class="line">[root@node01 sbin]# vi start-master.sh</span><br><span class="line">if [ &quot;$SPARK_MASTER_WEBUI_PORT&quot; = &quot;&quot; ]; then</span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8081</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<h5 id="5-同步配置"><a href="#5-同步配置" class="headerlink" title="5.同步配置"></a>5.同步配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 conf]# cd /opt/chant/</span><br><span class="line">[root@node01 chant]# scp -r spark-1.6.0 node02:`pwd`</span><br><span class="line">[root@node01 chant]# scp -r spark-1.6.0 node03:`pwd`</span><br><span class="line">[root@node01 chant]# scp -r spark-1.6.0 node04:`pwd`</span><br></pre></td></tr></table></figure>
<h5 id="6-进入spark安装目录的sbin目录下，启动集群"><a href="#6-进入spark安装目录的sbin目录下，启动集群" class="headerlink" title="6.进入spark安装目录的sbin目录下，启动集群"></a>6.进入spark安装目录的sbin目录下，启动集群</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 chant]# cd spark-1.6.0/sbin/</span><br><span class="line">[root@node01 sbin]# ./start-all.sh</span><br></pre></td></tr></table></figure>
<h5 id="7-访问web界面"><a href="#7-访问web界面" class="headerlink" title="7.访问web界面"></a>7.访问web界面</h5><p><a href="http://chant00.com/media/15052177677901.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052177677901.jpg" alt="img"></a></p>
<h5 id="8-提交Application验证集群是否工作正常"><a href="#8-提交Application验证集群是否工作正常" class="headerlink" title="8.提交Application验证集群是否工作正常"></a>8.提交Application验证集群是否工作正常</h5><p>以下scala代码是spark源码包自带的例子程序，用于计算圆周率，可传入参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.spark.examples</span><br><span class="line"></span><br><span class="line">import scala.math.random</span><br><span class="line"></span><br><span class="line">import org.apache.spark._</span><br><span class="line"></span><br><span class="line">/** Computes an approximation to pi */</span><br><span class="line">object SparkPi &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName(&quot;Spark Pi&quot;)</span><br><span class="line">    val spark = new SparkContext(conf)</span><br><span class="line">    val slices = if (args.length &gt; 0) args(0).toInt else 2</span><br><span class="line">// avoid overflow</span><br><span class="line">    val n = math.min(100000L * slices, Int.MaxValue).toInt </span><br><span class="line">    val count = spark.parallelize(1 to n, slices).map &#123; i =&gt;</span><br><span class="line">        val x = random * 2 - 1</span><br><span class="line">        val y = random * 2 - 1</span><br><span class="line">        if (x*x + y*y &lt; 1) 1 else 0</span><br><span class="line">      &#125;.reduce((v1,v2) =&gt; &#123;v1+v2&#125;)</span><br><span class="line">    println(&quot;Pi is roughly &quot; + 4.0 * count / n)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此程序的jar包路径为：SPARK_HOME/lib/spark-examples-1.6.0-hadoop2.6.0.jar</p>
<h4 id="Standalone模式下提交任务"><a href="#Standalone模式下提交任务" class="headerlink" title="Standalone模式下提交任务"></a>Standalone模式下提交任务</h4><p>Standalone模式：提交的任务在spark集群中管理，包括资源调度，计算</p>
<h5 id="客户端方式提交任务"><a href="#客户端方式提交任务" class="headerlink" title="客户端方式提交任务"></a>客户端方式提交任务</h5><p>进入客户端所在节点的spark安装目录的bin目录下，提交这个程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node04 bin]# ./spark-submit --master spark://node01:7077 #指定 master的地址</span><br><span class="line">&gt; --deploy-mode client #指定在客户端提交任务，这个选项可以不写，默认</span><br><span class="line">&gt; --class org.apache.spark.examples.SparkPi  #指定程序的全名</span><br><span class="line">&gt; ../lib/spark-examples-1.6.0-hadoop2.6.0.jar  #指定jar包路径</span><br><span class="line">&gt; 1000  #程序运行时传入的参数</span><br></pre></td></tr></table></figure>
<p>说明：<br>1.客户端提交，Driver进程就在客户端启动，进程名为SparkSubmit</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 注意：任务结束后，该进程就关闭了</span><br><span class="line">[root@node04 ~]# jps</span><br><span class="line">1646 Jps</span><br><span class="line">1592 SparkSubmit</span><br></pre></td></tr></table></figure>
<p>2.在客户端可以看到task执行情况和执行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">……</span><br><span class="line">17/08/04 02:55:47 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 6.602662 s</span><br><span class="line">Pi is roughly 3.1409092</span><br><span class="line">17/08/04 02:55:47 INFO SparkUI: Stopped Spark web UI at http://192.168.9.14:4040</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>3.适合场景：测试<br>原因：当提交的任务数量很多时，客户端资源不够用</p>
<h5 id="集群方式提交任务"><a href="#集群方式提交任务" class="headerlink" title="集群方式提交任务"></a>集群方式提交任务</h5><p>还是在客户端所在节点的spark安装目录的bin目录下提交程序，只是命令需要修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node04 bin]# ./spark-submit --master spark://node01:7077</span><br><span class="line">&gt; --deploy-mode cluster #指定在集群中提交任务，这个选项必须写</span><br><span class="line">&gt; --class org.apache.spark.examples.SparkPi</span><br><span class="line">&gt; ../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">&gt; 1000</span><br></pre></td></tr></table></figure>
<p>说明：<br>1.集群方式提交任务，Driver进程随机找一个Worker所在的节点启动，进程名为DriverWrapper</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# jps</span><br><span class="line">1108 Worker</span><br><span class="line">1529 Jps</span><br><span class="line">1514 DriverWrapper</span><br></pre></td></tr></table></figure>
<p>2.客户端看不到task执行情况和执行结果，可以在web界面查看<br><a href="http://chant00.com/media/15052183410200.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052183410200.jpg" alt="img"></a></p>
<p>3.适合场景：生产环境<br>原因：当task数量很多时，集群方式可以做到负载均衡，解决多次网卡流量激增问题（分摊到集群的Worker节点上），但无法解决单次网卡流量激增问题。</p>
<h4 id="Yarn模式下提交任务"><a href="#Yarn模式下提交任务" class="headerlink" title="Yarn模式下提交任务"></a>Yarn模式下提交任务</h4><p>yarn模式：把spark任务提交给yarn集群，由yarn集群进行管理，包括资源分配和计算<br>编辑客户端节点中spark配置文件，加入：<br><code>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</code>,启<strong>动hadoop集群，不需要启动spark集群</strong></p>
<h5 id="客户端方式提交任务-1"><a href="#客户端方式提交任务-1" class="headerlink" title="客户端方式提交任务"></a>客户端方式提交任务</h5><p>1.命令<br><code>./spark-submit --master yarn --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</code><br>2.流程<br><a href="http://chant00.com/media/15052186244693.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052186244693.jpg" alt="img"></a></p>
<p>① 在客户端执行提交命令<br>② 上传应用程序(jar包)及其依赖的jar包到HDFS上，开启Driver进程执行应用程序<br>③ 客户端会向RS发送请求，为当前的Application启动一个ApplicationMaster进程<br>④ RS会找一台NM启动ApplicationMaster，ApplicationMaster进程启动成功后，会向RS申请资源（图画的有误，ApplicationMaster应该在NM上）<br>⑤ RS接受请求后，会向资源充足的NM发送消息：在当前的节点上启动一个Executor进程，去HDFS下载spark-assembly-1.6.0-hadoop2.6.0.jar包，这个jar包中有启动Executor进程的相关类，调用其中的方法就可以启动Executor进程<br>⑥ Executor启动成功后，Driver开始分发task，在集群中执行任务</p>
<p>3.总结<br>Driver负责任务的调度<br>ApplicationMaster负责资源的申请</p>
<h5 id="集群方式提交任务-1"><a href="#集群方式提交任务-1" class="headerlink" title="集群方式提交任务"></a>集群方式提交任务</h5><p>1.命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master yarn-cluster --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br></pre></td></tr></table></figure>
<p>2.流程<br><a href="http://chant00.com/media/15052189861132.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052189861132.jpg" alt="img"></a></p>
<p>① 在客户端执行提交命令<br>② 上传应用程序(jar包)及其依赖的jar包到HDFS上<br>③ 客户端会向RS发送请求，为当前的Application启动一个ApplicationMaster(Driver)进程，这个ApplicationMaster就是driver。<br>④ RS会找一台NM启动ApplicationMaster，ApplicationMaster(Driver)进程启动成功后，会向RS申请资源（图画的有误，ApplicationMaster应该在NM上），ApplicationMaster(Driver)进程启动成功后，会向RS申请资源<br>⑤ RS接受请求后，会向资源充足的NM发送消息：在当前的节点上启动一个Executor进程，去HDFS下载spark-assembly-1.6.0-hadoop2.6.0.jar包，这个jar包中有启动Executor进程的相关类，调用其中的方法就可以启动Executor进程<br>⑥ Executor启动成功后，ApplicationMaster(Driver)开始分发task，在集群中执行任务<br>3.总结<br>在cluster提交方式中，ApplicationMaster进程就是Driver进程，任务调度和资源申请都是由一个进程来做的</p>
<h3 id="Spark-HA集群搭建"><a href="#Spark-HA集群搭建" class="headerlink" title="Spark HA集群搭建"></a>Spark HA集群搭建</h3><h4 id="Spark高可用的原理"><a href="#Spark高可用的原理" class="headerlink" title="Spark高可用的原理"></a>Spark高可用的原理</h4><p><a href="http://chant00.com/media/15052194149122.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052194149122.jpg" alt="img"></a></p>
<p>说明：<br>主备切换的过程中，不能提交新的Application。<br>已经提交的Application在执行过程中，集群进行主备切换，是没有影响的，因为spark是粗粒度的资源调度。</p>
<h4 id="角色划分-1"><a href="#角色划分-1" class="headerlink" title="角色划分"></a>角色划分</h4><p><a href="http://chant00.com/media/15052195976097.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052195976097.jpg" alt="img"></a></p>
<h5 id="1-修改spark-env-sh配置文件"><a href="#1-修改spark-env-sh配置文件" class="headerlink" title="1.修改spark-env.sh配置文件"></a>1.修改spark-env.sh配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cd /opt/chant/spark-1.6.0/conf/</span><br><span class="line">[root@node01 conf]# vi spark-env.sh</span><br><span class="line">加入以下配置</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER </span><br><span class="line">-Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181 </span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark/ha&quot;</span><br></pre></td></tr></table></figure>
<h5 id="2-同步配置文件"><a href="#2-同步配置文件" class="headerlink" title="2. 同步配置文件"></a>2. 同步配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 conf]# scp spark-env.sh node02:`pwd`</span><br><span class="line">[root@node01 conf]# scp spark-env.sh node03:`pwd`</span><br><span class="line">[root@node01 conf]# scp spark-env.sh node04:`pwd`</span><br></pre></td></tr></table></figure>
<h5 id="3-修改node02的spark配置文件"><a href="#3-修改node02的spark配置文件" class="headerlink" title="3. 修改node02的spark配置文件"></a>3. 修改node02的spark配置文件</h5><p>把master的IP改为node02，把node02的masterUI port改为8082<br>因为node01的masterUI的port设置为8081，同步后，node02的masterUI的port也为8081，那么在node02启动master进程时，日志中会有警告：<br>WARN Utils: Service ‘MasterUI’ could not bind on port 8081<br>导致我们不能通过该port访问node02的masterUI，所以修改的和node01不一样就可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# cd /opt/chant/spark-1.6.0/conf</span><br><span class="line">[root@node02 conf]# vi spark-env.sh</span><br><span class="line">export SPARK_MASTER_IP=node02</span><br><span class="line">[root@node02 conf]# cd ../sbin</span><br><span class="line">[root@node02 sbin]# vi start-master.sh</span><br><span class="line">if [ &quot;$SPARK_MASTER_WEBUI_PORT&quot; = &quot;&quot; ]; then</span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8082</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<h5 id="4-启动Zookeeper集群"><a href="#4-启动Zookeeper集群" class="headerlink" title="4. 启动Zookeeper集群"></a>4. 启动Zookeeper集群</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 ~]# zkServer.sh start</span><br><span class="line">[root@node03 ~]# zkServer.sh start</span><br><span class="line">[root@node04 ~]# zkServer.sh start</span><br></pre></td></tr></table></figure>
<h5 id="5-在node01上启动Spark集群"><a href="#5-在node01上启动Spark集群" class="headerlink" title="5.在node01上启动Spark集群"></a>5.在node01上启动Spark集群</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 conf]# cd ../sbin</span><br><span class="line">[root@node01 sbin]# pwd</span><br><span class="line">/opt/chant/spark-1.6.0/sbin</span><br><span class="line">[root@node01 sbin]# ./start-all.sh</span><br></pre></td></tr></table></figure>
<h5 id="6-在node02上启动Master进程"><a href="#6-在node02上启动Master进程" class="headerlink" title="6.在node02上启动Master进程"></a>6.在node02上启动Master进程</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node02 bin]# cd ../sbin</span><br><span class="line">[root@node02 sbin]# pwd</span><br><span class="line">/opt/chant/spark-1.6.0/sbin</span><br><span class="line">[root@node02 sbin]# ./start-master.sh</span><br></pre></td></tr></table></figure>
<h5 id="7-验证集群高可用"><a href="#7-验证集群高可用" class="headerlink" title="7.验证集群高可用"></a>7.验证集群高可用</h5><p><a href="http://chant00.com/media/15052199735375.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052199735375.jpg" alt="img"></a><br><a href="http://chant00.com/media/15052199860032.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052199860032.jpg" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 sbin]# jps</span><br><span class="line">1131 Master</span><br><span class="line">1205 Jps</span><br><span class="line">[root@node01 sbin]# kill -9 1131</span><br></pre></td></tr></table></figure>
<p><a href="http://chant00.com/media/15052200135780.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052200135780.jpg" alt="img"></a><br><a href="http://chant00.com/media/15052200295354.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052200295354.jpg" alt="img"></a></p>
<p>再次启动node01的master进程，node01成为standby<br><code>[root@node01 sbin]# ./start-master.sh</code><br><a href="http://chant00.com/media/15052200673813.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052200673813.jpg" alt="img"></a></p>
<h3 id="Spark-History-Server配置"><a href="#Spark-History-Server配置" class="headerlink" title="Spark History Server配置"></a>Spark History Server配置</h3><p>提交一个Application：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node04 ~]# cd /opt/chant/spark-1.6.0/bin</span><br><span class="line">[root@node04 bin]# ./spark-shell --name &quot;testSparkShell&quot; --master spark://node02:7077</span><br></pre></td></tr></table></figure>
<p><a href="http://chant00.com/media/15052209033503.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052209033503.jpg" alt="img"></a><br>点击ApplicationID<br><a href="http://chant00.com/media/15052209140931.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052209140931.jpg" alt="img"></a><br>点击appName查看job信息<br><a href="http://chant00.com/media/15052209386045.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052209386045.jpg" alt="img"></a></p>
<p>提交一个job</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(&quot;/tmp/wordcount_data&quot;)</span><br><span class="line">.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/tmp/wordcount_result&quot;)</span><br></pre></td></tr></table></figure>
<p><a href="http://chant00.com/media/15052210085405.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052210085405.jpg" alt="img"></a><br>点击job查看stage信息<br><a href="http://chant00.com/media/15052210190366.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052210190366.jpg" alt="img"></a><br>点击stage查看task信息<br><a href="http://chant00.com/media/15052210621811.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052210621811.jpg" alt="img"></a></p>
<p>退出Spark-shell后，这个Application的信息是不被保存的，需要做一些配置才会保存历史记录，有两种方法设置保存历史记录<br>1.提交命令时指定<br><code>./spark-shell --master spark://node02:7077 --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=&quot;/tmp/spark/historyLog&quot;</code><br>注意：保存历史数据的目录需要先创建好<br>2.启动history-server<br>修改conf/spark-defaults.conf文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled true</span><br><span class="line">spark.eventLog.dir hdfs://node01:9000/spark/historyLog</span><br><span class="line">spark.history.fs.logDirectory hdfs://node01:9000/spark/historyLog</span><br></pre></td></tr></table></figure>
<p>spark.eventLog.compress true 可以设置保存历史日志时进行压缩<br>注意：保存历史数据的目录需要先创建好<br>然后启动history server：sbin/start-history-server.sh<br>之后提交的所有的Application的执行记录都会被保存，访问18080端口就可以查看</p>
<h3 id="Spark-Shuffle"><a href="#Spark-Shuffle" class="headerlink" title="Spark Shuffle"></a><a href="https://tech.meituan.com/spark-tuning-pro.html" target="_blank" rel="noopener">Spark Shuffle</a></h3><p>reduceByKey会将上一个RDD中的每一个key对应的所有value聚合成一个value，然后生成一个新的RDD，元素类型是对的形式，这样每一个key对应一个聚合起来的value</p>
<p>存在的问题：<br>每一个key对应的value不一定都是在一个partition中，也不太可能在同一个节点上，因为RDD是分布式的弹性的数据集，他的partition极有可能分布在各个节点上。</p>
<p>那么如何进行聚合？<br>Shuffle Write：上一个stage的每个map task就必须保证将自己处理的当前分区中的数据相同的key写入一个分区文件中，可能会写入多个不同的分区文件中<br>Shuffle Read：reduce task就会从上一个stage的所有task所在的机器上寻找属于自己的那些分区文件，这样就可以保证每一个key所对应的value都会汇聚到同一个节点上去处理和聚合</p>
<h4 id="普通的HashShuffle"><a href="#普通的HashShuffle" class="headerlink" title="普通的HashShuffle"></a>普通的HashShuffle</h4><p><a href="http://chant00.com/media/15052213964855.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052213964855.jpg" alt="img"></a><br>上图中，每个节点启动一个Executor来运行Application，每个Executor使用1个core，其中有2条task，所以2条task不是并行执行的。Map task每计算一条数据之后，就写到对应的buffer（默认32K）中（比如key为hello的写入到蓝色buffer，key为world的写入到紫色buffer中），当buffer到达阈值后，把其中的数据溢写到磁盘，当task0执行完后，task2开始执行，在这个过程中，每一个map task产生reduce的个数个小文件，假如总共有m个map task，r个reduce，最终会产生m*r个小文件，磁盘小文件和缓存过多，造成耗时且低效的IO操作，可能造成OOM</p>
<h5 id="内存估算"><a href="#内存估算" class="headerlink" title="内存估算"></a>内存估算</h5><p>假设：一台服务器24核，支持超线程，1000个maptask，1000个reduceTask。<br>超线程相当于48核，所以并行48个mapTask，每个mapTask产生1000个（reduceTask数）磁盘小文件，也就是对应产生<code>48*1000</code>个buffer，而每个buffer默认值为32k。<br>所以使用内存为：<code>maptask数*reduceTask数*buffer大小 = 48*1000*32k=1536M</code>约<code>1.5G</code></p>
<h4 id="优化的HashShuffle"><a href="#优化的HashShuffle" class="headerlink" title="优化的HashShuffle"></a>优化的HashShuffle</h4><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。<br><a href="http://chant00.com/media/15052299928793.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052299928793.jpg" alt="img"></a><br>每个map task 之间可以共享buffer，task0执行完成后，task1开始执行，继续使用task0使用的buffer，假如总共有c个core， r个reduce，最终会产生c*r个小文件，因为复用buffer后，每个core执行的所有map task产生r个小文件</p>
<h4 id="普通的SortShuffle"><a href="#普通的SortShuffle" class="headerlink" title="普通的SortShuffle"></a>普通的SortShuffle</h4><p>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种<strong>聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存</strong>；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。<br><a href="http://chant00.com/media/15052328013148.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052328013148.jpg" alt="img"></a></p>
<ol>
<li>每个maptask将计算结果写入内存数据结构中，这个内存默认大小为<strong>5M</strong></li>
<li>会有一个“监控器”来不定时的检查这个内存的大小，如果写满了5M，比如达到了5.01M，那么再给这个内存申请5.02M（5.01M * 2 – 5M = 5.02）的内存，此时这个内存空间的总大小为10.02M</li>
<li>当“定时器”再次发现数据已经写满了，大小10.05M，会再次给它申请内存，大小为 10.05M * 2 – 10.02M = 10.08M</li>
<li>假如此时总的内存只剩下5M，不足以再给这个内存分配10.08M，那么这个内存会被锁起来，把里面的数据按照相同的key为一组，进行排序后，分别写到不同的缓存中，然后溢写到不同的小文件中，而map task产生的新的计算结果会写入总内存剩余的5M中</li>
<li>buffer中的数据（已经排好序）溢写的时候，会分批溢写，默认一次溢写10000条数据，假如最后一部分数据不足10000条，那么剩下多少条就一次性溢写多少条</li>
<li>每个map task产生的小文件，最终合并成一个大文件来让reduce拉取数据，合成大文件的同时也会生成这个大文件的索引文件，里面记录着分区信息和偏移量（比如：key为hello的数据在第5个字节到第8097个字节）</li>
<li>最终产生的小文件数为2*m（map task的数量）</li>
</ol>
<h4 id="SortShuffle的bypass机制"><a href="#SortShuffle的bypass机制" class="headerlink" title="SortShuffle的bypass机制"></a>SortShuffle的bypass机制</h4><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p>
<ul>
<li>shuffle map task数量小于<code>spark.shuffle.sort.bypassMergeThreshold</code>参数的值（默认200）。</li>
<li>不是聚合类的shuffle算子（比如reduceByKey）。因为如果是聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存，而bypass机制下是无法聚合的。</li>
</ul>
<p><a href="http://chant00.com/media/15052329433749.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052329433749.jpg" alt="img"></a></p>
<p>有条件的sort，当shuffle reduce task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值（默认200）时，会触发bypass机制，不进行sort，假如目前有300个reduce task，如果要触发bypass机制，就就设置spark.shuffle.sort.bypassMergeThreshold的值大于300，bypass机制最终产生2*m（map task的数量）的小文件。</p>
<h4 id="SparkShuffle详解"><a href="#SparkShuffle详解" class="headerlink" title="SparkShuffle详解"></a>SparkShuffle详解</h4><p>先了解一些角色：</p>
<h5 id="MapOutputTracker：管理磁盘小文件的地址"><a href="#MapOutputTracker：管理磁盘小文件的地址" class="headerlink" title="MapOutputTracker：管理磁盘小文件的地址"></a>MapOutputTracker：管理磁盘小文件的地址</h5><ul>
<li>主：MapOutputTrackerMaster</li>
<li>从：MapOutputTrackerWorker</li>
</ul>
<h5 id="BlockManager："><a href="#BlockManager：" class="headerlink" title="BlockManager："></a>BlockManager：</h5><h6 id="主：BlockManagerMaster，存在于Driver端"><a href="#主：BlockManagerMaster，存在于Driver端" class="headerlink" title="主：BlockManagerMaster，存在于Driver端"></a>主：BlockManagerMaster，存在于Driver端</h6><p>管理范围：RDD的缓存数据、广播变量、shuffle过程产生的磁盘小文件<br>包含4个重要对象：</p>
<ol>
<li>ConnectionManager：负责连接其他的BlockManagerSlave</li>
<li>BlockTransferService：负责数据传输</li>
<li>DiskStore：负责磁盘管理</li>
<li>Memstore：负责内存管理</li>
</ol>
<h6 id="从：BlockManagerSlave，存在于Executor端"><a href="#从：BlockManagerSlave，存在于Executor端" class="headerlink" title="从：BlockManagerSlave，存在于Executor端"></a>从：BlockManagerSlave，存在于Executor端</h6><p>包含4个重要对象：</p>
<ol>
<li>ConnectionManager：负责连接其他的BlockManagerSlave</li>
<li>BlockTransferService：负责数据传输</li>
<li>DiskStore：负责磁盘管理</li>
<li>Memstore：负责内存管理</li>
</ol>
<p><a href="http://chant00.com/media/15052834285291.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052834285291.jpg" alt="img"></a></p>
<h4 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h4><h5 id="配置参数的三种方式"><a href="#配置参数的三种方式" class="headerlink" title="配置参数的三种方式"></a>配置参数的三种方式</h5><ol>
<li>在程序中硬编码&lt;<br>br&gt;例如<code>sparkConf.set(&quot;spark.shuffle.file.buffer&quot;,&quot;64k&quot;)</code></li>
<li>提交application时在命令行指定&lt;<br>br&gt;例如<code>spark-submit --conf spark.shuffle.file.buffer=64k --conf 配置信息=配置值 ...</code></li>
<li>修改<code>SPARK_HOME/conf/spark-default.conf</code>配置文件<br>推<br>荐使用第2种方式</li>
</ol>
<h5 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h5><p>默认值：32K<br>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p>
<h5 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h5><p>默认值：48M<br>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96M），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p>
<h5 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h5><p>默认值：3<br>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。<br>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</p>
<h5 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h5><p>默认值：5s<br>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。<br>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</p>
<h5 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h5><p>默认值：0.2<br>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。<br>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</p>
<h5 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h5><p>默认值：sort<br>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。<br>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</p>
<h5 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h5><p>默认值：200<br>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。<br>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</p>
<h5 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h5><p>默认值：false<br>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。<br>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</p>
<h3 id="Spark内存管理"><a href="#Spark内存管理" class="headerlink" title="Spark内存管理"></a>Spark内存管理</h3><p>spark1.5之前默认为静态内存管理，之后默认为统一的内存管理，如果对数据比较了解，那么选用静态内存管理可调控的参数多，若想使用静态内存管理，将<code>spark.memory.useLegacyMode</code>从默认值<code>false</code>改为<code>true</code>即可。<br>这里阐述的是spark1.6版本的内存管理机制，想了解更前卫的版本，请戳<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html?ca=drs-&amp;utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">spark2.1内存管理机制</a></p>
<h4 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h4><p><a href="http://chant00.com/media/15053969470635.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053969470635.jpg" alt="img"></a></p>
<h6 id="Unrolling"><a href="#Unrolling" class="headerlink" title="Unrolling"></a><a href="https://issues.apache.org/jira/secure/attachment/12765646/unified-memory-management-spark-10000.pdf" target="_blank" rel="noopener">Unrolling</a></h6><blockquote>
<p>The memory used for unrolling is borrowed from the storage space. If there are no existing blocks, unrolling can use all of the storage space. Otherwise, unrolling can drop up to M bytes worth of blocks from memory, where M is a fraction of the storage space configurable through <code>spark.storage.unrollFraction</code>(default0.2). Note that this sub­region is not staticallyreserved, but dynamically allocated by dropping existing blocks.</p>
</blockquote>
<p>所以这里的unrollFraction内存其实是个上限，不是静态固定的0.2，而是动态分配的。</p>
<p>关于Unrolling的详细解读，请戳<a href="http://www.jianshu.com/p/58288b862030" target="_blank" rel="noopener">RDD缓存的过程</a></p>
<p>RDD在缓存到存储内存之后，Partition被转换成Block，Record在堆内或堆外存储内存中占用一块连续的空间。将Partition由不连续的存储空间转换为连续存储空间的过程，Spark称之为“展开”（Unroll）。Block有序列化和非序列化两种存储格式，具体以哪种方式取决于该RDD的存储级别。非序列化的Block以一种<code>DeserializedMemoryEntry</code>的数据结构定义，用一个数组存储所有的Java对象，序列化的Block则以<code>SerializedMemoryEntry</code>的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个Executor的Storage模块用一个链式Map结构（<code>LinkedHashMap</code>）来管理堆内和堆外存储内存中所有的Block对象的实例[6]，对这个<code>LinkedHashMap</code>新增和删除间接记录了内存的申请和释放。</p>
<h6 id="Reduce-OOM怎么办？"><a href="#Reduce-OOM怎么办？" class="headerlink" title="Reduce OOM怎么办？"></a>Reduce OOM怎么办？</h6><ol>
<li>减少每次拉取的数据量</li>
<li>提高shuffle聚合的内存比例</li>
<li>增加executor的内存</li>
</ol>
<h4 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h4><p><a href="http://chant00.com/media/15052969860133.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052969860133.jpg" alt="img"></a></p>
<p>统一内存管理中互相借用（申请，检查，借用，归还）这一环节会产生额外的计算开销。<br>其中最重要的优化在于动态占用机制，其规则如下：</p>
<ul>
<li>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围</li>
<li>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</li>
<li>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间</li>
</ul>
<p><a href="http://chant00.com/media/15053921543814.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053921543814.jpg" alt="img"></a><br>凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的 [5] 。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。</p>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Spark SQL的前身是shark，Shark是基于Spark计算框架之上且兼容Hive语法的SQL执行引擎，由于底层的计算采用了Spark，性能比MapReduce的Hive普遍快2倍以上，当数据全部load在内存的话，将快10倍以上，因此Shark可以作为交互式查询应用服务来使用。除了基于Spark的特性外，Shark是完全兼容Hive的语法，表结构以及UDF函数等，已有的HiveSql可以直接进行迁移至Shark上。Shark底层依赖于Hive的解析器，查询优化器，但正是由于Shark的整体设计架构对Hive的依赖性太强，难以支持其长远发展，比如不能和Spark的其他组件进行很好的集成，无法满足Spark的一栈式解决大数据处理的需求<br>Hive是Shark的前身，Shark是SparkSQL的前身，相对于Shark，SparkSQL有什么优势呢？</p>
<ul>
<li>SparkSQL产生的根本原因，是因为它完全脱离了Hive的限制</li>
<li>SparkSQL支持查询原生的RDD，这点就极为关键了。RDD是Spark平台的核心概念，是Spark能够高效的处理大数据的各种场景的基础</li>
<li>能够在Scala中写SQL语句。支持简单的SQL语法检查，能够在Scala中写Hive语句访问Hive数据，并将结果取回作为RDD使用</li>
</ul>
<h5 id="Spark和Hive有两种组合"><a href="#Spark和Hive有两种组合" class="headerlink" title="Spark和Hive有两种组合"></a>Spark和Hive有两种组合</h5><p>Hive on Spark类似于Shark，相对过时，现在公司一般都采用Spark on Hive。</p>
<h6 id="Spark-on-Hive"><a href="#Spark-on-Hive" class="headerlink" title="Spark on Hive"></a>Spark on Hive</h6><p>Hive只是作为了存储的角色<br>SparkSQL作为计算的角色</p>
<h6 id="Hive-on-Spark"><a href="#Hive-on-Spark" class="headerlink" title="Hive on Spark"></a>Hive on Spark</h6><p>Hive承担了一部分计算（解析SQL，优化SQL…）的和存储<br>Spark作为了执行引擎的角色</p>
<h4 id="Dataframe"><a href="#Dataframe" class="headerlink" title="Dataframe"></a>Dataframe</h4><h5 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h5><p>Spark SQL是Spark的核心组件之一，于2014年4月随Spark 1.0版一同面世，在Spark 1.3当中，Spark SQL终于从alpha(内测版本)阶段毕业。Spark 1.3更加完整的表达了Spark SQL的愿景：让开发者用更精简的代码处理尽量少的数据，同时让Spark SQL自动优化执行过程，以达到降低开发成本，提升数据分析执行效率的目的。与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还掌握数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。</p>
<h5 id="RDD-VS-DataFrame"><a href="#RDD-VS-DataFrame" class="headerlink" title="RDD VS DataFrame"></a>RDD VS DataFrame</h5><p><code>DataFrame = SchemaRDD = RDD</code><br><a href="http://chant00.com/media/15052983161313.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052983161313.jpg" alt="img"></a><br>从图虫颜色来区分，DataFrame是列式存储。当要取Age这一列时，RDD必须先取出person再取Age，而DataFrame可以直接取Age这一列。</p>
<h5 id="DataFrame底层架构"><a href="#DataFrame底层架构" class="headerlink" title="DataFrame底层架构"></a>DataFrame底层架构</h5><p><a href="http://chant00.com/media/15052983913084.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052983913084.jpg" alt="img"></a></p>
<h4 id="Predicate-Pushdown谓词下推机制"><a href="#Predicate-Pushdown谓词下推机制" class="headerlink" title="Predicate Pushdown谓词下推机制"></a>Predicate Pushdown谓词下推机制</h4><p>执行如下SQL语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT table1.name,table2.score</span><br><span class="line">FROM table1 JOIN table2 ON (table1.id=table2.id)</span><br><span class="line">WHERE table1.age&gt;25 AND table2.score&gt;90</span><br></pre></td></tr></table></figure>
<p>我们比较一下普通SQL执行流程和Spark SQL的执行流程<br><a href="http://chant00.com/media/15052992976209.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15052992976209.jpg" alt="img"></a></p>
<h4 id="DataFrame创建方式"><a href="#DataFrame创建方式" class="headerlink" title="DataFrame创建方式"></a>DataFrame创建方式</h4><h5 id="1-读JSON文件-不能嵌套"><a href="#1-读JSON文件-不能嵌套" class="headerlink" title="1.读JSON文件(不能嵌套)"></a>1.读JSON文件(不能嵌套)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * people.json</span><br><span class="line"> * &#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line">   &#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">   &#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br><span class="line"> */</span><br><span class="line">object DataFrameOpsFromFile &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf() </span><br><span class="line">    conf.setAppName(&quot;SparkSQL&quot;)</span><br><span class="line">    conf.setMaster(&quot;local&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">val sqlContext = new SQLContext(sc)</span><br><span class="line">//val df = sqlContext.read.format(&quot;json&quot;).load(&quot;people.json&quot;)    </span><br><span class="line">val df = sqlContext.read.json(&quot;people.json&quot;)</span><br><span class="line">    </span><br><span class="line">    //将DF注册成一张临时表，这张表是逻辑上的，数据并不会落地</span><br><span class="line">    //people是临时表的表名，后面的SQL直接FROM这个表名</span><br><span class="line">    df.registerTempTable(&quot;people&quot;)</span><br><span class="line">    //打印DataFrame的结构</span><br><span class="line">df.printSchema()</span><br><span class="line">/*</span><br><span class="line">     * 结果：nullable=true代表该字段可以为空</span><br><span class="line">     * root</span><br><span class="line">       |-- age: long (nullable = true)</span><br><span class="line">       |-- name: string (nullable = true)</span><br><span class="line">     */</span><br><span class="line">      //查看DataFrame中的数据, df.show(int n)可以指定显示多少条数据</span><br><span class="line">     df.show()</span><br><span class="line">     /*</span><br><span class="line">      * 结果：</span><br><span class="line">      * +----+-------+</span><br><span class="line">        | age|   name|</span><br><span class="line">        +----+-------+</span><br><span class="line">        |null|Michael|</span><br><span class="line">        |  30|   Andy|</span><br><span class="line">        |  19| Justin|</span><br><span class="line">        +----+-------+</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">      //SELECT name from table</span><br><span class="line">df.select(&quot;name&quot;).show()</span><br><span class="line">    </span><br><span class="line">//SELECT name,age+10 from table</span><br><span class="line">df.select(df(&quot;name&quot;), df(&quot;age&quot;).plus(10)).show()</span><br><span class="line">    </span><br><span class="line">//SELECT * FROM table WHERE age &gt; 10</span><br><span class="line">df.filter(df(&quot;age&quot;)&gt;10).show()</span><br><span class="line">    </span><br><span class="line">//SELECT count(*) FROM table GROUP BY age</span><br><span class="line">df.groupBy(&quot;age&quot;).count.show()</span><br><span class="line"></span><br><span class="line">sqlContext.sql(&quot;select * from people where age &gt; 20&quot;).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-JSON格式的RDD转为DataFrame"><a href="#2-JSON格式的RDD转为DataFrame" class="headerlink" title="2.JSON格式的RDD转为DataFrame"></a>2.JSON格式的RDD转为DataFrame</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">public class DataFrameOpsFromJsonRdd &#123;</span><br><span class="line">     public static void main(String[] args) &#123;</span><br><span class="line">         SparkConf conf = new SparkConf()</span><br><span class="line">.setAppName(&quot;DataFrameFromJsonRdd&quot;).setMaster(&quot;local&quot;);</span><br><span class="line">         JavaSparkContext sc = new JavaSparkContext(conf);</span><br><span class="line">         //若想使用SparkSQL必须创建SQLContext,必须是传入SparkContext,不能是SparkConf</span><br><span class="line">         SQLContext sqlContext = new SQLContext(sc);</span><br><span class="line">         </span><br><span class="line">         //创建一个本地的集合,类型String,集合中元素的格式为json格式       </span><br><span class="line">         List&lt;String&gt; nameList = Arrays.asList(</span><br><span class="line">                            &quot;&#123;&apos;name&apos;:&apos;Tom&apos;, &apos;age&apos;:20&#125;&quot;,</span><br><span class="line">                            &quot;&#123;&apos;name&apos;:&apos;Jed&apos;, &apos;age&apos;:30&#125;&quot;,</span><br><span class="line">                            &quot;&#123;&apos;name&apos;:&apos;Tony&apos;, &apos;age&apos;:22&#125;&quot;,</span><br><span class="line">                            &quot;&#123;&apos;name&apos;:&apos;Jack&apos;, &apos;age&apos;:24&#125;&quot;);</span><br><span class="line">         List&lt;String&gt; scoreList = Arrays.asList(</span><br><span class="line">                  &quot;&#123;&apos;name&apos;:&apos;Tom&apos;,&apos;score&apos;:100&#125;&quot;,</span><br><span class="line">                  &quot;&#123;&apos;name&apos;:&apos;Jed&apos;,&apos;score&apos;:99&#125;&quot; );</span><br><span class="line">         </span><br><span class="line">         JavaRDD&lt;String&gt; nameRDD = sc.parallelize(nameList);</span><br><span class="line">         JavaRDD&lt;String&gt; scoreRDD = sc.parallelize(scoreList);</span><br><span class="line">         </span><br><span class="line">         DataFrame nameDF = sqlContext.read().json(nameRDD);</span><br><span class="line">         DataFrame scoreDF = sqlContext.read().json(scoreRDD);</span><br><span class="line">         </span><br><span class="line">         /**</span><br><span class="line">          * SELECT nameTable.name,nameTable.age,scoreTable.score</span><br><span class="line">      FROM nameTable JOIN nameTable ON (nameTable.name = scoreTable.name)</span><br><span class="line">          */</span><br><span class="line">         nameDF.join(</span><br><span class="line">scoreDF, nameDF.col(&quot;name&quot;).$eq$eq$eq(scoreDF.col(&quot;name&quot;))</span><br><span class="line">).select(</span><br><span class="line">nameDF.col(&quot;name&quot;),nameDF.col(&quot;age&quot;),scoreDF.col(&quot;score&quot;))</span><br><span class="line">.show();         </span><br><span class="line">         </span><br><span class="line">          nameDF.registerTempTable(&quot;name&quot;);</span><br><span class="line">         scoreDF.registerTempTable(&quot;score&quot;);</span><br><span class="line">         String sql = &quot;SELECT name.name,name.age,score.score &quot;</span><br><span class="line">                  + &quot;FROM name join score ON (name.name = score.name)&quot;;</span><br><span class="line">         </span><br><span class="line">         sqlContext.sql(sql).show();</span><br><span class="line">         /*</span><br><span class="line">          * +----+---+-----+</span><br><span class="line">            |name|age|score|</span><br><span class="line">            +----+---+-----+</span><br><span class="line">            | Tom| 20|  100|</span><br><span class="line">            | Jed| 30|   99|</span><br><span class="line">            +----+---+-----+</span><br><span class="line">          */</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-非JSON格式的RDD转为DataFrame"><a href="#3-非JSON格式的RDD转为DataFrame" class="headerlink" title="3.非JSON格式的RDD转为DataFrame"></a>3.非JSON格式的RDD转为DataFrame</h5><h6 id="1-反射的方式"><a href="#1-反射的方式" class="headerlink" title="1.反射的方式"></a>1.反射的方式</h6><p>Person类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line"></span><br><span class="line">object RDD2DataFrameByReflectionScala &#123;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    *  * 使用反射的方式将RDD转换成为DataFrame</span><br><span class="line">    *  * 1.自定义的类必须是public</span><br><span class="line">    *  * 2.自定义的类必须是可序列化的</span><br><span class="line">    *  * 3.RDD转成DataFrame的时候，会根据自定义类中的字段名进行排序</span><br><span class="line">    *  *所以这里直接用case class类</span><br><span class="line">    *  * Peoples.txt内容：</span><br><span class="line">    *      1,Tom,7</span><br><span class="line">    *      2,Tony,11</span><br><span class="line">    *      3,Jack,5</span><br><span class="line">    *  * @author root</span><br><span class="line">    *  */</span><br><span class="line">  case class Person(name: String, age: Int)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf() //创建sparkConf对象</span><br><span class="line">    conf.setAppName(&quot;My First Spark App&quot;) //设置应用程序的名称，在程序运行的监控页面可以看到名称</span><br><span class="line">    conf.setMaster(&quot;local&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">    import sqlContext.implicits._</span><br><span class="line"></span><br><span class="line">    //传入进去Person.class的时候，sqlContext是通过反射的方式创建DataFrame</span><br><span class="line">    //在底层通过反射的方式或得Person的所有field，结合RDD本身，就生成了DataFrame</span><br><span class="line">    val people = sc.textFile(&quot;/Users/Chant/Documents/大数据0627/14spark/code/SparkJavaOperator/Peoples.txt&quot;)</span><br><span class="line">      .map(_.split(&quot;,&quot;)).map(p =&gt; Person(p(1), p(2).trim.toInt)).toDF()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    people.registerTempTable(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val teenagers = sqlContext.sql(&quot;SELECT name, age FROM people WHERE age &gt;= 6 AND age &lt;= 19&quot;)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 对dataFrame使用map算子后，返回类型是RDD&lt;Row&gt;</span><br><span class="line">      */</span><br><span class="line">    //    teenagers.map(t =&gt; &quot;Name: &quot; + t(0)).foreach(println)</span><br><span class="line">    //    teenagers.map(t =&gt; &quot;Name: &quot; + t.get(0)).foreach(println)</span><br><span class="line">    // 不推荐此方法，因为RDD转成DataFrame的时候，他会根据自定义类中的字段名(按字典序)进行排序。</span><br><span class="line">    // 如果列很多，那你就得自己去按字典序排序列名，然后才知道该列对应的索引位。</span><br><span class="line"></span><br><span class="line">    // or by field name: 推荐用列名来获取列值</span><br><span class="line">    //    teenagers.map(t =&gt; &quot;Name: &quot; + t.getAs[String](&quot;name&quot;)).foreach(println)</span><br><span class="line">    //    teenagers.map(t =&gt; t.getAs(&quot;name&quot;)).foreach(println)//这样会报错java.lang.ClassCastException: java.lang.String cannot be cast to scala.runtime.Nothing$</span><br><span class="line">    teenagers.map(t =&gt; t.getAs[String](&quot;name&quot;)).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h6 id="2-动态创建Schema"><a href="#2-动态创建Schema" class="headerlink" title="2.动态创建Schema"></a>2.动态创建Schema</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.&#123;Row, SQLContext&#125;</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Created by Chant.</span><br><span class="line">  */</span><br><span class="line">object RDD2DataFrameByProgrammatically &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = new SparkContext(new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;RDD2DataFrameByProgrammatically&quot;))</span><br><span class="line">    val sQLContext = new SQLContext(sc)</span><br><span class="line">//将数据封装到Row中，RDD[Row]就可以转成DataFrame</span><br><span class="line">    val RowRDD = sc.textFile(&quot;/Users/Chant/Documents/大数据0627/14spark/code/SparkScalaOperator/Peoples.txt&quot;)</span><br><span class="line">      .map(_.split(&quot;,&quot;)).map(x =&gt; Row(x(0).toInt, x(1), x(2).toInt))</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 读取配置文件的方式</span><br><span class="line">      */</span><br><span class="line">    val schemaString = &quot;id:Int name:String age:Int&quot;</span><br><span class="line">    val schema = StructType(schemaString.split(&quot; &quot;)</span><br><span class="line">      .map(x =&gt; StructField(x.split(&quot;:&quot;)(0), if (x.split(&quot;:&quot;)(1) == &quot;String&quot;) StringType else IntegerType, true)))</span><br><span class="line">    //true代表该字段是否可以为空</span><br><span class="line">    //构建StructType，用于最后DataFrame元数据的描述</span><br><span class="line">    //基于已有的MetaData以及RDD&lt;Row&gt; 来构造DataFrame</span><br><span class="line">    </span><br><span class="line">//    //最后一定要写else，不像java可以只有个if。</span><br><span class="line">//    val schema = StructType(schemaString.split(&quot; &quot;)</span><br><span class="line">//      .map(x =&gt;&#123;</span><br><span class="line">//        val pair = x.split(&quot;:&quot;)</span><br><span class="line">//        if (pair(1) == &quot;String&quot;) StructField(pair(0), StringType,true)</span><br><span class="line">//        else if(pair(1) == &quot;Int&quot;) StructField(pair(0), IntegerType,true)</span><br><span class="line">//        else  StructField(pair(0), IntegerType,true)</span><br><span class="line">//      &#125;))</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 如果列的数据类型比较多，可以用match case</span><br><span class="line">      */</span><br><span class="line">//    val schemaString = &quot;id:Int name:String age:Int&quot;</span><br><span class="line">//</span><br><span class="line">//    val schema = StructType(schemaString.split(&quot; &quot;)</span><br><span class="line">//      .map(x =&gt; &#123;</span><br><span class="line">//        val pair = x.split(&quot;:&quot;)</span><br><span class="line">////        var dataType = null.asInstanceOf[DataType]//巧用这一招</span><br><span class="line">//        var dataType : DataType = null</span><br><span class="line">//        pair(1) match &#123;</span><br><span class="line">//          case &quot;String&quot; =&gt; dataType = StringType</span><br><span class="line">//          case &quot;Int&quot;    =&gt; dataType = IntegerType</span><br><span class="line">//          case &quot;Double&quot; =&gt; dataType = DoubleType</span><br><span class="line">//          case &quot;Long&quot;   =&gt; dataType = LongType</span><br><span class="line">//          case _        =&gt; println(&quot;default, can&apos;t match&quot;)</span><br><span class="line">//        &#125;</span><br><span class="line">//</span><br><span class="line">//        StructField(pair(0),dataType,true)</span><br><span class="line">//      &#125;))</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 傻瓜式</span><br><span class="line">      */</span><br><span class="line">    //    val structField = Array(StructField(&quot;id&quot;, IntegerType, true), StructField(&quot;name&quot;, StringType, true), StructField(&quot;age&quot;, IntegerType, true))</span><br><span class="line">    //    //    val schema = StructType.apply(structField)</span><br><span class="line">    //    val schema = StructType(structField)</span><br><span class="line"></span><br><span class="line">    val df = sQLContext.createDataFrame(RowRDD, schema)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    df.registerTempTable(&quot;people&quot;)</span><br><span class="line">    val res = sQLContext.sql(&quot;select * from people where age &gt; 6&quot;)</span><br><span class="line">    res.show()</span><br><span class="line">    res.map(x =&gt; &quot;Name: &quot; + x.getAs[String](&quot;name&quot;) + &quot;\t Age: &quot; + x.getAs[Int](&quot;age&quot;)).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-读取MySQL中的数据来创建DataFrame"><a href="#4-读取MySQL中的数据来创建DataFrame" class="headerlink" title="4. 读取MySQL中的数据来创建DataFrame"></a>4. 读取MySQL中的数据来创建DataFrame</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">package com.chant.sql.jdbc</span><br><span class="line"></span><br><span class="line">import java.sql.&#123;Connection, DriverManager, PreparedStatement&#125;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Created by Chant.</span><br><span class="line">  */</span><br><span class="line">object JDBCDataSource2 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = new SparkContext(new SparkConf().setAppName(&quot;JDBCDataSource2&quot;).setMaster(&quot;local&quot;))</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line"></span><br><span class="line">    val reader = sqlContext.read.format(&quot;jdbc&quot;)</span><br><span class="line">    reader.option(&quot;url&quot;, &quot;jdbc:mysql://node01:3306/testdb&quot;)</span><br><span class="line">    reader.option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">    reader.option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">    reader.option(&quot;password&quot;, &quot;123456&quot;)</span><br><span class="line">    reader.option(&quot;dbtable&quot;, &quot;student_info&quot;)</span><br><span class="line"></span><br><span class="line">    val stuInfoDF = reader.load()</span><br><span class="line">    reader.option(&quot;dbtable&quot;,&quot;student_score&quot;)</span><br><span class="line">    val stuScoreDF = reader.load()</span><br><span class="line">//    分别将mysql中两张表的数据加载并注册为DataFrame</span><br><span class="line">    stuInfoDF.registerTempTable(&quot;stuInfos&quot;)</span><br><span class="line">    stuScoreDF.registerTempTable(&quot;stuScores&quot;)</span><br><span class="line"></span><br><span class="line">    val sql = &quot;select stuInfos.name, age, score from stuInfos join stuScores &quot; +</span><br><span class="line">      &quot;on (stuInfos.name = stuScores.name) where stuScores.score &gt; 80&quot;</span><br><span class="line">    //执行sql，join两个DF</span><br><span class="line">    val resDF = sqlContext.sql(sql)</span><br><span class="line">    resDF.show()</span><br><span class="line"></span><br><span class="line">//    将join后的数据写入的数据库</span><br><span class="line">    resDF.rdd.foreachPartition(p =&gt;&#123;</span><br><span class="line">      Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">      var conn : Connection = null//这样写，在finnaly的时候才能访问到并将其关闭</span><br><span class="line">      var ps : PreparedStatement = null</span><br><span class="line">      val sql2 = &quot;insert into good_student_info values(?,?,?)&quot;</span><br><span class="line"></span><br><span class="line">      try&#123;</span><br><span class="line">        conn = DriverManager.getConnection(&quot;jdbc:mysql://node01:3306/testdb&quot;, &quot;root&quot;, &quot;123456&quot;)//获取数据库链接</span><br><span class="line">        conn.setAutoCommit(false)//关闭自动提交</span><br><span class="line">        ps = conn.prepareStatement(sql2)//准备sql</span><br><span class="line">        //迭代添加数据，并添加带batch</span><br><span class="line">        p.foreach(row =&gt;&#123;</span><br><span class="line">          ps.setString(1, row.getAs[String](&quot;name&quot;))</span><br><span class="line">          ps.setInt(2, row.getAs[Int](&quot;age&quot;))</span><br><span class="line">          ps.setInt(3, row.getAs[Int](&quot;score&quot;))</span><br><span class="line">          ps.addBatch()</span><br><span class="line">        &#125;)</span><br><span class="line">        //执行并提交</span><br><span class="line">        ps.executeBatch()</span><br><span class="line">        conn.commit()//貌似数据量少的时候，不提交也会有数据写入数据库？？？尚存疑问。但是肯定要写</span><br><span class="line">      &#125;catch&#123;</span><br><span class="line">        case e:Exception =&gt; e.printStackTrace()</span><br><span class="line">      &#125;finally &#123;</span><br><span class="line">        if(ps != null) ps.close()</span><br><span class="line">        if(conn != null) conn.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="5-读取Hive中的数据创建一个DataFrame（Spark-on-Hive）"><a href="#5-读取Hive中的数据创建一个DataFrame（Spark-on-Hive）" class="headerlink" title="5. 读取Hive中的数据创建一个DataFrame（Spark on Hive）"></a>5. 读取Hive中的数据创建一个DataFrame（Spark on Hive）</h5><p>Spark与Hive整合：<br>1) 编辑spark客户端的配置文件hive-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">node04：vi /opt/chant/spark-1.6.0/conf/hive-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">  		&lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">    		&lt;value&gt;thrift://node04:9083&lt;/value&gt;</span><br><span class="line">      		&lt;description&gt;</span><br><span class="line">Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.</span><br><span class="line">&lt;/description&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>2) 把hadoop的core-site.xml和hdfs-site.xml copy到SPARK_HOME/conf/下<br>3) node0{1,2,3}：zkServer.sh start<br>4) node01：start-dfs.sh<br>5) node01：service mysqld start<br>6) node04：hive –service metastore<br>7) node01：/opt/chant/spark-1.6.0/sbin/start-all.sh<br>8) node02：/opt/chant/spark-1.6.0/sbin/start-master.sh<br>9) node04：/opt/chant/spark-1.6.0/bin/spark-submit<br>–master spark://node01:7077,node02:7077<br>–class com.bjchant.java.spark.sql.hive.HiveDataSource<br>../TestHiveContext.jar<br>jar包中的测试代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext</span><br><span class="line"></span><br><span class="line">object HiveDataSource &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(&quot;HiveDataSource&quot;);</span><br><span class="line">    val sc = new SparkContext(conf);</span><br><span class="line">    val hiveContext = new HiveContext(sc);</span><br><span class="line">    hiveContext.sql(&quot;DROP TABLE IF EXISTS student_infos&quot;);</span><br><span class="line">    hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS student_infos (name STRING, age INT) row format delimited fields terminated by &apos;\t&apos;&quot;);</span><br><span class="line">    hiveContext.sql(&quot;LOAD DATA &quot;</span><br><span class="line">      + &quot;LOCAL INPATH &apos;/root/resource/student_infos&apos; &quot;</span><br><span class="line">      + &quot;INTO TABLE student_infos&quot;);</span><br><span class="line"></span><br><span class="line">    hiveContext.sql(&quot;DROP TABLE IF EXISTS student_scores&quot;);</span><br><span class="line">    hiveContext.sql(&quot;CREATE TABLE IF NOT EXISTS student_scores (name STRING, score INT) row format delimited fields terminated by &apos;\t&apos;&quot;);</span><br><span class="line">    hiveContext.sql(&quot;LOAD DATA &quot;</span><br><span class="line">      + &quot;LOCAL INPATH &apos;/root/resource/student_scores&apos; &quot;</span><br><span class="line">      + &quot;INTO TABLE student_scores&quot;);</span><br><span class="line"></span><br><span class="line">    val goodStudentsDF = hiveContext.sql(&quot;SELECT si.name, si.age, ss.score &quot;</span><br><span class="line">      + &quot;FROM student_infos si &quot;</span><br><span class="line">      + &quot;JOIN student_scores ss ON si.name=ss.name &quot;</span><br><span class="line">      + &quot;WHERE ss.score&gt;=80&quot;);</span><br><span class="line"></span><br><span class="line">    hiveContext.sql(&quot;DROP TABLE IF EXISTS good_student_infos&quot;);</span><br><span class="line">//    goodStudentsDF.saveAsTable(&quot;good_student_infos&quot;);</span><br><span class="line">    hiveContext.sql(&quot;USE result&quot;)</span><br><span class="line">    </span><br><span class="line">    //将goodStudentsDF里面的值写入到Hive表中，如果表不存在，会自动创建然后将数据插入到表中</span><br><span class="line">    goodStudentsDF.write.saveAsTable(&quot;good_student_infos&quot;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="DataFrame数据存储"><a href="#DataFrame数据存储" class="headerlink" title="DataFrame数据存储"></a>DataFrame数据存储</h4><ol>
<li>存储到hive表中<br>把hive表读取为dataFrame<br><code>dataFrame = hiveContext().table(&quot;table_name&quot;);</code><br>把dataFrame转为hive表存储到hive中，若table不存在，自动创建<br><code>dataFrame.write().saveAsTable(&quot;table_name&quot;);</code></li>
</ol>
<p>2.存储到MySQL/HBase/Redis…中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataFrame.javaRDD().foreachPartition(new VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line">	……</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>3.存储到parquet文件(压缩比大，节省空间)中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DataFrame usersDF = </span><br><span class="line">sqlContext.read().format(&quot;parquet&quot;).load(&quot;hdfs://node01:9000/input/users.parquet&quot;);</span><br><span class="line">usersDF.registerTempTable(&quot;users&quot;);</span><br><span class="line">DataFrame resultDF = sqlContext.sql(&quot;SELECT * FROM users WHERE name = &apos;Tom&apos;&quot;);</span><br><span class="line">resultDF.write().format(&quot;parquet &quot;).mode(SaveMode.Ignore)</span><br><span class="line">.save(&quot;hdfs://node01:9000/output/result. parquet &quot;);</span><br><span class="line">resultDF.write().format(&quot;json&quot;).mode(SaveMode. Overwrite)</span><br><span class="line">.save(&quot;hdfs://node01:9000/output/result.json&quot;);</span><br><span class="line"></span><br><span class="line">public enum SaveMode &#123;</span><br><span class="line">  Append, //如果文件已经存在，追加</span><br><span class="line">  Overwrite, //如果文件已经存在，覆盖</span><br><span class="line">  ErrorIfExists, //如果文件已经存在，报错</span><br><span class="line">  Ignore//如果文件已经存在，不对原文件进行任何修改，即不存储DataFrame</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>parquet数据源会自动推断分区</strong>，类似hive里面的分区表的概念<br>文件存储的目录结构如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/users</span><br><span class="line">|/country=US</span><br><span class="line">	|data：id,name</span><br><span class="line">|/country=ZH</span><br><span class="line">	|data：id,name</span><br></pre></td></tr></table></figure>
<p>当执行以下代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataFrame usersDF = sqlContext.read().parquet(</span><br><span class="line">				&quot;hdfs://node01:9000/users&quot;); //路径只写到users</span><br><span class="line">usersDF.printSchema();</span><br><span class="line">/*</span><br><span class="line">*(id int, name string, country string)</span><br><span class="line">*/</span><br><span class="line">usersDF.show();</span><br><span class="line">usersDF.registerTempTable(&quot;table1&quot;);</span><br><span class="line">sqlContext.sql(&quot;SELECT count(0) FROM table1 WHERE country = &apos;ZH&apos;&quot;).show();</span><br><span class="line">//执行这个sql只需要去country=ZH文件夹下去遍历即可</span><br></pre></td></tr></table></figure>
<h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><h5 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.sql.SQLContext</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.types.StructType</span><br><span class="line">import org.apache.spark.sql.types.StructField</span><br><span class="line">import org.apache.spark.sql.types.StringType</span><br><span class="line"></span><br><span class="line">object UDF &#123;</span><br><span class="line">  </span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">        .setMaster(&quot;local&quot;) </span><br><span class="line">        .setAppName(&quot;UDF&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line">    val sqlContext = new SQLContext(sc)</span><br><span class="line">  </span><br><span class="line">    val names = Array(&quot;yarn&quot;, &quot;Marry&quot;, &quot;Jack&quot;, &quot;Tom&quot;) </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    val namesRDD = sc.parallelize(names, 4) </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    val namesRowRDD = namesRDD.map &#123; name =&gt; Row(name) &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    val structType = StructType(Array(StructField(&quot;name&quot;, StringType, true)))  </span><br><span class="line">    </span><br><span class="line">    val namesDF = sqlContext.createDataFrame(namesRowRDD, structType) </span><br><span class="line">    </span><br><span class="line">    // 注册一张names表</span><br><span class="line">    namesDF.registerTempTable(&quot;names&quot;)  </span><br><span class="line">    </span><br><span class="line">    sqlContext.udf.register(&quot;strLen&quot;, (str: String) =&gt; str.length()) </span><br><span class="line">  </span><br><span class="line">    // 使用自定义函数</span><br><span class="line">    sqlContext.sql(&quot;select name,strLen(name) from names&quot;).show</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="UDAF：实现对某个字段进行count"><a href="#UDAF：实现对某个字段进行count" class="headerlink" title="UDAF：实现对某个字段进行count"></a>UDAF：实现对某个字段进行count</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.expressions.UserDefinedAggregateFunction</span><br><span class="line">import org.apache.spark.sql.types.StructType</span><br><span class="line">import org.apache.spark.sql.types.DataType</span><br><span class="line">import org.apache.spark.sql.expressions.MutableAggregationBuffer</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">import org.apache.spark.sql.types.StructField</span><br><span class="line">import org.apache.spark.sql.types.StringType</span><br><span class="line">import org.apache.spark.sql.types.IntegerType</span><br><span class="line"></span><br><span class="line">class StringCount extends UserDefinedAggregateFunction &#123;</span><br><span class="line"></span><br><span class="line">  //输入数据的类型</span><br><span class="line">  def inputSchema: StructType = &#123;</span><br><span class="line">    StructType(Array(StructField(&quot;12321&quot;, StringType, true)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 聚合操作时，所处理的数据的类型</span><br><span class="line">  def bufferSchema: StructType = &#123;</span><br><span class="line">    StructType(Array(StructField(&quot;count&quot;, IntegerType, true)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def deterministic: Boolean = &#123;</span><br><span class="line">    true</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 为每个分组的数据执行初始化值</span><br><span class="line">  def initialize(buffer: MutableAggregationBuffer): Unit = &#123;</span><br><span class="line">    buffer(0) = 0</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * update可以认为是，一个一个地将组内的字段值传递进来实现拼接的逻辑</span><br><span class="line">    * buffer.getInt(0)获取的是上一次聚合后的值</span><br><span class="line">    * 相当于map端的combiner,combiner就是对每一个map task的处理结果进行一次小聚合</span><br><span class="line">    * 大聚和发生在reduce端</span><br><span class="line">    */</span><br><span class="line">  //每个组，有新的值进来的时候，进行分组对应的聚合值的计算</span><br><span class="line">  def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;</span><br><span class="line">    buffer(0) = buffer.getAs[Int](0) + 1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 合并 update操作,可能是针对一个分组内的部分数据,在某个节点上发生的</span><br><span class="line">    * 但是可能一个分组内的数据，会分布在多个节点上处理</span><br><span class="line">    * 此时就要用merge操作，将各个节点上分布式拼接好的串，合并起来</span><br><span class="line">    * buffer1.getInt(0) : 大聚和的时候上一次聚合后的值      </span><br><span class="line">    * buffer2.getInt(0) : 这次计算传入进来的update的结果</span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">  // 最后merger的时候，在各个节点上的聚合值，要进行merge，也就是合并</span><br><span class="line">  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;</span><br><span class="line">    buffer1(0) = buffer1.getAs[Int](0) + buffer2.getAs[Int](0)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 最终函数返回值的类型</span><br><span class="line">  def dataType: DataType = &#123;</span><br><span class="line">    IntegerType</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 最后返回一个最终的聚合值     要和dataType的类型一一对应</span><br><span class="line">  def evaluate(buffer: Row): Any = &#123;</span><br><span class="line">    buffer.getAs[Int](0)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="开窗函数"><a href="#开窗函数" class="headerlink" title="开窗函数"></a>开窗函数</h4><p>row_number()开窗函数的作用：<br>按照我们每一个分组的数据，按其照顺序，打上一个分组内的行号<br>id=2016 [111,112,113]<br>那么对这个分组的每一行使用row_number()开窗函数后，三行数据会一次得到一个组内的行号<br>id=2016 [111 1,112 2,113 3]<br><a href="http://chant00.com/media/15053155944357.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053155944357.jpg" alt="img"></a></p>
<h3 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h3><h4 id="Strom-VS-SparkStreaming"><a href="#Strom-VS-SparkStreaming" class="headerlink" title="Strom VS SparkStreaming"></a>Strom VS SparkStreaming</h4><ol>
<li>Storm是一个纯实时的流式处理框，SparkStreaming是一个准实时的流式处理框架，（微批处理：可以设置时间间隔）</li>
<li>SparkStreaming的吞吐量比Storm高（因为它是微批处理）</li>
<li>Storm的事务机制要比SparkStreaming好（每个数据只处理一次）</li>
<li>Storm支持动态资源调度（ 可以在数据的低谷期使用更少的资源，而spark的粗粒度资源调度默认是不支持这样的）</li>
<li>SparkStreaming的应用程序中可以写SQL语句来处理数据，可以和spark core及sparkSQL无缝结合，所以SparkingStreaming擅长复杂的业务处理，而Storm不擅长复杂的业务处理，它擅长简单的汇总型计算（天猫双十一销量）</li>
</ol>
<h4 id="SparkStreaming执行流程"><a href="#SparkStreaming执行流程" class="headerlink" title="SparkStreaming执行流程"></a>SparkStreaming执行流程</h4><p><a href="http://chant00.com/media/15053158002878.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053158002878.jpg" alt="img"></a></p>
<p>总结：<br>receiver task是7*24h一直在执行，一直接收数据，将接收到的数据保存到batch中，假设batch interval为5s，那么把接收到的数据每隔5s切割到一个batch，因为batch是没有分布式计算的特性的，而RDD有，所以把batch封装到RDD中，又把RDD封装到DStream中进行计算，在第5s的时候，计算前5s的数据，假设计算5s的数据只需要3s，那么第5-8s一边计算任务，一边接收数据，第9-11s只是接收数据，然后在第10s的时候，循环上面的操作。</p>
<p>如果job执行时间大于batch interval，那么未执行的数据会越攒越多，最终导致Spark集群崩溃。<br>测试：<br>1.开启scoket server<br><code>[root@node01 ~]# nc -lk 9999</code><br>2.启动spark集群</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# /opt/chant/spark-1.6.0/sbin /start-all.sh</span><br><span class="line">[root@node02 ~]# /opt/chant/spark-1.6.0/sbin /start-master.sh</span><br></pre></td></tr></table></figure>
<p>3.运行测试程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.StreamingContext</span><br><span class="line">import org.apache.spark.streaming.Durations</span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 1.local的模拟线程数必须大于等于2,因为一条线程被receiver(接受数据的线程)占用，另外一个线程是job执行</span><br><span class="line"> * 2.Durations时间的设置，就是我们能接受的延迟度，这个我们需要根据集群的资源情况以及监控，要考虑每一个job的执行时间</span><br><span class="line"> * 3.创建StreamingContext有两种方式 (sparkconf、sparkcontext)</span><br><span class="line"> * 4.业务逻辑完成后，需要有一个output operator</span><br><span class="line"> * 5.StreamingContext.start(),straming框架启动之后是不能在次添加业务逻辑</span><br><span class="line"> * 6.StreamingContext.stop()无参的stop方法会将sparkContext一同关闭，如果只想关闭StreamingContext,在stop()方法内传入参数false</span><br><span class="line"> * 7.StreamingContext.stop()停止之后是不能在调用start  </span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">object WordCountOnline &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    </span><br><span class="line">    val sparkConf = new SparkConf()</span><br><span class="line">    sparkConf.setMaster(&quot;local[2]&quot;)</span><br><span class="line">    sparkConf.setAppName(&quot;WordCountOnline&quot;)</span><br><span class="line"></span><br><span class="line">    //在创建streaminContext的时候设置batch Interval</span><br><span class="line">    val ssc = new StreamingContext(sparkConf,Durations.seconds(5))</span><br><span class="line">    </span><br><span class="line">    val linesDStream = ssc.socketTextStream(&quot;node01&quot;, 9999, StorageLevel.MEMORY_AND_DISK)</span><br><span class="line">    </span><br><span class="line">//    val wordsDStream = linesDStream.flatMap &#123; _.split(&quot; &quot;) &#125;</span><br><span class="line">    val wordsDStream = linesDStream.flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">    val pairDStream = wordsDStream.map &#123; (_,1) &#125;</span><br><span class="line">    </span><br><span class="line">    val resultDStream = pairDStream.reduceByKey(_+_)</span><br><span class="line">    </span><br><span class="line">    resultDStream.print()</span><br><span class="line">     //outputoperator类的算子 </span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    ssc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>结果：<br>在server 端输入数据，例如，hello world，控制台实时打印wordwount结果：(hello,1)(world,1)</p>
<h4 id="Output-Operations-on-DStreams"><a href="#Output-Operations-on-DStreams" class="headerlink" title="Output Operations on DStreams"></a>Output Operations on DStreams</h4><h5 id="foreachRDD-func"><a href="#foreachRDD-func" class="headerlink" title="foreachRDD(func)"></a>foreachRDD(func)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    // ConnectionPool is a static, lazily initialized pool of connections</span><br><span class="line">    val connection = ConnectionPool.getConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">ConnectionPool.returnConnection(connection)  </span><br><span class="line">// return to the pool for future reuse</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="saveAsTextFiles-prefix-suffix"><a href="#saveAsTextFiles-prefix-suffix" class="headerlink" title="saveAsTextFiles(prefix, [suffix])"></a>saveAsTextFiles(prefix, [suffix])</h5><p>Save this DStream’s contents as text files.</p>
<h5 id="saveAsObjectFiles-prefix-suffix"><a href="#saveAsObjectFiles-prefix-suffix" class="headerlink" title="saveAsObjectFiles(prefix, [suffix])"></a>saveAsObjectFiles(prefix, [suffix])</h5><p>Save this DStream’s contents as SequenceFiles of serialized Java objects.</p>
<h5 id="saveAsHadoopFiles-prefix-suffix"><a href="#saveAsHadoopFiles-prefix-suffix" class="headerlink" title="saveAsHadoopFiles(prefix, [suffix])"></a>saveAsHadoopFiles(prefix, [suffix])</h5><p>Save this DStream’s contents as Hadoop files.</p>
<h4 id="Transformations-on-Dstreams"><a href="#Transformations-on-Dstreams" class="headerlink" title="Transformations on Dstreams"></a>Transformations on Dstreams</h4><h5 id="transform-func"><a href="#transform-func" class="headerlink" title="transform(func)"></a>transform(func)</h5><p>Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) </span><br><span class="line">// RDD containing spam information</span><br><span class="line">val cleanedDStream = wordCounts.transform(rdd =&gt; &#123;</span><br><span class="line">  rdd.join(spamInfoRDD).filter(...) </span><br><span class="line">// join data stream with spam information to do data cleaning</span><br><span class="line">  ...</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h5 id="updateStateByKey-func"><a href="#updateStateByKey-func" class="headerlink" title="updateStateByKey(func)"></a>updateStateByKey(func)</h5><p>Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.<br>UpdateStateByKey的主要功能:<br>1．Spark Streaming中为每一个Key维护一份state状态，state类型可以是任意类型的，可以是一个自定义的对象，那么更新函数也可以是自定义的。<br>2．通过更新函数对该key的状态不断更新，对于每个新的batch而言，Spark Streaming会在使用updateStateByKey的时候为已经存在的key进行state的状态更新</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.&#123;Durations, StreamingContext&#125;</span><br><span class="line">import spire.std.option</span><br><span class="line">/** wordcount,实时计算结果，与之前的worCountOnline不同，它是增量计算的。</span><br><span class="line">  *</span><br><span class="line">  * UpdateStateByKey的主要功能:</span><br><span class="line">  * 1、Spark Streaming中为每一个Key维护一份state状态，state类型可以是任意类型的的， 可以是一个自定义的对象，那么更新函数也可以是自定义的。</span><br><span class="line">  * 2、通过更新函数对该key的状态不断更新，对于每个新的batch而言，Spark Streaming会在使用updateStateByKey的时候为已经存在的key进行state的状态更新</span><br><span class="line">  * 第6s的计算结果1-5s</span><br><span class="line">  * hello,3</span><br><span class="line">  * world,2</span><br><span class="line">  *</span><br><span class="line">  * 第11s的时候 6-11秒</span><br><span class="line">  * 接收数据</span><br><span class="line">  * hello 1</span><br><span class="line">  * hello 1</span><br><span class="line">  * world 1</span><br><span class="line">  *计算逻辑</span><br><span class="line">  * hello 3+1+1</span><br><span class="line">  * world 2+1</span><br><span class="line">  *</span><br><span class="line">  * 第16s 11-15s</span><br><span class="line">  * 接收数据</span><br><span class="line">  * hello 1</span><br><span class="line">  * hello 1</span><br><span class="line">  * world 1</span><br><span class="line">  * 计算逻辑</span><br><span class="line">  * hello 5+1+1</span><br><span class="line">  * world 3+1</span><br><span class="line">  * 如果要不断的更新每个key的state，就一定涉及到了状态的保存和容错，这个时候就需要开启checkpoint机制和功能</span><br><span class="line">  *</span><br><span class="line">  * 全面的广告点击分析</span><br><span class="line">  *</span><br><span class="line">  *         有何用？   统计广告点击流量，统计这一天的车流量，统计。。。。点击量</span><br><span class="line">  */</span><br><span class="line">object UpdateStateByKeyOperator &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;UpdateStateByKeyOperator&quot;)</span><br><span class="line">    val streamContext = new StreamingContext(conf, Durations.seconds(5))</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">    * 上一次的计算结果会保存两份：</span><br><span class="line">      * 1.内存</span><br><span class="line">      * 2.我们设置的checkPoint目录下</span><br><span class="line">      * 因为内存不稳定，放在checkPoint目录下更安全。</span><br><span class="line">      * 多久会将内存中的数据（每一个key所对应的状态）写入到磁盘上一份呢？</span><br><span class="line">      * 	如果你的batch interval小于10s  那么10s会将内存中的数据写入到磁盘一份</span><br><span class="line">      * 	如果bacth interval 大于10s，那么就以bacth interval为准</span><br><span class="line">      */</span><br><span class="line">    streamContext.checkpoint(&quot;hdfs://node01:8020/sscheckpoint01&quot;)</span><br><span class="line"></span><br><span class="line">    val lines = streamContext.socketTextStream(&quot;node01&quot;,8888)</span><br><span class="line">    val wordsPair = lines.flatMap(_.split(&quot; &quot;)).map((_,1))</span><br><span class="line">//    val wordsPair = streamContext.socketTextStream(&quot;node01&quot;,8888).flatMap(_.split(&quot; &quot;)).map((_,1))</span><br><span class="line"></span><br><span class="line">    //注意这里的各种泛型</span><br><span class="line">    val counts = wordsPair.updateStateByKey[Int]((values:Seq[Int], state:Option[Int]) =&gt;&#123;</span><br><span class="line">      var updateValue = 0</span><br><span class="line">      if(!state.isEmpty) updateValue = state.get</span><br><span class="line"></span><br><span class="line">      values.foreach(x =&gt;&#123;updateValue += x&#125;)</span><br><span class="line">//      Option.apply[Int](updateValue)</span><br><span class="line">      Some(updateValue)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    counts.print()</span><br><span class="line"></span><br><span class="line">    streamContext.start()</span><br><span class="line">    streamContext.awaitTermination()</span><br><span class="line">    streamContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Window-Operations"><a href="#Window-Operations" class="headerlink" title="Window Operations"></a>Window Operations</h5><p><a href="http://chant00.com/media/15053179353512.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053179353512.jpg" alt="img"></a></p>
<p>总结：<br>batch interval：5s<br>每隔5s切割一次batch封装成DStream<br>window length：15s<br>进行计算的DStream中包含15s的数据,这里也就是3个batch。<br>sliding interval：10s<br>每隔10s取3个batch封装的DStream，封装成一个更大的DStream进行计算<br>window length和sliding interval必须是batch interval的整数倍<br>问题：<br>time3的RDD被计算两次</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.&#123;Durations, StreamingContext&#125;</span><br><span class="line"></span><br><span class="line">object WindowOperator &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setAppName(&quot;WindowOperator&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val streamingContext = new StreamingContext(conf, Durations.seconds(5))</span><br><span class="line">    //优化版必须设置cehckpoint目录，因为内存不稳定，保证数据不丢失。</span><br><span class="line">    streamingContext.checkpoint(&quot;hdfs://node01:8020/sscheckpoint02&quot;)</span><br><span class="line"></span><br><span class="line">    val logDStrem = streamingContext.socketTextStream(&quot;node01&quot;, 8888)</span><br><span class="line"></span><br><span class="line">    val wordsPair = logDStrem.flatMap(_.split(&quot; &quot;)).map((_, 1))</span><br><span class="line">    /** batch interval:5s</span><br><span class="line">      * sliding interval:10s</span><br><span class="line">      * window length：60s</span><br><span class="line">      * 所以每隔10s会取12个rdd,在计算的时候会将这12个rdd聚合起来</span><br><span class="line">      * 然后一起执行reduceByKeyAndWindow操作</span><br><span class="line">      * reduceByKeyAndWindow是针对窗口操作的而不是针对DStream操作的</span><br><span class="line">      */</span><br><span class="line">//    val wordCountDStrem = wordsPair.reduceByKeyAndWindow((a, b) =&gt; a + b, Durations.seconds(60))</span><br><span class="line">    //为什么这里一定要声明参数类型？？？什么时候需要声明，什么时候不用，什么时候必须不声明？</span><br><span class="line">//    val wordCountDStrem = wordsPair.reduceByKeyAndWindow((a: Int, b: Int) =&gt; a+b, Durations.seconds(60), Durations.seconds(10))</span><br><span class="line"></span><br><span class="line">    //优化版</span><br><span class="line">    val wordCountDStrem = wordsPair.reduceByKeyAndWindow((a, b) =&gt; a+b, (a, b) =&gt; a-b, Durations.minutes(1), Durations.seconds(10))</span><br><span class="line">    wordCountDStrem.print()</span><br><span class="line"></span><br><span class="line">    streamingContext.start()</span><br><span class="line">    streamingContext.awaitTermination()</span><br><span class="line">    streamingContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>优化：<br><a href="http://chant00.com/media/15053180739268.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053180739268.jpg" alt="img"></a></p>
<p>假设batch=1s，window length=5s，sliding interval=1s，那么每个DStream重复计算了5次，优化后，(t+4)时刻的Window由(t+3)时刻的Window和(t+4)时刻的DStream组成，由于(t+3)时刻的Window包含(t-1)时刻的DStream，而(t+4)时刻的Window中不需要包含(t-1)时刻的DStream，所以还需要减去(t-1)时刻的DStream，所以：<br><code>Window(t+4) = Window(t+3) + DStream(t+4) - DStream(t-1)</code></p>
<p>优化后的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//优化版必须要设置checkPoint目录</span><br><span class="line">    val wordCountDStrem = wordsPair.reduceByKeyAndWindow((a, b) =&gt; a+b, (a, b) =&gt; a-b, Durations.minutes(1), Durations.seconds(10))</span><br></pre></td></tr></table></figure>
<p><strong>NOTE:</strong> updateStateByKey和优化版的reduceByKeyAndWindow都必须要设置checkPoint目录。</p>
<h4 id="Driver-HA"><a href="#Driver-HA" class="headerlink" title="Driver HA"></a>Driver HA</h4><p>提交任务时设置<br><code>spark-submit –supervise</code><br>Spark standalone or Mesos with cluster deploy mode only:<br>–supervise If given, restarts the driver on failure.<br>以集群方式提交到yarn上时，Driver挂掉会自动重启，不需要任何设置<br>提交任务，在客户端启动Driver，那么不管是提交到standalone还是yarn，Driver挂掉后都无法重启<br>代码中配置<br>上面的方式重新启动的Driver需要重新读取application的信息然后进行任务调度，实际需求是，新启动的Driver可以直接恢复到上一个Driver的状态（可以直接读取上一个StreamingContext的DSstream操作逻辑和job执行进度，所以需要把上一个StreamingContext的元数据保存到HDFS上），直接进行任务调度，这就需要在代码层面进行配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.&#123;Durations, StreamingContext&#125;</span><br><span class="line">/**  Spark standalone or Mesos 1with cluster deploy mode only:</span><br><span class="line">  *  在提交application的时候  添加 --supervise 选项  如果Driver挂掉 会自动启动一个Driver</span><br><span class="line">  *  	SparkStreaming</span><br><span class="line">  */</span><br><span class="line">object SparkStreamingOnHDFS2 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val checkpointPath = &quot;hdfs://node01:8020/sscheckpoint03&quot;</span><br><span class="line"></span><br><span class="line">//    val ssc = new StreamingContext(conf, Durations.seconds(5))</span><br><span class="line"></span><br><span class="line">    val ssc = StreamingContext.getOrCreate(checkpointPath,() =&gt; &#123;</span><br><span class="line">      println(&quot;Creating new context&quot;)</span><br><span class="line">      //这里可以设置一个线程，因为不需要一个专门接收数据的线程，而是监控一个目录</span><br><span class="line">      val conf = new SparkConf().setAppName(&quot;SparkStreamingOnHDFS&quot;).setMaster(&quot;local[1]&quot;)</span><br><span class="line">      //每隔15s查看一下监控的目录中是否新增了文件</span><br><span class="line">      val ssc  = new StreamingContext(conf, Durations.seconds(15))</span><br><span class="line">      ssc.checkpoint(checkpointPath)</span><br><span class="line">      /** 只是监控文件夹下新增的文件，减少的文件是监控不到的  文件内容有改动也是监控不到 */</span><br><span class="line">      ssc</span><br><span class="line">    &#125;)</span><br><span class="line">    val wordCount = ssc.textFileStream(&quot;hdfs://node01:8020/hdfs/&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_)</span><br><span class="line">    wordCount.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    ssc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行一次程序后，JavaStreamingContext会在checkpointDirectory中保存，当修改了业务逻辑后，再次运行程序，JavaStreamingContext.getOrCreate(checkpointDirectory, factory);<br>因为checkpointDirectory中有这个application的JavaStreamingContext，所以不会调用JavaStreamingContextFactory来创建JavaStreamingContext，而是直接checkpointDirectory中的JavaStreamingContext，所以即使业务逻辑改变了，执行的效果也是之前的业务逻辑，如果需要执行修改过的业务逻辑，可以修改或删除checkpointDirectory。</p>
<h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>简介<br>kafka是一个高吞吐的分部式消息系统<br><a href="http://chant00.com/media/15053191033367.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053191033367.jpg" alt="img"></a><br>使用kafka和SparkStreaming组合的好处：</p>
<ol>
<li>解耦，SparkStreaming不用关心数据源是什么，只需要消费数据即可</li>
<li>缓冲，数据提交给kafka消息队列，SparkStreaming按固定时间和顺序处理数据，减轻数据量过大造成的负载</li>
<li>异步通信，kafka把请求队列提交给服务端，服务端可以把响应消息也提交给kafka的消息队列中，互不影响</li>
</ol>
<h4 id="消息系统特点"><a href="#消息系统特点" class="headerlink" title="消息系统特点"></a>消息系统特点</h4><ol>
<li><p>生产者消费者模式</p>
</li>
<li><p>可靠性</p>
<p>自己不丢数据</p>
<p>当消费者消费一条数据后，这条数据还会保存在kafka中，在一周（默认）后再删除</p>
<p>消费者不丢数据</p>
<p>消费者消费数据的策略，“至少一次”，即消费者至少处理这条数据一次，“严格一次”，即消费者必须且只能消费这条数据一次</p>
</li>
</ol>
<h4 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h4><p><a href="http://chant00.com/media/15053668820845.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053668820845.jpg" alt="img"></a></p>
<p>图里还少了个zookeeper，用于存储元数据和消费偏移量（根据偏移量来保证消费至少一次和严格消费一次）<br>producer：消息生产者<br>consumer：消息消费者<br>broker：kafka集群的每一台节点叫做broker，负责处理消息读、写请求，存储消息<br>topic：消息队列/分类<br>kafka里面的消息是有topic来组织的，简单的我们可以想象为一个队列，一个队列就是一个topic，然后它把每个topic又分为很多个partition，这个是为了做并行的，在每个partition里面是有序的，相当于有序的队列，其中每个消息都有个序号，比如0到12，从前面读往后面写。一个partition对应一个broker，一个broker可以管多个partition，比如说，topic有6个partition，有两个broker，那每个broker就管理3个partition。这个partition可以很简单想象为一个文件，当数据发过来的时候它就往这个partition上面append，追加就行，kafka和很多消息系统不一样，很多消息系统是消费完了我就把它删掉，而kafka是根据时间策略删除，而不是消费完就删除，在kafka里面没有消费完这个概念，只有过期这个概念</p>
<p>一个topic分成多个partition，每个partition内部消息强有序，其中的每个消息都有一个序号叫offset，一个partition只对应一个broker，一个broker可以管多个partition，消息直接写入文件，并不是存储在内存中，根据时间策略（默认一周）删除，而不是消费完就删除，producer自己决定往哪个partition写消息，可以是轮询的负载均衡，或者是基于hash的partition策略，而这样容易造成数据倾斜。所以<strong>建议使用轮询的负载均衡</strong>。<br><a href="http://chant00.com/media/15053673731094.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053673731094.jpg" alt="img"></a></p>
<p>consumer自己维护消费到哪个offset，每个consumer都有对应的group，group内部是queue消费模型，各个consumer消费不同的partition，一个消息在group内只消费一次，各个group各自独立消费，互不影响<br><strong>partition内部是FIFO的，partition之间不是FIFO的</strong>，当然我们可以把topic设为一个partition，这样就是严格的FIFO（First Input First Output，先入先出队列）</p>
<h4 id="kafka特点"><a href="#kafka特点" class="headerlink" title="kafka特点"></a>kafka特点</h4><ul>
<li><p>高性能：单节点支持上千个客户端，百MB/s吞吐</p>
</li>
<li><p>持久性：消息直接持久化在普通磁盘上，性能好，直接写到磁盘里面去，就是直接append到磁盘里面去，这样的好处是直接持久话，数据不会丢，第二个好处是顺序写，然后消费数据也是顺序的读，所以持久化的同时还能保证顺序读写</p>
</li>
<li><p>分布式：数据副本冗余、流量负载均衡、可扩展</p>
<p>分布式，数据副本，也就是同一份数据可以到不同的broker上面去，也就是当一份数据，磁盘坏掉的时候，数据不会丢失，比如3个副本，就是在3个机器磁盘都坏掉的情况下数据才会丢。</p>
</li>
<li><p>灵活性：消息长时间持久化+Client维护消费状态</p>
<p>消费方式非常灵活，第一原因是消息持久化时间跨度比较长，一天或者一星期等，第二消费状态自己维护消费到哪个地方了，可以自定义消费偏移量</p>
</li>
</ul>
<h4 id="kafka与其他消息队列对比"><a href="#kafka与其他消息队列对比" class="headerlink" title="kafka与其他消息队列对比"></a>kafka与其他消息队列对比</h4><ul>
<li><p>RabbitMQ:分布式，支持多种MQ协议，重量级</p>
</li>
<li><p>ActiveMQ：与RabbitMQ类似</p>
</li>
<li><p>ZeroMQ：以库的形式提供，使用复杂，无持久化</p>
</li>
<li><p>redis：单机、纯内存性好，持久化较差</p>
<p>本身是一个内存的KV系统，但是它也有队列的一些数据结构，能够实现一些消息队列的功能，当然它在单机纯内存的情况下，性能会比较好，持久化做的稍差，当持久化的时候性能下降的会比较厉害</p>
</li>
<li><p>kafka：分布式，较长时间持久化，高性能，轻量灵活</p>
<p>天生是分布式的，不需要你在上层做分布式的工作，另外有较长时间持久化，在长时间持久化下性能还比较高，顺序读和顺序写，还通过sendFile这样0拷贝的技术直接从文件拷贝到网络，减少内存的拷贝，还有批量读批量写来提高网络读取文件的性能</p>
</li>
</ul>
<h4 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h4><p><a href="http://chant00.com/media/15053704805816.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053704805816.jpg" alt="img"></a><br>“零拷贝”是指计算机操作的过程中，CPU不需要为数据在内存之间的拷贝消耗资源。而它通常是指计算机在网络上发送文件时，不需要将文件内容拷贝到用户空间（User Space）而直接在内核空间（Kernel Space）中传输到网络的方式。<br><a href="http://chant00.com/media/15053705012302.jpg" target="_blank" rel="noopener"><img src="http://chant00.com/media/15053705012302.jpg" alt="img"></a></p>
<h4 id="Kafka集群搭建"><a href="#Kafka集群搭建" class="headerlink" title="Kafka集群搭建"></a>Kafka集群搭建</h4><p>node01,node02,node03</p>
<h5 id="1-解压"><a href="#1-解压" class="headerlink" title="1. 解压"></a>1. 解压</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 chant]# tar zxvf kafka_2.10-0.8.2.2.tgz</span><br></pre></td></tr></table></figure>
<h5 id="2-修改server-properties配置文件"><a href="#2-修改server-properties配置文件" class="headerlink" title="2. 修改server.properties配置文件"></a>2. 修改server.properties配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 chant]# cd kafka_2.10-0.8.2.2/config</span><br><span class="line">[root@node01 config]# vi server.properties</span><br><span class="line">broker.id=0  #node01为0，node02为1，node03为2</span><br><span class="line">log.dirs=/var/kafka/logs #真实数据存储路径</span><br><span class="line">auto.leader.rebalance.enable=true  #leader均衡机制开启</span><br><span class="line">zookeeper.connect=node02:2181,node03:2181,node04:2181 #zookeeper集群</span><br></pre></td></tr></table></figure>
<h5 id="3-同步配置，记得修改每台机器的broker-id"><a href="#3-同步配置，记得修改每台机器的broker-id" class="headerlink" title="3. 同步配置，记得修改每台机器的broker.id"></a>3. 同步配置，记得修改每台机器的broker.id</h5><h5 id="4-启动zookeeper集群"><a href="#4-启动zookeeper集群" class="headerlink" title="4. 启动zookeeper集群"></a>4. 启动zookeeper集群</h5><h5 id="5-在每台kafka节点上启动kafka集群"><a href="#5-在每台kafka节点上启动kafka集群" class="headerlink" title="5. 在每台kafka节点上启动kafka集群"></a>5. 在每台kafka节点上启动kafka集群</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup /opt/chant/kafka_2.10-0.8.2.2/bin/kafka-server-start.sh /opt/chant/kafka_2.10-0.8.2.2/config/server.properties &amp;</span><br></pre></td></tr></table></figure>
<h5 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h5><p>在node01上<br>创建topic：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/opt/chant/kafka_2.10-0.8.2.2/bin/kafka-topics.sh </span><br><span class="line">--create </span><br><span class="line">--zookeeper node02:2181,node03:2181,node04:2181 </span><br><span class="line">--replication-factor 3 </span><br><span class="line">--partitions 3 </span><br><span class="line">--topic test_create_topic</span><br></pre></td></tr></table></figure>
<p>生产数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/chant/kafka_2.10-0.8.2.2/bin/kafka-console-producer.sh </span><br><span class="line">--broker-list node01:9092,node02:9092,node03:9092 </span><br><span class="line">--topic test_create_topic</span><br></pre></td></tr></table></figure>
<p>在node02上启动消费者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/opt/chant/kafka_2.10-0.8.2.2/bin/kafka-console-consumer.sh </span><br><span class="line">--zookeeper node02:2181,node03:2181,node04:2181 </span><br><span class="line">--from-beginning </span><br><span class="line">--topic test_create_topic</span><br></pre></td></tr></table></figure>
<p>在node01输入消息，在node02会接收并打印</p>
<p>查看在集群中有哪些topic：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/chant/kafka_2.10-0.8.2.2/bin/kafka-topics.sh </span><br><span class="line">--list </span><br><span class="line">--zookeeper node02:2181,node03:2181,node04:2181</span><br></pre></td></tr></table></figure>
<p>结果：test_create_topic</p>
<p>查看某个topic信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/opt/chant/kafka_2.10-0.8.2.2/bin/kafka-topics.sh </span><br><span class="line">--describe </span><br><span class="line">--zookeeper node02:2181,node03:2181,node04:2181 --topic test_create_topic</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">Topic:test_create_topic	PartitionCount:3	ReplicationFactor:3	Configs:</span><br><span class="line">	Topic: test_create_topic	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2</span><br><span class="line">	Topic: test_create_topic	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0</span><br><span class="line">	Topic: test_create_topic	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1</span><br></pre></td></tr></table></figure>
<p>解释一下leader均衡机制(auto.leader.rebalance.enable=true)：<br>每个partition是有主备结构的，当partition 1的leader，就是broker.id = 1的节点挂掉后，那么leader 0 或leader 2成为partition 1 的leader，那么leader 0 或leader 2 会管理两个partition的读写，性能会下降，当leader 1 重新启动后，如果开启了leader均衡机制，那么leader 1会重新成为partition 1 的leader，降低leader 0 或leader 2 的负载</p>
<h4 id="Kafka和SparkStreaming整合"><a href="#Kafka和SparkStreaming整合" class="headerlink" title="Kafka和SparkStreaming整合"></a>Kafka和SparkStreaming整合</h4><h5 id="Receiver方式"><a href="#Receiver方式" class="headerlink" title="Receiver方式"></a>Receiver方式</h5><h6 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h6><p><a href="http://chant00.com/media/SparkStreaming和Kafka整合原理.png" target="_blank" rel="noopener"><img src="http://chant00.com/media/SparkStreaming%E5%92%8CKafka%E6%95%B4%E5%90%88%E5%8E%9F%E7%90%86.png" alt="SparkStreaming和Kafka整合原理"></a></p>
<h6 id="获取kafka传递的数据来计算："><a href="#获取kafka传递的数据来计算：" class="headerlink" title="获取kafka传递的数据来计算："></a>获取kafka传递的数据来计算：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">    .setAppName(&quot;SparkStreamingOnKafkaReceiver&quot;)</span><br><span class="line">    .setMaster(&quot;local[2]&quot;)</span><br><span class="line">    .set(&quot;spark.streaming.receiver.writeAheadLog.enable&quot;,&quot;true&quot;);      </span><br><span class="line">JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(5));</span><br><span class="line">//设置持久化数据的目录</span><br><span class="line">jsc.checkpoint(&quot;hdfs://node01:8020/spark/checkpoint&quot;);</span><br><span class="line">Map&lt;String, Integer&gt; topicConsumerConcurrency = new HashMap&lt;String, Integer&gt;();</span><br><span class="line">//topic名    receiver task数量</span><br><span class="line">topicConsumerConcurrency.put(&quot;test_create_topic&quot;, 1);</span><br><span class="line">JavaPairReceiverInputDStream&lt;String,String&gt; lines = KafkaUtils.createStream(</span><br><span class="line">    jsc,</span><br><span class="line">    &quot;node02:2181,node03:2181,node04:2181&quot;,</span><br><span class="line">    &quot;MyFirstConsumerGroup&quot;,</span><br><span class="line">    topicConsumerConcurrency,</span><br><span class="line">    StorageLevel.MEMORY_AND_DISK_SER());</span><br><span class="line">/*</span><br><span class="line"> * 第一个参数是StreamingContext</span><br><span class="line"> * 第二个参数是ZooKeeper集群信息（接受Kafka数据的时候会从Zookeeper中获得Offset等元数据信息）</span><br><span class="line"> * 第三个参数是Consumer Group</span><br><span class="line"> * 第四个参数是消费的Topic以及并发读取Topic中Partition的线程数</span><br><span class="line"> * 第五个参数是持久化数据的级别，可以自定义</span><br><span class="line"> */</span><br><span class="line">//对lines进行其他操作……</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong></p>
<ol>
<li>需要spark-examples-1.6.0-hadoop2.6.0.jar</li>
<li>kafka_2.10-0.8.2.2.jar和kafka-clients-0.8.2.2.jar版本号要一致</li>
</ol>
<h6 id="kafka客户端生产数据的代码："><a href="#kafka客户端生产数据的代码：" class="headerlink" title="kafka客户端生产数据的代码："></a>kafka客户端生产数据的代码：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">public class SparkStreamingDataManuallyProducerForKafka extends Thread &#123;</span><br><span class="line">    private String topic; //发送给Kafka的数据的类别</span><br><span class="line">    private Producer&lt;Integer, String&gt; producerForKafka;</span><br><span class="line">    public SparkStreamingDataManuallyProducerForKafka(String topic)&#123;</span><br><span class="line">        this.topic = topic;</span><br><span class="line">        Properties conf = new Properties();</span><br><span class="line">        conf.put(&quot;metadata.broker.list&quot;,</span><br><span class="line">&quot;node01:9092,node02:9092,node03:9092&quot;);</span><br><span class="line">        conf.put(&quot;serializer.class&quot;,  StringEncoder.class.getName());</span><br><span class="line">        producerForKafka = new Producer&lt;Integer, String&gt;(</span><br><span class="line">            new ProducerConfig(conf)) ;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">        while(true)&#123;</span><br><span class="line">             counter ++;</span><br><span class="line">             String userLog = createUserLog();</span><br><span class="line">//生产数据这个方法可以根据实际需求自己编写</span><br><span class="line">             producerForKafka.send(new KeyedMessage&lt;Integer, String&gt;(topic, userLog));</span><br><span class="line">             try &#123;</span><br><span class="line">                 Thread.sleep(1000);</span><br><span class="line">             &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                 e.printStackTrace();</span><br><span class="line">             &#125;</span><br><span class="line">        &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">new SparkStreamingDataManuallyProducerForKafka(</span><br><span class="line">&quot;test_create_topic&quot;).start();</span><br><span class="line">//test_create_topic是topic名 </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Direct方式"><a href="#Direct方式" class="headerlink" title="Direct方式"></a>Direct方式</h5><p>把kafka当作一个存储系统，直接从kafka中读数据，SparkStreaming自己维护消费者的消费偏移量,不再将其存储到zookeeper。<br>与Receiver方式相比，他有的优点是：</p>
<ol>
<li><p>one-to-one,直接读取卡夫卡中数据，Partion数与kafka一致。</p>
</li>
<li><p>Efficiency,不开启WAL也不会丢失数据，因为事实上kafka的的数据保留时间只要设定合适，可以直接从kafka恢复。</p>
</li>
<li><p>Exactly-once semantics,其实这只保证kafka与spark是一致的，之后写数据的时候还需要自己注意才能保证整个事务的一致性。而在receiver模式下，offset交由kafka管理，而kafaka实际交由zookeeper管理，可能出现数据不一致。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = new SparkConf()</span><br><span class="line">         .setAppName(&quot;SparkStreamingOnKafkaDirected&quot;)</span><br><span class="line">         .setMaster(&quot;local[1]&quot;);</span><br><span class="line">         </span><br><span class="line">JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(10));</span><br><span class="line">         </span><br><span class="line">Map&lt;String, String&gt; kafkaParameters = new HashMap&lt;String, String&gt;();</span><br><span class="line">kafkaParameters.put(&quot;metadata.broker.list&quot;, &quot;node01:9092,node02:9092,node03:9092&quot;);</span><br><span class="line">         </span><br><span class="line">HashSet&lt;String&gt; topics = new HashSet&lt;String&gt;();</span><br><span class="line">topics.add(&quot;test_create_topic&quot;);</span><br><span class="line">JavaPairInputDStream&lt;String,String&gt; lines = KafkaUtils.createDirectStream(jsc,</span><br><span class="line">        String.class,</span><br><span class="line">        String.class,</span><br><span class="line">        StringDecoder.class,</span><br><span class="line">        StringDecoder.class,</span><br><span class="line">        kafkaParameters,</span><br><span class="line">        topics);</span><br><span class="line">//对lines进行其他操作……</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h5 id="两种方式下提高SparkStreaming并行度的方法"><a href="#两种方式下提高SparkStreaming并行度的方法" class="headerlink" title="两种方式下提高SparkStreaming并行度的方法"></a>两种方式下提高SparkStreaming并行度的方法</h5><h6 id="Receiver方式调整SparkStreaming的并行度的方法："><a href="#Receiver方式调整SparkStreaming的并行度的方法：" class="headerlink" title="Receiver方式调整SparkStreaming的并行度的方法："></a>Receiver方式调整SparkStreaming的并行度的方法：</h6><ul>
<li><p><code>spark.streaming.blockInterval</code></p>
<p>假设batch interval为5s，Receiver Task会每隔200ms(spark.streaming.blockInterval默认)将接收来的数据封装到一个block中，那么每个batch中包括25个block，batch会被封装到RDD中，所以RDD中会包含25个partition，所以提高接收数据时的并行度的方法是：调低<code>spark.streaming.blockInterval</code>的值，建议不低于50ms<br>其他配置：</p>
</li>
<li><p><code>spark.streaming.backpressure.enabled</code></p>
<p>默认false，设置为true后，sparkstreaming会根据上一个batch的接收数据的情况来动态的调整本次接收数据的速度，但是最大速度不能超过<code>spark.streaming.receiver.maxRate</code>设置的值（设置为n，那么速率不能超过n/s）</p>
</li>
<li><p><code>spark.streaming.receiver.writeAheadLog.enable</code> 默认false 是否开启WAL机制</p>
</li>
</ul>
<h6 id="Direct方式并行度的设置："><a href="#Direct方式并行度的设置：" class="headerlink" title="Direct方式并行度的设置："></a>Direct方式并行度的设置：</h6><p>第一个DStream的分区数是由读取的topic的分区数决定的，可以通过增加topic的partition数来提高SparkStreaming的并行度</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol>
<li><a href="http://www.cnblogs.com/arachis/p/Spark_Shuffle.html" target="_blank" rel="noopener">Spark Shuffle原理、Shuffle操作问题解决和参数调优</a></li>
<li><a href="https://tech.meituan.com/spark-tuning-pro.html" target="_blank" rel="noopener">Spark性能优化指南——高级篇</a></li>
<li><a href="https://tech.meituan.com/spark-tuning-basic.html" target="_blank" rel="noopener">Spark性能优化指南——基础篇</a></li>
<li><a href="https://issues.apache.org/jira/secure/attachment/12765646/unified-memory-management-spark-10000.pdf" target="_blank" rel="noopener">Unified Memory Management in Spark 1.6</a></li>
<li><a href="http://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning" target="_blank" rel="noopener">Garbage Collection Tuning</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html?ca=drs-&amp;utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">spark2.1内存管理机制</a></li>
<li><a href="http://www.jianshu.com/p/58288b862030" target="_blank" rel="noopener">Spark内存管理详解（下）——内存管理</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/09/Flink源码调试/" rel="next" title="Flink源码调试（一）">
                <i class="fa fa-chevron-left"></i> Flink源码调试（一）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/03/09/kudu_test/" rel="prev" title="Kudu测试报告">
                Kudu测试报告 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80Mzk4NC8yMDUyMA=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Fly Hugh">
            
              <p class="site-author-name" itemprop="name">Fly Hugh</p>
              <p class="site-description motion-element" itemprop="description">TRUST THE PROCESS</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">37</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/FlyMeToTheMars" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yourname@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/3200892914" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/Fly__HoBo" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark简介"><span class="nav-number">1.</span> <span class="nav-text">Spark简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark比MapReduce快的原因"><span class="nav-number">1.1.</span> <span class="nav-text">Spark比MapReduce快的原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD（Resilient-Distributed-Dataset-弹性分布式数据集"><span class="nav-number">1.2.</span> <span class="nav-text">RDD（Resilient Distributed Dataset )-弹性分布式数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark任务执行流程"><span class="nav-number">1.3.</span> <span class="nav-text">Spark任务执行流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#写一个Spark应用程序的流程"><span class="nav-number">1.4.</span> <span class="nav-text">写一个Spark应用程序的流程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-加载数据集（获得RDD）"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.加载数据集（获得RDD）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-使用transformations算子对RDD进行操作"><span class="nav-number">1.4.2.</span> <span class="nav-text">2.使用transformations算子对RDD进行操作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-使用actions算子触发执行"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.使用actions算子触发执行</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算子"><span class="nav-number">2.</span> <span class="nav-text">算子</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Actions"><span class="nav-number">2.1.</span> <span class="nav-text">Actions</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#count：统计RDD中元素的个数"><span class="nav-number">2.1.1.</span> <span class="nav-text">count：统计RDD中元素的个数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#foreach：遍历RDD中的元素"><span class="nav-number">2.1.2.</span> <span class="nav-text">foreach：遍历RDD中的元素</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#foreachPartition"><span class="nav-number">2.1.3.</span> <span class="nav-text">foreachPartition</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#collect：把运行结果拉回到Driver端"><span class="nav-number">2.1.4.</span> <span class="nav-text">collect：把运行结果拉回到Driver端</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#take-n-：取RDD中的前n个元素"><span class="nav-number">2.1.5.</span> <span class="nav-text">take(n)：取RDD中的前n个元素</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#first-：相当于take-1"><span class="nav-number">2.1.6.</span> <span class="nav-text">first ：相当于take(1)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#reduce：按照指定规则聚合RDD中的元素"><span class="nav-number">2.1.7.</span> <span class="nav-text">reduce：按照指定规则聚合RDD中的元素</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#countByKey：统计出KV格式的RDD中相同的K的个数"><span class="nav-number">2.1.8.</span> <span class="nav-text">countByKey：统计出KV格式的RDD中相同的K的个数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#countByValue：统计出RDD中每个元素的个数"><span class="nav-number">2.1.9.</span> <span class="nav-text">countByValue：统计出RDD中每个元素的个数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformations"><span class="nav-number">2.2.</span> <span class="nav-text">Transformations</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#filter：过滤"><span class="nav-number">2.2.1.</span> <span class="nav-text">filter：过滤</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#map-和flatMap"><span class="nav-number">2.2.2.</span> <span class="nav-text">map 和flatMap</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sample-：随机抽样"><span class="nav-number">2.2.3.</span> <span class="nav-text">sample ：随机抽样</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#groupByKey和reduceByKey"><span class="nav-number">2.2.4.</span> <span class="nav-text">groupByKey和reduceByKey</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sortByKey：按key进行排序"><span class="nav-number">2.2.5.</span> <span class="nav-text">sortByKey：按key进行排序</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sortBy：自定义排序规则"><span class="nav-number">2.2.6.</span> <span class="nav-text">sortBy：自定义排序规则</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#distinct：去掉重复数据"><span class="nav-number">2.2.7.</span> <span class="nav-text">distinct：去掉重复数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#join"><span class="nav-number">2.2.8.</span> <span class="nav-text">join</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-Inner-join"><span class="nav-number">2.2.8.1.</span> <span class="nav-text">1.Inner join</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-Left-outer-join"><span class="nav-number">2.2.8.2.</span> <span class="nav-text">2.Left outer join</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-Right-outer-join"><span class="nav-number">2.2.8.3.</span> <span class="nav-text">3.Right outer join</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4-Full-outer-join（MySQL不支持）"><span class="nav-number">2.2.8.4.</span> <span class="nav-text">4.Full outer join（MySQL不支持）</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#union：把两个RDD进行逻辑上的合并"><span class="nav-number">2.2.9.</span> <span class="nav-text">union：把两个RDD进行逻辑上的合并</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#map和mapPartitions"><span class="nav-number">2.2.10.</span> <span class="nav-text">map和mapPartitions</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mapPartitionsWithIndex"><span class="nav-number">2.2.11.</span> <span class="nav-text">mapPartitionsWithIndex</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#coalesce：改变RDD的分区数"><span class="nav-number">2.2.12.</span> <span class="nav-text">coalesce：改变RDD的分区数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#repartition：改变RDD分区数"><span class="nav-number">2.2.13.</span> <span class="nav-text">repartition：改变RDD分区数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#partitionBy：通过自定义分区器改变RDD分区数"><span class="nav-number">2.2.14.</span> <span class="nav-text">partitionBy：通过自定义分区器改变RDD分区数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#randomSplit：拆分RDD"><span class="nav-number">2.2.15.</span> <span class="nav-text">randomSplit：拆分RDD</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zip"><span class="nav-number">2.2.16.</span> <span class="nav-text">zip</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算子案例"><span class="nav-number">3.</span> <span class="nav-text">算子案例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#WordCount-Java版"><span class="nav-number">3.1.</span> <span class="nav-text">WordCount-Java版</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#过滤掉出现次数最多的数据-Scala版"><span class="nav-number">3.2.</span> <span class="nav-text">过滤掉出现次数最多的数据-Scala版</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#统计每个页面的UV"><span class="nav-number">3.3.</span> <span class="nav-text">统计每个页面的UV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二次排序-scala版"><span class="nav-number">3.4.</span> <span class="nav-text">二次排序-scala版</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分组取TopN问题：找出每个班级中排名前三的分数-Java版"><span class="nav-number">3.5.</span> <span class="nav-text">分组取TopN问题：找出每个班级中排名前三的分数-Java版</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#广播变量"><span class="nav-number">4.</span> <span class="nav-text">广播变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#累加器"><span class="nav-number">5.</span> <span class="nav-text">累加器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#累加器-Accumulator-陷阱及解决办法"><span class="nav-number">5.1.</span> <span class="nav-text">累加器(Accumulator)陷阱及解决办法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#计数器的测试代码"><span class="nav-number">5.1.1.</span> <span class="nav-text">计数器的测试代码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#累加器的错误用法"><span class="nav-number">5.1.2.</span> <span class="nav-text">累加器的错误用法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#原因分析"><span class="nav-number">5.1.3.</span> <span class="nav-text">原因分析</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解决办法"><span class="nav-number">5.1.4.</span> <span class="nav-text">解决办法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#总结"><span class="nav-number">5.1.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cache：把RDD持久化到内存"><span class="nav-number">5.2.</span> <span class="nav-text">cache：把RDD持久化到内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#persist：可以选择多种持久化方式"><span class="nav-number">5.3.</span> <span class="nav-text">persist：可以选择多种持久化方式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#checkpoint-可以把RDD持久化到HDFS，同时切断RDD之间的依赖"><span class="nav-number">5.3.1.</span> <span class="nav-text">checkpoint : 可以把RDD持久化到HDFS，同时切断RDD之间的依赖</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark任务调度和资源调度"><span class="nav-number">6.</span> <span class="nav-text">Spark任务调度和资源调度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#一些术语"><span class="nav-number">6.1.</span> <span class="nav-text">一些术语</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#宽窄依赖"><span class="nav-number">6.2.</span> <span class="nav-text">宽窄依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#宽窄依赖的作用"><span class="nav-number">6.3.</span> <span class="nav-text">宽窄依赖的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD中为什么不存储数据以及stage的计算模式"><span class="nav-number">6.4.</span> <span class="nav-text">RDD中为什么不存储数据以及stage的计算模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#任务调度"><span class="nav-number">6.5.</span> <span class="nav-text">任务调度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#资源调度"><span class="nav-number">6.6.</span> <span class="nav-text">资源调度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#粗粒度和细粒度的资源申请"><span class="nav-number">6.7.</span> <span class="nav-text">粗粒度和细粒度的资源申请</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#粗粒度的资源申请：Spark"><span class="nav-number">6.7.1.</span> <span class="nav-text">粗粒度的资源申请：Spark</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#细粒度的资源申请：MapReduce"><span class="nav-number">6.7.2.</span> <span class="nav-text">细粒度的资源申请：MapReduce</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yarn如何同时调度粗细两种方式"><span class="nav-number">6.7.3.</span> <span class="nav-text">yarn如何同时调度粗细两种方式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#资源调度源码分析"><span class="nav-number">7.</span> <span class="nav-text">资源调度源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#分析以集群方式提交命令后的资源调度源码"><span class="nav-number">7.1.</span> <span class="nav-text">分析以集群方式提交命令后的资源调度源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#资源调度的三个结论"><span class="nav-number">7.2.</span> <span class="nav-text">资源调度的三个结论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#验证资源调度的三个结论"><span class="nav-number">7.3.</span> <span class="nav-text">验证资源调度的三个结论</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#任务调度源码分析"><span class="nav-number">8.</span> <span class="nav-text">任务调度源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Standalone集群搭建"><span class="nav-number">9.</span> <span class="nav-text">Spark Standalone集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#角色划分"><span class="nav-number">9.1.</span> <span class="nav-text">角色划分</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-解压安装包"><span class="nav-number">9.1.1.</span> <span class="nav-text">1.解压安装包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-编辑spark-env-sh文件"><span class="nav-number">9.1.2.</span> <span class="nav-text">2.编辑spark-env.sh文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-编辑slaves文件"><span class="nav-number">9.1.3.</span> <span class="nav-text">3.编辑slaves文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-Spark的web端口默认为8080，与Tomcat冲突，进行修改"><span class="nav-number">9.1.4.</span> <span class="nav-text">4.Spark的web端口默认为8080，与Tomcat冲突，进行修改</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-同步配置"><span class="nav-number">9.1.5.</span> <span class="nav-text">5.同步配置</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-进入spark安装目录的sbin目录下，启动集群"><span class="nav-number">9.1.6.</span> <span class="nav-text">6.进入spark安装目录的sbin目录下，启动集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-访问web界面"><span class="nav-number">9.1.7.</span> <span class="nav-text">7.访问web界面</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-提交Application验证集群是否工作正常"><span class="nav-number">9.1.8.</span> <span class="nav-text">8.提交Application验证集群是否工作正常</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Standalone模式下提交任务"><span class="nav-number">9.2.</span> <span class="nav-text">Standalone模式下提交任务</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#客户端方式提交任务"><span class="nav-number">9.2.1.</span> <span class="nav-text">客户端方式提交任务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#集群方式提交任务"><span class="nav-number">9.2.2.</span> <span class="nav-text">集群方式提交任务</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Yarn模式下提交任务"><span class="nav-number">9.3.</span> <span class="nav-text">Yarn模式下提交任务</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#客户端方式提交任务-1"><span class="nav-number">9.3.1.</span> <span class="nav-text">客户端方式提交任务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#集群方式提交任务-1"><span class="nav-number">9.3.2.</span> <span class="nav-text">集群方式提交任务</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-HA集群搭建"><span class="nav-number">10.</span> <span class="nav-text">Spark HA集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark高可用的原理"><span class="nav-number">10.1.</span> <span class="nav-text">Spark高可用的原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#角色划分-1"><span class="nav-number">10.2.</span> <span class="nav-text">角色划分</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-修改spark-env-sh配置文件"><span class="nav-number">10.2.1.</span> <span class="nav-text">1.修改spark-env.sh配置文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-同步配置文件"><span class="nav-number">10.2.2.</span> <span class="nav-text">2. 同步配置文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-修改node02的spark配置文件"><span class="nav-number">10.2.3.</span> <span class="nav-text">3. 修改node02的spark配置文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-启动Zookeeper集群"><span class="nav-number">10.2.4.</span> <span class="nav-text">4. 启动Zookeeper集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-在node01上启动Spark集群"><span class="nav-number">10.2.5.</span> <span class="nav-text">5.在node01上启动Spark集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-在node02上启动Master进程"><span class="nav-number">10.2.6.</span> <span class="nav-text">6.在node02上启动Master进程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-验证集群高可用"><span class="nav-number">10.2.7.</span> <span class="nav-text">7.验证集群高可用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-History-Server配置"><span class="nav-number">11.</span> <span class="nav-text">Spark History Server配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Shuffle"><span class="nav-number">12.</span> <span class="nav-text">Spark Shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#普通的HashShuffle"><span class="nav-number">12.1.</span> <span class="nav-text">普通的HashShuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#内存估算"><span class="nav-number">12.1.1.</span> <span class="nav-text">内存估算</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化的HashShuffle"><span class="nav-number">12.2.</span> <span class="nav-text">优化的HashShuffle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#普通的SortShuffle"><span class="nav-number">12.3.</span> <span class="nav-text">普通的SortShuffle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SortShuffle的bypass机制"><span class="nav-number">12.4.</span> <span class="nav-text">SortShuffle的bypass机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SparkShuffle详解"><span class="nav-number">12.5.</span> <span class="nav-text">SparkShuffle详解</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#MapOutputTracker：管理磁盘小文件的地址"><span class="nav-number">12.5.1.</span> <span class="nav-text">MapOutputTracker：管理磁盘小文件的地址</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#BlockManager："><span class="nav-number">12.5.2.</span> <span class="nav-text">BlockManager：</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#主：BlockManagerMaster，存在于Driver端"><span class="nav-number">12.5.2.1.</span> <span class="nav-text">主：BlockManagerMaster，存在于Driver端</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#从：BlockManagerSlave，存在于Executor端"><span class="nav-number">12.5.2.2.</span> <span class="nav-text">从：BlockManagerSlave，存在于Executor端</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shuffle调优"><span class="nav-number">12.6.</span> <span class="nav-text">Shuffle调优</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#配置参数的三种方式"><span class="nav-number">12.6.1.</span> <span class="nav-text">配置参数的三种方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-shuffle-file-buffer"><span class="nav-number">12.6.2.</span> <span class="nav-text">spark.shuffle.file.buffer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-reducer-maxSizeInFlight"><span class="nav-number">12.6.3.</span> <span class="nav-text">spark.reducer.maxSizeInFlight</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-shuffle-io-maxRetries"><span class="nav-number">12.6.4.</span> <span class="nav-text">spark.shuffle.io.maxRetries</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-shuffle-io-retryWait"><span class="nav-number">12.6.5.</span> <span class="nav-text">spark.shuffle.io.retryWait</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-shuffle-memoryFraction"><span class="nav-number">12.6.6.</span> <span class="nav-text">spark.shuffle.memoryFraction</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-shuffle-manager"><span class="nav-number">12.6.7.</span> <span class="nav-text">spark.shuffle.manager</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-shuffle-sort-bypassMergeThreshold"><span class="nav-number">12.6.8.</span> <span class="nav-text">spark.shuffle.sort.bypassMergeThreshold</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spark-shuffle-consolidateFiles"><span class="nav-number">12.6.9.</span> <span class="nav-text">spark.shuffle.consolidateFiles</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark内存管理"><span class="nav-number">13.</span> <span class="nav-text">Spark内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#静态内存管理"><span class="nav-number">13.1.</span> <span class="nav-text">静态内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Unrolling"><span class="nav-number">13.1.0.1.</span> <span class="nav-text">Unrolling</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Reduce-OOM怎么办？"><span class="nav-number">13.1.0.2.</span> <span class="nav-text">Reduce OOM怎么办？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#统一内存管理"><span class="nav-number">13.2.</span> <span class="nav-text">统一内存管理</span></a></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">14.</span> <span class="nav-text">Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简介"><span class="nav-number">14.1.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Spark和Hive有两种组合"><span class="nav-number">14.1.1.</span> <span class="nav-text">Spark和Hive有两种组合</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Spark-on-Hive"><span class="nav-number">14.1.1.1.</span> <span class="nav-text">Spark on Hive</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Hive-on-Spark"><span class="nav-number">14.1.1.2.</span> <span class="nav-text">Hive on Spark</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dataframe"><span class="nav-number">14.2.</span> <span class="nav-text">Dataframe</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#简介-1"><span class="nav-number">14.2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-VS-DataFrame"><span class="nav-number">14.2.2.</span> <span class="nav-text">RDD VS DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DataFrame底层架构"><span class="nav-number">14.2.3.</span> <span class="nav-text">DataFrame底层架构</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Predicate-Pushdown谓词下推机制"><span class="nav-number">14.3.</span> <span class="nav-text">Predicate Pushdown谓词下推机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame创建方式"><span class="nav-number">14.4.</span> <span class="nav-text">DataFrame创建方式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-读JSON文件-不能嵌套"><span class="nav-number">14.4.1.</span> <span class="nav-text">1.读JSON文件(不能嵌套)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-JSON格式的RDD转为DataFrame"><span class="nav-number">14.4.2.</span> <span class="nav-text">2.JSON格式的RDD转为DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-非JSON格式的RDD转为DataFrame"><span class="nav-number">14.4.3.</span> <span class="nav-text">3.非JSON格式的RDD转为DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-反射的方式"><span class="nav-number">14.4.3.1.</span> <span class="nav-text">1.反射的方式</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-动态创建Schema"><span class="nav-number">14.4.3.2.</span> <span class="nav-text">2.动态创建Schema</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-读取MySQL中的数据来创建DataFrame"><span class="nav-number">14.4.4.</span> <span class="nav-text">4. 读取MySQL中的数据来创建DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-读取Hive中的数据创建一个DataFrame（Spark-on-Hive）"><span class="nav-number">14.4.5.</span> <span class="nav-text">5. 读取Hive中的数据创建一个DataFrame（Spark on Hive）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame数据存储"><span class="nav-number">14.5.</span> <span class="nav-text">DataFrame数据存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自定义函数"><span class="nav-number">14.6.</span> <span class="nav-text">自定义函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#UDF"><span class="nav-number">14.6.1.</span> <span class="nav-text">UDF</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#UDAF：实现对某个字段进行count"><span class="nav-number">14.6.2.</span> <span class="nav-text">UDAF：实现对某个字段进行count</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#开窗函数"><span class="nav-number">14.7.</span> <span class="nav-text">开窗函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkStreaming"><span class="nav-number">15.</span> <span class="nav-text">SparkStreaming</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Strom-VS-SparkStreaming"><span class="nav-number">15.1.</span> <span class="nav-text">Strom VS SparkStreaming</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SparkStreaming执行流程"><span class="nav-number">15.2.</span> <span class="nav-text">SparkStreaming执行流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Output-Operations-on-DStreams"><span class="nav-number">15.3.</span> <span class="nav-text">Output Operations on DStreams</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#foreachRDD-func"><span class="nav-number">15.3.1.</span> <span class="nav-text">foreachRDD(func)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsTextFiles-prefix-suffix"><span class="nav-number">15.3.2.</span> <span class="nav-text">saveAsTextFiles(prefix, [suffix])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsObjectFiles-prefix-suffix"><span class="nav-number">15.3.3.</span> <span class="nav-text">saveAsObjectFiles(prefix, [suffix])</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#saveAsHadoopFiles-prefix-suffix"><span class="nav-number">15.3.4.</span> <span class="nav-text">saveAsHadoopFiles(prefix, [suffix])</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformations-on-Dstreams"><span class="nav-number">15.4.</span> <span class="nav-text">Transformations on Dstreams</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#transform-func"><span class="nav-number">15.4.1.</span> <span class="nav-text">transform(func)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#updateStateByKey-func"><span class="nav-number">15.4.2.</span> <span class="nav-text">updateStateByKey(func)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Window-Operations"><span class="nav-number">15.4.3.</span> <span class="nav-text">Window Operations</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Driver-HA"><span class="nav-number">15.5.</span> <span class="nav-text">Driver HA</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka"><span class="nav-number">16.</span> <span class="nav-text">Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#消息系统特点"><span class="nav-number">16.1.</span> <span class="nav-text">消息系统特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka架构"><span class="nav-number">16.2.</span> <span class="nav-text">Kafka架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kafka特点"><span class="nav-number">16.3.</span> <span class="nav-text">kafka特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kafka与其他消息队列对比"><span class="nav-number">16.4.</span> <span class="nav-text">kafka与其他消息队列对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#零拷贝"><span class="nav-number">16.5.</span> <span class="nav-text">零拷贝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka集群搭建"><span class="nav-number">16.6.</span> <span class="nav-text">Kafka集群搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-解压"><span class="nav-number">16.6.1.</span> <span class="nav-text">1. 解压</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-修改server-properties配置文件"><span class="nav-number">16.6.2.</span> <span class="nav-text">2. 修改server.properties配置文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-同步配置，记得修改每台机器的broker-id"><span class="nav-number">16.6.3.</span> <span class="nav-text">3. 同步配置，记得修改每台机器的broker.id</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-启动zookeeper集群"><span class="nav-number">16.6.4.</span> <span class="nav-text">4. 启动zookeeper集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-在每台kafka节点上启动kafka集群"><span class="nav-number">16.6.5.</span> <span class="nav-text">5. 在每台kafka节点上启动kafka集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-测试"><span class="nav-number">16.6.6.</span> <span class="nav-text">6. 测试</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka和SparkStreaming整合"><span class="nav-number">16.7.</span> <span class="nav-text">Kafka和SparkStreaming整合</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Receiver方式"><span class="nav-number">16.7.1.</span> <span class="nav-text">Receiver方式</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#原理："><span class="nav-number">16.7.1.1.</span> <span class="nav-text">原理：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#获取kafka传递的数据来计算："><span class="nav-number">16.7.1.2.</span> <span class="nav-text">获取kafka传递的数据来计算：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#kafka客户端生产数据的代码："><span class="nav-number">16.7.1.3.</span> <span class="nav-text">kafka客户端生产数据的代码：</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Direct方式"><span class="nav-number">16.7.2.</span> <span class="nav-text">Direct方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#两种方式下提高SparkStreaming并行度的方法"><span class="nav-number">16.7.3.</span> <span class="nav-text">两种方式下提高SparkStreaming并行度的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Receiver方式调整SparkStreaming的并行度的方法："><span class="nav-number">16.7.3.1.</span> <span class="nav-text">Receiver方式调整SparkStreaming的并行度的方法：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Direct方式并行度的设置："><span class="nav-number">16.7.3.2.</span> <span class="nav-text">Direct方式并行度的设置：</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考资料"><span class="nav-number">17.</span> <span class="nav-text">参考资料</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fly Hugh</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">224.1k</span>
  
</div>


<!-- 
  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>

-->



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

       <!-- 页面点击小红心 -->
    <script type="text/javascript" src="/js/src/src/clicklove.js"></script>

    
  </div>



  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("72l8G3xFxrAReJk0PBs8jCeC-gzGzoHsz", "r0TXzhifsg8y5LTKokFte6fz");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  





</body>


</html>

