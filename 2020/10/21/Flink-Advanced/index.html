<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/xingqiushangcheng.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/xingqiushangcheng.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-mac-osx.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Flink进阶的知识点和文章整理">
<meta name="keywords" content="Apache Flink">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink Advanced">
<meta property="og:url" content="http://yoursite.com/2020/10/21/Flink-Advanced/index.html">
<meta property="og:site_name" content="Mars">
<meta property="og:description" content="Flink进阶的知识点和文章整理">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gjy2m9uj9pj20zk0np77z.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gkoqzw3ghyj20u30guk85.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gkoyk15s9qj20zg0hst9j.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gkozw4iza2j215l0g6q3y.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gkp3n2m556j20d805qgtj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gjwqeh9dfkj20k40l9ab4.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gjwrvcz2rjj20pf0d70tf.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gjws4z21vij20jv0b2wev.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gjwsnd19ebj20li0bkaak.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gk54bjvcx9j20ee0nkjvz.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gk54yygegpj20iw0yota9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gkmcz3c92rj20u00ki0z9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gkmd1v7t1nj20u009rjua.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1gkmd8wroh7j20u00ezgp1.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1gkmdb2gkhoj20u00fjwgy.jpg">
<meta property="og:updated_time" content="2020-11-14T16:39:25.014Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Flink Advanced">
<meta name="twitter:description" content="Flink进阶的知识点和文章整理">
<meta name="twitter:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1gjy2m9uj9pj20zk0np77z.jpg">

<link rel="canonical" href="http://yoursite.com/2020/10/21/Flink-Advanced/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Flink Advanced | Mars</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Mars" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mars</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/21/Flink-Advanced/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Fly Hugh">
      <meta itemprop="description" content="WE CHOOSE TO  GO TO THE MARS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mars">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Flink Advanced
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-21 09:42:56" itemprop="dateCreated datePublished" datetime="2020-10-21T09:42:56+08:00">2020-10-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-11-15 00:39:25" itemprop="dateModified" datetime="2020-11-15T00:39:25+08:00">2020-11-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gjy2m9uj9pj20zk0np77z.jpg" alt="undefined">

<blockquote>
<p>Flink进阶的知识点和文章整理</p>
</blockquote>
<a id="more"></a> 

<h1 id="Flink的应用场景"><a href="#Flink的应用场景" class="headerlink" title="Flink的应用场景"></a>Flink的应用场景</h1><p>前段时间被人问了一个很简单的问题，那就是Flink应该使用在什么应用场景下</p>
<p>乍一听很简单的问题，我却没法提供让我自己满意的回答，只是说需要用到实时计算的地方都刻意用Flink，表述的不够明确，OK，这边有空来用专业的术语和详细的例子总结一下</p>
<ul>
<li>事件驱动型应用</li>
<li>数据分析型应用</li>
<li>数据管道型应用</li>
</ul>
<h2 id="事件驱动型应用场景"><a href="#事件驱动型应用场景" class="headerlink" title="事件驱动型应用场景"></a>事件驱动型应用场景</h2><h3 id="社交领域"><a href="#社交领域" class="headerlink" title="社交领域"></a>社交领域</h3><p>比如在twitter上，我们点击关注某人，点击之后，点击作为一个事件，以数据的方式被接受计算，被关注者的粉丝数量+1，关注者的关注数量+1，两个数据收到了影响，这个就叫事件驱动型应用。</p>
<h3 id="电商"><a href="#电商" class="headerlink" title="电商"></a>电商</h3><p>当刷单者疯狂买东西然后每天给很多个好评，持续很多天的时候，这个事件作为一条数据被抛出警告，提醒淘宝平台这个账号有问题，可能在刷单，这也是事件驱动。</p>
<p>如果某个商家收到来自不同用户的大量投诉，那么可能是店出现了问题，同样提醒淘宝平台。</p>
<p>在淘宝的推荐系统中</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gkoqzw3ghyj20u30guk85.jpg" alt="微信截图_20201114154629.png"></p>
<p>购买东西作为事件，可以看到会实时触发淘宝的推荐系统进行类似推荐。</p>
<h3 id="金融行业"><a href="#金融行业" class="headerlink" title="金融行业"></a>金融行业</h3><p>比如说，有人在银行ATM机里面通过摄像头被算法识别到遮挡面部或者遮挡摄像头等特征的时候，会触发一些事件，比如说通知安保人员等等。</p>
<blockquote>
<p>事件驱动型应用是一类具有<strong>状态</strong>的应用，该应用会根据事件流中的时间触发计算，更新状态或进行外部系统操作。事件驱动型应用常见于实时计算业务中，比如上面的：实时推荐系统，金融反欺诈系统，实时规则预警系统等。</p>
</blockquote>
<p>上述的这种场景毫无疑问是优先于Flink出现而出现的，那么我们来看一下原来我们是如何解决这些问题的</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gkoyk15s9qj20zg0hst9j.jpg" alt="微信截图_20201114200806.png"></p>
<p>这个是我们最开始的解决此类问题的架构</p>
<p>对于一些数据规模是很大，实时性要求不是很高的场景来说，效果是很好的，但是随着数量的不断增加，尤其是数据爆炸时代背景下的海量数据，以关系型数据为例，这里我们需要使用分库分表的方式来支持海量数据，这种方式由于数据库的各种限制，比如说事务一致性的控制，数据备份机制，全局索引等等数据库的实际的架构方面的原因，随着数据量的增加，写入和查询的性能会受到很大的挑战，尤其在实时性要求更高的场景，我们需要更优秀的架构。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gkozw4iza2j215l0g6q3y.jpg" alt="微信截图_20201114205428.png"></p>
<p>这个是引入了Flink之后的架构，在一个完整的时间驱动型架构中往往会涉及到已有数据实体的存储，实体之间关系的维护，统计报表的持久化等需求，所以我们在这个架构中仍然需要用到传统关系型数据库，那么在这个架构中，触发计算的部分，我们交给Apache Flink处理，计算结果既可以存储到支持海量数据的Hadoop生态中的HDFS上，我们也可以将计算结果和统计信息存储在传统数据库中供实时查询。</p>
<p>那么这种解决方案有什么优势呢？</p>
<p><strong>实时性</strong>。为了数据价值最大化，从上面的场景中，我们可以很明显的体会到数据实时反馈的重要性，Flink为了实时计算处理，进行了针对实时流计算的定制特性，比如丰富的状态支持，多窗口语义支持，灵活的Timer和Trigger机制，强大的CEP机制等等等，同时进行了大量的性能优化，这里限于篇幅不能一一列举，后面有机会会再介绍。</p>
<h2 id="数据分析型应用"><a href="#数据分析型应用" class="headerlink" title="数据分析型应用"></a>数据分析型应用</h2><p>So，怎样的应用才是数据分析型应用呢？</p>
<p>数据分析型应用是一个非常宽泛的范围，凡是数据分析型应用都是<strong>从原始数据中提取有价值的信息和指标</strong>。</p>
<p>传统的数据分析方案是一次查询，获得一次查询 获得一次结果，为了实现始终得到最新的结果，需要自己定义触发器（到了某个设定的条件，自动执行查询）也就是我们说的Batch方式的查询方式。</p>
<p>而得益于Timer和State的支持，Flink能够持续查询，Flink的Timer能够持续触发计算，State能够保存上次结果增加计算，一次查询能够源源不断的获得最新的结果。</p>
<p>为了体现Flink的优秀性能，我举一些特定例子</p>
<p>比如说，几乎要求零延迟的天猫/京东双十一大屏</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gkp3n2m556j20d805qgtj.jpg" alt="微信截图_20201114230411.png"></p>
<p>这个统计结果几乎是要求零延迟展示给大家的，现在几家巨头电商的双十一计算逻辑都是在Flink中实现的，侧面验证了Flink在快和处理数据量巨大这两点上的可靠性。</p>
<p>实际上更多的业务可能既既是事件驱动型应用又是数据分析型应用，如果一定要定义这两种区别的话，我们可以从处理目的上来定义，事件驱动型应用是数据最后会直接派发新动作，比如上面的ATM应用检测到不正常动作就触发报警。而数据分析我们只是产生数据，让决策者根据结果数据为现实事件做决定。</p>
<h1 id="数据管道型应用"><a href="#数据管道型应用" class="headerlink" title="数据管道型应用"></a>数据管道型应用</h1><p>我们在采集数据的时候往往会根据数据的特性（数据量的大小，数据的结构）采用不同的数据库来存储数据，但是我们在使用数据的时候如果需要进行流处理，那么我们要针对不用的数据库开发的数据库开发上面所述的不同的触发器，大大增加开发成本。</p>
<p>现在Flink已经集成了CDC功能，正在完善对各个流行的数据库的解析模块，随着时间的推移，最后会成为一个完美的数据管道。</p>
<hr>
<p>如果针对以上场景，读者能有一个清晰的了解，那介绍的目的也就达到了。</p>
<h1 id="Flink-原理与实现：内存管理"><a href="#Flink-原理与实现：内存管理" class="headerlink" title="Flink 原理与实现：内存管理"></a>Flink 原理与实现：内存管理</h1><p>如今，大数据领域的开源框架（Hadoop，Spark，Storm）都使用的 JVM，当然也包括 Flink。基于 JVM 的数据分析引擎都需要面对将大量数据存到内存中，这就不得不面对 JVM 存在的几个问题：</p>
<ol>
<li>Java 对象存储密度低。一个只包含 boolean 属性的对象占用了16个字节内存：对象头占了8个，boolean 属性占了1个，对齐填充占了7个。而实际上只需要一个bit（1/8字节）就够了。</li>
<li>Full GC 会极大地影响性能，尤其是为了处理更大数据而开了很大内存空间的JVM来说，GC 会达到秒级甚至分钟级。</li>
<li>OOM 问题影响稳定性。OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会发生OutOfMemoryError错误，导致JVM崩溃，分布式框架的健壮性和性能都会受到影响。</li>
</ol>
<p>所以目前，越来越多的大数据项目开始自己管理JVM内存了，像 Spark、Flink、HBase，为的就是获得像 C 一样的性能以及避免 OOM 的发生。本文将会讨论 Flink 是如何解决上面的问题的，主要内容包括内存管理、定制的序列化工具、缓存友好的数据结构和算法、堆外内存、JIT编译优化（Just In Time Compiler 即时编译器）等。</p>
<h2 id="积极的内存管理"><a href="#积极的内存管理" class="headerlink" title="积极的内存管理"></a>积极的内存管理</h2><p>Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上，这个内存块叫做 <code>MemorySegment</code>，它代表了一段固定长度的内存（默认大小为 32KB），也是 Flink 中最小的内存分配单元，并且提供了非常高效的读写方法。你可以把 MemorySegment 想象成是为 Flink 定制的 <code>java.nio.ByteBuffer</code>。它的底层可以是一个普通的 Java 字节数组（<code>byte[]</code>），也可以是一个申请在堆外的 <code>ByteBuffer</code>。每条记录都会以序列化的形式存储在一个或多个<code>MemorySegment</code>中。</p>
<p>Flink 中的 Worker 名叫 TaskManager，是用来运行用户代码的 JVM 进程。TaskManager 的堆内存主要被分成了三个部分：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gjwqeh9dfkj20k40l9ab4.jpg" alt="undefined"></p>
<ul>
<li><strong>Network Buffers:</strong> 一定数量的32KB大小的 buffer，主要用于数据的网络传输。在 TaskManager 启动的时候就会分配。默认数量是 2048 个，可以通过 <code>taskmanager.network.numberOfBuffers</code> 来配置。（阅读<a href="http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/?spm=a2c6h.12873639.0.0.506a2e39Kl3K9L" target="_blank" rel="noopener">这篇文章</a>了解更多Network Buffer的管理）</li>
<li><strong>Memory Manager Pool:</strong> 这是一个由 <code>MemoryManager</code> 管理的，由众多<code>MemorySegment</code>组成的超大集合。Flink 中的算法（如 sort/shuffle/join）会向这个内存池申请 MemorySegment，将序列化后的数据存于其中，使用完后释放回内存池。默认情况下，池子占了堆内存的 70% 的大小。</li>
<li><strong>Remaining (Free) Heap:</strong> 这部分的内存是留给用户代码以及 TaskManager 的数据结构使用的。因为这些数据结构一般都很小，所以基本上这些内存都是给用户代码使用的。从GC的角度来看，可以把这里看成的新生代，也就是说这里主要都是由用户代码生成的短期对象。</li>
</ul>
<p><em>注意：Memory Manager Pool 主要在Batch模式下使用。在Steaming模式下，该池子不会预分配内存，也不会向该池子请求内存块。也就是说该部分的内存都是可以给用户代码使用的。不过社区是打算在 Streaming 模式下也能将该池子利用起来。</em></p>
<p>Flink 采用类似 DBMS 的 sort 和 join 算法，直接操作二进制数据，从而使序列化/反序列化带来的开销达到最小。所以 Flink 的内部实现更像 C/C++ 而非 Java。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。如果要操作多块MemorySegment就像操作一块大的连续内存一样，Flink会使用逻辑视图（<code>AbstractPagedInputView</code>）来方便操作。下图描述了 Flink 如何存储序列化后的数据到内存块中，以及在需要的时候如何将数据存储到磁盘上。</p>
<p>从上面我们能够得出 Flink 积极的内存管理以及直接操作二进制数据有以下几点好处：</p>
<ol>
<li><strong>减少GC压力。</strong>显而易见，因为所有常驻型数据都以二进制的形式存在 Flink 的<code>MemoryManager</code>中，这些<code>MemorySegment</code>一直呆在老年代而不会被GC回收。其他的数据对象基本上是由用户代码生成的短生命周期对象，这部分对象可以被 Minor GC 快速回收。只要用户不去创建大量类似缓存的常驻型对象，那么老年代的大小是不会变的，Major GC也就永远不会发生。从而有效地降低了垃圾回收的压力。另外，这里的内存块还可以是堆外内存，这可以使得 JVM 内存更小，从而加速垃圾回收。</li>
<li><strong>避免了OOM。</strong>所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。在内存吃紧的情况下，算法（sort/join等）会高效地将一大批内存块写到磁盘，之后再读回来。因此，<code>OutOfMemoryErrors</code>可以有效地被避免。</li>
<li><strong>节省内存空间。</strong>Java 对象在存储上有很多额外的消耗（如上一节所谈）。如果只存储实际数据的二进制内容，就可以避免这部分消耗。</li>
<li><strong>高效的二进制操作 &amp; 缓存友好的计算。</strong>二进制数据以定义好的格式存储，可以高效地比较与操作。另外，该二进制形式可以把相关的值，以及hash值，键值和指针等相邻地放进内存中。这使得数据结构可以对高速缓存更友好，可以从 L1/L2/L3 缓存获得性能的提升（下文会详细解释）。</li>
</ol>
<h2 id="为-Flink-量身定制的序列化框架"><a href="#为-Flink-量身定制的序列化框架" class="headerlink" title="为 Flink 量身定制的序列化框架"></a>为 Flink 量身定制的序列化框架</h2><p>目前 Java 生态圈提供了众多的序列化框架：Java serialization, Kryo, Apache Avro 等等。但是 Flink 实现了自己的序列化框架。因为在 Flink 中处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。</p>
<p>Flink支持任意的Java或是Scala类型。Flink 在数据类型上有很大的进步，不需要实现一个特定的接口（像Hadoop中的<code>org.apache.hadoop.io.Writable</code>），Flink 能够自动识别数据类型。Flink 通过 Java Reflection 框架分析基于 Java 的 Flink 程序 UDF (User Define Function)的返回类型的类型信息，通过 Scala Compiler 分析基于 Scala 的 Flink 程序 UDF 的返回类型的类型信息。类型信息由 <code>TypeInformation</code> 类表示，TypeInformation 支持以下几种类型：</p>
<ul>
<li><code>BasicTypeInfo</code>: 任意Java 基本类型（装箱的）或 String 类型。</li>
<li><code>BasicArrayTypeInfo</code>: 任意Java基本类型数组（装箱的）或 String 数组。</li>
<li><code>WritableTypeInfo</code>: 任意 Hadoop Writable 接口的实现类。</li>
<li><code>TupleTypeInfo</code>: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现。</li>
<li><code>CaseClassTypeInfo</code>: 任意的 Scala CaseClass(包括 Scala tuples)。</li>
<li><code>PojoTypeInfo</code>: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法。</li>
<li><code>GenericTypeInfo</code>: 任意无法匹配之前几种类型的类。</li>
</ul>
<p>前六种数据类型基本上可以满足绝大部分的Flink程序，针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。对于最后一种数据类型，Flink会使用Kryo进行序列化和反序列化。每个TypeInformation中，都包含了serializer，类型会自动通过serializer进行序列化，然后用Java Unsafe接口写入MemorySegments。对于可以用作key的数据类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据进行compare、hash等操作。对于 Tuple、CaseClass、POJO 等组合类型，其TypeSerializer和TypeComparator也是组合的，序列化和比较时会委托给对应的serializers和comparators。如下图展示 一个内嵌型的Tuple3 对象的序列化过程。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gjwrvcz2rjj20pf0d70tf.jpg" alt="undefined"></p>
<p>可以看出这种序列化方式存储密度是相当紧凑的。其中 int 占4字节，double 占8字节，POJO多个一个字节的header，PojoSerializer只负责将header序列化进去，并委托每个字段对应的serializer对字段进行序列化。</p>
<p>Flink 的类型系统可以很轻松地扩展出自定义的TypeInformation、Serializer以及Comparator，来提升数据类型在序列化和比较时的性能。</p>
<h2 id="Flink-如何直接操作二进制数据"><a href="#Flink-如何直接操作二进制数据" class="headerlink" title="Flink 如何直接操作二进制数据"></a>Flink 如何直接操作二进制数据</h2><p>Flink 提供了如 group、sort、join 等操作，这些操作都需要访问海量数据。这里，我们以sort为例，这是一个在 Flink 中使用非常频繁的操作。</p>
<p>首先，Flink 会从 MemoryManager 中申请一批 MemorySegment，我们把这批 MemorySegment 称作 sort buffer，用来存放排序的数据。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gjws4z21vij20jv0b2wev.jpg" alt="undefined"></p>
<p>我们会把 sort buffer 分成两块区域。一个区域是用来存放所有对象完整的二进制数据。另一个区域用来存放指向完整二进制数据的指针以及定长的序列化后的key（key+pointer）。如果需要序列化的key是个变长类型，如String，则会取其前缀序列化。如上图所示，当一个对象要加到 sort buffer 中时，它的二进制数据会被加到第一个区域，指针（可能还有key）会被加到第二个区域。</p>
<p>将实际的数据和指针加定长key分开存放有两个目的。第一，交换定长块（key+pointer）更高效，不用交换真实的数据也不用移动其他key和pointer。第二，这样做是缓存友好的，因为key都是连续存储在内存中的，可以大大减少 cache miss（后面会详细解释）。</p>
<p>排序的关键是比大小和交换。Flink 中，会先用 key 比大小，这样就可以直接用二进制的key比较而不需要反序列化出整个对象。因为key是定长的，所以如果key相同（或者没有提供二进制key），那就必须将真实的二进制数据反序列化出来，然后再做比较。之后，只需要交换key+pointer就可以达到排序的效果，真实的数据不用移动。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gjwsnd19ebj20li0bkaak.jpg" alt="undefined"></p>
<p>最后，访问排序后的数据，可以沿着排好序的key+pointer区域顺序访问，通过pointer找到对应的真实数据，并写到内存或外部（更多细节可以看这篇文章 <a href="http://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html" target="_blank" rel="noopener">Joins in Flink</a>）。</p>
<h2 id="缓存友好的数据结构和算法"><a href="#缓存友好的数据结构和算法" class="headerlink" title="缓存友好的数据结构和算法"></a>缓存友好的数据结构和算法</h2><p>随着磁盘IO和网络IO越来越快，CPU逐渐成为了大数据领域的瓶颈。从 L1/L2/L3 缓存读取数据的速度比从主内存读取数据的速度快好几个量级。通过性能分析可以发现，CPU时间中的很大一部分都是浪费在等待数据从主内存过来上。如果这些数据可以从 L1/L2/L3 缓存过来，那么这些等待时间可以极大地降低，并且所有的算法会因此而受益。</p>
<p>在上面讨论中我们谈到的，Flink 通过定制的序列化框架将算法中需要操作的数据（如sort中的key）连续存储，而完整数据存储在其他地方。因为对于完整的数据来说，key+pointer更容易装进缓存，这大大提高了缓存命中率，从而提高了基础算法的效率。这对于上层应用是完全透明的，可以充分享受缓存友好带来的性能提升。</p>
<h2 id="走向堆外内存"><a href="#走向堆外内存" class="headerlink" title="走向堆外内存"></a>走向堆外内存</h2><p>Flink 基于堆内存的内存管理机制已经可以解决很多JVM现存问题了，为什么还要引入堆外内存？</p>
<ol>
<li>启动超大内存（上百GB）的JVM需要很长时间，GC停留时间也会很长（分钟级）。使用堆外内存的话，可以极大地减小堆内存（只需要分配Remaining Heap那一块），使得 TaskManager 扩展到上百GB内存不是问题。</li>
<li>高效的 IO 操作。堆外内存在写磁盘或网络传输时是 <code>zero-copy</code>，而堆内存的话，至少需要 copy 一次。</li>
<li>堆外内存是进程间共享的。也就是说，即使JVM进程崩溃也不会丢失数据。这可以用来做故障恢复（Flink暂时没有利用起这个，不过未来很可能会去做）。</li>
</ol>
<p>但是强大的东西总是会有其负面的一面，不然为何大家不都用堆外内存呢。</p>
<ol>
<li>堆内存的使用、监控、调试都要简单很多。堆外内存意味着更复杂更麻烦。</li>
<li>Flink 有时需要分配短生命周期的 <code>MemorySegment</code>，这个申请在堆上会更廉价。</li>
<li>有些操作在堆内存上会快一点点。</li>
</ol>
<p>Flink用通过<code>ByteBuffer.allocateDirect(numBytes)</code>来申请堆外内存，用 <code>sun.misc.Unsafe</code> 来操作堆外内存。</p>
<p>基于 Flink 优秀的设计，实现堆外内存是很方便的。Flink 将原来的 <code>MemorySegment</code> 变成了抽象类，并生成了两个子类。<code>HeapMemorySegment</code> 和 <code>HybridMemorySegment</code>。从字面意思上也很容易理解，前者是用来分配堆内存的，后者是用来分配堆外内存<strong>和堆内存</strong>的。是的，你没有看错，后者既可以分配堆外内存又可以分配堆内存。为什么要这样设计呢？</p>
<p>首先假设<code>HybridMemorySegment</code>只提供分配堆外内存。在上述堆外内存的不足中的第二点谈到，Flink 有时需要分配短生命周期的 buffer，这些buffer用<code>HeapMemorySegment</code>会更高效。那么当使用堆外内存时，为了也满足堆内存的需求，我们需要同时加载两个子类。这就涉及到了 JIT 编译优化的问题。因为以前 <code>MemorySegment</code> 是一个单独的 final 类，没有子类。JIT 编译时，所有要调用的方法都是确定的，所有的方法调用都可以被去虚化（de-virtualized）和内联（inlined），这可以极大地提高性能（MemroySegment的使用相当频繁）。然而如果同时加载两个子类，那么 JIT 编译器就只能在真正运行到的时候才知道是哪个子类，这样就无法提前做优化。实际测试的性能差距在 2.7 倍左右。</p>
<p>Flink 使用了两种方案：</p>
<p><strong>方案1：只能有一种 MemorySegment 实现被加载</strong></p>
<p>代码中所有的短生命周期和长生命周期的MemorySegment都实例化其中一个子类，另一个子类根本没有实例化过（使用工厂模式来控制）。那么运行一段时间后，JIT 会意识到所有调用的方法都是确定的，然后会做优化。</p>
<p><strong>方案2：提供一种实现能同时处理堆内存和堆外内存</strong></p>
<p>这就是 <code>HybridMemorySegment</code> 了，能同时处理堆与堆外内存，这样就不需要子类了。这里 Flink 优雅地实现了一份代码能同时操作堆和堆外内存。这主要归功于 <code>sun.misc.Unsafe</code>提供的一系列方法，如getLong方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sun.misc.Unsafe.getLong(Object reference, long offset)</span><br></pre></td></tr></table></figure>

<ul>
<li>如果reference不为空，则会取该对象的地址，加上后面的offset，从相对地址处取出8字节并得到 long。这对应了堆内存的场景。</li>
<li>如果reference为空，则offset就是要操作的绝对地址，从该地址处取出数据。这对应了堆外内存的场景。</li>
</ul>
<p>这里我们看下 <code>MemorySegment</code> 及其子类的实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 堆内存引用</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] heapMemory;</span><br><span class="line">  <span class="comment">// 堆外内存地址</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">long</span> address;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//堆内存的初始化</span></span><br><span class="line">  MemorySegment(<span class="keyword">byte</span>[] buffer, Object owner) &#123;</span><br><span class="line">    <span class="comment">//一些先验检查</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.heapMemory = buffer;</span><br><span class="line">    <span class="keyword">this</span>.address = BYTE_ARRAY_BASE_OFFSET;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//堆外内存的初始化</span></span><br><span class="line">  MemorySegment(<span class="keyword">long</span> offHeapAddress, <span class="keyword">int</span> size, Object owner) &#123;</span><br><span class="line">    <span class="comment">//一些先验检查</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.heapMemory = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">this</span>.address = offHeapAddress;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">getLong</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> pos = address + index;</span><br><span class="line">    <span class="keyword">if</span> (index &gt;= <span class="number">0</span> &amp;&amp; pos &lt;= addressLimit - <span class="number">8</span>) &#123;</span><br><span class="line">      <span class="comment">// 这是我们关注的地方，使用 Unsafe 来操作 on-heap &amp; off-heap</span></span><br><span class="line">      <span class="keyword">return</span> UNSAFE.getLong(heapMemory, pos);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (address &gt; addressLimit) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"segment has been freed"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// index is in fact invalid</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapMemorySegment</span> <span class="keyword">extends</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 指向heapMemory的额外引用，用来如数组越界的检查</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">byte</span>[] memory;</span><br><span class="line">  <span class="comment">// 只能初始化堆内存</span></span><br><span class="line">  HeapMemorySegment(<span class="keyword">byte</span>[] memory, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(Objects.requireNonNull(memory), owner);</span><br><span class="line">    <span class="keyword">this</span>.memory = memory;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HybridMemorySegment</span> <span class="keyword">extends</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ByteBuffer offHeapBuffer;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 堆外内存初始化</span></span><br><span class="line">  HybridMemorySegment(ByteBuffer buffer, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(checkBufferAndGetAddress(buffer), buffer.capacity(), owner);</span><br><span class="line">    <span class="keyword">this</span>.offHeapBuffer = buffer;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 堆内存初始化</span></span><br><span class="line">  HybridMemorySegment(<span class="keyword">byte</span>[] buffer, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(buffer, owner);</span><br><span class="line">    <span class="keyword">this</span>.offHeapBuffer = <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以发现，HybridMemorySegment 中的很多方法其实都下沉到了父类去实现。包括堆内堆外内存的初始化。<code>MemorySegment</code> 中的 <code>getXXX</code>/<code>putXXX</code> 方法都是调用了 unsafe 方法，可以说<code>MemorySegment</code>已经具有了些 Hybrid 的意思了。<code>HeapMemorySegment</code>只调用了父类的<code>MemorySegment(byte[] buffer, Object owner)</code>方法，也就只能申请堆内存。另外，阅读代码你会发现，许多方法（大量的 getXXX/putXXX）都被标记成了 final，两个子类也是 final 类型，为的也是优化 JIT 编译器，会提醒 JIT 这个方法是可以被去虚化和内联的。</p>
<p>对于堆外内存，使用 <code>HybridMemorySegment</code> 能同时用来代表堆和堆外内存。这样只需要一个类就能代表长生命周期的堆外内存和短生命周期的堆内存。既然<code>HybridMemorySegment</code>已经这么全能，为什么还要方案1呢？因为我们需要工厂模式来保证只有一个子类被加载（为了更高的性能），而且HeapMemorySegment比heap模式的HybridMemorySegment要快。</p>
<p>下方是一些性能测试数据，更详细的数据请参考<a href="http://flink.apache.org/news/2015/09/16/off-heap-memory.html#appendix-detailed-micro-benchmarks" target="_blank" rel="noopener">这篇文章</a>。</p>
<table>
<thead>
<tr>
<th align="left">Segment</th>
<th align="left">Time</th>
</tr>
</thead>
<tbody><tr>
<td align="left">HeapMemorySegment, exclusive</td>
<td align="left">1,441 msecs</td>
</tr>
<tr>
<td align="left">HeapMemorySegment, mixed</td>
<td align="left">3,841 msecs</td>
</tr>
<tr>
<td align="left">HybridMemorySegment, heap, exclusive</td>
<td align="left">1,626 msecs</td>
</tr>
<tr>
<td align="left">HybridMemorySegment, off-heap, exclusive</td>
<td align="left">1,628 msecs</td>
</tr>
<tr>
<td align="left">HybridMemorySegment, heap, mixed</td>
<td align="left">3,848 msecs</td>
</tr>
<tr>
<td align="left">HybridMemorySegment, off-heap, mixed</td>
<td align="left">3,847 msecs</td>
</tr>
</tbody></table>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要总结了 Flink 面对 JVM 存在的问题，而在内存管理的道路上越走越深。从自己管理内存，到序列化框架，再到堆外内存。其实纵观大数据生态圈，其实会发现各个开源项目都有同样的趋势。比如最近炒的很火热的 Spark Tungsten 项目，与 Flink 在内存管理上的思想是及其相似的。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://flink.apache.org/news/2015/09/16/off-heap-memory.html" target="_blank" rel="noopener">Off-heap Memory in Apache Flink and the curious JIT compiler</a></li>
<li><a href="https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html" target="_blank" rel="noopener">Juggling with Bits and Bytes</a></li>
<li><a href="https://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html" target="_blank" rel="noopener">Peeking into Apache Flink’s Engine Room</a></li>
<li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=53741525" target="_blank" rel="noopener">Flink: Memory Management</a></li>
<li><a href="http://www.bigsynapse.com/addressing-big-data-performance" target="_blank" rel="noopener">Big Data Performance Engineering</a></li>
<li><a href="http://mishadoff.com/blog/java-magic-part-4-sun-dot-misc-dot-unsafe/" target="_blank" rel="noopener">sun.misc.misc.Unsafe usage for C style memory management</a></li>
<li><a href="http://howtodoinjava.com/core-java/related-concepts/usage-of-class-sun-misc-unsafe/" target="_blank" rel="noopener">sun.misc.misc.Unsafe usage for C style memory management - How to do it.</a></li>
<li><a href="http://www.javamex.com/tutorials/memory/object_memory_usage.shtml" target="_blank" rel="noopener">Memory usage of Java objects: general guide</a></li>
<li><a href="http://www.36dsj.com/archives/33650" target="_blank" rel="noopener">脱离JVM？ Hadoop生态圈的挣扎与演化</a></li>
</ul>
<h1 id="Flink-内存设置思路"><a href="#Flink-内存设置思路" class="headerlink" title="Flink 内存设置思路"></a>Flink 内存设置思路</h1><p>Flink内存设置思路分为两个版本</p>
<p>分别是1.9之前和之后的</p>
<h2 id="Flink-lt-1-9"><a href="#Flink-lt-1-9" class="headerlink" title="Flink &lt;= 1.9"></a>Flink &lt;= 1.9</h2><p>这里用Flink1.8为例，计算内存的代码位于<code>org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters</code>类的<code>create</code>方法。</p>
<p>按照计算步骤，以<code>taskmanager.heap.size=6g</code>为例子，其他参数保持不动，最终得到的参数如下：</p>
<p>​                        -Xms4148m -Xmx4148m  -XX:MaxDirectMemorySize=1996m</p>
<p>两块内存加起来是6144m = 6g jvm的设置符合参数。</p>
<p>Flink Dashboard上面显示的是：</p>
<p>​        JVM Heap Size：3.95 GB  Flink Managed Memory：2.74 GB</p>
<p>​        JVM (Heap/Non-Heap) Commit： Heap：3.95 GB Non-Heap：141 MB Total：4.09 GB</p>
<p>​        Outside JVM：Capacity：457 MB</p>
<p>​        NetWork: count: xxxxx</p>
<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p>设容器内存总大小是x</p>
<p>详细看create方法：</p>
<pre><code>1.  cutoff：容器超过3g, 简单可以记成 0.25x. flink为了防止内存溢出，计算的时候先切了一块内存下来不参与后续计算，这块就是cutoff</code></pre>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cutoff = Math.max(containerized.heap-cutoff-min, taskmanager.heap.size * containerized.heap-cutoff-ratio)</span><br></pre></td></tr></table></figure>

<p>默认值是600和0.25，所以6g的时候=Math.max(600, 6144*0.25) = 1536m</p>
<p>剩余大小 0.75x6g = 4608m</p>
<ol start="2">
<li>networkBufMB：简单记成 0.75*0.1x，最大1g</li>
</ol>
<p>网络buffer使用内存分成新旧版，这里只关注新版，涉及参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">taskmanager.memory.segment-size：32kb</span><br><span class="line">taskmanager.network.memory.fraction：0.1</span><br><span class="line">taskmanager.network.memory.min：64mb</span><br><span class="line">taskmanager.network.memory.max：1g</span><br></pre></td></tr></table></figure>

<p>计算参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Math.min(taskmanager.network.memory.max，Math.max(taskmanager.network.memory.min, taskmanager.network.memory.fraction * (x - cutoff))</span><br></pre></td></tr></table></figure>

<p>这里的结果就是：Math.min(1g, Math.max(64mb, 0.1 * 4608m) = 460.8m</p>
<ol start="3">
<li>heapSizeMB：0.75 * 0.9x</li>
</ol>
<p>taskmanager.memory.off-heap默认为false，主要指的是Flink Managed Memory使用Heap还是Non-heap，默认使用Heap，如果开启使用Non-heap将再减少一部分资源。</p>
<p>计算公式：<code>x - cutoff - networkBufMB</code></p>
<p>这里就是：4147.2　　　　（注意：这个就是-xmx 4148m）</p>
<p>​    4. offHeapSizeMB： x - heapSizeMB</p>
<p>就是1996m　　　　　　　(注意：这个就是XX:MaxDirectMemorySize: 1996m)</p>
<p>后续：上面只是一个jvm的参数预估设置，实际设置还与运行中环境有关，TaskManagerServices.fromConfiguration</p>
<p>会计算一个 freeHeapMemoryWithDefrag，计算之前会手动触发gc，然后用Jvm最大内存 - 总内存 + 空闲内存。</p>
<p>这个值可以认为是一个空运行的flink任务剩余的堆内存了。</p>
<p>后面将计算Flink管理的内存，这个指的是Flink Managed Memory Segment: taskmanager.memory.fraction默认是0.7，</p>
<p>被Flink管理的内存就是：freeHeapMemoryWithDefrag * 0.7</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gk54bjvcx9j20ee0nkjvz.jpg" alt="undefined"></p>
<p>所以虽然6g内存计算出来后，heap是4148，但是在dashbord中显示不足4148, 为3.95G=4044.8, Flink managed内存小于 0.75<em>0.9</em>0.7 = 2903.04 , dashboard上显示2.74g = 2805.76m</p>
<p>框架运行需要：4148 - 4044.8 = 103.2m，3.95 * 0.7 = 2.765 &gt; 2.74。没有相等，其他的内存使用暂时没有探究了。</p>
<p>Flink Managed内存一般用于批处理作业，流处理作业可以调整 taskmanager.memory.fraction，使得这部分内存用于用户代码。</p>
<p>Non - heap空间一般用于 JVM 的栈空间、方法区等堆外开销外，还包括网络 buffer、batch 缓存、RocksDB</p>
<h2 id="Flink-gt-1-10"><a href="#Flink-gt-1-10" class="headerlink" title="Flink &gt;= 1.10"></a>Flink &gt;= 1.10</h2><p>Flink后面内存进行了较大的变动，也就是说之前上面云邪写的内存管理已经过时了</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/memory/mem_migration.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/memory/mem_migration.html</a></p>
<p>这里设置单个taskmanager为14g，taskmanager.memory.managed.fraction为0.5，将会得到以下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-Xmx5721030656  = 5456MB = 5.328g</span><br><span class="line">-=1207959552  = 1152MB = 1.125g</span><br><span class="line">-XX:MaxMetaspaceSize=100663296 = 96MB</span><br></pre></td></tr></table></figure>

<p>可以发现，上面的加起来等于6704MB，远远不足14g，和1.8版本有很大的不同。</p>
<p>再看dashboard：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JVM Heap Size：5.19 GB   Flink Managed Memory：6.45 GB</span><br><span class="line">JVM (Heap/Non-Heap) ： Heap：5.19 GB  Non-Heap：1.33 GB  Total：6.52 GB</span><br><span class="line">Outside JVM：Capacity：1.01GB</span><br><span class="line">NetWork: count:  xxxxx</span><br></pre></td></tr></table></figure>

<p>可以计算得到6.45+6.52+1.01 = 13.98 等于14</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">taskmanager.memory.process.size 设置的是容器的内存大小，等于之前的 taskmanager.heap.size</span><br><span class="line"></span><br><span class="line">计算过程在org.apache.flink.runtime.clusterframework.TaskExecutorProcessUtils中processSpecFromConfig方法，TaskExecutorProcessSpec类展示了1.10版本整个内存的组成。</span><br><span class="line"></span><br><span class="line">计算方法分成3种：</span><br><span class="line">1.指定了taskmanager.memory.task.heap.size和taskmanager.memory.managed.size   </span><br><span class="line">见方法：deriveProcessSpecWithExplicitTaskAndManagedMemory</span><br><span class="line">2.指定了taskmanager.memory.flink.size  </span><br><span class="line">见方法：deriveProcessSpecWithTotalFlinkMemory</span><br><span class="line">3.指定了taskmanager.memory.process.size（容器环境一般指定这个，决定全局容量）</span><br><span class="line">totalProcessMemorySize = 设置的值 14g,   jvmMetaspaceSize = taskmanager.memory.jvm-metaspace.size,默认96m,这个对应参数-XX:MaxMetaspaceSize=100663296。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jvmOverheadSize：</span><br><span class="line"></span><br><span class="line">	taskmanager.memory.jvm-overhead.min  192m</span><br><span class="line">	taskmanager.memory.jvm-overhead.max 1g</span><br><span class="line">	taskmanager.memory.jvm-overhead.fraction 0.1</span><br></pre></td></tr></table></figure>

<p>公式 14g * 0.1 = 1.4g 必须在[192m, 1g]之间，所以jvmOverheadSize的大小是1g</p>
<p>totalFlinkMemorySize = 14g - 1g - 96m = 13216m</p>
<p>frameworkHeapMemorySize：taskmanager.memory.framework.heap.size 默认128m</p>
<p>frameworkOffHeapMemorySize：taskmanager.memory.framework.off-heap.size 默认128m</p>
<p>taskOffHeapMemorySize：taskmanager.memory.task.off-heap.size 默认0</p>
<p>确定好上面这些参数后，就是最重要的三个指标的计算了：</p>
<p>taskHeapMemorySize，networkMemorySize，managedMemorySize</p>
<p>计算分成确定了：taskmanager.memory.task.heap.size还是没确定。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">确定了taskmanager.memory.task.heap.size</span><br><span class="line">	taskHeapMemorySize = 设置值</span><br><span class="line">	managedMemorySize = 设置了使用设置值，否则使用 0.4 * totalFlinkMemorySize</span><br><span class="line">	如果 taskHeapMemorySize + taskOffHeapMemorySize + frameworkHeapMemorySize + frameworkOffHeapMemorySize + managedMemorySize &gt; totalFlinkMemorySize异常</span><br><span class="line">	networkMemorySize 等于剩余的大小，之后还会check这块内存是否充足，可以自己查看对应代码</span><br><span class="line">未设置heap大小</span><br><span class="line">	先确定 managedMemorySize = 设置了使用设置值，否则使用 0.4 * totalFlinkMemorySize，这里就是 0.5 * 13216m = 6608 = 6.45g (这里就是dashboard的显示内容)</span><br><span class="line">	再确定network buffer大小，这个也是有两种情况，不细说。 [64mb, 1g] 0.1 * totalFlinkMemorySize = 1321.6, 所以是1g</span><br><span class="line">	最后剩余的就是taskHeapMemorySize,不能为负数，这里等于  13216 - 6608 - 1024 - 128 - 128 = 5328 = 5.2g (这里约等于dashboard的显示heap大小)</span><br><span class="line">	最后jvm的参数的计算过程：</span><br><span class="line">	jvmHeapSize = frameworkHeapSize + taskHeapSize = 5328 + 128 = 5456</span><br><span class="line">	jvmDirectSize = frameworkOffHeapMemorySize + taskOffHeapSize + networkMemSize = 128 + 1024 = 1152</span><br><span class="line">	jvmMetaspaceSize = 96m</span><br></pre></td></tr></table></figure>

<h3 id="全新内存划分"><a href="#全新内存划分" class="headerlink" title="全新内存划分"></a>全新内存划分</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_detail.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_detail.html</a></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gk54yygegpj20iw0yota9.jpg" alt="undefined"></p>
<p>从计算过程，结合上图可以看出Flink 1.10中的一个内存划分了。</p>
<p>总内存 = Flink 内存 + JVM Metaspace （96m）+ JVM Overhead （计算为0.1 * 全局大小，结果必须在[192m, 1g]之间）</p>
<p>Flink内存被划分成6部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">框架运行需要的Heap和Non Heap，默认都是128m</span><br><span class="line">任务需要的Heap和Non Heap(默认0), Heap是通过计算其他5部分内存，Flink内存剩余得到</span><br><span class="line">网络缓冲 (0.1 * Flink内存，结果必须在[64mb, 1g]之间）</span><br><span class="line">Flink管理内存：0.4 * Flink内存</span><br></pre></td></tr></table></figure>

<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>Flink 1.10之前对内存的划分比较简单，主要就是Heap + Non-Heap，之后对内存做了更细致的切分。</p>
<p>Flink 1.8可以调整taskmanager.memory.fraction 减少Heap中的管理的内存，增大用户代码的内存使用，调整containerized.heap-cutoff-ratio，控制Non-heap空间，这个影响rocksdb。</p>
<p>Flink 1.10可以调整taskmanager.memory.managed.fraction 控制managed内存，这个影响rocksdb，也会影响taskHeap大小，需要衡量。</p>
<p>也可以看到Flink内存模型的变化managed内存位置也发生了变化，作用也有了些许变化。</p>
<p>JVM 主要划分 Heap 和 Non-Heap，Non-Heap又划分为Direct和Native等。</p>
<p>1.8的Non-Heap都是通过XX:MaxDirectMemorySize设置的</p>
<p>1.10的Network buffer在Direct里面，另一部分是Native(包括Managed Memory)，主要用于rocksdb，如果使用的是Heap状态后台，可以设置小点，也用于Batch。</p>
<h1 id="多流合并"><a href="#多流合并" class="headerlink" title="多流合并"></a>多流合并</h1><p>遇到了一个多流合并的问题，目前给出的方案是：</p>
<ol>
<li><p>分析业务数据源，很多需要多个流的join的场景 是伪命题，用union即可。</p>
</li>
<li><p>union + group by ，在基于key的流中可以取代join。优势： 在join 发生数据倾斜或者反压，很难 checkpoint时，用union可以回避这个问题。</p>
</li>
<li><p>例如三个流join，可以 tempstream = stream1.join(stream2)  ResultStream = tempstream.join(stream3)。语法支持，看了下生成的图，不确定是不是想要的效果。</p>
</li>
<li><p>将第三点的 join 换成 cogroup操作。这个是社区直播中，提问多流join后  得到的回复</p>
</li>
<li><p>基于blink引擎，多表join 直接用sql表达出来。select * from table1 a left join table2 b on a.id = b.id left join table3 c on b.id = c.id 这个方案也是在社区直播，提问多流join后 得到的回复。</p>
</li>
</ol>
<p>SQL暂时是不用的，那么只剩下了第一种。</p>
<p>我尝试拆分一下逻辑。</p>
<h1 id="Flink-的算子链机制"><a href="#Flink-的算子链机制" class="headerlink" title="Flink 的算子链机制"></a>Flink 的算子链机制</h1><p>“为什么我的 Flink 作业 Web UI 中只显示出了一个框，并且 Records Sent 和Records Received 指标都是 0 ？是我的程序写得有问题吗？”</p>
<p>在 Flink 社区群里经常能看到类似这样的疑问。这种情况几乎都不是程序有问题，而是因为 Flink 的 operator chain ——即算子链机制导致的，即提交的作业的执行计划中，所有算子的并发实例（即 sub-task ）都因为满足特定条件而串成了整体来执行，自然就观察不到算子之间的数据流量了。</p>
<p>当然上述是一种特殊情况。我们更常见到的是只有部分算子得到了算子链机制的优化，如官方文档中出现过多次的下图所示，注意 Source 和 map() 算子。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gkmcz3c92rj20u00ki0z9.jpg" alt="undefined"></p>
<p>算子链机制的好处是显而易见的：<strong>所有 chain 在一起的 sub-task 都会在同一个线程（即 TaskManager 的 slot）中执行，能够减少不必要的数据交换、序列化和上下文切换，从而提高作业的执行效率。</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gkmd1v7t1nj20u009rjua.jpg" alt="undefined"></p>
<h2 id="逻辑计划中的算子链"><a href="#逻辑计划中的算子链" class="headerlink" title="逻辑计划中的算子链"></a>逻辑计划中的算子链</h2><p>对 Flink Runtime 稍有了解的看官应该知道，Flink 作业的执行计划会用三层图结构来表示，即：</p>
<ul>
<li>StreamGraph —— 原始逻辑执行计划</li>
<li>JobGraph —— 优化的逻辑执行计划（Web UI 中看到的就是这个）</li>
<li>ExecutionGraph —— 物理执行计划</li>
</ul>
<p>算子链是在优化逻辑计划时加入的，也就是由 StreamGraph 生成 JobGraph 的过程中。那么我们来到负责生成 JobGraph 的 o.a.f.streaming.api.graph.StreamingJobGraphGenerator 类，查看其核心方法 createJobGraph() 的源码。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> JobGraph <span class="title">createJobGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// make sure that all vertices start immediately</span></span><br><span class="line">    jobGraph.setScheduleMode(streamGraph.getScheduleMode());</span><br><span class="line">    <span class="comment">// Generate deterministic hashes for the nodes in order to identify them across</span></span><br><span class="line">    <span class="comment">// submission iff they didn't change.</span></span><br><span class="line">    Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph);</span><br><span class="line">    <span class="comment">// Generate legacy version hashes for backwards compatibility</span></span><br><span class="line">    List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes = <span class="keyword">new</span> ArrayList&lt;&gt;(legacyStreamGraphHashers.size());</span><br><span class="line">    <span class="keyword">for</span> (StreamGraphHasher hasher : legacyStreamGraphHashers) &#123;</span><br><span class="line">        legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph));</span><br><span class="line">    &#125;</span><br><span class="line">    Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    setChaining(hashes, legacyHashes, chainedOperatorHashes);</span><br><span class="line"></span><br><span class="line">    setPhysicalEdges();</span><br><span class="line">    <span class="comment">// 略......</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jobGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可见，该方法会先计算出 StreamGraph 中各个节点的哈希码作为唯一标识，并创建一个空的 Map 结构保存即将被链在一起的算子的哈希码，然后调用 setChaining() 方法，如下源码所示。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setChaining</span><span class="params">(Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes, List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes, Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Integer sourceNodeId : streamGraph.getSourceIDs()) &#123;</span><br><span class="line">        createChain(sourceNodeId, sourceNodeId, hashes, legacyHashes, <span class="number">0</span>, chainedOperatorHashes);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可见是逐个遍历 StreamGraph 中的 Source 节点，并调用 createChain() 方法。createChain() 是逻辑计划层创建算子链的核心方法，完整源码如下，有点长。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;StreamEdge&gt; <span class="title">createChain</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        Integer startNodeId,</span></span></span><br><span class="line"><span class="function"><span class="params">        Integer currentNodeId,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes,</span></span></span><br><span class="line"><span class="function"><span class="params">        List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> chainIndex,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!builtVertices.contains(startNodeId)) &#123;</span><br><span class="line">        List&lt;StreamEdge&gt; transitiveOutEdges = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">        List&lt;StreamEdge&gt; chainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">        List&lt;StreamEdge&gt; nonChainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">        StreamNode currentNode = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line">        <span class="keyword">for</span> (StreamEdge outEdge : currentNode.getOutEdges()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isChainable(outEdge, streamGraph)) &#123;</span><br><span class="line">                chainableOutputs.add(outEdge);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                nonChainableOutputs.add(outEdge);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge chainable : chainableOutputs) &#123;</span><br><span class="line">            transitiveOutEdges.addAll(</span><br><span class="line">                    createChain(startNodeId, chainable.getTargetId(), hashes, legacyHashes, chainIndex + <span class="number">1</span>, chainedOperatorHashes));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge nonChainable : nonChainableOutputs) &#123;</span><br><span class="line">            transitiveOutEdges.add(nonChainable);</span><br><span class="line">            createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes, legacyHashes, <span class="number">0</span>, chainedOperatorHashes);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; operatorHashes =</span><br><span class="line">            chainedOperatorHashes.computeIfAbsent(startNodeId, k -&gt; <span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span>[] primaryHashBytes = hashes.get(currentNodeId);</span><br><span class="line">        OperatorID currentOperatorId = <span class="keyword">new</span> OperatorID(primaryHashBytes);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map&lt;Integer, <span class="keyword">byte</span>[]&gt; legacyHash : legacyHashes) &#123;</span><br><span class="line">            operatorHashes.add(<span class="keyword">new</span> Tuple2&lt;&gt;(primaryHashBytes, legacyHash.get(currentNodeId)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        chainedNames.put(currentNodeId, createChainedName(currentNodeId, chainableOutputs));</span><br><span class="line">        chainedMinResources.put(currentNodeId, createChainedMinResources(currentNodeId, chainableOutputs));</span><br><span class="line">        chainedPreferredResources.put(currentNodeId, createChainedPreferredResources(currentNodeId, chainableOutputs));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNode.getInputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId).addInputFormat(currentOperatorId, currentNode.getInputFormat());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (currentNode.getOutputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId).addOutputFormat(currentOperatorId, currentNode.getOutputFormat());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        StreamConfig config = currentNodeId.equals(startNodeId)</span><br><span class="line">                ? createJobVertex(startNodeId, hashes, legacyHashes, chainedOperatorHashes)</span><br><span class="line">                : <span class="keyword">new</span> StreamConfig(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        setVertexConfig(currentNodeId, config, chainableOutputs, nonChainableOutputs);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNodeId.equals(startNodeId)) &#123;</span><br><span class="line">            config.setChainStart();</span><br><span class="line">            config.setChainIndex(<span class="number">0</span>);</span><br><span class="line">            config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName());</span><br><span class="line">            config.setOutEdgesInOrder(transitiveOutEdges);</span><br><span class="line">            config.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges());</span><br><span class="line">            <span class="keyword">for</span> (StreamEdge edge : transitiveOutEdges) &#123;</span><br><span class="line">                connect(startNodeId, edge);</span><br><span class="line">            &#125;</span><br><span class="line">            config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            chainedConfigs.computeIfAbsent(startNodeId, k -&gt; <span class="keyword">new</span> HashMap&lt;Integer, StreamConfig&gt;());</span><br><span class="line">            config.setChainIndex(chainIndex);</span><br><span class="line">            StreamNode node = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line">            config.setOperatorName(node.getOperatorName());</span><br><span class="line">            chainedConfigs.get(startNodeId).put(currentNodeId, config);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        config.setOperatorID(currentOperatorId);</span><br><span class="line">        <span class="keyword">if</span> (chainableOutputs.isEmpty()) &#123;</span><br><span class="line">            config.setChainEnd();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> transitiveOutEdges;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>先解释一下方法开头创建的 3 个 List 结构：</p>
<ul>
<li>transitiveOutEdges：当前算子链在 JobGraph 中的出边列表，同时也是 createChain() 方法的最终返回值；</li>
<li>chainableOutputs：当前能够链在一起的 StreamGraph 边列表；</li>
<li>nonChainableOutputs：当前不能够链在一起的 StreamGraph 边列表。</li>
</ul>
<p>接下来，从 Source 开始遍历 StreamGraph 中当前节点的所有出边，调用 isChainable() 方法判断是否可以被链在一起（这个判断逻辑稍后会讲到）。可以链接的出边被放入 chainableOutputs 列表，否则放入 nonChainableOutputs 列表。</p>
<p>对于 chainableOutputs 中的边，就会以这些边的直接下游为起点，继续递归调用createChain() 方法延展算子链。对于 nonChainableOutputs 中的边，由于当前算子链的延展已经到头，就会以这些“断点”为起点，继续递归调用 createChain() 方法试图创建新的算子链。也就是说，逻辑计划中整个创建算子链的过程都是递归的，亦即实际返回时，是从 Sink 端开始返回的。</p>
<p>然后要判断当前节点是不是算子链的起始节点。如果是，则调用 createJobVertex()方法为算子链创建一个 JobVertex（ 即 JobGraph 中的节点），也就形成了我们在Web UI 中看到的 JobGraph 效果：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1gkmd8wroh7j20u00ezgp1.jpg" alt="undefined"></p>
<p>最后，还需要将各个节点的算子链数据写入各自的 StreamConfig 中，算子链的起始节点要额外保存下 transitiveOutEdges。StreamConfig 在后文的物理执行阶段会再次用到。</p>
<h2 id="形成算子链的条件"><a href="#形成算子链的条件" class="headerlink" title="形成算子链的条件"></a>形成算子链的条件</h2><p>来看看 isChainable() 方法的代码。 由此可得，上下游算子能够 chain 在一起的条件还是非常苛刻的（老生常谈了），列举如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isChainable</span><span class="params">(StreamEdge edge, StreamGraph streamGraph)</span> </span>&#123;</span><br><span class="line">    StreamNode upStreamVertex = streamGraph.getSourceVertex(edge);</span><br><span class="line">    StreamNode downStreamVertex = streamGraph.getTargetVertex(edge);</span><br><span class="line"></span><br><span class="line">    StreamOperatorFactory&lt;?&gt; headOperator = upStreamVertex.getOperatorFactory();</span><br><span class="line">    StreamOperatorFactory&lt;?&gt; outOperator = downStreamVertex.getOperatorFactory();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> downStreamVertex.getInEdges().size() == <span class="number">1</span></span><br><span class="line">            &amp;&amp; outOperator != <span class="keyword">null</span></span><br><span class="line">            &amp;&amp; headOperator != <span class="keyword">null</span></span><br><span class="line">            &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex)</span><br><span class="line">            &amp;&amp; outOperator.getChainingStrategy() == ChainingStrategy.ALWAYS</span><br><span class="line">            &amp;&amp; (headOperator.getChainingStrategy() == ChainingStrategy.HEAD ||</span><br><span class="line">                headOperator.getChainingStrategy() == ChainingStrategy.ALWAYS)</span><br><span class="line">            &amp;&amp; (edge.getPartitioner() <span class="keyword">instanceof</span> ForwardPartitioner)</span><br><span class="line">            &amp;&amp; edge.getShuffleMode() != ShuffleMode.BATCH</span><br><span class="line">            &amp;&amp; upStreamVertex.getParallelism() == downStreamVertex.getParallelism()</span><br><span class="line">            &amp;&amp; streamGraph.isChainingEnabled();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>上下游算子实例处于同一个 SlotSharingGroup 中（之后再提）；</p>
</li>
<li><p>下游算子的链接策略（ChainingStrategy）为 ALWAYS ——既可以与上游链接，也可以与下游链接。我们常见的 map()、filter() 等都属此类；</p>
</li>
<li><p>上游算子的链接策略为 HEAD 或 ALWAYS。HEAD 策略表示只能与下游链接，这在正常情况下是 Source 算子的专属；</p>
</li>
<li><p>两个算子间的物理分区逻辑是 ForwardPartitioner ，可参见之前写过的《聊聊Flink DataStream 的八种物理分区逻辑》；</p>
</li>
<li><p>两个算子间的 shuffle 方式不是批处理模式；</p>
</li>
<li><p>上下游算子实例的并行度相同；</p>
</li>
<li><p>没有禁用算子链。</p>
</li>
</ul>
<h2 id="禁用算子链"><a href="#禁用算子链" class="headerlink" title="禁用算子链"></a>禁用算子链</h2><p>用户可以在一个算子上调用 startNewChain() 方法强制开始一个新的算子链，或者调用 disableOperatorChaining() 方法指定它不参与算子链。代码位于 SingleOutputStreamOperator 类中，都是通过改变算子的链接策略实现的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@PublicEvolving</span><br><span class="line">public SingleOutputStreamOperator&lt;T&gt; disableChaining() &#123;</span><br><span class="line">    return setChainingStrategy(ChainingStrategy.NEVER);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PublicEvolving</span><br><span class="line">public SingleOutputStreamOperator&lt;T&gt; startNewChain() &#123;</span><br><span class="line">    return setChainingStrategy(ChainingStrategy.HEAD);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果要在整个运行时环境中禁用算子链，调用 StreamExecutionEnvironment.disableOperatorChaining() 方法即可。</p>
<h2 id="物理计划中的算子链"><a href="#物理计划中的算子链" class="headerlink" title="物理计划中的算子链"></a>物理计划中的算子链</h2><p>在 JobGraph 转换成 ExecutionGraph 并交由 TaskManager 执行之后，会生成调度执行的基本任务单元 ——StreamTask，负责执行具体的 StreamOperator 逻辑。在StreamTask.invoke() 方法中，初始化了状态后端、checkpoint 存储和定时器服务之后，可以发现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">operatorChain = new OperatorChain&lt;&gt;(this, recordWriters);</span><br><span class="line">headOperator = operatorChain.getHeadOperator();</span><br></pre></td></tr></table></figure>

<p>构造出了一个 OperatorChain 实例，这就是算子链在实际执行时的形态。解释一下OperatorChain 中的几个主要属性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">private final StreamOperator&lt;?&gt;[] allOperators;</span><br><span class="line">private final RecordWriterOutput&lt;?&gt;[] streamOutputs;</span><br><span class="line">private final WatermarkGaugeExposingOutput&lt;StreamRecord&lt;OUT&gt;&gt; chainEntryPoint;</span><br><span class="line">private final OP headOperator;</span><br></pre></td></tr></table></figure>

<ul>
<li>headOperator：算子链的第一个算子，对应 JobGraph 中的算子链起始节点；</li>
<li>allOperators：算子链中的所有算子，倒序排列，即 headOperator 位于该数组的末尾；</li>
<li>streamOutputs：算子链的输出，可以有多个；</li>
<li>chainEntryPoint：算子链的“入口点”，它的含义将在后文说明。</li>
</ul>
<p>由上可知，所有 StreamTask 都会创建 OperatorChain。如果一个算子无法进入算子链，也会形成一个只有 headOperator 的单个算子的 OperatorChain。</p>
<p>OperatorChain 构造方法中的核心代码如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">for (int i = 0; i &lt; outEdgesInOrder.size(); i++) &#123;</span><br><span class="line">    StreamEdge outEdge = outEdgesInOrder.get(i);</span><br><span class="line">    RecordWriterOutput&lt;?&gt; streamOutput = createStreamOutput(</span><br><span class="line">        recordWriters.get(i),</span><br><span class="line">        outEdge,</span><br><span class="line">        chainedConfigs.get(outEdge.getSourceId()),</span><br><span class="line">        containingTask.getEnvironment());</span><br><span class="line">    this.streamOutputs[i] = streamOutput;</span><br><span class="line">    streamOutputMap.put(outEdge, streamOutput);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// we create the chain of operators and grab the collector that leads into the chain</span><br><span class="line">List&lt;StreamOperator&lt;?&gt;&gt; allOps = new ArrayList&lt;&gt;(chainedConfigs.size());</span><br><span class="line">this.chainEntryPoint = createOutputCollector(</span><br><span class="line">    containingTask,</span><br><span class="line">    configuration,</span><br><span class="line">    chainedConfigs,</span><br><span class="line">    userCodeClassloader,</span><br><span class="line">    streamOutputMap,</span><br><span class="line">    allOps);</span><br><span class="line"></span><br><span class="line">if (operatorFactory != null) &#123;</span><br><span class="line">    WatermarkGaugeExposingOutput&lt;StreamRecord&lt;OUT&gt;&gt; output = getChainEntryPoint();</span><br><span class="line">    headOperator = operatorFactory.createStreamOperator(containingTask, configuration, output);</span><br><span class="line">    headOperator.getMetricGroup().gauge(MetricNames.IO_CURRENT_OUTPUT_WATERMARK, output.getWatermarkGauge());</span><br><span class="line">&#125; else &#123;</span><br><span class="line">    headOperator = null;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// add head operator to end of chain</span><br><span class="line">allOps.add(headOperator);</span><br><span class="line">this.allOperators = allOps.toArray(new StreamOperator&lt;?&gt;[allOps.size()]);</span><br></pre></td></tr></table></figure>

<p>首先会遍历算子链整体的所有出边，并调用 createStreamOutput() 方法创建对应的下游输出 RecordWriterOutput。然后就会调用 createOutputCollector() 方法创建物理的算子链，并返回 chainEntryPoint，这个方法比较重要，部分代码如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">private &lt;T&gt; WatermarkGaugeExposingOutput&lt;StreamRecord&lt;T&gt;&gt; createOutputCollector(</span><br><span class="line">        StreamTask&lt;?, ?&gt; containingTask,</span><br><span class="line">        StreamConfig operatorConfig,</span><br><span class="line">        Map&lt;Integer, StreamConfig&gt; chainedConfigs,</span><br><span class="line">        ClassLoader userCodeClassloader,</span><br><span class="line">        Map&lt;StreamEdge, RecordWriterOutput&lt;?&gt;&gt; streamOutputs,</span><br><span class="line">        List&lt;StreamOperator&lt;?&gt;&gt; allOperators) &#123;</span><br><span class="line">    List&lt;Tuple2&lt;WatermarkGaugeExposingOutput&lt;StreamRecord&lt;T&gt;&gt;, StreamEdge&gt;&gt; allOutputs = new ArrayList&lt;&gt;(4);</span><br><span class="line"></span><br><span class="line">    // create collectors for the network outputs</span><br><span class="line">    for (StreamEdge outputEdge : operatorConfig.getNonChainedOutputs(userCodeClassloader)) &#123;</span><br><span class="line">        @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">        RecordWriterOutput&lt;T&gt; output = (RecordWriterOutput&lt;T&gt;) streamOutputs.get(outputEdge);</span><br><span class="line">        allOutputs.add(new Tuple2&lt;&gt;(output, outputEdge));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Create collectors for the chained outputs</span><br><span class="line">    for (StreamEdge outputEdge : operatorConfig.getChainedOutputs(userCodeClassloader)) &#123;</span><br><span class="line">        int outputId = outputEdge.getTargetId();</span><br><span class="line">        StreamConfig chainedOpConfig = chainedConfigs.get(outputId);</span><br><span class="line">        WatermarkGaugeExposingOutput&lt;StreamRecord&lt;T&gt;&gt; output = createChainedOperator(</span><br><span class="line">            containingTask,</span><br><span class="line">            chainedOpConfig,</span><br><span class="line">            chainedConfigs,</span><br><span class="line">            userCodeClassloader,</span><br><span class="line">            streamOutputs,</span><br><span class="line">            allOperators,</span><br><span class="line">            outputEdge.getOutputTag());</span><br><span class="line">        allOutputs.add(new Tuple2&lt;&gt;(output, outputEdge));</span><br><span class="line">    &#125;</span><br><span class="line">    // 以下略......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该方法从上一节提到的 StreamConfig 中分别取出出边和链接边的数据，并创建各自的 Output。出边的 Output 就是将数据发往算子链之外下游的 RecordWriterOutput，而链接边的输出要靠 createChainedOperator() 方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">private &lt;IN, OUT&gt; WatermarkGaugeExposingOutput&lt;StreamRecord&lt;IN&gt;&gt; createChainedOperator(</span><br><span class="line">        StreamTask&lt;?, ?&gt; containingTask,</span><br><span class="line">        StreamConfig operatorConfig,</span><br><span class="line">        Map&lt;Integer, StreamConfig&gt; chainedConfigs,</span><br><span class="line">        ClassLoader userCodeClassloader,</span><br><span class="line">        Map&lt;StreamEdge, RecordWriterOutput&lt;?&gt;&gt; streamOutputs,</span><br><span class="line">        List&lt;StreamOperator&lt;?&gt;&gt; allOperators,</span><br><span class="line">        OutputTag&lt;IN&gt; outputTag) &#123;</span><br><span class="line">    // create the output that the operator writes to first. this may recursively create more operators</span><br><span class="line">    WatermarkGaugeExposingOutput&lt;StreamRecord&lt;OUT&gt;&gt; chainedOperatorOutput = createOutputCollector(</span><br><span class="line">        containingTask,</span><br><span class="line">        operatorConfig,</span><br><span class="line">        chainedConfigs,</span><br><span class="line">        userCodeClassloader,</span><br><span class="line">        streamOutputs,</span><br><span class="line">        allOperators);</span><br><span class="line"></span><br><span class="line">    // now create the operator and give it the output collector to write its output to</span><br><span class="line">    StreamOperatorFactory&lt;OUT&gt; chainedOperatorFactory = operatorConfig.getStreamOperatorFactory(userCodeClassloader);</span><br><span class="line">    OneInputStreamOperator&lt;IN, OUT&gt; chainedOperator = chainedOperatorFactory.createStreamOperator(</span><br><span class="line">            containingTask, operatorConfig, chainedOperatorOutput);</span><br><span class="line"></span><br><span class="line">    allOperators.add(chainedOperator);</span><br><span class="line"></span><br><span class="line">    WatermarkGaugeExposingOutput&lt;StreamRecord&lt;IN&gt;&gt; currentOperatorOutput;</span><br><span class="line">    if (containingTask.getExecutionConfig().isObjectReuseEnabled()) &#123;</span><br><span class="line">        currentOperatorOutput = new ChainingOutput&lt;&gt;(chainedOperator, this, outputTag);</span><br><span class="line">    &#125;</span><br><span class="line">    else &#123;</span><br><span class="line">        TypeSerializer&lt;IN&gt; inSerializer = operatorConfig.getTypeSerializerIn1(userCodeClassloader);</span><br><span class="line">        currentOperatorOutput = new CopyingChainingOutput&lt;&gt;(chainedOperator, inSerializer, outputTag, this);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // wrap watermark gauges since registered metrics must be unique</span><br><span class="line">    chainedOperator.getMetricGroup().gauge(MetricNames.IO_CURRENT_INPUT_WATERMARK, currentOperatorOutput.getWatermarkGauge()::getValue);</span><br><span class="line">    chainedOperator.getMetricGroup().gauge(MetricNames.IO_CURRENT_OUTPUT_WATERMARK, chainedOperatorOutput.getWatermarkGauge()::getValue);</span><br><span class="line">    return currentOperatorOutput;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们一眼就可以看到，这个方法递归调用了上述 createOutputCollector() 方法，与逻辑计划阶段类似，通过不断延伸 Output 来产生 chainedOperator（即算子链中除了headOperator 之外的算子），并逆序返回，这也是 allOperators 数组中的算子顺序为倒序的原因。</p>
<p>chainedOperator 产生之后，将它们通过 ChainingOutput 连接起来，形成如下图所示的结构。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1gkmdb2gkhoj20u00fjwgy.jpg" alt="undefined"></p>
<p>最后来看看 ChainingOutput.collect() 方法是如何输出数据流的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void collect(StreamRecord&lt;T&gt; record) &#123;</span><br><span class="line">    if (this.outputTag != null) &#123;</span><br><span class="line">        // we are only responsible for emitting to the main input</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    pushToOperator(record);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public &lt;X&gt; void collect(OutputTag&lt;X&gt; outputTag, StreamRecord&lt;X&gt; record) &#123;</span><br><span class="line">    if (this.outputTag == null || !this.outputTag.equals(outputTag)) &#123;</span><br><span class="line">        // we are only responsible for emitting to the side-output specified by our</span><br><span class="line">        // OutputTag.</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    pushToOperator(record);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">protected &lt;X&gt; void pushToOperator(StreamRecord&lt;X&gt; record) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        // we know that the given outputTag matches our OutputTag so the record</span><br><span class="line">        // must be of the type that our operator expects.</span><br><span class="line">        @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">        StreamRecord&lt;T&gt; castRecord = (StreamRecord&lt;T&gt;) record;</span><br><span class="line">        numRecordsIn.inc();</span><br><span class="line">        operator.setKeyContextElement1(castRecord);</span><br><span class="line">        operator.processElement(castRecord);</span><br><span class="line">    &#125;</span><br><span class="line">    catch (Exception e) &#123;</span><br><span class="line">        throw new ExceptionInChainedOperatorException(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可见是通过调用链接算子的 processElement() 方法，直接将数据推给下游处理了。也就是说，OperatorChain 完全可以看做一个由 headOperator 和 streamOutputs组成的单个算子，其内部的 chainedOperator 和 ChainingOutput 都像是被黑盒遮蔽，同时没有引入任何 overhead。</p>
<p>打通了算子链在执行层的逻辑，看官应该会明白 chainEntryPoint 的含义了。由于它位于递归返回的终点，所以它就是流入算子链的起始 Output，即上图中指向 headOperator 的 RecordWriterOutput。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Apache-Flink/" rel="tag"># Apache Flink</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/10/13/Drools/" rel="prev" title="Drools">
      <i class="fa fa-chevron-left"></i> Drools
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/10/21/Docker/" rel="next" title="Docker">
      Docker <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80Mzk4NC8yMDUyMA=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink的应用场景"><span class="nav-number">1.</span> <span class="nav-text">Flink的应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#事件驱动型应用场景"><span class="nav-number">1.1.</span> <span class="nav-text">事件驱动型应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#社交领域"><span class="nav-number">1.1.1.</span> <span class="nav-text">社交领域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#电商"><span class="nav-number">1.1.2.</span> <span class="nav-text">电商</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#金融行业"><span class="nav-number">1.1.3.</span> <span class="nav-text">金融行业</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据分析型应用"><span class="nav-number">1.2.</span> <span class="nav-text">数据分析型应用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据管道型应用"><span class="nav-number">2.</span> <span class="nav-text">数据管道型应用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink-原理与实现：内存管理"><span class="nav-number">3.</span> <span class="nav-text">Flink 原理与实现：内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#积极的内存管理"><span class="nav-number">3.1.</span> <span class="nav-text">积极的内存管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为-Flink-量身定制的序列化框架"><span class="nav-number">3.2.</span> <span class="nav-text">为 Flink 量身定制的序列化框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flink-如何直接操作二进制数据"><span class="nav-number">3.3.</span> <span class="nav-text">Flink 如何直接操作二进制数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#缓存友好的数据结构和算法"><span class="nav-number">3.4.</span> <span class="nav-text">缓存友好的数据结构和算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#走向堆外内存"><span class="nav-number">3.5.</span> <span class="nav-text">走向堆外内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">3.6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">3.7.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink-内存设置思路"><span class="nav-number">4.</span> <span class="nav-text">Flink 内存设置思路</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Flink-lt-1-9"><span class="nav-number">4.1.</span> <span class="nav-text">Flink &lt;= 1.9</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#计算过程"><span class="nav-number">4.1.1.</span> <span class="nav-text">计算过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flink-gt-1-10"><span class="nav-number">4.2.</span> <span class="nav-text">Flink &gt;= 1.10</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#全新内存划分"><span class="nav-number">4.2.1.</span> <span class="nav-text">全新内存划分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-1"><span class="nav-number">4.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#多流合并"><span class="nav-number">5.</span> <span class="nav-text">多流合并</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink-的算子链机制"><span class="nav-number">6.</span> <span class="nav-text">Flink 的算子链机制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑计划中的算子链"><span class="nav-number">6.1.</span> <span class="nav-text">逻辑计划中的算子链</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#形成算子链的条件"><span class="nav-number">6.2.</span> <span class="nav-text">形成算子链的条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#禁用算子链"><span class="nav-number">6.3.</span> <span class="nav-text">禁用算子链</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#物理计划中的算子链"><span class="nav-number">6.4.</span> <span class="nav-text">物理计划中的算子链</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fly Hugh"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Fly Hugh</p>
  <div class="site-description" itemprop="description">WE CHOOSE TO  GO TO THE MARS</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">67</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/FlyMeToTheMars" title="GitHub → https://github.com/FlyMeToTheMars" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/flyhobo@live.com" title="E-Mail → flyhobo@live.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/3200892914" title="Weibo → https://weibo.com/u/3200892914" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Fly__HoBo" title="Twitter → https://twitter.com/Fly__HoBo" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fly Hugh</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
