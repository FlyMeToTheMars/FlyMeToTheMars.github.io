<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mars</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-05-07T03:56:58.949Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Fly Hugh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink Table &amp; SQL</title>
    <link href="http://yoursite.com/2020/05/07/Flink%20Table%20&amp;%20SQL%20%E6%A6%82%E8%A7%88/"/>
    <id>http://yoursite.com/2020/05/07/Flink Table &amp; SQL 概览/</id>
    <published>2020-05-07T02:43:53.432Z</published>
    <updated>2020-05-07T03:56:58.949Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Flink Table 和 SQL 整体的脉络</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gejppurwd5j20ib046t9h.jpg" alt="3.png"></p><a id="more"></a> <h1 id="Flink-Table-amp-SQL"><a href="#Flink-Table-amp-SQL" class="headerlink" title="Flink Table &amp; SQL"></a>Flink Table &amp; SQL</h1><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>依赖没啥好说的，要想一下的是Zeppelin是否需要手动把这些依赖全都加上去</p><h2 id="两种Planner的区别"><a href="#两种Planner的区别" class="headerlink" title="两种Planner的区别"></a>两种Planner的区别</h2><ul><li>最大区别 流批一体，blink不支持和dataset之间的转换了</li><li>取消了BatchTableSource，使用有界的StreamTableSource</li><li>Blink只支持全新的catalog，旧的ExternalCatalog不再支持</li><li>基于字符串的键值配置选项仅适用于Blink planner</li><li>PlannerConfig在两个planner中的实现不同</li><li>Blink planner会将多个sink优化在一个DAG中（仅在TableEnvironment上受支持，而在StreamTableEnvironment上不受支持）。而旧planner的优化总是将每一个sink放在一个新的DAG中，其中所有DAG彼此独立</li><li>旧的planner不支持目录统计，而Blink planner支持‘’</li></ul><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><h3 id="基本程序结构"><a href="#基本程序结构" class="headerlink" title="基本程序结构"></a>基本程序结构</h3><p>Table API 和 SQL 的程序结构，与流式处理的程序结构类似；也可以近似地认为有这么几步：首先创建执行环境，然后定义source、transform和sink</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tableEnv = ...     <span class="comment">// 创建表的执行环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一张表，用于读取数据</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"inputTable"</span>)</span><br><span class="line"><span class="comment">// 注册一张表，用于把计算结果输出</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"outputTable"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 Table API 查询算子，得到一张结果表</span></span><br><span class="line"><span class="keyword">val</span> result = tableEnv.from(<span class="string">"inputTable"</span>).select(...)</span><br><span class="line"><span class="comment">// 通过 SQL查询语句，得到一张结果表</span></span><br><span class="line"><span class="keyword">val</span> sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM inputTable ..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果表写入输出表中</span></span><br><span class="line">result.insertInto(<span class="string">"outputTable"</span>)</span><br></pre></td></tr></table></figure><h3 id="创建表环境"><a href="#创建表环境" class="headerlink" title="创建表环境"></a>创建表环境</h3><p>创建表环境最简单的方式，就是基于流处理执行环境，调create方法直接创建:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(env)</span><br></pre></td></tr></table></figure><p>表环境（TableEnvironment）是flink中集成Table API &amp; SQL的核心概念。它负责:</p><ul><li>注册catalog</li><li>在内部 catalog 中注册表</li><li>执行SQL查询</li><li>注册用户自定义函数</li><li>将DataStream或DataSet转换成表</li><li>保存对ExecutionEnvironment或者StreamExecutionEnvironment的引用</li></ul><p>在创建TableEnv的时候，可以多传入一个EnvironmentSettings 或者 TableConfig 参数，可以用来配置tEnv的一些特性</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">  .useOldPlanner()      <span class="comment">// 使用老版本planner</span></span><br><span class="line">  .inStreamingMode()    <span class="comment">// 流处理模式</span></span><br><span class="line">  .build()</span><br><span class="line"><span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br></pre></td></tr></table></figure><p>基于老版本的批处理环境（Flink-Batch-Query）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> batchEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> batchTableEnv = <span class="type">BatchTableEnvironment</span>.create(batchEnv)</span><br></pre></td></tr></table></figure><p>基于blink版本的流处理环境(Blink-Streaming-Query):</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">.useBlinkPlanner()</span><br><span class="line">.inStreamingMode().build()</span><br><span class="line"><span class="keyword">val</span> bsTableEnv = <span class="type">StreamTableEnvironment</span>.create(env, bsSettings)</span><br></pre></td></tr></table></figure><p>基于Blink版本的批处理环境(Blink-Batch-Query)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> bbSettings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">.useBlinkPlanner()</span><br><span class="line">.inBatchMode().build()</span><br><span class="line"><span class="keyword">val</span> bbTableEnv = <span class="type">TableEnvironment</span>.create(bbSettings)</span><br></pre></td></tr></table></figure><h3 id="注册表"><a href="#注册表" class="headerlink" title="注册表"></a>注册表</h3><p>TableEnvironment可以注册目录Catalog，并可以基于Catalog注册表。它会维护一个Catalog-Table表之间的map。</p><p>表（Table）是由一个“标识符”来指定的，由3部分组成：Catalog名、数据库（database）名和对象名（表名）。如果没有指定目录或数据库，就使用当前的默认值。</p><p>表可以是常规的（Table，表），或者虚拟的（View，视图）。常规表（Table）一般可以用来描述外部数据，比如文件、数据库表或消息队列的数据，也可以直接从 DataStream转换而来。视图可以从现有的表中创建，通常是table API或者SQL查询的一个结果。</p><h3 id="连接到文件系统（CSV）"><a href="#连接到文件系统（CSV）" class="headerlink" title="连接到文件系统（CSV）"></a>连接到文件系统（CSV）</h3><p>连接外部系统在Catalog中注册表，直接调用tableEnv.connect()就可以，里面参数要传入一个ConnectorDescriptor，也就是connector描述器。对于文件系统的connector而言，flink内部已经提供了，就叫做FileSystem()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tableEnv</span><br><span class="line">.connect( <span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"sensor.txt"</span>))  <span class="comment">// 定义表数据来源，外部连接</span></span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">OldCsv</span>())    <span class="comment">// 定义从外部系统读取数据之后的格式化方法</span></span><br><span class="line">  .withSchema( <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">    .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">    .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">    .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  )    <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"inputTable"</span>)    <span class="comment">// 创建临时表</span></span><br></pre></td></tr></table></figure><p>这是旧版本的csv格式描述器。由于它是非标的，跟外部系统对接并不通用，所以将被弃用，以后会被一个符合RFC-4180标准的新format描述器取代。新的描述器就叫Csv()，但flink没有直接提供，需要引入依赖flink-csv：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>代码非常类似，只需要把withFormat里的OldCsv改成Csv就可以了。</p><h3 id="连接到Kafka"><a href="#连接到Kafka" class="headerlink" title="连接到Kafka"></a>连接到Kafka</h3><p>kafka的连接器flink-kafka-connector中，1.10版本的已经提供了Table API的支持。我们可以在 connect方法中直接传入一个叫做Kafka的类，这就是kafka连接器的描述器ConnectorDescriptor</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Kafka</span>()</span><br><span class="line">    .version(<span class="string">"0.11"</span>) <span class="comment">// 定义kafka的版本</span></span><br><span class="line">    .topic(<span class="string">"sensor"</span>) <span class="comment">// 定义主题</span></span><br><span class="line">    .property(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>) </span><br><span class="line">    .property(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">)</span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>())</span><br><span class="line">  .withSchema(<span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">  .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">  .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">  .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">)</span><br><span class="line">  .createTemporaryTable(<span class="string">"kafkaInputTable"</span>)</span><br></pre></td></tr></table></figure><p>当然也可以连接到ElasticSearch、MySql、HBase、Hive等外部系统，实现方式基本上是类似的。</p><h3 id="表的查询"><a href="#表的查询" class="headerlink" title="表的查询"></a>表的查询</h3><p>利用外部系统的连接器connector，我们可以读写数据，并在环境的Catalog中注册表。接下来就可以对表做查询转换了。</p><p>Flink给我们提供了两种查询方式：Table API和 SQL。</p><h3 id="TableAPI的调用"><a href="#TableAPI的调用" class="headerlink" title="TableAPI的调用"></a>TableAPI的调用</h3><p>Table API是集成在Scala和Java语言内的查询API。与SQL不同，Table API的查询不会用字符串表示，而是在宿主语言中一步一步调用完成的。</p><p>Table API基于代表一张“表”的Table类，并提供一整套操作处理的方法API。这些方法会返回一个新的Table对象，这个对象就表示对输入表应用转换操作的结果。有些关系型转换操作，可以由多个方法调用组成，构成链式调用结构。例如table.select(…).filter(…)，其中select（…）表示选择表中指定的字段，filter(…)表示筛选条件。</p><p>代码中的实现如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorTable: <span class="type">Table</span> = tableEnv.from(<span class="string">"inputTable"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultTable: <span class="type">Table</span> = senorTable</span><br><span class="line">.select(<span class="string">"id, temperature"</span>)</span><br><span class="line">.filter(<span class="string">"id ='sensor_1'"</span>)</span><br></pre></td></tr></table></figure><h3 id="SQL查询"><a href="#SQL查询" class="headerlink" title="SQL查询"></a>SQL查询</h3><p>Flink的SQL集成，基于的是<code>ApacheCalcite</code>，</p><p>它实现了SQL标准。</p><p>在Flink中，用常规字符串来定义SQL查询语句。</p><p>SQL 查询的结果，是一个新的 Table。</p><p>代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv.sqlQuery(<span class="string">"select id, temperature from inputTable where id ='sensor_1'"</span>)</span><br></pre></td></tr></table></figure><p>或者：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |select id, temperature</span></span><br><span class="line"><span class="string">    |from inputTable</span></span><br><span class="line"><span class="string">    |where id = 'sensor_1'</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure><p>当然，也可以加上聚合操作，比如我们统计每个sensor温度数据出现的个数，做个count统计：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val aggResultTable = sensorTable</span><br><span class="line">    .groupBy(&apos;id)</span><br><span class="line">    .select(&apos;id, &apos;id.count as &apos;count)</span><br></pre></td></tr></table></figure><p>SQL:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> aggResultSqlTable = tableEnv.sqlQuery(<span class="string">"select id, count(id) as cnt from inputTable group by id"</span>)</span><br></pre></td></tr></table></figure><p>这里Table API里指定的字段，前面加了一个单引号’，这是Table API中定义的Expression类型的写法，可以很方便地表示一个表中的字段。</p><p>字段可以直接全部用双引号引起来，也可以用半边单引号+字段名的方式。以后的代码中，一般都用后一种形式。</p><h3 id="将DataStream转换成表"><a href="#将DataStream转换成表" class="headerlink" title="将DataStream转换成表"></a>将DataStream转换成表</h3><p>Flink允许我们把Table和DataStream做转换：</p><p>我们可以基于一个DataStream，</p><p>先流式地读取数据源，</p><p>然后map成样例类，</p><p>再把它转成Table。</p><p>Table的列字段（column fields），</p><p>就是样例类里的字段，</p><p>这样就不用再麻烦地定义schema了。</p><p><strong>Code</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val inputStream: DataStream[String] = env.readTextFile(&quot;sensor.txt&quot;)</span><br><span class="line">val dataStream: DataStream[SensorReading] = inputStream</span><br><span class="line">  .map(data =&gt; &#123;</span><br><span class="line">    val dataArray = data.split(&quot;,&quot;)</span><br><span class="line">    SensorReading(dataArray(0), dataArray(1).toLong, dataArray(2).toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">val sensorTable: Table = tableEnv.fromDataStream(dataStream)</span><br><span class="line"></span><br><span class="line">val sensorTable2 = tableEnv.fromDataStream(dataStream, &apos;id, &apos;timestamp as &apos;ts)</span><br></pre></td></tr></table></figure><h3 id="数据类型与-Table-schema的对应"><a href="#数据类型与-Table-schema的对应" class="headerlink" title="数据类型与 Table schema的对应"></a>数据类型与 Table schema的对应</h3><p>在上面的例子中，DataStream 中的数据类型，与表的 Schema 之间的对应关系，是按照样例类中的字段名来对应的（name-based mapping），所以还可以用as做重命名。</p><p>另外一种对应方式是，直接按照字段的位置来对应（position-based mapping），对应的过程中，就可以直接指定新的字段名了。</p><p><strong>基于名称的对应</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorTable = tableEnv.fromDataStream(dataStream, <span class="symbol">'timestamp</span> as <span class="symbol">'ts</span>, <span class="symbol">'id</span> as <span class="symbol">'myId</span>, <span class="symbol">'temperature</span>)</span><br></pre></td></tr></table></figure><p><strong>基于位置的对应</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sensorTable = tableEnv.fromDataStream(dataStream, <span class="symbol">'myId</span>, <span class="symbol">'ts</span>)</span><br></pre></td></tr></table></figure><p>Flink的DataStream和DataSet API支持多种类型。</p><p>组合类型，比如元组（内置Scala和Java元组）、POJO、Scala case类和Flink的Row类型等，允许具有多个字段的嵌套数据结构，这些字段可以在Table的表达式中访问。其他类型，则被视为原子类型。</p><p>元组类型和原子类型，一般用位置对应会好一些；如果非要用名称对应，也是可以的：</p><p>元组类型，默认的名称是 “_1”, “_2”；而原子类型，默认名称是 ”f0”。</p><h3 id="创建临时视图（Temporary-View）"><a href="#创建临时视图（Temporary-View）" class="headerlink" title="创建临时视图（Temporary View）"></a>创建临时视图（Temporary View）</h3><p>创建临时视图的第一种方式，就是直接从DataStream传唤而来。同样，可以直接对应字段转换；也可以在转换的时候，指定相应的字段。</p><p><strong>Code</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">"sensorView"</span>, dataStream)</span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"sensorView"</span>, dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span> as <span class="symbol">'ts</span>)</span><br></pre></td></tr></table></figure><p>另外，当然还可以基于Table创建视图：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">"sensorView"</span>, sensorTable)</span><br></pre></td></tr></table></figure><p>View和Table的Schema完全相同。事实上，在Table API中，可以认为View和Table是等价的。</p><h3 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h3><p>表的输出，是通过将数据写入 TableSink 来实现的。TableSink 是一个通用接口，可以支持不同的文件格式、存储数据库和消息队列。</p><p>具体实现，输出表最直接的方法，就是通过 Table.insertInto() 方法将一个 Table 写入注册过的 TableSink 中。</p><p>输出到文件</p><p><strong>Code</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注册输出表</span></span><br><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"…\\resources\\out.txt"</span>)</span><br><span class="line">) <span class="comment">// 定义到文件系统的连接</span></span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>()) <span class="comment">// 定义格式化方法，Csv格式</span></span><br><span class="line">  .withSchema(<span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">  .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">  .field(<span class="string">"temp"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">) <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"outputTable"</span>) <span class="comment">// 创建临时表</span></span><br><span class="line"></span><br><span class="line">resultSqlTable.insertInto(<span class="string">"outputTable"</span>)</span><br></pre></td></tr></table></figure><h3 id="更新模式"><a href="#更新模式" class="headerlink" title="更新模式"></a>更新模式</h3><p>在流处理过程中，表的处理并不像传统定义的那样简单。</p><p>对于流式查询（Streaming Queries），需要声明如何在（动态）表和外部连接器之间执行转换。与外部系统交换的消息类型，由<strong>更新模式</strong>（update mode）指定。</p><p>Flink Table API中的更新模式有以下三种：</p><p><strong>1) 追加模式 Append Mode</strong></p><p>在追加模式下，表（动态表）和外部连接器只插入（Insert）消息。</p><p><strong>2)撤回模式 Retract Mode</strong></p><p>撤回模式下，表和外部连接器交换的是：添加ADD 和撤回Retract 消息。</p><p>插入（Insert）会被编码为添加消息。</p><p>删除（Delete）则编码为撤回消息。</p><p>更新（Update）则会编码为。已更新行（上一行）的撤回消息，和更新行（新行）的添加消息。</p><p>从模式下，不能定义key，这一点跟upsert模式完全不同。</p><p><strong>3)更新插入模式 Upsert</strong></p><p>在Upsert模式下，动态表和外部连接器交换Upsert和Delete消息。</p><p>这个模式需要一个唯一的key，通过这个key可以传递更新消息。为了正确应用消息，外部连接器需要知道这个唯一key的属性。</p><ul><li><p>插入（Insert）和更新（Update）都被编码为Upsert消息；</p></li><li><p>删除（Delete）编码为Delete信息。</p></li></ul><p>这种模式和Retract模式的主要区别在于，Update操作是用单个消息编码的，所以效率会更高。</p><h3 id="输出到Kafka"><a href="#输出到Kafka" class="headerlink" title="输出到Kafka"></a>输出到Kafka</h3><p>除了输出到文件，也可以输出到Kafka。我们可以结合前面Kafka作为输入数据，构建数据管道，kafka进，kafka出</p><p><strong>Code</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Kafka</span>()</span><br><span class="line">    .version(<span class="string">"0.11"</span>)</span><br><span class="line">    .topic(<span class="string">"sinkTest"</span>)</span><br><span class="line">    .property(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>)</span><br><span class="line">    .property(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">)</span><br><span class="line">  .withFormat( <span class="keyword">new</span> <span class="type">Csv</span>() )</span><br><span class="line">  .withSchema( <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">    .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">    .field(<span class="string">"temp"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  )</span><br><span class="line">  .createTemporaryTable(<span class="string">"kafkaOutputTable"</span>)</span><br><span class="line"></span><br><span class="line">resultTable.insertInto(<span class="string">"kafkaOutputTable"</span>)</span><br></pre></td></tr></table></figure><h3 id="输出到ES"><a href="#输出到ES" class="headerlink" title="输出到ES"></a>输出到ES</h3><p>ElasticSearch的connector可以在upsert（update+insert，更新插入）模式下操作，这样就可以使用Query定义的键（key）与外部系统交换UPSERT/DELETE消息。</p><p>另外，对于“仅追加”（append-only）的查询，connector还可以在append 模式下操作，这样就可以与外部系统只交换insert消息。</p><p>es目前支持的数据格式，只有Json，而flink本身并没有对应的支持，所以还需要引入依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-json&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p><strong>Code</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出到es</span></span><br><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Elasticsearch</span>()</span><br><span class="line">    .version(<span class="string">"6"</span>)</span><br><span class="line">    .host(<span class="string">"localhost"</span>, <span class="number">9200</span>, <span class="string">"http"</span>)</span><br><span class="line">    .index(<span class="string">"sensor"</span>)</span><br><span class="line">    .documentType(<span class="string">"temp"</span>)</span><br><span class="line">)</span><br><span class="line">  .inUpsertMode()           <span class="comment">// 指定是 Upsert 模式</span></span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Json</span>())</span><br><span class="line">  .withSchema( <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">    .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">    .field(<span class="string">"count"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">  )</span><br><span class="line">  .createTemporaryTable(<span class="string">"esOutputTable"</span>)</span><br><span class="line"></span><br><span class="line">aggResultTable.insertInto(<span class="string">"esOutputTable"</span>)</span><br></pre></td></tr></table></figure><h3 id="输出到MySQL"><a href="#输出到MySQL" class="headerlink" title="输出到MySQL"></a>输出到MySQL</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-jdbc_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">1.10</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>jdbc连接的代码实现比较特殊，因为没有对应的java/scala类实现ConnectorDescriptor，所以不能直接tableEnv.connect()。不过Flink SQL留下了执行DDL的接口：tableEnv.sqlUpdate()。</p><p>对于jdbc的创建表操作，天生就适合直接写DDL来实现，所以我们的代码可以这样写：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出到 Mysql</span></span><br><span class="line"><span class="keyword">val</span> sinkDDL: <span class="type">String</span> =</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |create table jdbcOutputTable (</span></span><br><span class="line"><span class="string">    |  id varchar(20) not null,</span></span><br><span class="line"><span class="string">    |  cnt bigint not null</span></span><br><span class="line"><span class="string">    |) with (</span></span><br><span class="line"><span class="string">    |  'connector.type' = 'jdbc',</span></span><br><span class="line"><span class="string">    |  'connector.url' = 'jdbc:mysql://localhost:3306/test',</span></span><br><span class="line"><span class="string">    |  'connector.table' = 'sensor_count',</span></span><br><span class="line"><span class="string">    |  'connector.driver' = 'com.mysql.jdbc.Driver',</span></span><br><span class="line"><span class="string">    |  'connector.username' = 'root',</span></span><br><span class="line"><span class="string">    |  'connector.password' = '123456'</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL)</span><br><span class="line">aggResultSqlTable.insertInto(<span class="string">"jdbcOutputTable"</span>)</span><br></pre></td></tr></table></figure><h3 id="Table转换为DataStream"><a href="#Table转换为DataStream" class="headerlink" title="Table转换为DataStream"></a>Table转换为DataStream</h3><p>表可以转换为DataStream或DataSet。这样，自定义流处理或批处理程序就可以继续在 Table API或SQL查询的结果上运行了。</p><p>将表转换为DataStream或DataSet时，需要指定生成的数据类型，即要将表的每一行转换成的数据类型。通常，最方便的转换类型就是Row。当然，因为结果的所有字段类型都是明确的，我们也经常会用元组类型来表示。</p><p>表作为流式查询的结果，是动态更新的。</p><p>所以，将这种动态查询转换成的数据流，同样需要对表的更新操作进行编码，</p><p>进而有不同的转换模式。</p><p>Table API中表到DataStream有两种模式</p><ul><li>追加 Append Mode</li></ul><p>用于表只会被插入（Insert）操作更改的场景。</p><ul><li>撤回 RetractMode</li></ul><p>得到的数据会增加一个Boolean类型的标识位（返回的第一个字段），用它来表示到底是新增的数据（Insert），还是被删除的数据（老数据， Delete）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resultStream: <span class="type">DataStream</span>[<span class="type">Row</span>] = tableEnv.toAppendStream[<span class="type">Row</span>](resultTable)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> aggResultStream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = </span><br><span class="line">tableEnv.toRetractStream[(<span class="type">String</span>, <span class="type">Long</span>)](aggResultTable)</span><br><span class="line"></span><br><span class="line">resultStream.print(<span class="string">"result"</span>)</span><br><span class="line">aggResultStream.print(<span class="string">"aggResult"</span>)</span><br></pre></td></tr></table></figure><p>所以，没有经过groupby之类聚合操作，可以直接用 toAppendStream 来转换；而如果经过了聚合，有更新操作，一般就必须用 toRetractDstream。</p><h3 id="Query的解释和执行"><a href="#Query的解释和执行" class="headerlink" title="Query的解释和执行"></a>Query的解释和执行</h3><p>Table API提供了一种机制来解释（Explain）计算表的逻辑和优化查询计划。这是通过TableEnvironment.explain（table）方法或TableEnvironment.explain（）方法完成的</p><p>explain方法会返回一个字符串，描述三个计划：</p><ul><li><p>未优化的逻辑查询计划</p></li><li><p>优化后的逻辑查询计划</p></li><li><p>实际执行计划</p></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> explaination: <span class="type">String</span> = tableEnv.explain(resultTable)</span><br><span class="line">println(explaination)</span><br></pre></td></tr></table></figure><p>Query的解释和执行过程，老planner和blink planner大体是一致的，又有所不同。整体来讲，Query都会表示成一个逻辑查询计划，然后分两步解释：</p><p>1.优化查询计划</p><p>2.解释成DataStream或者DataSet程序</p><p>而Blink版本是批流统一的，所以所有的Query，只会被解释成DataStream程序；另外在批处理环境TableEnvironment下，Blink版本要到tableEnv.execute()执行调用才开始解释。</p><h2 id="流处理中的特殊概念"><a href="#流处理中的特殊概念" class="headerlink" title="流处理中的特殊概念"></a>流处理中的特殊概念</h2><p>Table API和SQL，本质上还是基于关系型表的操作方式；而关系型表、关系代数，以及SQL本身，一般是有界的，更适合批处理的场景。这就导致在进行流处理的过程中，理解会稍微复杂一些，需要引入一些特殊概念。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1geby8il5f3j20gj08g0sx.jpg" alt="1.png"></p><p>可以看到，其实关系代数（主要就是指关系型数据库中的表）和SQL，主要就是针对批处理的，这和流处理有天生的隔阂。</p><h3 id="Dynamic-Tables"><a href="#Dynamic-Tables" class="headerlink" title="Dynamic Tables"></a>Dynamic Tables</h3><p>因为流处理面对的数据，是连续不断的，这和我们熟悉的关系型数据库中保存的“表”完全不同。所以，如果我们把流数据转换成Table，然后执行类似于table的select操作，结果就不是一成不变的，而是随着新数据的到来，会不停更新。</p><p>我们可以随着新数据的到来，不停地在之前的基础上更新结果。这样得到的表，在Flink Table API概念里，就叫做“<strong>动态表</strong>”（Dynamic Tables）。</p><p>动态表是Flink对流数据的Table API和SQL支持的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。动态表可以像静态的批处理表一样进行查询，查询一个动态表会产生持续查询（Continuous Query）。连续查询永远不会终止，并会生成另一个动态表。查询（Query）会不断更新其动态结果表，以反映其动态输入表上的更改。</p><h3 id="流式持续查询的过程"><a href="#流式持续查询的过程" class="headerlink" title="流式持续查询的过程"></a>流式持续查询的过程</h3><p>下图显示了流、动态表和连续查询的关系：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gebyby4r2yj20ji035jt2.jpg" alt="2.png"></p><p>流式持续查询的过程为：</p><ol><li><p>流被转换为动态表。</p></li><li><p>对动态表计算连续查询，生成新的动态表。</p></li><li><p>生成的动态表被转换回流。</p></li></ol><h3 id="将流转换成表（Table）"><a href="#将流转换成表（Table）" class="headerlink" title="将流转换成表（Table）"></a>将流转换成表（Table）</h3><p>为了处理带有关系查询的流，必须先将其转换为表。</p><p>从概念上讲，流的每个数据记录，都被解释为对结果表的插入（Insert）修改。因为流式持续不断的，而且之前的输出结果无法改变。本质上，我们其实是从一个、只有插入操作的changelog（更新日志）流，来构建一个表。</p><p>为了更好地说明动态表和持续查询的概念，我们来举一个具体的例子。</p><p>比如，我们现在的输入数据，就是用户在网站上的访问行为，数据类型（Schema）如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  user:  VARCHAR,   // 用户名</span><br><span class="line">  cTime: TIMESTAMP, // 访问某个URL的时间戳</span><br><span class="line">  url:   VARCHAR    // 用户访问的URL</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>下图显示了如何将访问URL事件流，或者叫点击事件流（左侧）转换为表（右侧）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gebyly5qi6j20ex05e74v.jpg" alt="3.png"></p><p>随着插入更多的访问事件流记录，生成的表将不断增长。</p><h3 id="持续查询（Continuous-Query）"><a href="#持续查询（Continuous-Query）" class="headerlink" title="持续查询（Continuous Query）"></a>持续查询（Continuous Query）</h3><p>持续查询，会在动态表上做计算处理，并作为结果生成新的动态表。与批处理查询不同，连续查询从不终止，并根据输入表上的更新更新其结果表。</p><p>在任何时间点，连续查询的结果在语义上，等同于在输入表的快照上，以批处理模式执行的同一查询的结果。</p><p>在下面的示例中，我们展示了对点击事件流中的一个持续查询。</p><p>这个Query很简单，是一个分组聚合做count统计的查询。它将用户字段上的clicks表分组，并统计访问的url数。图中显示了随着时间的推移，当clicks表被其他行更新时如何计算查询。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gebyo6k1awj20lr0aljsg.jpg" alt="4.png"></p><h3 id="将动态表转换成流"><a href="#将动态表转换成流" class="headerlink" title="将动态表转换成流"></a>将动态表转换成流</h3><p>与常规的数据库表一样，动态表可以通过插入（Insert）、更新（Update）和删除（Delete）更改，进行持续的修改。将动态表转换为流或将其写入外部系统时，需要对这些更改进行编码。Flink的Table API和SQL支持三种方式对动态表的更改进行编码：</p><p>1).仅追加（Append-only）流</p><p>仅通过插入（Insert）更改，来修改的动态表，可以直接转换为“仅追加”流。这个流中发出的数据，就是动态表中新增的每一行。</p><p>2).撤回（Retract）流</p><p>Retract流是包含两类消息的流，添加（Add）消息和撤回（Retract）消息。</p><p>动态表通过将INSERT 编码为add消息、DELETE 编码为retract消息、UPDATE编码为被更改行（前一行）的retract消息和更新后行（新行）的add消息，转换为retract流。</p><p>下图显示了将动态表转换为Retract流的过程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gebyrhzrcdj20kf081t9b.jpg" alt="5.png"></p><p>3).Upsert（更新插入）流</p><p>Upsert流包含两种类型的消息：Upsert消息和delete消息。转换为upsert流的动态表，需要有唯一的键（key）。</p><p>通过将INSERT和UPDATE更改编码为upsert消息，将DELETE更改编码为DELETE消息，就可以将具有唯一键（Unique Key）的动态表转换为流。</p><p>下图显示了将动态表转换为upsert流的过程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gebz2a1sjmj20kk089js9.jpg" alt="6.png"></p><p>这些概念我们之前都已提到过。需要注意的是，在代码里将动态表转换为DataStream时，仅支持Append和Retract流。而向外部系统输出动态表的TableSink接口，则可以有不同的实现，比如之前我们讲到的ES，就可以有Upsert模式。</p><h2 id="时间特性"><a href="#时间特性" class="headerlink" title="时间特性"></a>时间特性</h2><p>基于时间的操作（比如Table API和SQL中窗口操作），需要定义相关的时间语义和时间数据来源的信息。所以，Table可以提供一个逻辑上的时间字段，用于在表处理程序中，指示时间和访问相应的时间戳。</p><p>时间属性，可以是每个表schema的一部分。一旦定义了时间属性，它就可以作为一个字段引用，并且可以在基于时间的操作中使用。</p><p>时间属性的行为类似于常规时间戳，可以访问，并且进行计算。</p><h3 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h3><p>处理时间语义下，允许表处理程序根据机器的本地时间生成结果。它是时间的最简单概念。它既不需要提取时间戳，也不需要生成watermark。</p><p>定义处理时间属性有三种方法：在DataStream转化时直接指定；在定义Table Schema时指定；在创建表的DDL中指定。</p><h4 id="DataStream转换成Table时指定"><a href="#DataStream转换成Table时指定" class="headerlink" title="DataStream转换成Table时指定"></a>DataStream转换成Table时指定</h4><p>由DataStream转换成表的时候，可以在后面指定字段名来定义Schema。在定义Schema期间，可以使用.proctime定义处理时间字段。</p><p>注意，这个proctime属性只能通过附加逻辑字段，来拓展物理schema，因此，</p><p><strong>只能在schema定义的末尾定义它。</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义好schema</span></span><br><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"\\sensor.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream</span><br><span class="line">  .map(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">    <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>), dataArray(<span class="number">1</span>).toLong, dataArray(<span class="number">2</span>).toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream转换为 Table，并指定时间字段</span></span><br><span class="line"><span class="keyword">val</span> sensorTable = tableEnv.fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span>, <span class="symbol">'pt</span>.proctime)</span><br></pre></td></tr></table></figure><h4 id="定义Table-Schema-时指定"><a href="#定义Table-Schema-时指定" class="headerlink" title="定义Table Schema 时指定"></a>定义Table Schema 时指定</h4><p>定义Schema的时候，加上一个新字段，指定成proctime就可以。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"..\\sensor.txt"</span>))</span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>())</span><br><span class="line">  .withSchema(<span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">    .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">    .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">    .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">    .field(<span class="string">"pt"</span>, <span class="type">DataTypes</span>.<span class="type">TIMESTAMP</span>(<span class="number">3</span>))</span><br><span class="line">      .proctime()    <span class="comment">// 指定 pt字段为处理时间</span></span><br><span class="line">  ) <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"inputTable"</span>) <span class="comment">// 创建临时表</span></span><br></pre></td></tr></table></figure><h4 id="创建表的DDL中指定"><a href="#创建表的DDL中指定" class="headerlink" title="创建表的DDL中指定"></a>创建表的DDL中指定</h4><p>在创建表的DDL中，增加一个字段并指定成proctime，也可以指定当前的时间字段。</p><p>代码如下:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sinkDDL: <span class="type">String</span> =</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |create table dataTable (</span></span><br><span class="line"><span class="string">    |  id varchar(20) not null,</span></span><br><span class="line"><span class="string">    |  ts bigint,</span></span><br><span class="line"><span class="string">    |  temperature double,</span></span><br><span class="line"><span class="string">    |  pt AS PROCTIME()</span></span><br><span class="line"><span class="string">    |) with (</span></span><br><span class="line"><span class="string">    |  'connector.type' = 'filesystem',</span></span><br><span class="line"><span class="string">    |  'connector.path' = 'file:///D:\\..\\sensor.txt',</span></span><br><span class="line"><span class="string">    |  'format.type' = 'csv'</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL) <span class="comment">// 执行 DDL</span></span><br></pre></td></tr></table></figure><p>注意：运行这段DDL，必须使用Blink Planner。</p><h3 id="事件时间（Event-Time）"><a href="#事件时间（Event-Time）" class="headerlink" title="事件时间（Event Time）"></a>事件时间（Event Time）</h3><p>事件时间语义，允许表处理程序根据每个记录中包含的时间生成结果。这样即使在有乱序事件或者延迟事件时，也可以获得正确的结果。</p><p>为了处理无序事件，并区分流中的准时和迟到事件；Flink需要从事件数据中，提取时间戳，并用来推进事件时间的进展（watermark）。</p><h4 id="DataStream转化成Table时指定"><a href="#DataStream转化成Table时指定" class="headerlink" title="DataStream转化成Table时指定"></a>DataStream转化成Table时指定</h4><p>在DataStream转换成Table，schema的定义期间，使用.rowtime可以定义事件时间属性。</p><p>注意，必须在转换的数据流中分配时间戳和watermark。</p><p>在将数据流转换为表时，有两种定义时间属性的方法。根据指定的.rowtime字段名是否存在于数据流的架构中，timestamp字段可以：</p><ol><li><p><strong>作为新字段追加到schema</strong></p></li><li><p><strong>替换现有字段</strong></p></li></ol><p>在这两种情况下，定义的事件时间戳字段，都将保存DataStream中事件时间戳的值。</p><p>代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"\\sensor.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream</span><br><span class="line">    .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">        <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>), dataArray(<span class="number">1</span>).toLong, dataArray(<span class="number">2</span>).toDouble)</span><br><span class="line">      &#125;)</span><br><span class="line">    .assignAscendingTimestamps(_.timestamp * <span class="number">1000</span>L)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream转换为 Table，并指定时间字段</span></span><br><span class="line"><span class="keyword">val</span> sensorTable = tableEnv.fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'timestamp</span>.rowtime, <span class="symbol">'temperature</span>)</span><br><span class="line"><span class="comment">// 或者，直接追加字段</span></span><br><span class="line"><span class="keyword">val</span> sensorTable2 = tableEnv.fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span>, <span class="symbol">'rt</span>.rowtime)</span><br></pre></td></tr></table></figure><h4 id="定义Schema时指定"><a href="#定义Schema时指定" class="headerlink" title="定义Schema时指定"></a>定义Schema时指定</h4><p>这种方法只要在定义Schema的时候，将事件时间指定，并指定成rowtime就可以了。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.connect(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FileSystem</span>().path(<span class="string">"sensor.txt"</span>))</span><br><span class="line">  .withFormat(<span class="keyword">new</span> <span class="type">Csv</span>())</span><br><span class="line">  .withSchema(<span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">    .field(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">    .field(<span class="string">"timestamp"</span>, <span class="type">DataTypes</span>.<span class="type">BIGINT</span>())</span><br><span class="line">      .rowtime(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Rowtime</span>()</span><br><span class="line">          .timestampsFromField(<span class="string">"timestamp"</span>)    <span class="comment">// 从字段中提取时间戳</span></span><br><span class="line">          .watermarksPeriodicBounded(<span class="number">1000</span>)    <span class="comment">// watermark延迟1秒</span></span><br><span class="line">      )</span><br><span class="line">    .field(<span class="string">"temperature"</span>, <span class="type">DataTypes</span>.<span class="type">DOUBLE</span>())</span><br><span class="line">  ) <span class="comment">// 定义表结构</span></span><br><span class="line">  .createTemporaryTable(<span class="string">"inputTable"</span>) <span class="comment">// 创建临时表</span></span><br></pre></td></tr></table></figure><h4 id="创建表的DDL中指定-1"><a href="#创建表的DDL中指定-1" class="headerlink" title="创建表的DDL中指定"></a>创建表的DDL中指定</h4><p>事件时间属性，是使用CREATE TABLE DDL中的WARDMARK语句定义的。watermark语句，定义现有事件时间字段上的watermark生成表达式，该表达式将事件时间字段标记为事件时间属性。</p><p>代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sinkDDL: <span class="type">String</span> =</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |create table dataTable (</span></span><br><span class="line"><span class="string">    |  id varchar(20) not null,</span></span><br><span class="line"><span class="string">    |  ts bigint,</span></span><br><span class="line"><span class="string">    |  temperature double,</span></span><br><span class="line"><span class="string">    |  rt AS TO_TIMESTAMP( FROM_UNIXTIME(ts) ),</span></span><br><span class="line"><span class="string">    |  watermark for rt as rt - interval '1' second</span></span><br><span class="line"><span class="string">    |) with (</span></span><br><span class="line"><span class="string">    |  'connector.type' = 'filesystem',</span></span><br><span class="line"><span class="string">    |  'connector.path' = 'file:///D:\\..\\sensor.txt',</span></span><br><span class="line"><span class="string">    |  'format.type' = 'csv'</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">tableEnv.sqlUpdate(sinkDDL) <span class="comment">// 执行 DDL</span></span><br></pre></td></tr></table></figure><p>这里<em>FROM_UNIXTIME</em>是系统内置的时间函数，用来将一个整数（秒数）转换成“YYYY-MM-DD hh:mm:ss”格式（默认，也可以作为第二个String参数传入）的日期时间字符串（date time string）；然后再用<em>TO_TIMESTAMP</em>将其转换成Timestamp。</p><h2 id="窗口（Windows）"><a href="#窗口（Windows）" class="headerlink" title="窗口（Windows）"></a>窗口（Windows）</h2><p>时间语义，要配合窗口操作才能发挥作用。最主要的用途，当然就是开窗口、根据时间段做计算了。下面我们就来看看Table API和SQL中，怎么利用时间字段做窗口操作。</p><p>在Table API和SQL中，主要有两种窗口：Group Windows和Over Windows</p><h4 id="分组窗口"><a href="#分组窗口" class="headerlink" title="分组窗口"></a>分组窗口</h4><p>分组窗口（Group Windows）会根据时间或行计数间隔，将行聚合到有限的组（Group）中，并对每个组的数据执行一次聚合函数。</p><p>Table API中的Group Windows都是使用.window（w:GroupWindow）子句定义的，并且必须由as子句指定一个别名。为了按窗口对表进行分组，窗口的别名必须在group by子句中，像常规的分组字段一样引用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> table = input</span><br><span class="line">  .window([w: <span class="type">GroupWindow</span>] as <span class="symbol">'w</span>) <span class="comment">// 定义窗口，别名 w</span></span><br><span class="line">  .groupBy(<span class="symbol">'w</span>, <span class="symbol">'a</span>)  <span class="comment">// 以属性a和窗口w作为分组的key </span></span><br><span class="line">  .select(<span class="symbol">'a</span>, <span class="symbol">'b</span>.sum)  <span class="comment">// 聚合字段b的值，求和</span></span><br></pre></td></tr></table></figure><p>或者，还可以把窗口的相关信息，作为字段添加到结果表中：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> table = input</span><br><span class="line">  .window([w: <span class="type">GroupWindow</span>] as <span class="symbol">'w</span>) </span><br><span class="line">  .groupBy(<span class="symbol">'w</span>, <span class="symbol">'a</span>) </span><br><span class="line">  .select(<span class="symbol">'a</span>, <span class="symbol">'w</span>.start, <span class="symbol">'w</span>.end, <span class="symbol">'w</span>.rowtime, <span class="symbol">'b</span>.count)</span><br></pre></td></tr></table></figure><p>Table API提供了一组具有特定语义的预定义Window类，这些类会被转换为底层DataStream或DataSet的窗口操作。</p><p>Table API支持的窗口定义，和我们熟悉的一样，主要也是三种：滚动（Tumbling）、滑动（Sliding）和会话（Session）。</p><h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p>滚动窗口（Tumbling windows）要用Tumble类来定义，另外还有三个方法：</p><p>over：定义窗口长度</p><p>on：用来分组（按时间间隔）或者排序（按行数）的时间字段</p><p>as：别名，必须出现在后面的groupBy中</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Tumbling Event-time Window（事件时间字段rowtime）</span></span><br><span class="line">.window(<span class="type">Tumble</span> over <span class="number">10.</span>minutes on <span class="symbol">'rowtime</span> as <span class="symbol">'w</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Tumbling Processing-time Window（处理时间字段proctime）</span></span><br><span class="line">.window(<span class="type">Tumble</span> over <span class="number">10.</span>minutes on <span class="symbol">'proctime</span> as <span class="symbol">'w</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Tumbling Row-count Window (类似于计数窗口，按处理时间排序，10行一组)</span></span><br><span class="line">.window(<span class="type">Tumble</span> over <span class="number">10.</span>rows on <span class="symbol">'proctime</span> as <span class="symbol">'w</span>)</span><br></pre></td></tr></table></figure><h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h4><p>滑动窗口（Sliding windows）要用Slide类来定义，另外还有四个方法：</p><p>over：定义窗口长度</p><p>every：定义滑动步长</p><p>on：用来分组（按时间间隔）或者排序（按行数）的时间字段</p><p>as：别名，必须出现在后面的groupBy中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Sliding Event-time Window</span></span><br><span class="line">.window(Slide over <span class="number">10</span>.minutes every <span class="number">5</span>.minutes on <span class="string">'rowtime as '</span>w)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Sliding Processing-time window </span></span><br><span class="line">.window(Slide over <span class="number">10</span>.minutes every <span class="number">5</span>.minutes on <span class="string">'proctime as '</span>w)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Sliding Row-count window</span></span><br><span class="line">.window(Slide over <span class="number">10</span>.rows every <span class="number">5</span>.rows on <span class="string">'proctime as '</span>w)</span><br></pre></td></tr></table></figure><h4 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h4><p>会话窗口（Session windows）要用Session类来定义，另外还有三个方法：</p><ul><li><p>withGap：会话时间间隔</p></li><li><p>on：用来分组（按时间间隔）或者排序（按行数）的时间字段</p></li><li><p>as：别名，必须出现在后面的groupBy中</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Session Event-time Window</span></span><br><span class="line">.window(Session withGap <span class="number">10</span>.minutes on <span class="string">'rowtime as '</span>w)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Session Processing-time Window </span></span><br><span class="line">.window(Session withGap <span class="number">10</span>.minutes on <span class="string">'proctime as '</span>w)</span><br></pre></td></tr></table></figure><h4 id="Over-Windows"><a href="#Over-Windows" class="headerlink" title="Over Windows"></a>Over Windows</h4><p>Over window聚合是标准SQL中已有的（Over子句），可以在查询的SELECT子句中定义。Over window 聚合，会针对每个输入行，计算相邻行范围内的聚合。Over windows<br>使用.window（w:overwindows*）子句定义，并在select（）方法中通过别名来引用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val table = input</span><br><span class="line">  .window([w: OverWindow] as <span class="string">'w)</span></span><br><span class="line"><span class="string">  .select('</span>a, <span class="string">'b.sum over '</span>w, <span class="string">'c.min over '</span>w)</span><br></pre></td></tr></table></figure><p>Table API提供了Over类，来配置Over窗口的属性。可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定义Over windows。</p><p>无界的over window是使用常量指定的。也就是说，时间间隔要指定UNBOUNDED_RANGE，或者行计数间隔要指定UNBOUNDED_ROW。而有界的over window是用间隔的大小指定的。</p><p>实际代码应用如下：</p><p>1） 无界的 over window</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 无界的事件时间over window (时间字段 "rowtime")</span></span><br><span class="line">.window(Over partitionBy <span class="string">'a orderBy '</span>rowtime preceding UNBOUNDED_RANGE as <span class="string">'w)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">//无界的处理时间over window (时间字段"proctime")</span></span><br><span class="line"><span class="string">.window(Over partitionBy '</span>a orderBy <span class="string">'proctime preceding UNBOUNDED_RANGE as '</span>w)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 无界的事件时间Row-count over window (时间字段 "rowtime")</span></span><br><span class="line">.window(Over partitionBy <span class="string">'a orderBy '</span>rowtime preceding UNBOUNDED_ROW as <span class="string">'w)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">//无界的处理时间Row-count over window (时间字段 "rowtime")</span></span><br><span class="line"><span class="string">.window(Over partitionBy '</span>a orderBy <span class="string">'proctime preceding UNBOUNDED_ROW as '</span>w)</span><br></pre></td></tr></table></figure><p>2） 有界的over window</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 有界的事件时间over window (时间字段 "rowtime"，之前1分钟)</span></span><br><span class="line">.window(Over partitionBy <span class="string">'a orderBy '</span>rowtime preceding <span class="number">1</span>.minutes as <span class="string">'w)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 有界的处理时间over window (时间字段 "rowtime"，之前1分钟)</span></span><br><span class="line"><span class="string">.window(Over partitionBy '</span>a orderBy <span class="string">'proctime preceding 1.minutes as '</span>w)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 有界的事件时间Row-count over window (时间字段 "rowtime"，之前10行)</span></span><br><span class="line">.window(Over partitionBy <span class="string">'a orderBy '</span>rowtime preceding <span class="number">10</span>.rows as <span class="string">'w)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 有界的处理时间Row-count over window (时间字段 "rowtime"，之前10行)</span></span><br><span class="line"><span class="string">.window(Over partitionBy '</span>a orderBy <span class="string">'proctime preceding 10.rows as '</span>w)</span><br></pre></td></tr></table></figure><h3 id="SQL中窗口的定义"><a href="#SQL中窗口的定义" class="headerlink" title="SQL中窗口的定义"></a>SQL中窗口的定义</h3><p>我们已经了解了在Table API里window的调用方式，同样，我们也可以在SQL中直接加入窗口的定义和使用。</p><h4 id="Group-Windows"><a href="#Group-Windows" class="headerlink" title="Group Windows"></a>Group Windows</h4><p>Group Windows在SQL查询的Group BY子句中定义。与使用常规GROUP BY子句的查询一样，使用GROUP BY子句的查询会计算每个组的单个结果行。</p><p>SQL支持以下Group窗口函数:</p><ul><li>TUMBLE(time_attr, interval)</li></ul><p>定义一个滚动窗口，第一个参数是时间字段，第二个参数是窗口长度。</p><ul><li>HOP(time_attr, interval, interval)</li></ul><p>定义一个滑动窗口，第一个参数是时间字段，第二个参数是窗口滑动步长，第三个是窗口长度。</p><ul><li>SESSION(time_attr, interval)</li></ul><p>定义一个会话窗口，第一个参数是时间字段，第二个参数是窗口间隔（Gap）。</p><p>另外还有一些辅助函数，可以用来选择Group Window的开始和结束时间戳，以及时间属性。</p><p>这里只写TUMBLE_<em>，滑动和会话窗口是类似的（HOP_</em>，SESSION_*）。</p><ul><li>TUMBLE_START(time_attr, interval)</li><li>TUMBLE_END(time_attr, interval)</li><li>TUMBLE_ROWTIME(time_attr, interval)</li><li>TUMBLE_PROCTIME(time_attr, interval)</li></ul><h4 id="Over-Windows-1"><a href="#Over-Windows-1" class="headerlink" title="Over Windows"></a>Over Windows</h4><p>由于Over本来就是SQL内置支持的语法，所以这在SQL中属于基本的聚合操作。所有聚合必须在同一窗口上定义，也就是说，必须是相同的分区、排序和范围。目前仅支持在当前行范围之前的窗口（无边界和有边界）。</p><p>注意，ORDER BY必须在单一的时间属性上指定。</p><p>代码如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(amount) <span class="keyword">OVER</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line">  <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"></span><br><span class="line">// 也可以做多个聚合</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(amount) <span class="keyword">OVER</span> w, <span class="keyword">SUM</span>(amount) <span class="keyword">OVER</span> w</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line">  <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br></pre></td></tr></table></figure><h3 id="CASE"><a href="#CASE" class="headerlink" title="CASE"></a>CASE</h3><p>开一个滚动窗口，统计10秒内出现的每个sensor的个数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> streamFromFile: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"sensor.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = streamFromFile</span><br><span class="line">.map( data =&gt; </span><br><span class="line">     &#123;</span><br><span class="line"><span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line"><span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">&#125; )</span><br><span class="line">.assignTimestampsAndWatermarks( <span class="keyword">new</span> <span class="type">BoundedOutOfOrdernessTimestampExtractor</span>[<span class="type">SensorReading</span>](<span class="type">Time</span>.seconds(<span class="number">1</span>)) &#123;</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">SensorReading</span>): <span class="type">Long</span> = element.timestamp * <span class="number">1000</span>L</span><br><span class="line">&#125; )</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span></span><br><span class="line">.newInstance()</span><br><span class="line">                                        .useOldPlanner()</span><br><span class="line">                                        .inStreamingMode()</span><br><span class="line">                                        .build()</span><br><span class="line"><span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = </span><br><span class="line"><span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataTable: <span class="type">Table</span> = tableEnv</span><br><span class="line">.fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span>.rowtime)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultTable: <span class="type">Table</span> = dataTable</span><br><span class="line">                                .window(<span class="type">Tumble</span> over <span class="number">10.</span>seconds on <span class="symbol">'timestamp</span> as <span class="symbol">'tw</span>)</span><br><span class="line">                                .groupBy(<span class="symbol">'id</span>, <span class="symbol">'tw</span>)</span><br><span class="line">                                .select(<span class="symbol">'id</span>, <span class="symbol">'id</span>.count)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sqlDataTable: <span class="type">Table</span> = dataTable</span><br><span class="line">.select(<span class="symbol">'id</span>, <span class="symbol">'temperature</span>, <span class="symbol">'timestamp</span> as <span class="symbol">'ts</span>)</span><br><span class="line"><span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv</span><br><span class="line">.sqlQuery(<span class="string">"select id, count(id) from "</span> </span><br><span class="line">+ sqlDataTable </span><br><span class="line">+ <span class="string">" group by id,tumble(ts,interval '10' second)"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把 Table转化成数据流</span></span><br><span class="line"><span class="keyword">val</span> resultDstream: <span class="type">DataStream</span>[(<span class="type">Boolean</span>, (<span class="type">String</span>, <span class="type">Long</span>))] = resultSqlTable.toRetractStream[(<span class="type">String</span>, <span class="type">Long</span>)]</span><br><span class="line"></span><br><span class="line">resultDstream.filter(_._1).print()</span><br><span class="line">env.execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>Flink Table 和 SQL内置了很多SQL中支持的函数；如果有无法满足的需要，则可以实现用户自定义的函数（UDF）来解决。</p><h3 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a>系统内置函数</h3><p>Flink Table API 和 SQL为用户提供了一组用于数据转换的内置函数。SQL中支持的很多函数，Table API和SQL都已经做了实现，其它还在快速开发扩展中。</p><p>以下是一些典型函数的举例，全部的内置函数，可以参考官网介绍。</p><table><thead><tr><th>内置函数</th><th>SQL</th><th>Table API</th></tr></thead><tbody><tr><td>判断比较</td><td>value1 = value2</td><td>ANY1 === ANY2</td></tr><tr><td></td><td>value1 &gt; value2</td><td>ANY1 &gt; ANY2</td></tr><tr><td>逻辑函数</td><td>boolean1 OR boolean2</td><td>BOOLEAN1</td></tr><tr><td></td><td>boolean IS FALSE</td><td>BOOLEAN.isFalse</td></tr><tr><td></td><td>NOT boolean</td><td>!BOOLEAN</td></tr><tr><td>算数函数</td><td>numeric1 + numeric2</td><td>NUMERIC1 + NUMERIC2</td></tr><tr><td></td><td>POWER(numeric1, numeric2)</td><td>NUMERIC1.power(NUMERIC2)</td></tr><tr><td>字符串函数</td><td>string1 丨丨 string2</td><td>string1 + string2</td></tr><tr><td></td><td>UPPER(string)</td><td>String.upperCase()</td></tr><tr><td></td><td>CHAR_LENGTH(string)</td><td>STRING.charLength()</td></tr><tr><td>时间函数</td><td>DATE string</td><td>STRING.toDate</td></tr><tr><td></td><td>TIMESTAMP string</td><td>STRING.toTimestamp</td></tr><tr><td></td><td>CURRENT_TIME</td><td>currentTime()</td></tr><tr><td></td><td>INTERVAL string range</td><td>NUMERIC.days</td></tr><tr><td></td><td></td><td>NUMERIC.minutes</td></tr><tr><td>聚合函数</td><td>COUNT(*)</td><td>FIELD.count</td></tr><tr><td></td><td>SUM([ALL丨DISTINCT] expression)</td><td>FIELD.sum0</td></tr><tr><td></td><td>RANK()</td><td></td></tr><tr><td></td><td>ROW_NUMBER()</td></tr></tbody></table><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><p>在大多数情况下，用户定义的函数必须先注册，然后才能在查询中使用。不需要专门为Scala 的Table API注册函数。</p><p>函数通过调用registerFunction（）方法在TableEnvironment中注册。当用户定义的函数被注册时，它被插入到TableEnvironment的函数目录中，这样Table API或SQL解析器就可以识别并正确地解释它。</p><h4 id="标量函数"><a href="#标量函数" class="headerlink" title="标量函数"></a>标量函数</h4><p>用户定义的标量函数，可以将0、1或多个标量值，映射到新的标量值。</p><p>为了定义标量函数，必须在org.apache.flink.table.functions中扩展基类Scalar Function，并实现（一个或多个）求值（evaluation，eval）方法。标量函数的行为由求值方法决定，求值方法必须公开声明并命名为eval（直接def声明，没有override）。求值方法的参数类型和返回类型，确定了标量函数的参数和返回类型。</p><p>在下面的代码中，我们定义自己的HashCode函数，在TableEnvironment中注册它，并在查询中调用它。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义一个标量函数</span></span><br><span class="line">class HashCode( factor: Int ) extends ScalarFunction &#123;</span><br><span class="line">  <span class="function">def <span class="title">eval</span><span class="params">( s: String )</span>: Int </span>= &#123;</span><br><span class="line">    s.hashCode * factor</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主函数中调用，计算sensor id的哈希值（前面部分照抄，流环境、表环境、读取source、建表）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">  env.setParallelism(<span class="number">1</span>)</span><br><span class="line">  env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span></span><br><span class="line">    .newInstance()</span><br><span class="line">    .useOldPlanner()</span><br><span class="line">    .inStreamingMode()</span><br><span class="line">    .build()</span><br><span class="line">  <span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create( env, settings )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义好 DataStream</span></span><br><span class="line">  <span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">"..\\sensor.txt"</span>)</span><br><span class="line">  <span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream</span><br><span class="line">    .map(data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> dataArray = data.split(<span class="string">","</span>)</span><br><span class="line">      <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>), dataArray(<span class="number">1</span>).toLong, dataArray(<span class="number">2</span>).toDouble)</span><br><span class="line">    &#125;)</span><br><span class="line">    .assignAscendingTimestamps(_.timestamp * <span class="number">1000</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将 DataStream转换为 Table，并指定时间字段</span></span><br><span class="line">  <span class="keyword">val</span> sensorTable = tableEnv.fromDataStream(dataStream, <span class="symbol">'id</span>, <span class="symbol">'timestamp</span>.rowtime, <span class="symbol">'temperature</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Table API中使用</span></span><br><span class="line">  <span class="keyword">val</span> hashCode = <span class="keyword">new</span> <span class="type">HashCode</span>(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> resultTable = sensorTable</span><br><span class="line">    .select( <span class="symbol">'id</span>, hashCode(<span class="symbol">'id</span>) )</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// SQL 中使用</span></span><br><span class="line">  tableEnv.createTemporaryView(<span class="string">"sensor"</span>, sensorTable)</span><br><span class="line">  tableEnv.registerFunction(<span class="string">"hashCode"</span>, hashCode)</span><br><span class="line">  <span class="keyword">val</span> resultSqlTable = tableEnv.sqlQuery(<span class="string">"select id, hashCode(id) from sensor"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 转换成流，打印输出</span></span><br><span class="line">  resultTable.toAppendStream[<span class="type">Row</span>].print(<span class="string">"table"</span>)</span><br><span class="line">  resultSqlTable.toAppendStream[<span class="type">Row</span>].print(<span class="string">"sql"</span>)</span><br><span class="line"> </span><br><span class="line">  env.execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="表函数（Table-Functions）"><a href="#表函数（Table-Functions）" class="headerlink" title="表函数（Table Functions）"></a>表函数（Table Functions）</h4><p>与用户定义的标量函数类似，用户定义的表函数，可以将0、1或多个标量值作为输入参数；与标量函数不同的是，它可以返回任意数量的行作为输出，而不是单个值。</p><p>为了定义一个表函数，必须扩展org.apache.flink.table.functions中的基类TableFunction并实现（一个或多个）求值方法。表函数的行为由其求值方法决定，求值方法必须是public的，并命名为eval。求值方法的参数类型，决定表函数的所有有效参数。</p><p>返回表的类型由TableFunction的泛型类型确定。求值方法使用protected collect（T）方法发出输出行。</p><p>在Table API中，Table函数需要与.joinLateral或.leftOuterJoinLateral一起使用。</p><p>joinLateral算子，会将外部表中的每一行，与表函数（TableFunction，算子的参数是它的表达式）计算得到的所有行连接起来。</p><p>而leftOuterJoinLateral算子，则是左外连接，它同样会将外部表中的每一行与表函数计算生成的所有行连接起来；并且，对于表函数返回的是空表的外部行，也要保留下来。</p><p>在SQL中，则需要使用Lateral Table（<tablefunction>），或者带有ON TRUE条件的左连接。</tablefunction></p><p>下面的代码中，我们将定义一个表函数，在表环境中注册它，并在查询中调用它。</p><p>自定义TableFunction：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义TableFunction</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Split</span>(<span class="params">separator: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">TableFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>)]</span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    str.split(separator).foreach(</span><br><span class="line">      word =&gt; collect((word, word.length))</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来，就是在代码中调用。首先是Table API的方式：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Table API中调用，需要用joinLateral</span></span><br><span class="line">    <span class="keyword">val</span> resultTable = sensorTable</span><br><span class="line">      .joinLateral(split(<span class="symbol">'id</span>) as (<span class="symbol">'word</span>, <span class="symbol">'length</span>))   <span class="comment">// as对输出行的字段重命名</span></span><br><span class="line">      .select(<span class="symbol">'id</span>, <span class="symbol">'word</span>, <span class="symbol">'length</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 或者用leftOuterJoinLateral</span></span><br><span class="line">    <span class="keyword">val</span> resultTable2 = sensorTable</span><br><span class="line">      .leftOuterJoinLateral(split(<span class="symbol">'id</span>) as (<span class="symbol">'word</span>, <span class="symbol">'length</span>))</span><br><span class="line">      .select(<span class="symbol">'id</span>, <span class="symbol">'word</span>, <span class="symbol">'length</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 转换成流打印输出</span></span><br><span class="line">    resultTable.toAppendStream[<span class="type">Row</span>].print(<span class="string">"1"</span>)</span><br><span class="line">    resultTable2.toAppendStream[<span class="type">Row</span>].print(<span class="string">"2"</span>)</span><br></pre></td></tr></table></figure><p>然后是SQL的方式</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">    tableEnv.createTemporaryView(<span class="string">"sensor"</span>, sensorTable)</span><br><span class="line">    tableEnv.registerFunction(<span class="string">"split"</span>, split)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> resultSqlTable = tableEnv.sqlQuery(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |select id, word, length</span></span><br><span class="line"><span class="string">        |from</span></span><br><span class="line"><span class="string">        |sensor, LATERAL TABLE(split(id)) AS newsensor(word, length)</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 或者用左连接的方式</span></span><br><span class="line">    <span class="keyword">val</span> resultSqlTable2 = tableEnv.sqlQuery(</span><br><span class="line">      <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |SELECT id, word, length</span></span><br><span class="line"><span class="string">        |FROM</span></span><br><span class="line"><span class="string">        |sensor</span></span><br><span class="line"><span class="string">|  LEFT JOIN </span></span><br><span class="line"><span class="string">|  LATERAL TABLE(split(id)) AS newsensor(word, length) </span></span><br><span class="line"><span class="string">|  ON TRUE</span></span><br><span class="line"><span class="string">      "</span><span class="string">""</span>.stripMargin</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 转换成流打印输出</span></span><br><span class="line">    resultSqlTable.toAppendStream[<span class="type">Row</span>].print(<span class="string">"1"</span>)</span><br><span class="line">    resultSqlTable2.toAppendStream[<span class="type">Row</span>].print(<span class="string">"2"</span>)</span><br></pre></td></tr></table></figure><h4 id="聚合函数-aggregate-Function"><a href="#聚合函数-aggregate-Function" class="headerlink" title="聚合函数(aggregate Function)"></a>聚合函数(aggregate Function)</h4><p>用户自定义聚合函数（User-Defined Aggregate Functions，UDAGGs）可以把一个表中的数据，聚合成一个标量值。用户定义的聚合函数，是通过继承AggregateFunction抽象类实现的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gejnzvbpitj20jf0b6jsm.jpg" alt="1.png"></p><p>上图中显示了一个聚合的例子。</p><p>假设现在有一张表，包含了各种饮料的数据。该表由三列（id、name和price）、五行组成数据。现在我们需要找到表中所有饮料的最高价格，即执行max（）聚合，结果将是一个数值。</p><p>AggregateFunction的工作原理如下。</p><ul><li><p>首先，它需要一个累加器，用来保存聚合中间结果的数据结构（状态）。可以通过调用AggregateFunction的createAccumulator（）方法创建空累加器。</p></li><li><p>随后，对每个输入行调用函数的accumulate（）方法来更新累加器。</p></li><li><p>处理完所有行后，将调用函数的getValue（）方法来计算并返回最终结果。</p></li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li><p>createAccumulator()</p></li><li><p>accumulate()</p></li><li><p>getValue()</p></li></ul><p>除了上述方法之外，还有一些可选择实现的方法。其中一些方法，可以让系统执行查询更有效率，而另一些方法，对于某些场景是必需的。例如，如果聚合函数应用在会话窗口（session group window）的上下文中，则merge（）方法是必需的。</p><ul><li><p>retract() </p></li><li><p>merge() </p></li><li><p>resetAccumulator()</p></li></ul><p>除了上述方法之外，还有一些可选择实现的方法。其中一些方法，可以让系统执行查询更有效率，而另一些方法，对于某些场景是必需的。例如，如果聚合函数应用在会话窗口（session group window）的上下文中，则merge（）方法是必需的。</p><ul><li>retract()</li><li>merge()</li><li>resetAccumulator()</li></ul><p>接下来自定义AggregateFunction,计算一下每个sensor的平均温度值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义AggregateFunction的Accumulator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgTempAcc</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sum: <span class="type">Double</span> = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">var</span> count: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgTemp</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">Double</span>, <span class="type">AvgTempAcc</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(accumulator: <span class="type">AvgTempAcc</span>): <span class="type">Double</span> =</span><br><span class="line">    accumulator.sum / accumulator.count</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">AvgTempAcc</span> = <span class="keyword">new</span> <span class="type">AvgTempAcc</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(accumulator: <span class="type">AvgTempAcc</span>, temp: <span class="type">Double</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    accumulator.sum += temp</span><br><span class="line">    accumulator.count += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就可以在代码中调用了</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个聚合函数实例</span></span><br><span class="line"><span class="keyword">val</span> avgTemp = <span class="keyword">new</span> <span class="type">AvgTemp</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Table API的调用 </span></span><br><span class="line"><span class="keyword">val</span> resultTable = sensorTable.groupBy(<span class="symbol">'id</span>)</span><br><span class="line">  .aggregate(avgTemp(<span class="symbol">'temperature</span>) as <span class="symbol">'avgTemp</span>)</span><br><span class="line">  .select(<span class="symbol">'id</span>, <span class="symbol">'avgTemp</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL的实现</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"sensor"</span>, sensorTable)</span><br><span class="line">tableEnv.registerFunction(<span class="string">"avgTemp"</span>, avgTemp)</span><br><span class="line"><span class="keyword">val</span> resultSqlTable = tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |SELECT</span></span><br><span class="line"><span class="string">    |id, avgTemp(temperature)</span></span><br><span class="line"><span class="string">    |FROM</span></span><br><span class="line"><span class="string">    |sensor</span></span><br><span class="line"><span class="string">    |GROUP BY id</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换成流打印输出</span></span><br><span class="line">resultTable.toRetractStream[(<span class="type">String</span>, <span class="type">Double</span>)].print(<span class="string">"agg temp"</span>)</span><br><span class="line">resultSqlTable.toRetractStream[<span class="type">Row</span>].print(<span class="string">"agg temp sql"</span>)</span><br></pre></td></tr></table></figure><h3 id="表聚合函数（Table-Aggregate-Functions）"><a href="#表聚合函数（Table-Aggregate-Functions）" class="headerlink" title="表聚合函数（Table Aggregate Functions）"></a>表聚合函数（Table Aggregate Functions）</h3><p>用户定义的表聚合函数（User-Defined Table Aggregate Functions，UDTAGGs），可以把一个表中数据，聚合为具有多行和多列的结果表。这跟AggregateFunction非常类似，只是之前聚合结果是一个标量值，现在变成了一张表。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gejo6fdbb4j20je09cta0.jpg" alt="2.png"></p><p>比如现在我们需要找到表中所有饮料的前2个最高价格，即执行top2（）表聚合。我们需要检查5行中的每一行，得到的结果将是一个具有排序后前2个值的表。</p><p>用户定义的表聚合函数，是通过继承TableAggregateFunction抽象类来实现的。</p><p>TableAggregateFunction的工作原理如下。</p><ul><li><p>首先，它同样需要一个累加器（Accumulator），它是保存聚合中间结果的数据结构。通过调用TableAggregateFunction的createAccumulator（）方法可以创建空累加器。</p></li><li><p>随后，对每个输入行调用函数的accumulate（）方法来更新累加器。</p></li><li><p>处理完所有行后，将调用函数的emitValue（）方法来计算并返回最终结果。</p></li></ul><p>AggregationFunction要求必须实现的方法：</p><ul><li><p>createAccumulator()</p></li><li><p>accumulate()</p></li></ul><p>除了上述方法之外，还有一些可选择实现的方法。</p><ul><li>retract() </li><li>merge()  </li><li>resetAccumulator() </li><li>emitValue() </li><li>emitUpdateWithRetract()</li></ul><p>接下来我们写一个自定义TableAggregateFunction，用来提取每个sensor最高的两个温度值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 先定义一个 Accumulator </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Top2TempAcc</span></span>&#123;</span><br><span class="line">  <span class="keyword">var</span> highestTemp: <span class="type">Double</span> = <span class="type">Int</span>.<span class="type">MinValue</span></span><br><span class="line">  <span class="keyword">var</span> secondHighestTemp: <span class="type">Double</span> = <span class="type">Int</span>.<span class="type">MinValue</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义 TableAggregateFunction</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Top2Temp</span> <span class="keyword">extends</span> <span class="title">TableAggregateFunction</span>[(<span class="type">Double</span>, <span class="type">Int</span>), <span class="type">Top2TempAcc</span>]</span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">Top2TempAcc</span> = <span class="keyword">new</span> <span class="type">Top2TempAcc</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">Top2TempAcc</span>, temp: <span class="type">Double</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">if</span>( temp &gt; acc.highestTemp )&#123;</span><br><span class="line">      acc.secondHighestTemp = acc.highestTemp</span><br><span class="line">      acc.highestTemp = temp</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span>( temp &gt; acc.secondHighestTemp )&#123;</span><br><span class="line">      acc.secondHighestTemp = temp</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">emitValue</span></span>(acc: <span class="type">Top2TempAcc</span>, out: <span class="type">Collector</span>[(<span class="type">Double</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    out.collect(acc.highestTemp, <span class="number">1</span>)</span><br><span class="line">    out.collect(acc.secondHighestTemp, <span class="number">2</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就可以在代码中调用了。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个表聚合函数实例</span></span><br><span class="line"><span class="keyword">val</span> top2Temp = <span class="keyword">new</span> <span class="type">Top2Temp</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Table API的调用</span></span><br><span class="line"><span class="keyword">val</span> resultTable = sensorTable.groupBy(<span class="symbol">'id</span>)</span><br><span class="line">  .flatAggregate( top2Temp(<span class="symbol">'temperature</span>) as (<span class="symbol">'temp</span>, <span class="symbol">'rank</span>) )</span><br><span class="line">  .select(<span class="symbol">'id</span>, <span class="symbol">'temp</span>, <span class="symbol">'rank</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换成流打印输出</span></span><br><span class="line">resultTable.toRetractStream[(<span class="type">String</span>, <span class="type">Double</span>, <span class="type">Int</span>)].print(<span class="string">"agg temp"</span>)</span><br><span class="line">resultSqlTable.toRetractStream[<span class="type">Row</span>].print(<span class="string">"agg temp sql"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Flink Table 和 SQL 整体的脉络&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2gy1gejppurwd5j20ib046t9h.jpg&quot; alt=&quot;3.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Apache/Flink/"/>
    
    
  </entry>
  
  <entry>
    <title>Flink State</title>
    <link href="http://yoursite.com/2020/04/26/Flink%20State/"/>
    <id>http://yoursite.com/2020/04/26/Flink State/</id>
    <published>2020-04-26T03:38:39.811Z</published>
    <updated>2020-04-26T03:42:22.120Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Flink State的使用经验和平时一些注意不到的属性</p></blockquote><a id="more"></a> <h1 id="Flink-State"><a href="#Flink-State" class="headerlink" title="Flink State"></a>Flink State</h1><h2 id="Flink-State-的种类"><a href="#Flink-State-的种类" class="headerlink" title="Flink State 的种类"></a>Flink State 的种类</h2><p>定义：流式计算中持久化来的状态</p><p>两种不同的 state：operator state 以及 keyed state。</p><p>区别：</p><p><strong>是否存在当前处理的 key</strong>（current key）：operator state 是没有当前 key 的概念，而 keyed state 的数值总是与一个 current key 对应。</p><p><strong>是否存在当前处理的 key</strong>（current key）：operator state 是没有当前 key 的概念，而 keyed state 的数值总是与一个 current key 对应。</p><p><strong>是否需要手动声明快照</strong>（snapshot）<strong>和恢复</strong> (restore) <strong>方法</strong>：operator state 需要手动实现 snapshot 和 restore 方法；而 keyed state 则由 backend 自行实现，对用户透明。</p><p><strong>数据大小</strong>：一般而言，我们认为 operator state 的数据规模是比较小的；认为 keyed state 规模是相对比较大的。需要注意的是，这是一个经验判断，不是一个绝对的判断区分标准。</p><p>生产中，我们会在 FsStateBackend 和 RocksDBStateBackend 间选择：</p><ul><li><strong>FsStateBackend</strong>：性能更好；日常存储是在堆内存中，面临着 OOM 的风险，不支持增量 checkpoint。</li><li><strong>RocksDBStateBackend</strong>：无需担心 OOM 风险，是大部分时候的选择。</li></ul><p><strong>RocksDB StateBackend 概览和相关配置讨论</strong></p><p>RocksDB 是 Facebook 开源的 LSM 的键值存储数据库，被广泛应用于大数据系统的单机组件中。Flink 的 keyed state 本质上来说就是一个键值对，所以与 RocksDB 的数据模型是吻合的。下图分别是 “window state” 和 “value state” 在 RocksDB 中的存储格式，所有存储的 key，value 均被序列化成 bytes 进行存储。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ge6ze2rcs8j20oz09sglx.jpg" alt="undefined"></p><p>在 RocksDB 中，每个 state 独享一个 Column Family，而每个 Column family 使用各自独享的 write buffer 和 block cache，上图中的 window state 和 value state实际上分属不同的 column family。</p><p>下面介绍一些对 RocksDB 性能比较有影响的参数，并整理了一些相关的推荐配置，至于其他配置项，可以参阅社区相关文档。</p><table><thead><tr><th>state.backend.rocksdb.thread.num</th><th>后台 flush 和 compaction 的线程数. 默认值 ‘1‘. 建议调大</th></tr></thead><tbody><tr><td>state.backend.rocksdb.writebuffer.count</td><td>每个 column family 的 write buffer 数目，默认值 ‘2‘. 如果有需要可以适当调大</td></tr><tr><td>state.backend.rocksdb.writebuffer.size</td><td>每个 write buffer 的 size，默认值‘64MB‘. 对于写频繁的场景，建议调大</td></tr><tr><td>state.backend.rocksdb.block.cache-size</td><td>每个 column family 的 block cache大小，默认值‘8MB’，如果存在重复读的场景，建议调大</td></tr></tbody></table><h2 id="State-best-practice"><a href="#State-best-practice" class="headerlink" title="State best practice"></a>State best practice</h2><p>■ <strong>慎重使用长 list</strong></p><p>下图展示的是目前 task 端 operator state 在执行完 checkpoint 返回给 job master 端的 StateMetaInfo 的代码片段。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ge6zual0gdj20oz0a0jrv.jpg" alt="undefined"></p><p>由于 operator state 没有 key group 的概念，所以为了实现改并发恢复的功能，需要对 operator state 中的每一个序列化后的元素存储一个位置偏移 offset，也就是构成了上图红框中的 offset 数组。</p><p>那么如果你的 operator state 中的 list 长度达到一定规模时，这个 offset 数组就可能会有几十 MB 的规模，关键这个数组是会返回给 job master，当 operator 的并发数目很大时，很容易触发 job master 的内存超用问题。我们遇到过用户把 operator state 当做黑名单存储，结果这个黑名单规模很大，导致一旦开始执行 checkpoint，job master 就会因为收到 task 发来的“巨大”的 offset 数组，而内存不断增长直到超用无法正常响应。</p><p>■ <strong>正确使用 UnionListState</strong></p><p>union list state 目前被广泛使用在 kafka connector 中，不过可能用户日常开发中较少遇到，他的语义是从检查点恢复之后每个并发 task 内拿到的是原先所有operator 上的 state，如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ge6zuxdx96j20nl0930sn.jpg" alt="undefined"></p><p>kafka connector 使用该功能，为的是从检查点恢复时，可以拿到之前的全局信息，如果用户需要使用该功能，需要切记恢复的 task 只取其中的一部分进行处理和用于下一次 snapshot，否则有可能随着作业不断的重启而导致 state 规模不断增长。</p><h2 id="Keyed-state-使用建议"><a href="#Keyed-state-使用建议" class="headerlink" title="Keyed state 使用建议"></a>Keyed state 使用建议</h2><p>■ <strong>如何正确清空当前的 state</strong></p><p>state.clear() 实际上只能清理当前 key 对应的 value 值，如果想要清空整个 state，需要借助于 applyToAllKeys 方法，具体代码片段如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ge6zvn0chzj20oz04odfx.jpg" alt="undefined"></p><p>如果你的需求中只是对 state 有过期需求，借助于 state TTL 功能来清理会是一个性能更好的方案。</p><p><strong>■ RocksDB 中考虑 value 值很大的极限场景</strong></p><p>受限于 JNI bridge API 的限制，单个 value 只支持 2^31 bytes 大小，如果存在很极限的情况，可以考虑使用 MapState 来替代 ListState 或者 ValueState，因为RocksDB 的 map state 并不是将整个 map 作为 value 进行存储，而是将 map 中的一个条目作为键值对进行存储。</p><p><strong>■ 如何知道当前 RocksDB 的运行情况</strong></p><p>比较直观的方式是打开 RocksDB 的 native metrics ，在默认使用 Flink managed memory 方式的情况下，state.backend.rocksdb.metrics.block-cache-usage ，state.backend.rocksdb.metrics.mem-table-flush-pending，state.backend.rocksdb.metrics.num-running-compactions 以及 state.backend.rocksdb.metrics.num-running-flushes 是比较重要的相关 metrics。</p><p>下面这张图是 Flink-1.10 之后，打开相关 metrics 的示例图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ge6zw28irmj20oz0e2aaq.jpg" alt="undefined"></p><p>而下面这张是 Flink-1.10 之前或者关闭 state.backend.rocksdb.memory.managed  的效果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ge6zwe0hczj20oz0e0aas.jpg" alt="undefined"></p><p>■ <strong>容器内运行的 RocksDB 的内存超用问题</strong></p><p>在 Flink-1.10 之前，由于一个 state 独占若干 write buffer 和一块 block cache，所以我们会建议用户不要在一个 operator 内创建过多的 state，否则需要考虑到相应的额外内存使用量，否则容易造成在容器内运行时，相关进程被容器环境所杀。对于用户来说，需要考虑一个 slot 内有多少 RocksDB 实例在运行，一个 RocksDB 中有多少 state，整体的计算规则就很复杂，很难真得落地实施。</p><p>Flink-1.10 之后，由于引入了 RocksDB 的内存托管机制，在绝大部分情况下， RocksDB 的这一部分 native 内存是可控的，不过受限于 RocksDB 的相关 cache 实现限制（这里暂不展开，后续会有文章讨论），在某些场景下，无法做到完美控制，这时候建议打开上文提到的 native metrics，观察相关 block cache 内存使用是否存在超用情况，可以将相关内存添加到 taskmanager.memory.task.off-heap.size 中，使得 Flink 有更多的空间给 native 内存使用。</p><p>■ <strong>Checkpoint 间隔不要太短</strong></p><p>虽然理论上 Flink 支持很短的 checkpoint 间隔，但是在实际生产中，过短的间隔对于底层分布式文件系统而言，会带来很大的压力。另一方面，由于检查点的语义，所以实际上 Flink 作业处理 record 与执行 checkpoint 存在互斥锁，过于频繁的 checkpoint，可能会影响整体的性能。当然，这个建议的出发点是底层分布式文件系统的压力考虑。</p><p>■ <strong>合理设置超时时间</strong></p><p>默认的超时时间是 10min，如果 state 规模大，则需要合理配置。最坏情况是分布式地创建速度大于单点（job master 端）的删除速度，导致整体存储集群可用空间压力较大。建议当检查点频繁因为超时而失败时，增大超时时间。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Flink State的使用经验和平时一些注意不到的属性&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Apache/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Git详解</title>
    <link href="http://yoursite.com/2020/04/13/Git%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/04/13/Git详解/</id>
    <published>2020-04-13T06:48:17.587Z</published>
    <updated>2020-04-13T08:24:45.247Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>讨论了许多种git的情况，非常详细的git报告</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gds54g3odrj206402kmwz.jpg" alt="undefined"><br><a id="more"></a> </p><h1 id="Git超详细"><a href="#Git超详细" class="headerlink" title="Git超详细"></a>Git超详细</h1><h4 id="Git是什么"><a href="#Git是什么" class="headerlink" title="Git是什么"></a>Git是什么</h4><p>Git是目前世界上最先进的分布式版本控制系统。</p><h4 id="SVN与Git的最主要的区别"><a href="#SVN与Git的最主要的区别" class="headerlink" title="SVN与Git的最主要的区别"></a><strong>SVN与Git的最主要的区别</strong></h4><p>SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而干活的时候，用的都是自己的电脑，所以首先要从中央服务器哪里得到最新的版本，然后干活，干完后，需要把自己做完的活推送到中央服务器。集中式版本控制系统是必须联网才能工作，如果在局域网还可以，带宽够大，速度够快，如果在互联网下，如果网速慢的话，就纳闷了。</p><p>   Git是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。</p><h4 id="创建版本库"><a href="#创建版本库" class="headerlink" title="创建版本库"></a>创建版本库</h4><p>   什么是版本库？版本库又名仓库，英文名repository,你可以简单的理解一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改，删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻还可以将文件”还原”。</p><p>  所以创建一个版本库也非常简单，如下我是D盘 –&gt; www下 目录下新建一个testgit版本库。</p><p>pwd 命令是用于显示当前的目录。</p><p>   \1. 通过命令 git init 把这个目录变成git可以管理的仓库，如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyr2rpcnj20en025mx9.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyr2rpcnj20en025mx9.jpg" alt="img"></a></p><p>   这时候你当前testgit目录下会多了一个.git的目录，这个目录是Git来跟踪管理版本的，没事千万不要手动乱改这个目录里面的文件，否则，会把git仓库给破坏了。如下：</p><p>  <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyr1x3lzj20h004tgm1.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyr1x3lzj20h004tgm1.jpg" alt="img"></a></p><h4 id="把文件添加到版本库中"><a href="#把文件添加到版本库中" class="headerlink" title="把文件添加到版本库中"></a>把文件添加到版本库中</h4><p>​     首先要明确下，所有的版本控制系统，只能跟踪文本文件的改动，比如txt文件，网页，所有程序的代码等，Git也不列外，版本控制系统可以告诉你每次的改动，但是图片，视频这些二进制文件，虽能也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是知道图片从1kb变成2kb，但是到底改了啥，版本控制也不知道。</p><p>  <strong>下面先看下**</strong>demo<strong>**如下演示：</strong></p><p>   我在版本库testgit目录下新建一个记事本文件 readme.txt 内容如下：11111111</p><p>   第一步：使用命令 git add readme.txt添加到暂存区里面去。如下：</p><p>  <a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyr0wkxbj20ch028dfu.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyr0wkxbj20ch028dfu.jpg" alt="img"></a></p><p>  如果和上面一样，没有任何提示，说明已经添加成功了。</p><p>  第二步：用命令 git commit告诉Git，把文件提交到仓库。</p><p>  <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqz56axj20dp03djrr.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqz56axj20dp03djrr.jpg" alt="img"></a></p><p> 现在我们已经提交了一个readme.txt文件了，我们下面可以通过命令git status来查看是否还有文件未提交，如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqrg067j20d102zwen.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqrg067j20d102zwen.jpg" alt="img"></a></p><p> 说明没有任何文件未提交，但是我现在继续来改下readme.txt内容，比如我在下面添加一行2222222222内容，继续使用git status来查看下结果，如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqq7ts6j20h504r74x.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqq7ts6j20h504r74x.jpg" alt="img"></a></p><p>上面的命令告诉我们 readme.txt文件已被修改，但是未被提交的修改。</p><p>接下来我想看下readme.txt文件到底改了什么内容，如何查看呢？可以使用如下命令：</p><p>git diff readme.txt 如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqnrvxgj20ds05maal.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqnrvxgj20ds05maal.jpg" alt="img"></a></p><p>如上可以看到，readme.txt文件内容从一行11111111改成 二行 添加了一行22222222内容。</p><p>知道了对readme.txt文件做了什么修改后，我们可以放心的提交到仓库了，提交修改和提交文件是一样的2步(第一步是git add 第二步是：git commit)。</p><p>如下：</p><p> <a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqmcupsj20h609i402.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqmcupsj20h609i402.jpg" alt="img"></a></p><h4 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h4><p>   如上，我们已经学会了修改文件，现在我继续对readme.txt文件进行修改，再增加一行</p><p>内容为33333333333333.继续执行命令如下：</p><p>  <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyql1473j20cp03vdga.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyql1473j20cp03vdga.jpg" alt="img"></a></p><p>现在我已经对readme.txt文件做了三次修改了，那么我现在想查看下历史记录，如何查呢？我们现在可以使用命令 git log 演示如下所示：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyqd9m1dj20gt08ggn8.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyqd9m1dj20gt08ggn8.jpg" alt="img"></a></p><p>  git log命令显示从最近到最远的显示日志，我们可以看到最近三次提交，最近的一次是,增加内容为333333.上一次是添加内容222222，第一次默认是 111111.如果嫌上面显示的信息太多的话，我们可以使用命令 git log –pretty=oneline 演示如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqc3ziwj20gs02paai.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqc3ziwj20gs02paai.jpg" alt="img"></a></p><p>  现在我想使用版本回退操作，我想把当前的版本回退到上一个版本，要使用什么命令呢？可以使用如下2种命令，第一种是：git reset –hard HEAD^ 那么如果要回退到上上个版本只需把HEAD^ 改成 HEAD^^ 以此类推。那如果要回退到前100个版本的话，使用上面的方法肯定不方便，我们可以使用下面的简便命令操作：git reset –hard HEAD~100 即可。未回退之前的readme.txt内容如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqavyf7j20ch04laap.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqavyf7j20ch04laap.jpg" alt="img"></a></p><p>如果想回退到上一个版本的命令如下操作：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqa5xjfj20ct02xaad.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqa5xjfj20ct02xaad.jpg" alt="img"></a></p><p>再来查看下 readme.txt内容如下：通过命令cat readme.txt查看</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq9fck2j20c402d74c.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq9fck2j20c402d74c.jpg" alt="img"></a></p><p>可以看到，内容已经回退到上一个版本了。我们可以继续使用git log 来查看下历史记录信息，如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq6bhrlj20dc063dgk.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq6bhrlj20dc063dgk.jpg" alt="img"></a></p><p>我们看到 增加333333 内容我们没有看到了，但是现在我想回退到最新的版本，如：有333333的内容要如何恢复呢？我们可以通过版本号回退，使用命令方法如下：</p><p>git reset –hard 版本号 ，但是现在的问题假如我已经关掉过一次命令行或者333内容的版本号我并不知道呢？要如何知道增加3333内容的版本号呢？可以通过如下命令即可获取到版本号：git reflog 演示如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyq5dtfrj20e603e0t5.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyq5dtfrj20e603e0t5.jpg" alt="img"></a></p><p>通过上面的显示我们可以知道，增加内容3333的版本号是 6fcfc89.我们现在可以命令</p><p>git reset –hard 6fcfc89来恢复了。演示如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq4m3oqj20e104974t.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq4m3oqj20e104974t.jpg" alt="img"></a></p><p>可以看到 目前已经是最新的版本了。</p><h4 id="理解工作区与暂存区的区别"><a href="#理解工作区与暂存区的区别" class="headerlink" title="理解工作区与暂存区的区别"></a>理解工作区与暂存区的区别</h4><p><strong>工作区：</strong>就是你在电脑上看到的目录，比如目录下testgit里的文件(.git隐藏目录版本库除外)。或者以后需要再新建的目录文件等等都属于工作区范畴。</p><p>   <strong>版本库**</strong>(Repository)<strong>**：</strong>工作区有一个隐藏目录.git,这个不属于工作区，这是版本库。其中版本库里面存了很多东西，其中最重要的就是stage(暂存区)，还有Git为我们自动创建了第一个分支master,以及指向master的一个指针HEAD。</p><p>我们前面说过使用Git提交文件到版本库有两步：</p><p> 第一步：是使用 git add 把文件添加进去，实际上就是把文件添加到暂存区。</p><p> 第二步：使用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支上。</p><p>我们继续使用demo来演示下：</p><p>我们在readme.txt再添加一行内容为4444444，接着在目录下新建一个文件为test.txt 内容为test，我们先用命令 git status来查看下状态，如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyq3ykzsj20hv06pwfi.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyq3ykzsj20hv06pwfi.jpg" alt="img"></a></p><p>现在我们先使用git add 命令把2个文件都添加到暂存区中，再使用git status来查看下状态，如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq2gn7sj20d206p0t8.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq2gn7sj20d206p0t8.jpg" alt="img"></a></p><p>接着我们可以使用git commit一次性提交到分支上，如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq1gpk0j20h704mdgm.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq1gpk0j20h704mdgm.jpg" alt="img"></a></p><h4 id="Git撤销修改和删除文件操作"><a href="#Git撤销修改和删除文件操作" class="headerlink" title="Git撤销修改和删除文件操作"></a>Git撤销修改和删除文件操作</h4><h4 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a><strong>撤销修改</strong></h4><p>  比如我现在在readme.txt文件里面增加一行 内容为555555555555，我们先通过命令查看如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq0rzrcj20ax03vaaa.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq0rzrcj20ax03vaaa.jpg" alt="img"></a></p><p>在我未提交之前，我发现添加5555555555555内容有误，所以我得马上恢复以前的版本，现在我可以有如下几种方法可以做修改：</p><p>第一：如果我知道要删掉那些内容的话，直接手动更改去掉那些需要的文件，然后add添加到暂存区，最后commit掉。</p><p>第二：我可以按以前的方法直接恢复到上一个版本。使用 git reset –hard HEAD^</p><p>但是现在我不想使用上面的2种方法，我想直接想使用撤销命令该如何操作呢？首先在做撤销之前，我们可以先用 git status 查看下当前的状态。如下所示：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq034qhj20hs04oaam.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq034qhj20hs04oaam.jpg" alt="img"></a></p><p>可以发现，Git会告诉你，git checkout — file 可以丢弃工作区的修改，如下命令：</p><p>git checkout — readme.txt,如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypz44y5j20eh03w0t4.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypz44y5j20eh03w0t4.jpg" alt="img"></a></p><p>命令 git checkout –readme.txt 意思就是，把readme.txt文件在工作区做的修改全部撤销，这里有2种情况，如下：</p><ol><li>readme.txt自动修改后，还没有放到暂存区，使用 撤销修改就回到和版本库一模一样的状态。</li><li>另外一种是readme.txt已经放入暂存区了，接着又作了修改，撤销修改就回到添加暂存区后的状态。</li></ol><p>对于第二种情况，我想我们继续做demo来看下，假如现在我对readme.txt添加一行 内容为6666666666666，我git add 增加到暂存区后，接着添加内容7777777，我想通过撤销命令让其回到暂存区后的状态。如下所示：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypybh8pj20h40deq52.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypybh8pj20h40deq52.jpg" alt="img"></a></p><p><strong>注意：</strong>命令git checkout — readme.txt 中的 — 很重要，如果没有 — 的话，那么命令变成创建分支了。</p><h4 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a><strong>删除文件</strong></h4><p> 假如我现在版本库testgit目录添加一个文件b.txt,然后提交。如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypxcttej20hr0awmzc.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypxcttej20hr0awmzc.jpg" alt="img"></a></p><p>如上：一般情况下，可以直接在文件目录中把文件删了，或者使用如上rm命令：rm b.txt ，如果我想彻底从版本库中删掉了此文件的话，可以再执行commit命令 提交掉，现在目录是这样的，</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypvtweyj20jj05cwf4.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypvtweyj20jj05cwf4.jpg" alt="img"></a></p><p>只要没有commit之前，如果我想在版本库中恢复此文件如何操作呢？</p><p>可以使用如下命令 git checkout — b.txt，如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyput1l8j20fh06s0tr.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyput1l8j20fh06s0tr.jpg" alt="img"></a></p><p>再来看看我们testgit目录，添加了3个文件了。如下所示：</p><h4 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h4><p> 在了解之前，先注册github账号，由于你的本地Git仓库和github仓库之间的传输是通过SSH加密的，所以需要一点设置：</p><p>   第一步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果有的话，直接跳过此如下命令，如果没有的话，打开命令行，输入如下命令：</p><p>ssh-keygen -t rsa –C “<a href="mailto:youremail@example.com" target="_blank" rel="noopener">youremail@example.com</a>”, 由于我本地此前运行过一次，所以本地有，如下所示：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypq7esij20kx04pt9c.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypq7esij20kx04pt9c.jpg" alt="img"></a></p><p>id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。</p><p>第二步：登录github,打开” settings”中的SSH Keys页面，然后点击“Add SSH Key”,填上任意title，在Key文本框里黏贴id_rsa.pub文件的内容。</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyppfdu3j20vh0nwdl0.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyppfdu3j20vh0nwdl0.jpg" alt="img"></a></p><p>点击 Add Key，你就应该可以看到已经添加的key。</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypnrj0cj20l60ad75p.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypnrj0cj20l60ad75p.jpg" alt="img"></a></p><ol><li>如何添加远程库？</li></ol><p>​     现在的情景是：我们已经在本地创建了一个Git仓库后，又想在github创建一个Git仓库，并且希望这两个仓库进行远程同步，这样github的仓库可以作为备份，又可以其他人通过该仓库来协作。</p><p>  首先，登录github上，然后在右上角找到“create a new repo”创建一个新的仓库。如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypmocbsj20u40gttbc.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypmocbsj20u40gttbc.jpg" alt="img"></a></p><p>在Repository name填入<code>testgit</code>，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypm6o2gj20si0idwh4.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypm6o2gj20si0idwh4.jpg" alt="img"></a></p><p>  目前，在GitHub上的这个<code>testgit</code>仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。</p><p>现在，我们根据GitHub的提示，在本地的<code>testgit</code>仓库下运行命令：</p><p>git remote add origin <a href="https://github.com/tugenhua0707/testgit.git" target="_blank" rel="noopener">https://github.com/tugenhua0707/testgit.git</a></p><p>所有的如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypk8b34j20hk070764.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypk8b34j20hk070764.jpg" alt="img"></a></p><p>把本地库的内容推送到远程，使用 git push命令，实际上是把当前分支master推送到远程。</p><p>由于远程库是空的，我们第一次推送master分支时，加上了 –u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。推送成功后，可以立刻在github页面中看到远程库的内容已经和本地一模一样了，上面的要输入github的用户名和密码如下所示：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypjhn5ij20t40i7mzp.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypjhn5ij20t40i7mzp.jpg" alt="img"></a></p><p>从现在起，只要本地作了提交，就可以通过如下命令：</p><p>git push origin master</p><p>把本地master分支的最新修改推送到github上了，现在你就拥有了真正的分布式版本库了。</p><p>\2. 如何从远程库克隆？</p><p>上面我们了解了先有本地库，后有远程库时候，如何关联远程库。</p><p>现在我们想，假如远程库有新的内容了，我想克隆到本地来 如何克隆呢？</p><p>首先，登录github，创建一个新的仓库，名字叫testgit2.如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyphv15sj20t10gs775.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyphv15sj20t10gs775.jpg" alt="img"></a></p><p>如下，我们看到：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypexzvuj20ss0dgabs.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypexzvuj20ss0dgabs.jpg" alt="img"></a></p><p>现在，远程库已经准备好了，下一步是使用命令git clone克隆一个本地库了。如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypec5t0j20hp03jwf6.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypec5t0j20hp03jwf6.jpg" alt="img"></a></p><p>接着在我本地目录下 生成testgit2目录了，如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypdbpwnj20jt05hmxr.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypdbpwnj20jt05hmxr.jpg" alt="img"></a></p><p>六：创建与合并分支。</p><p>在  版本回填退里，你已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。</p><p>首先，我们来创建dev分支，然后切换到dev分支上。如下操作：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypab36sj20bc04nweu.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypab36sj20bc04nweu.jpg" alt="img"></a></p><p>git checkout 命令加上 –b参数表示创建并切换，相当于如下2条命令</p><p>git branch dev</p><p>git checkout dev</p><p>git branch查看分支，会列出所有的分支，当前分支前面会添加一个星号。然后我们在dev分支上继续做demo，比如我们现在在readme.txt再增加一行 7777777777777</p><p>首先我们先来查看下readme.txt内容，接着添加内容77777777，如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp9es90j20at0awjsq.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp9es90j20at0awjsq.jpg" alt="img"></a></p><p>现在dev分支工作已完成，现在我们切换到主分支master上，继续查看readme.txt内容如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp8mng3j20hm05qaav.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp8mng3j20hm05qaav.jpg" alt="img"></a></p><p>现在我们可以把dev分支上的内容合并到分支master上了，可以在master分支上，使用如下命令 git merge dev 如下所示：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp83uksj20es073gmi.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp83uksj20es073gmi.jpg" alt="img"></a></p><p>git merge命令用于合并指定分支到当前分支上，合并后，再查看readme.txt内容，可以看到，和dev分支最新提交的是完全一样的。</p><p>注意到上面的<em>Fast-forward</em>信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。</p><p>合并完成后，我们可以接着删除dev分支了，操作如下：</p><p>总结创建与合并分支命令如下：</p><p>  查看分支：git branch</p><p>  创建分支：git branch name</p><p>  切换分支：git checkout name</p><p>创建+切换分支：git checkout –b name</p><p>合并某分支到当前分支：git merge name</p><p>删除分支：git branch –d name</p><h4 id="如何解决冲突"><a href="#如何解决冲突" class="headerlink" title="如何解决冲突"></a>如何解决冲突</h4><p>下面我们还是一步一步来，先新建一个新分支，比如名字叫fenzhi1，在readme.txt添加一行内容8888888，然后提交，如下所示：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp4jq8yj20ft0cu40a.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp4jq8yj20ft0cu40a.jpg" alt="img"></a></p><p>同样，我们现在切换到master分支上来，也在最后一行添加内容，内容为99999999，如下所示：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp3w0l1j20g80dwmz7.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp3w0l1j20g80dwmz7.jpg" alt="img"></a></p><p>现在我们需要在master分支上来合并fenzhi1，如下操作：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp1wo2ij20hm0gddi9.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp1wo2ij20hm0gddi9.jpg" alt="img"></a></p><p>Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，其中&lt;&lt;&lt;HEAD是指主分支修改的内容，&gt;&gt;&gt;&gt;&gt;fenzhi1 是指fenzhi1上修改的内容，我们可以修改下如下后保存：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp11x4zj20g107e3zd.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp11x4zj20g107e3zd.jpg" alt="img"></a></p><p>如果我想查看分支合并的情况的话，需要使用命令 git log.命令行演示如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp0aj6uj20dt0o5gph.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp0aj6uj20dt0o5gph.jpg" alt="img"></a></p><h4 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h4><p>通常合并分支时，git一般使用”Fast forward”模式，在这种模式下，删除分支后，会丢掉分支信息，现在我们来使用带参数 –no-ff来禁用”Fast forward”模式。首先我们来做demo演示下：</p><ol><li>创建一个dev分支。</li><li>修改readme.txt内容。</li><li>添加到暂存区。</li><li>切换回主分支(master)。</li><li>合并dev分支，使用命令 git merge –no-ff -m “注释” dev</li><li>查看历史记录</li></ol><p>截图如下：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoz5m31j20gr0lon0y.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoz5m31j20gr0lon0y.jpg" alt="img"></a></p><p><strong>分支策略：</strong>首先master主分支应该是非常稳定的，也就是用来发布新版本，一般情况下不允许在上面干活，干活一般情况下在新建的dev分支上干活，干完后，比如上要发布，或者说dev分支代码稳定后可以合并到主分支master上来。</p><p>七：bug分支：</p><p>   在开发中，会经常碰到bug问题，那么有了bug就需要修复，在Git中，分支是很强大的，每个bug都可以通过一个临时分支来修复，修复完成后，合并分支，然后将临时的分支删除掉。</p><p>比如我在开发中接到一个404 bug时候，我们可以创建一个404分支来修复它，但是，当前的dev分支上的工作还没有提交。比如如下：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoy0x5yj20he04m74v.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoy0x5yj20he04m74v.jpg" alt="img"></a></p><p>  并不是我不想提交，而是工作进行到一半时候，我们还无法提交，比如我这个分支bug要2天完成，但是我issue-404 bug需要5个小时内完成。怎么办呢？还好，Git还提供了一个stash功能，可以把当前工作现场 ”隐藏起来”，等以后恢复现场后继续工作。如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoxn4t8j20i3058dgo.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoxn4t8j20i3058dgo.jpg" alt="img"></a></p><p>  所以现在我可以通过创建issue-404分支来修复bug了。</p><p>首先我们要确定在那个分支上修复bug，比如我现在是在主分支master上来修复的，现在我要在master分支上创建一个临时分支，演示如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyowmdooj20gp0etq55.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyowmdooj20gp0etq55.jpg" alt="img"></a></p><p>修复完成后，切换到master分支上，并完成合并，最后删除issue-404分支。演示如下：</p><p>现在，我们回到dev分支上干活了。</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyou5898j20bq03s0t6.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyou5898j20bq03s0t6.jpg" alt="img"></a></p><p>工作区是干净的，那么我们工作现场去哪里呢？我们可以使用命令 git stash list来查看下。如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyot6ny2j20c202lmxg.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyot6ny2j20c202lmxg.jpg" alt="img"></a></p><p>工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，可以使用如下2个方法：</p><ol><li>git stash apply恢复，恢复后，stash内容并不删除，你需要使用命令git stash drop来删除。</li><li>另一种方式是使用git stash pop,恢复的同时把stash内容也删除了。</li></ol><p>​     演示如下</p><p> <a href="https://images2015.cnblogs.com/blog/762349/201610/762349-20161026134059296-2019917854.png" target="_blank" rel="noopener"><img src="https://images2015.cnblogs.com/blog/762349/201610/762349-20161026134059296-2019917854.png" alt="img"></a></p><h4 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h4><p>当你从远程库克隆时候，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且远程库的默认名称是origin。</p><ol><li>要查看远程库的信息 使用 git remote</li><li>要查看远程库的详细信息 使用 git remote –v</li></ol><p>如下演示：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyor8ayjj20h704pt9e.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyor8ayjj20h704pt9e.jpg" alt="img"></a></p><p><strong>一：推送分支：</strong></p><p>   推送分支就是把该分支上所有本地提交到远程库中，推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上：</p><p>   使用命令 git push origin master</p><p>比如我现在的github上的readme.txt代码如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyoolky9j20n00crt9x.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyoolky9j20n00crt9x.jpg" alt="img"></a></p><p>本地的readme.txt代码如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoljbdoj20bp05p74u.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoljbdoj20bp05p74u.jpg" alt="img"></a></p><p>现在我想把本地更新的readme.txt代码推送到远程库中，使用命令如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoklccxj20f105nmy8.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoklccxj20f105nmy8.jpg" alt="img"></a></p><p>我们可以看到如上，推送成功，我们可以继续来截图github上的readme.txt内容 如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyojp2l7j20mi0dgdh4.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyojp2l7j20mi0dgdh4.jpg" alt="img"></a></p><p>可以看到 推送成功了，如果我们现在要推送到其他分支，比如dev分支上，我们还是那个命令 git push origin dev</p><p>那么一般情况下，那些分支要推送呢？</p><ol><li><p>master分支是主分支，因此要时刻与远程同步。</p></li><li><p>一些修复bug分支不需要推送到远程去，可以先合并到主分支上，然后把主分支master推送到远程去。</p></li><li><h4 id="抓取分支"><a href="#抓取分支" class="headerlink" title="抓取分支"></a>抓取分支</h4></li><li><p>多人协作时，大家都会往master分支上推送各自的修改。现在我们可以模拟另外一个同事，可以在另一台电脑上（注意要把SSH key添加到github上）或者同一台电脑上另外一个目录克隆，新建一个目录名字叫testgit2</p><p>但是我首先要把dev分支也要推送到远程去，如下</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoilae8j20dz047jrw.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoilae8j20dz047jrw.jpg" alt="img"></a></p><p>接着进入testgit2目录，进行克隆远程的库到本地来，如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyofwtkzj20e404qdgn.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyofwtkzj20e404qdgn.jpg" alt="img"></a></p><p>现在目录下生成有如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoerppxj20jy07475a.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoerppxj20jy07475a.jpg" alt="img"></a></p><p>现在我们的小伙伴要在dev分支上做开发，就必须把远程的origin的dev分支到本地来，于是可以使用命令创建本地dev分支：git checkout –b dev origin/dev</p><p>现在小伙伴们就可以在dev分支上做开发了，开发完成后把dev分支推送到远程库时。</p><p>如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyodj3j4j20gq0katc9.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyodj3j4j20gq0katc9.jpg" alt="img"></a></p><p>小伙伴们已经向origin/dev分支上推送了提交，而我在我的目录文件下也对同样的文件同个地方作了修改，也试图推送到远程库时，如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyocm8nlj20hz0l3jvp.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyocm8nlj20hz0l3jvp.jpg" alt="img"></a></p><p>由上面可知：推送失败，因为我的小伙伴最新提交的和我试图推送的有冲突，解决的办法也很简单，上面已经提示我们，先用git pull把最新的提交从origin/dev抓下来，然后在本地合并，解决冲突，再推送。</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoblpvij20gi07ugmx.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoblpvij20gi07ugmx.jpg" alt="img"></a></p><p><em>git pull</em>也失败了，原因是没有指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接：如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyoab9gfj20hy05j0tu.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyoab9gfj20hy05j0tu.jpg" alt="img"></a></p><p>这回<em>git pull</em>成功，但是合并有冲突，需要手动解决，解决的方法和分支管理中的 解决冲突完全一样。解决后，提交，再push：</p><p>我们可以先来看看readme.txt内容了。</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyo7l3o6j20ef07p74y.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyo7l3o6j20ef07p74y.jpg" alt="img"></a></p><p>现在手动已经解决完了，我接在需要再提交，再push到远程库里面去。如下所示：<br><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyo5em1aj20gt0dcwgv.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyo5em1aj20gt0dcwgv.jpg" alt="img"></a></p><p>因此：多人协作工作模式一般是这样的：</p><ol><li><p>首先，可以试图用git push origin branch-name推送自己的修改.</p></li><li><p>如果推送失败，则因为远程分支比你的本地更新早，需要先用git pull试图合并。</p></li><li><p>如果合并有冲突，则需要解决冲突，并在本地提交。再用git push origin branch-name推送。</p></li><li><h4 id="Git-基本常用命令如下"><a href="#Git-基本常用命令如下" class="headerlink" title="Git**基本常用命令如下**"></a><strong>Git**</strong>基本常用命令如下**</h4></li><li><p>mkdir：     XX (创建一个空目录 XX指目录名)</p><p>pwd：     显示当前目录的路径。</p><p>git init     把当前的目录变成可以管理的git仓库，生成隐藏.git文件。</p><p>git add XX    把xx文件添加到暂存区去。</p><p>git commit –m “XX” 提交文件 –m 后面的是注释。</p><p>git status    查看仓库状态</p><p>git diff XX   查看XX文件修改了那些内容</p><p>git log     查看历史记录</p><p>git reset –hard HEAD^ 或者 git reset –hard HEAD~ 回退到上一个版本</p><p>​            (如果想回退到100个版本，使用git reset –hard HEAD~100 )</p><p>cat XX     查看XX文件内容</p><p>git reflog    查看历史记录的版本号id</p><p>git checkout — XX 把XX文件在工作区的修改全部撤销。</p><p>git rm XX     删除XX文件</p><p>git remote add origin <a href="https://github.com/ev-power/XiaoYong" target="_blank" rel="noopener">https://github.com/ev-power/XiaoYong</a> 关联一个远程库</p><p>git push –u(第一次要用-u 以后不需要) origin master 把当前master分支推送到远程库</p><p>git clone <a href="https://github.com/ev-power/XiaoYong" target="_blank" rel="noopener">https://github.com/ev-power/XiaoYong</a> 从远程库中克隆</p><p>git checkout –b dev 创建dev分支 并切换到dev分支上</p><p>git branch 查看当前所有的分支</p><p>git checkout master 切换回master分支</p><p>git merge dev  在当前的分支上合并dev分支</p><p>git branch –d dev 删除dev分支</p><p>git branch name 创建分支</p><p>git stash 把当前的工作隐藏起来 等以后恢复现场后继续工作</p><p>git stash list 查看所有被隐藏的文件列表</p><p>git stash apply 恢复被隐藏的文件，但是内容不删除</p><p>git stash drop 删除文件</p><p>git stash pop 恢复文件的同时 也删除文件</p><p>git remote 查看远程库的信息</p><p>git remote –v 查看远程库的详细信息</p><p>git push origin master Git会把master分支推送到远程库对应的远程分支上   </p></li><li><p>本文非原创博客，部分内容有所更改，原文出自：<a href="http://www.cnblogs.com/tugenhua0707/p/4050072.html" target="_blank" rel="noopener">http://www.cnblogs.com/tugenhua0707/p/4050072.html</a></p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;讨论了许多种git的情况，非常详细的git报告&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2gy1gds54g3odrj206402kmwz.jpg&quot; alt=&quot;undefined&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Git" scheme="http://yoursite.com/categories/Git/"/>
    
    
      <category term="Git" scheme="http://yoursite.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>朝花夕拾</title>
    <link href="http://yoursite.com/2020/04/09/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    <id>http://yoursite.com/2020/04/09/朝花夕拾/</id>
    <published>2020-04-09T07:29:23.635Z</published>
    <updated>2020-05-06T12:17:39.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>资质低下 三心二意 昨日知识 朝花夕拾</p></blockquote><a id="more"></a> <h3 id="更友好的创建对象方式"><a href="#更友好的创建对象方式" class="headerlink" title="更友好的创建对象方式"></a>更友好的创建对象方式</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gdnjra9kkrj20sk0c6myf.jpg" alt="3d5024b55687373af54fcb9ef4e0eb4.png"></p><p>上面的方式，对JVM来说是更友好的，因为堆内存的调用无法避免，所以从栈内存这边入手解决内存问题是一个不错的解决的方式</p><hr><h3 id="下面代码是否线程安全"><a href="#下面代码是否线程安全" class="headerlink" title="下面代码是否线程安全"></a>下面代码是否线程安全</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Singleton instance; </span><br><span class="line">    <span class="function"><span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span></span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span>(Singleton.class) &#123;</span><br><span class="line">                <span class="keyword">if</span> (instance == <span class="keyword">null</span>) instance = <span class="keyword">new</span> Singleton();</span><br><span class="line">            &#125; </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">return</span> instance; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>乍一看类似饿汉式的单例，线程安全，其实是有问题的</p><p>虽然只有一个线程能够获得锁，并且这个锁还是类锁，所有对象共享的</p><p>关键在于 jvm 对 new 的优化，这个变量没有声明 volatile，new 不是一个线程安全的操作，</p><p>对于 new 这个指令，一般的顺序是申请内存空间，初始化内存空间，然后把内存地址赋给 instance 对象，但是 jvm 会对这段指令进行优化，优化之后变成 申请内存空间，内存地址赋给 instance 对象，初始化内存空间，这就导致 第二层检查可能会出错，标准写法只需要在变量前声明 volatile 即可。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gdnkbsp9sij20pp0gy75i.jpg" alt="677701574e4f69f35e226ed6bc9a380.png"></p><hr><h3 id="volatile利用了什么协议来实现可见性"><a href="#volatile利用了什么协议来实现可见性" class="headerlink" title="volatile利用了什么协议来实现可见性"></a>volatile利用了什么协议来实现可见性</h3><p>volatile 是通过内存屏障实现的，MESI协议，缓存一致性协议</p><p>JVM推荐书《The Java Language Specification》<br>volatile 修饰的变量如果值发生变化 发现线程的高速缓存与主存数据不一致时候 由于缓存一致性协议 则总线将高速缓存中的值清空 其他线程只能通过访问主存来获取最新的值 并缓存到告诉缓存上。</p><hr><h3 id="Java-Trainsient-关键字"><a href="#Java-Trainsient-关键字" class="headerlink" title="Java Trainsient 关键字"></a>Java Trainsient 关键字</h3><p>1.一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 </p><p>2.transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口。 </p><p>3.一个静态变量不管是否被transient修饰，均不能被序列化。 </p><p>使用总结和场景：某个类的有些属性需要序列化，其他属性不需要被序列化，比如：敏感信息（如密码，银行卡号等），java 的transient关键字为我们提供了便利，你只需要实现Serilizable接口，将不需要序列化的属性前添加关键字transient，序列化对象的时候，这个属性就不会序列化到指定的目的地中。</p><h3 id="多线程中Random的使用"><a href="#多线程中Random的使用" class="headerlink" title="多线程中Random的使用"></a>多线程中Random的使用</h3><p>1.不要在多个线程间共享一个java.util.Random实例，而该把它放入ThreadLocal之中。</p><p>2.Java7以上我们更推荐使用java.util.concurrent.ThreadLocalRandom。</p><p>下面两条建议是 IDEA给的:</p><p>1.不要将将随机数放大10的若干倍然后取整，直接使用Random对象的nextInt或者nextLong方法</p><p>2.Math.random()应避免在多线程环境下使用</p><h3 id="为什么阿里禁止使用Executor创建线程池"><a href="#为什么阿里禁止使用Executor创建线程池" class="headerlink" title="为什么阿里禁止使用Executor创建线程池"></a>为什么阿里禁止使用Executor创建线程池</h3><p>阿里规约之所以强制要求手动创建线程池，也是和这些参数有关。具体为什么不允许，规约是这么说的：</p><p>线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。</p><p>Executor提供的四个静态方法创建线程池，但是阿里规约却并不建议使用它。</p><p>Executors各个方法的弊端：<br>1）newFixedThreadPool和newSingleThreadExecutor:<br>  主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至OOM。<br>2）newCachedThreadPool和newScheduledThreadPool:<br>  主要问题是线程数最大数是Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至OOM。</p><p>看一下这两种弊端怎么导致的。</p><p>第一种，newFixedThreadPool和newSingleThreadExecutor分别获得 FixedThreadPool 类型的线程池 和  SingleThreadExecutor 类型的线程池。　</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">new</span> ThreadPoolExecutor(nThreads, nThreads,</span><br><span class="line">                                     <span class="number">0L</span>, TimeUnit.MILLISECONDS,</span><br><span class="line">                                     <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public static ExecutorService newSingleThreadExecutor() &#123;</span><br><span class="line">       return new FinalizableDelegatedExecutorService</span><br><span class="line">           (new ThreadPoolExecutor(1, 1,</span><br><span class="line">                                   0L, TimeUnit.MILLISECONDS,</span><br><span class="line">                                   new LinkedBlockingQueue&lt;Runnable&gt;()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为，创建了一个无界队列LinkedBlockingQueuesize，是一个最大值为Integer.MAX_VALUE的线程阻塞队列，当添加任务的速度大于线程池处理任务的速度，可能会在队列堆积大量的请求，消耗很大的内存，甚至导致OOM。</p><h3 id="阿里开发手册上不推荐（禁止）使用Double的根本原因"><a href="#阿里开发手册上不推荐（禁止）使用Double的根本原因" class="headerlink" title="阿里开发手册上不推荐（禁止）使用Double的根本原因"></a>阿里开发手册上不推荐（禁止）使用Double的根本原因</h3><p>精度丢失就不谈了，稍微深入一下为什么精度会丢失，分为一些不同情况</p><p><strong>典型现象（一）：条件判断超预期</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">System.out.println( <span class="number">1f</span> == <span class="number">0.9999999f</span> );   <span class="comment">// 打印：false</span></span><br><span class="line">System.out.println( <span class="number">1f</span> == <span class="number">0.99999999f</span> );  <span class="comment">// 打印：true    纳尼？</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.0</span>（十进制）</span><br><span class="line">    ↓</span><br><span class="line"><span class="number">00111111</span> <span class="number">10000000</span> <span class="number">00000000</span> <span class="number">00000000</span>（二进制）</span><br><span class="line">    ↓</span><br><span class="line"><span class="number">0x3F800000</span>（十六进制）</span><br><span class="line">    </span><br><span class="line"><span class="number">0.99999999</span>（十进制）</span><br><span class="line">    ↓</span><br><span class="line"><span class="number">00111111</span> <span class="number">10000000</span> <span class="number">00000000</span> <span class="number">00000000</span>（二进制）</span><br><span class="line">    ↓</span><br><span class="line"><span class="number">0x3F800000</span>（十六进制）</span><br><span class="line">    </span><br><span class="line">果不其然，这两个十进制浮点数的底层二进制表示是一毛一样的，怪不得==的判断结果返回<span class="keyword">true</span>！</span><br><span class="line"></span><br><span class="line">浮点数的精度问题。</span><br><span class="line">    </span><br><span class="line">浮点数在计算机中的存储方式遵循IEEE <span class="number">754</span> 浮点数计数标准，可以用科学计数法表示为：</span><br><span class="line">    <span class="number">1</span> + <span class="number">2</span> + <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>、符号部分（S）</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>-正  <span class="number">1</span>-负</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、阶码部分（E）（指数部分）：</span><br><span class="line"></span><br><span class="line">对于<span class="keyword">float</span>型浮点数，指数部分<span class="number">8</span>位，考虑可正可负，因此可以表示的指数范围为-<span class="number">127</span> ~ <span class="number">128</span></span><br><span class="line">对于<span class="keyword">double</span>型浮点数，指数部分<span class="number">11</span>位，考虑可正可负，因此可以表示的指数范围为-<span class="number">1023</span> ~ <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>、尾数部分（M）：</span><br><span class="line"></span><br><span class="line">浮点数的精度是由尾数的位数来决定的：</span><br><span class="line"></span><br><span class="line">对于<span class="keyword">float</span>型浮点数，尾数部分<span class="number">23</span>位，换算成十进制就是 <span class="number">2</span>^<span class="number">23</span>=<span class="number">8388608</span>，所以十进制精度只有<span class="number">6</span> ~ <span class="number">7</span>位；</span><br><span class="line">对于<span class="keyword">double</span>型浮点数，尾数部分<span class="number">52</span>位，换算成十进制就是 <span class="number">2</span>^<span class="number">52</span> = <span class="number">4503599627370496</span>，所以十进制精度只有<span class="number">15</span> ~ <span class="number">16</span>位</span><br><span class="line"></span><br><span class="line">所以对于上面的数值<span class="number">0.99999999f</span>，很明显已经超过了<span class="keyword">float</span>型浮点数据的精度范围，出问题也是在所难免的。</span><br></pre></td></tr></table></figure><p><strong>典型现象（二）：数据转换超预期</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> f = <span class="number">1.1f</span>;</span><br><span class="line"><span class="keyword">double</span> d = (<span class="keyword">double</span>) f;</span><br><span class="line">System.out.println(f);  <span class="comment">// 打印：1.1</span></span><br><span class="line">System.out.println(d);  <span class="comment">// 打印：1.100000023841858  纳尼？</span></span><br></pre></td></tr></table></figure><p><strong>典型现象（三）：基本运算超预期</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">System.out.println( <span class="number">0.2</span> + <span class="number">0.7</span> );  </span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印：0.8999999999999999   纳尼？</span></span><br></pre></td></tr></table></figure><p><strong>典型现象（四）：数据自增超预期</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> f1 = <span class="number">8455263f</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    System.out.println(f1);</span><br><span class="line">    f1++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 打印：8455263.0</span></span><br><span class="line"><span class="comment">// 打印：8455264.0</span></span><br><span class="line"><span class="comment">// 打印：8455265.0</span></span><br><span class="line"><span class="comment">// 打印：8455266.0</span></span><br><span class="line"><span class="comment">// 打印：8455267.0</span></span><br><span class="line"><span class="comment">// 打印：8455268.0</span></span><br><span class="line"><span class="comment">// 打印：8455269.0</span></span><br><span class="line"><span class="comment">// 打印：8455270.0</span></span><br><span class="line"><span class="comment">// 打印：8455271.0</span></span><br><span class="line"><span class="comment">// 打印：8455272.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> f2 = <span class="number">84552631f</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    System.out.println(f2);</span><br><span class="line">    f2++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br><span class="line"><span class="comment">//    打印：8.4552632E7   纳尼？不是 +1了吗？</span></span><br></pre></td></tr></table></figure><p>解决办法：</p><p>1.我们我们可以用字符串或者数组来表示这种大数，然后按照四则运算的规则来手动模拟出具体计算过程，中间还需要考虑各种诸如：<strong>进位、借位、符号</strong>等等问题的处理，有点复杂。</p><ol start="2"><li>JDK早已为我们考虑到了浮点数的计算精度问题，因此提供了专用于高精度数值计算的<strong>大数类</strong>来方便我们使用。</li></ol><h3 id="mac-清理maven仓库的脚本"><a href="#mac-清理maven仓库的脚本" class="headerlink" title="mac 清理maven仓库的脚本"></a>mac 清理maven仓库的脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 这里写你的仓库路径</span><br><span class="line">REPOSITORY_PATH=~/Documents/tools/apache-maven-3.0.3/repository</span><br><span class="line">echo 正在搜索...</span><br><span class="line">find $REPOSITORY_PATH -name &quot;*lastUpdated*&quot; | xargs rm -fr</span><br><span class="line">echo 删除完毕</span><br><span class="line"></span><br><span class="line">mac（linux）系统-创建.sh文件脚本执行（mac用.command终端也可以）</span><br></pre></td></tr></table></figure><h3 id="idea目录较多，文件名较长产生的错误"><a href="#idea目录较多，文件名较长产生的错误" class="headerlink" title="idea目录较多，文件名较长产生的错误"></a>idea目录较多，文件名较长产生的错误</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Error running &apos;ServiceStarter&apos;: Command line is too long. Shorten command line for ServiceStarter or also for Application default configuration.</span><br><span class="line"></span><br><span class="line">修改项目下 .idea\workspace.xml，找到标签 &lt;component name=&quot;PropertiesComponent&quot;&gt; ， 在标签里加一行 &lt;property name=&quot;dynamic.classpath&quot; value=&quot;true&quot; /&gt;</span><br></pre></td></tr></table></figure><h3 id="Log4J-指定屏蔽某些特定报警信息"><a href="#Log4J-指定屏蔽某些特定报警信息" class="headerlink" title="Log4J 指定屏蔽某些特定报警信息"></a>Log4J 指定屏蔽某些特定报警信息</h3><p>Logger.getLogger(“org.apache.library”).setLevel(Level.OFF)</p><h3 id="Flink-Source并行度为1的意义"><a href="#Flink-Source并行度为1的意义" class="headerlink" title="Flink Source并行度为1的意义"></a>Flink Source并行度为1的意义</h3><p>对于需要设置EventTime的流来说，我们的TimestampAssigner应该在Source之后立即调用，原因是时间戳分配器看到的元素的顺序应该和source操作符产生数据的顺序是一样的，否则就乱了，也就是说，任何分区操作都会将元素的顺序打乱，例如：改变并行度 keyBy操作等等。，所以最佳实践是：</p><p>在尽量接近数据源source操作符的地方分配时间戳和产生水位线，甚至最好在SourceFunction中分配时间戳和产生水位线。当然在分配时间戳和产生水位线之前可以对流进行map和filter操作是没问题的，也就是说必须是窄依赖。</p><h3 id="JB套件的一个实用功能"><a href="#JB套件的一个实用功能" class="headerlink" title="JB套件的一个实用功能"></a>JB套件的一个实用功能</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">之前没注意，更改变量名字的时候直接使用refactor就可以了，真的实用</span><br></pre></td></tr></table></figure><h3 id="zk使用的分布式协议并不是paxos"><a href="#zk使用的分布式协议并不是paxos" class="headerlink" title="zk使用的分布式协议并不是paxos"></a>zk使用的分布式协议并不是paxos</h3><p>而是zab协议</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;资质低下 三心二意 昨日知识 朝花夕拾&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Interview" scheme="http://yoursite.com/categories/Interview/"/>
    
    
      <category term="PICKS" scheme="http://yoursite.com/tags/PICKS/"/>
    
  </entry>
  
  <entry>
    <title>clearLastUpdated</title>
    <link href="http://yoursite.com/2020/03/17/clearLastUpdated/"/>
    <id>http://yoursite.com/2020/03/17/clearLastUpdated/</id>
    <published>2020-03-17T02:46:49.404Z</published>
    <updated>2020-04-10T17:05:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>mvn库 windows清理脚本<br>需要把mvn的位置改成自己的<br><a id="more"></a><br>cls<br>@ECHO OFF<br>SET CLEAR_PATH=C:<br>SET CLEAR_DIR=C:\Users\Administrator.m2\repository<br>color 0a<br>TITLE ClearLastUpdated For Windows<br>GOTO MENU<br>:MENU<br>CLS<br>ECHO.<br>ECHO. <em> </em> <em> </em>  ClearLastUpdated For Windows  <em> </em> <em> </em><br>ECHO. <em> </em><br>ECHO. <em> 1 清理</em>.lastUpdated <em><br>ECHO. </em> <em><br>ECHO. </em> 2 查看<em>.lastUpdated </em><br>ECHO. <em> </em><br>ECHO. <em> 3 退 出 </em><br>ECHO. <em> </em><br>ECHO. <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em><br>ECHO.<br>ECHO.请输入选择项目的序号：<br>set /p ID=<br>IF “%id%”==”1” GOTO cmd1<br>IF “%id%”==”2” GOTO cmd2<br>IF “%id%”==”3” EXIT<br>PAUSE<br>:cmd1<br>ECHO. 开始清理<br>%CLEAR_PATH%<br>cd %CLEAR_DIR%<br>for /r %%i in (<em>.lastUpdated) do del %%i<br>ECHO.OK<br>PAUSE<br>GOTO MENU<br>:cmd2<br>ECHO. 查看</em>.lastUpdated文件<br>%CLEAR_PATH%<br>cd %CLEAR_DIR%<br>for /r %%i in (*.lastUpdated) do echo %%i<br>ECHO.OK<br>PAUSE<br>GOTO MENU </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;mvn库 windows清理脚本&lt;br&gt;需要把mvn的位置改成自己的&lt;br&gt;
    
    </summary>
    
      <category term="Script" scheme="http://yoursite.com/categories/Script/"/>
    
    
      <category term="Script" scheme="http://yoursite.com/tags/Script/"/>
    
  </entry>
  
  <entry>
    <title>Scala Note</title>
    <link href="http://yoursite.com/2020/03/09/Scala%20Note/"/>
    <id>http://yoursite.com/2020/03/09/Scala Note/</id>
    <published>2020-03-09T10:03:41.710Z</published>
    <updated>2019-08-16T08:06:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。</p></blockquote><a id="more"></a> <h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h2 id="Scala-介绍"><a href="#Scala-介绍" class="headerlink" title="Scala 介绍"></a>Scala 介绍</h2><p>Scala 是 Scalable Language 的简写，是一门多范式的编程语言</p><p>联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计Scala。</p><p>Funnel是把函数式编程思想和Petri网相结合的一种编程语言。</p><p>Odersky先前的工作是Generic Java和javac（Sun Java编译器）。Java平台的Scala于2003年底/2004年初发布。.NET平台的Scala发布于2004年6月。该语言第二个版本，v2.0，发布于2006年3月。</p><p>截至2009年9月，最新版本是版本2.7.6 。Scala 2.8预计的特性包括重写的Scala类库（Scala collections library）、方法的命名参数和默认参数、包对象（package object），以及Continuation。</p><p>2009年4月，Twitter宣布他们已经把大部分后端程序从Ruby迁移到Scala，其余部分也打算要迁移。此外， Wattzon已经公开宣称，其整个平台都已经是基于Scala基础设施编写的。</p><hr><h2 id="环境部分："><a href="#环境部分：" class="headerlink" title="环境部分："></a>环境部分：</h2><p>安装：和Java一样也要配置环境变量</p><p>配置IDEA：</p><p>先安装插件Scala</p><p>然后创建Maven项目</p><p>因为Maven默认不支持Scala</p><p>创建完毕之后</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izm7uublj20f30ch0t5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izmsgpxxj20pw0lnab5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iznzcv6ej20sw0o13zt.jpg" alt></p><p>Scala文件夹标记为Source</p><h2 id="语法部分"><a href="#语法部分" class="headerlink" title="语法部分"></a>语法部分</h2><h3 id="Hello-Scala"><a href="#Hello-Scala" class="headerlink" title="Hello Scala"></a>Hello Scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"hello Scala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令台执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala -cp C:\Users\61661\Desktop\scala-1.0-SNAPSHOT.jar HelloScala</span><br></pre></td></tr></table></figure><h3 id="声明值和变量"><a href="#声明值和变量" class="headerlink" title="声明值和变量"></a>声明值和变量</h3><p>Scala声明变量有两种方式：<code>val</code> 和 <code>var</code></p><p><code>val</code>定义的值是不可变的，它不是一个常量，是不可变量，或者称之为只读变量。</p><p>Tips：</p><ol><li>Scala的匿名变量（为了运行程序，系统自动添加的变量）分配<code>val</code>。</li><li><code>val</code>定义的变量虽然不能改变其引用的内存地址，但是可以改变其引用的对象的内部的其他属性值。</li><li>为了减少可变性引起的bug，应该尽可能地使用不可变变量。变量类型可以省略，解析器会根据值进行推断。<code>val</code>和<code>var</code>声明变量时都必须初始化。</li></ol><h3 id="常用类型"><a href="#常用类型" class="headerlink" title="常用类型"></a>常用类型</h3><p>8种常用类型</p><table><thead><tr><th>类型</th><th>属性</th></tr></thead><tbody><tr><td>Boolean</td><td><code>true</code> 或者 <code>false</code></td></tr><tr><td>Byte</td><td>8位， 有符号</td></tr><tr><td>Short</td><td>16位， 有符号</td></tr><tr><td>Int</td><td>32位， 有符号</td></tr><tr><td>Long</td><td>64位， 有符号</td></tr><tr><td>Char</td><td>16位， 无符号</td></tr><tr><td>Float</td><td>32位， 单精度浮点数</td></tr><tr><td>Double</td><td>64位， 双精度浮点数</td></tr><tr><td>String</td><td>由Char数组组成</td></tr></tbody></table><p>与Java中的数据类型不同，Scala并不区分基本类型和引用类型，所以这些类型<strong>都是对象</strong></p><p>可以调用相对应的方法，String直接使用的是<code>java.lang.String</code></p><p>由于String实际是一系列Char的不可变的集合，Scala中大部分针对集合的操作，都可以用于String，具体来说，String的这些方法存在于类<code>scala.collection.immutable.StringOps</code>中。</p><p>由于String在需要时能隐式转换为<code>StringOps</code>，因此不需要任何额外的转换，String就可以使用这些方法。</p><p>每一种数据类型都有对应的<code>Rich*</code>类型，如<code>RichInt</code>、<code>RichChar</code>等，为基本类型提供了更多的有用操作。</p><h3 id="常用类型结构图"><a href="#常用类型结构图" class="headerlink" title="常用类型结构图"></a>常用类型结构图</h3><p>Scala中，所有的值都是类对象，而所有的类，包括值类型，都最终继承自一个统一的根类型<code>Any</code>。统一类型，是Scala的又一大特点。更特别的是，Scala中还定义了几个底层类<code>Bottom Class</code>，比如<code>Null</code>和<code>Nothing</code>。</p><ol><li><code>Null</code>是所有引用类型的子类型，而<code>Nothing</code>是所有类型的子类型。<code>Null</code>类只有一个实例对象，<code>null</code>，类似于Java中的<code>null</code>引用。<code>null</code>可以赋值给任意引用类型，但是不能赋值给值类型。</li><li><code>Nothing</code>，可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于<code>Nothing</code>是其他任意类型的子类，他还能跟要求返回值的方法兼容。</li><li><code>Unit</code>类型用来标识过程，也就是没有明确返回值的函数。 由此可见，<code>Unit</code>类似于<code>Java</code>里的<code>void</code>。<code>Unit</code>只有一个实例，()，这个实例也没有实质的意义。</li></ol><p>关系图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3jcuo85e8j20of0gtta1.jpg" alt></p><h3 id="算数操作符重载"><a href="#算数操作符重载" class="headerlink" title="算数操作符重载"></a>算数操作符重载</h3><p><code>+</code> <code>-</code> <code>*</code> <code>/</code> <code>%</code>可以完成和Java中相同的工作，但是有一点区别，他们都是方法。你几乎可以用任何符号来为方法命名。</p><p><code>1 + 2</code> 等同于 <code>1.+(2)</code></p><p>Tips: Scala中没有++、–操作符，需要通过+=、-=来实现同样的效果。</p><h3 id="调用函数与方法"><a href="#调用函数与方法" class="headerlink" title="调用函数与方法"></a>调用函数与方法</h3><p>在Scala中，一般情况下我们不会刻意的去区分<code>函数</code>与<code>方法</code>的区别，但是他们确实是不同的东西。</p><p>后面我们再详细探讨。首先我们要学会使用Scala来调用函数与方法。</p><h4 id="1-调用函数，求方根"><a href="#1-调用函数，求方根" class="headerlink" title="1.调用函数，求方根"></a>1.调用函数，求方根</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.math</span><br><span class="line">sqrt(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h4 id="2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"><a href="#2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）" class="headerlink" title="2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"></a>2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">BigInt</span>.probablePrime(<span class="number">16</span>, scala.util.<span class="type">Random</span>)</span><br></pre></td></tr></table></figure><h4 id="3-调用方法，非静态方法，使用对象调用"><a href="#3-调用方法，非静态方法，使用对象调用" class="headerlink" title="3.调用方法，非静态方法，使用对象调用"></a>3.调用方法，非静态方法，使用对象调用</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"HelloWorld"</span>.distinct</span><br></pre></td></tr></table></figure><h4 id="4-apply与update方法"><a href="#4-apply与update方法" class="headerlink" title="4.apply与update方法"></a>4.apply与update方法</h4><p>apply方法是调用时可以省略方法名的方法。用于构造和获取元素：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Hello"</span>(<span class="number">4</span>)  等同于  <span class="string">"Hello"</span>.apply(<span class="number">4</span>)</span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) 等同于 <span class="type">Array</span>.apply(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">如：</span><br><span class="line">println(<span class="string">"Hello"</span>(<span class="number">4</span>))</span><br><span class="line">println(<span class="string">"Hello"</span>.apply(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>在<code>StringOps</code>中你会发现一个 <code>def apply(n: Int): Char</code>方法定义。<code>update</code>方法也是调用时可以省略方法名的方法，用于元素的更新：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr(<span class="number">4</span>) = <span class="number">5</span>  等同于  arr.update(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">如：</span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">5</span>)</span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">2</span></span><br><span class="line">arr1.update(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(arr1.mkString(<span class="string">","</span>))</span><br></pre></td></tr></table></figure><h4 id="Option类型"><a href="#Option类型" class="headerlink" title="Option类型"></a>Option类型</h4><p>Scala为单个值提供了对象的包装器，表示为那种可能存在也可能不存在的值。他只有两个有效的子类对象，一个是Some，表示某个值，另外一个是None，表示为空，通过Option的使用，避免了使用null、空字符串等方式来表示缺少某个值的做法。</p><p>如：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>(<span class="string">"Alice"</span> -&gt; <span class="number">20</span>, <span class="string">"Bob"</span> -&gt; <span class="number">30</span>)</span><br><span class="line">println(map1.get(<span class="string">"Alice"</span>))</span><br><span class="line">println(map1.get(<span class="string">"Jone"</span>))</span><br></pre></td></tr></table></figure><h3 id="控制结构和函数"><a href="#控制结构和函数" class="headerlink" title="控制结构和函数"></a>控制结构和函数</h3><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>Scala中没有三目运算符，因为根本不需要。Scala中if else表达式是有返回值的，如果if或者else返回的类型不一样，就返回Any类型（所有类型的公共超类型）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> a3 = <span class="number">10</span></span><br><span class="line">    <span class="keyword">val</span> a4 =</span><br><span class="line">      <span class="comment">//返回类型一样</span></span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">        <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="string">"a3小于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> a5 = </span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">          <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    println(a4)</span><br><span class="line">    <span class="comment">//a3小于20</span></span><br><span class="line">    println(a5)</span><br><span class="line">    <span class="comment">//()</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果缺少一个判断，什么都没有返回，但是Scala认为任何表达式都会有值，对于空值，使用Unit类，写做()，叫做无用占位符，相当于Java中的void。</p><p>Tips: 行尾的位置不需要分号，只要能够从上下文判断出语句的终止即可。但是如果在单行中写多个语句，则需要分号分割。在Scala中，{}块包含一系列表达式，其结果也是一个表达式。块中最后一个表达式的值就是块的值。</p><h4 id="while-表达式"><a href="#while-表达式" class="headerlink" title="while 表达式"></a>while 表达式</h4><p>Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> while1 = <span class="keyword">while</span>(n &lt;= <span class="number">10</span>)&#123;</span><br><span class="line">      n += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(while1) <span class="comment">//()</span></span><br><span class="line">    println(n) <span class="comment">//11</span></span><br><span class="line">    <span class="comment">//Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>while循环的中断</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span></span><br><span class="line"><span class="keyword">val</span> loop = <span class="keyword">new</span> <span class="type">Breaks</span></span><br><span class="line">loop.breakable&#123;</span><br><span class="line">  <span class="keyword">while</span>(n &lt;= <span class="number">20</span>)&#123;</span><br><span class="line">    n += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">18</span>)&#123;</span><br><span class="line">      loop.<span class="keyword">break</span>()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(n)</span><br></pre></td></tr></table></figure><p>Tips: Scala并没有提供break和continue语句来退出循环，如果需要break，可以通过几种方法来做1、使用Boolean型的控制变量 2、使用嵌套函数，从函数中return 3、使用Breaks对象的break方法。</p><h4 id="for表达式"><a href="#for表达式" class="headerlink" title="for表达式"></a>for表达式</h4><p>Scala也为for循环这一常见的控制结构提供了非常多的特性，这些for循环特性被称为for推导式(for comprehension)或for表达式(for expression).</p><h5 id="for示例1-to左右两边为前闭后闭的访问"><a href="#for示例1-to左右两边为前闭后闭的访问" class="headerlink" title="for示例1: to左右两边为前闭后闭的访问"></a>for示例1: to左右两边为前闭后闭的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span>)&#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例2：until左右两边为前闭后开的访问"><a href="#for示例2：until左右两边为前闭后开的访问" class="headerlink" title="for示例2：until左右两边为前闭后开的访问"></a>for示例2：until左右两边为前闭后开的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">3</span>; j &lt;- <span class="number">1</span> until <span class="number">3</span>) &#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"><a href="#for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue" class="headerlink" title="for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"></a>for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != <span class="number">2</span>) &#123;</span><br><span class="line">  print(i + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例4：引入变量"><a href="#for示例4：引入变量" class="headerlink" title="for示例4：引入变量"></a>for示例4：引入变量</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j = <span class="number">4</span> - i) &#123;</span><br><span class="line">  print(j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"><a href="#for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字" class="headerlink" title="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"></a>for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> for5 = <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i</span><br><span class="line">println(for5)</span><br></pre></td></tr></table></figure><h5 id="for示例6：使用花括号-代替小括号"><a href="#for示例6：使用花括号-代替小括号" class="headerlink" title="for示例6：使用花括号{}代替小括号()"></a>for示例6：使用花括号{}代替小括号()</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>&#123;</span><br><span class="line">  i &lt;- <span class="number">1</span> to <span class="number">3</span></span><br><span class="line">  j = <span class="number">4</span> - i&#125;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>Tips</strong>: {}和()对于for表达式来说都可以。for 推导式有一个不成文的约定：当for<br>推导式仅包含单一表达式时使用原括号，当其包含多个表达式时使用大括号。值得注意的是，使用原括号时，早前版本的Scala 要求表达式之间必须使用分号。</p><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><p>scala定义函数的标准格式为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">函数名</span></span>(参数名<span class="number">1</span>: 参数类型<span class="number">1</span>, 参数名<span class="number">2</span>: 参数类型<span class="number">2</span>) : 返回类型 = &#123;函数体&#125;</span><br></pre></td></tr></table></figure><p>函数示例1：返回Unit类型的函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">shout1</span><span class="params">(content: String)</span> : Unit </span>= &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例2：返回Unit类型的函数，但是没有显式指定返回类型。（当然也可以返回非Unit类型的值）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout2</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例3:返回值类型有多种可能，此时也可以省略Unit</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout3</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span>(content.length &gt;= <span class="number">3</span>)</span><br><span class="line">    content + <span class="string">"喵喵喵~"</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例4：带有默认值参数的函数，调用该函数时，可以只给无默认值的参数传递值，也可以都传递，新值会覆盖默认值；传递参数时如果不按照定义顺序，则可以通过参数名来指定。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout4</span></span>(content: <span class="type">String</span>, leg: <span class="type">Int</span> = <span class="number">4</span>) = &#123;</span><br><span class="line">  println(content + <span class="string">","</span> + leg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例5：变长参数（不确定个数参数，类似Java的…）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(args: <span class="type">Int</span>*) = &#123;</span><br><span class="line">  <span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span>(arg &lt;- args)</span><br><span class="line">    result += arg</span><br><span class="line">  result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>递归函数：递归函数在使用时必须有明确的返回值类型</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(n: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span>(n &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    n * factorial(n - <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Tips:</strong></p><p>1、Scala可以通过=右边的表达式  推断出函数的返回类型。如果函数体需要多个表达式，可以用代码块{}。</p><p>2、可以把return 当做  函数版本的break语句。</p><p>3、递归函数一定要指定返回类型。</p><p>4、变长参数通过* 来指定，所有参数会转化为一个seq序列。</p><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>我们将函数的返回类型为Unit的函数称之为过程。</p><h5 id="定义过程示例1："><a href="#定义过程示例1：" class="headerlink" title="定义过程示例1："></a>定义过程示例1：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) : <span class="type">Unit</span> = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例3：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>尖叫提示：这只是一个逻辑上的细分，如果因为该概念导致了理解上的混淆，可以暂时直接跳过过程这样的描述。毕竟过程，在某种意义上也是函数。</p><h4 id="懒值"><a href="#懒值" class="headerlink" title="懒值"></a>懒值</h4><p>当val被声明为lazy时，他的初始化将被推迟，直到我们首次对此取值，适用于初始化开销较大的场景。</p><p>lazy示例：通过lazy关键字的使用与否，来观察执行过程</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Lazy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    println(<span class="string">"init方法执行"</span>)</span><br><span class="line">    <span class="string">"嘿嘿嘿，我来了~"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> msg = init()</span><br><span class="line">    println(<span class="string">"lazy方法没有执行"</span>)</span><br><span class="line">    println(msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><p>当碰到异常情况时，方法抛出一个异常，终止方法本身的执行，异常传递到其调用者，调用者可以处理该异常，也可以升级到它的调用者。运行系统会一直这样升级异常，直到有调用者能处理它。 如果一直没有处理，则终止整个程序。</p><p>Scala的异常的工作机制和Java一样，但是Scala没有“checked”异常，你不需要声明说函数或者方法可能会抛出某种异常。受检异常在编译器被检查，java必须声明方法所会抛出的异常类型。</p><p><strong>抛出异常</strong>：用throw关键字，抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing，因为Nothing是所有类型的子类型，所以throw表达式可以用在需要类型的地方。</p><p><strong>捕捉异常：</strong>在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case字句。</p><p>异常捕捉的机制与其他语言中一样，如果有异常发生，catch字句是按次序捕捉的。因此，在catch字句中，越具体的异常越要靠前，越普遍的异常越靠后。 如果抛出的异常不在catch字句中，该异常则无法处理，会被升级到调用者处。</p><p>finally字句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作。</p><h5 id="异常示例："><a href="#异常示例：" class="headerlink" title="异常示例："></a>异常示例：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExceptionSyllabus</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">divider</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Float</span>= &#123;</span><br><span class="line">    <span class="keyword">if</span>(y == <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"0作为了除数"</span>)</span><br><span class="line">    <span class="keyword">else</span> x / y</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          println(divider(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; println(<span class="string">"捕获了异常："</span> + ex)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="数据结构特点"><a href="#数据结构特点" class="headerlink" title="数据结构特点"></a>数据结构特点</h4><p>Scala同时支持可变集合和不可变集合，不可变集合从不可变，可以安全的并发访问。</p><p>两个主要的包：</p><p>不可变集合：scala.collection.immutable</p><p>可变集合：  scala.collection.mutable</p><p>Scala优先采用不可变集合，对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本。</p><p>不可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hebn4l8j20qa0j63zo.jpg" alt></p><p>可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hf54tkwj20xc0l0q50.jpg" alt></p><h4 id="数组Array"><a href="#数组Array" class="headerlink" title="数组Array"></a>数组Array</h4><h5 id="1-定长数组"><a href="#1-定长数组" class="headerlink" title="1.定长数组"></a>1.定长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">7</span></span><br><span class="line">或：</span><br><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h5 id="2-变长数组"><a href="#2-变长数组" class="headerlink" title="2.变长数组"></a>2.变长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr2 = <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line"><span class="comment">//追加值</span></span><br><span class="line">arr2.append(<span class="number">7</span>)</span><br><span class="line"><span class="comment">//重新赋值</span></span><br><span class="line">arr2(<span class="number">0</span>) = <span class="number">7</span></span><br></pre></td></tr></table></figure><h5 id="3-定长数据与变长数据的装换"><a href="#3-定长数据与变长数据的装换" class="headerlink" title="3.定长数据与变长数据的装换"></a>3.定长数据与变长数据的装换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1.toBuffer</span><br><span class="line">arr2.toArray</span><br></pre></td></tr></table></figure><h5 id="4-多维数据"><a href="#4-多维数据" class="headerlink" title="4.多维数据"></a>4.多维数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr3 = <span class="type">Array</span>.ofDim[<span class="type">Double</span>](<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr3(<span class="number">1</span>)(<span class="number">1</span>) = <span class="number">11.11</span></span><br></pre></td></tr></table></figure><h5 id="5-与Java数组的互转"><a href="#5-与Java数组的互转" class="headerlink" title="5.与Java数组的互转"></a>5.与Java数组的互转</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala =&gt; Java</span></span><br><span class="line"><span class="keyword">val</span> arr4 = <span class="type">ArrayBuffer</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)</span><br><span class="line"><span class="comment">//Scala to Java</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.bufferAsJavaList</span><br><span class="line"><span class="keyword">val</span> javaArr = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(arr4)</span><br><span class="line">println(javaArr.command())</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java =&gt; scala</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.asScalaBuffer</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Buffer</span></span><br><span class="line"><span class="keyword">val</span> scalaArr: <span class="type">Buffer</span>[<span class="type">String</span>] = javaArr.command()</span><br><span class="line">println(scalaArr)</span><br></pre></td></tr></table></figure><p>6.数据的遍历</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(x &lt;- arr1) &#123;</span><br><span class="line">  println(x)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-元组-Tuple"><a href="#5-3-元组-Tuple" class="headerlink" title="5.3 元组 Tuple"></a>5.3 元组 Tuple</h4><p>元组可以理解为一个容器，可以存放各种相同或者不同类型的数据。</p><h5 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tuple1 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">"heiheihei"</span>)</span><br><span class="line">println(tuple1)</span><br></pre></td></tr></table></figure><h5 id="访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0"><a href="#访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0" class="headerlink" title="访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)"></a>访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val value1 = tuple1._4</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="元组的遍历"><a href="#元组的遍历" class="headerlink" title="元组的遍历"></a>元组的遍历</h5><p><strong>方式1</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (elem &lt;- tuple1.productIterator) &#123;</span><br><span class="line">  print(elem)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>方式2</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tuple1.productIterator.foreach(i =&gt; println(i))</span><br><span class="line">tuple1.productIterator.foreach(print(_))</span><br></pre></td></tr></table></figure><h4 id="列表List"><a href="#列表List" class="headerlink" title="列表List"></a>列表List</h4><p>如果List列表为空，则使用Nil来表示</p><h5 id="创建List"><a href="#创建List" class="headerlink" title="创建List"></a>创建List</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(list1)</span><br></pre></td></tr></table></figure><h5 id="访问List元素"><a href="#访问List元素" class="headerlink" title="访问List元素"></a>访问List元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = list1(<span class="number">1</span>)</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="List元素的追加"><a href="#List元素的追加" class="headerlink" title="List元素的追加"></a>List元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = list1 :+ <span class="number">99</span></span><br><span class="line">println(list2)</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="number">100</span> +: list1</span><br><span class="line">println(list3)</span><br></pre></td></tr></table></figure><p>List的创建与追加，符号“::”，注意观察去掉Nil和不去掉Nil的区别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list4 = <span class="number">1</span> :: <span class="number">2</span> :: <span class="number">3</span> :: list1 :: <span class="type">Nil</span></span><br><span class="line">println(list4)</span><br></pre></td></tr></table></figure><h4 id="队列Queue"><a href="#队列Queue" class="headerlink" title="队列Queue"></a>队列Queue</h4><p>队列数据存取符合先进先出的策略</p><h5 id="队列的创建"><a href="#队列的创建" class="headerlink" title="队列的创建"></a>队列的创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">val</span> q1 = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">Int</span>]</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="队列元素的追加"><a href="#队列元素的追加" class="headerlink" title="队列元素的追加"></a>队列元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1+=<span class="number">1</span></span><br><span class="line">print;n(q1)</span><br></pre></td></tr></table></figure><h5 id="队列的追加"><a href="#队列的追加" class="headerlink" title="队列的追加"></a>队列的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1 ++= <span class="type">List</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="按照进入队列的顺序删除元素"><a href="#按照进入队列的顺序删除元素" class="headerlink" title="按照进入队列的顺序删除元素"></a>按照进入队列的顺序删除元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1.dequeue()</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="塞入数据"><a href="#塞入数据" class="headerlink" title="塞入数据"></a>塞入数据</h5><h5 id="返回队列的第一个元素"><a href="#返回队列的第一个元素" class="headerlink" title="返回队列的第一个元素"></a>返回队列的第一个元素</h5><h5 id="返回队列的最后一个元素"><a href="#返回队列的最后一个元素" class="headerlink" title="返回队列的最后一个元素"></a>返回队列的最后一个元素</h5><h5 id="返回队列最后一个元素"><a href="#返回队列最后一个元素" class="headerlink" title="返回队列最后一个元素"></a>返回队列最后一个元素</h5><h5 id="返回除了第一个以外的元素"><a href="#返回除了第一个以外的元素" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h5 id="返回除了第一个以外的元素-1"><a href="#返回除了第一个以外的元素-1" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><h5 id="构造不可变映射"><a href="#构造不可变映射" class="headerlink" title="构造不可变映射"></a>构造不可变映射</h5><h5 id="构造可变映射"><a href="#构造可变映射" class="headerlink" title="构造可变映射"></a>构造可变映射</h5><h5 id="空的映射"><a href="#空的映射" class="headerlink" title="空的映射"></a>空的映射</h5><h5 id="对偶元组"><a href="#对偶元组" class="headerlink" title="对偶元组"></a>对偶元组</h5><h5 id="取值"><a href="#取值" class="headerlink" title="取值"></a>取值</h5><h5 id="更新值"><a href="#更新值" class="headerlink" title="更新值"></a>更新值</h5><h5 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h5><h4 id="集-Set"><a href="#集-Set" class="headerlink" title="集 Set"></a>集 Set</h4><h5 id="1-Set不可变集合的创建"><a href="#1-Set不可变集合的创建" class="headerlink" title="1.Set不可变集合的创建"></a>1.Set不可变集合的创建</h5><h5 id="2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"><a href="#2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合" class="headerlink" title="2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"></a>2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合</h5><h5 id="3-可变集合的元素添加"><a href="#3-可变集合的元素添加" class="headerlink" title="3.可变集合的元素添加"></a>3.可变集合的元素添加</h5><h5 id="4-可变集合的元素删除"><a href="#4-可变集合的元素删除" class="headerlink" title="4.可变集合的元素删除"></a>4.可变集合的元素删除</h5><h5 id="5-遍历"><a href="#5-遍历" class="headerlink" title="5.遍历"></a>5.遍历</h5><h5 id="6-Set更多常用操作"><a href="#6-Set更多常用操作" class="headerlink" title="6.Set更多常用操作"></a>6.Set更多常用操作</h5><h4 id="集合元素与函数的映射"><a href="#集合元素与函数的映射" class="headerlink" title="集合元素与函数的映射"></a>集合元素与函数的映射</h4><h5 id="map"><a href="#map" class="headerlink" title="map"></a>map</h5><h5 id="flatmap"><a href="#flatmap" class="headerlink" title="flatmap"></a>flatmap</h5><h4 id="化简、折叠、扫描"><a href="#化简、折叠、扫描" class="headerlink" title="化简、折叠、扫描"></a>化简、折叠、扫描</h4><h5 id="折叠，化简：将二次元函数引用于集合中的函数。"><a href="#折叠，化简：将二次元函数引用于集合中的函数。" class="headerlink" title="折叠，化简：将二次元函数引用于集合中的函数。"></a>折叠，化简：将二次元函数引用于集合中的函数。</h5><h5 id="折叠，化简：fold"><a href="#折叠，化简：fold" class="headerlink" title="折叠，化简：fold"></a>折叠，化简：fold</h5>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Language" scheme="http://yoursite.com/categories/Language/"/>
    
      <category term="Scala" scheme="http://yoursite.com/categories/Language/Scala/"/>
    
    
      <category term="Scala" scheme="http://yoursite.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Analog Data With TPCDS &amp; TPCH</title>
    <link href="http://yoursite.com/2020/03/09/analog_data/"/>
    <id>http://yoursite.com/2020/03/09/analog_data/</id>
    <published>2020-03-09T10:03:41.694Z</published>
    <updated>2020-04-10T17:05:22.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段<br>本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同</p></blockquote><a id="more"></a> <h3 id="大神fayson的日志："><a href="#大神fayson的日志：" class="headerlink" title="大神fayson的日志："></a>大神<code>fayson</code>的日志：</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488108&amp;idx=1&amp;sn=8f34c674bc12990d61a8f4de4ca3c728&amp;chksm=ec2ac265db5d4b731b93c4b7da0b3a24f0bf200274dd763531873bb4dd205e37d704ea2719b6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用TPC-DS生成测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488190&amp;idx=1&amp;sn=3f34824bdadbfa0823823121f86cafd4&amp;chksm=ec2ac2b7db5d4ba1484d6a6cf3161fdb90798d2605d2e79fa8bf633d32766c42cc8e2b41e206&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用hive-testbench生成Hive基准测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247489095&amp;idx=1&amp;sn=5af481742664f79146c58f425c9429d3&amp;chksm=ec2ac64edb5d4f58860db96ae4b452fda70b108527e4cc78c1978461ed62e67fcf997631d270&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Impala TPC-DS基准测试</a></p><h3 id="一、遇到的问题"><a href="#一、遇到的问题" class="headerlink" title="一、遇到的问题"></a>一、遇到的问题</h3><h4 id="1-源码无法编译"><a href="#1-源码无法编译" class="headerlink" title="1.源码无法编译"></a>1.源码无法编译</h4><p>源码下载下来之后build，需要的组件根本下载不了</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330ii05j5j20tt0bt75b.jpg" alt></p><p>这里Google到了一个办法</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330lkku7yj20tn0a7wf6.jpg" alt></p><p>先把包下载下来，放进对应的文件夹里然后编译</p><h4 id="2-安装遇到的问题"><a href="#2-安装遇到的问题" class="headerlink" title="2.安装遇到的问题"></a>2.安装遇到的问题</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330stw7enj20of0ebdgx.jpg" alt></p><p>类似配置冲突的问题 不知道为什么</p><p>yes和no我都分别选过，但是都不对，配置完成之后执行都有问题</p><p>我初步怀疑可能是版本问题，我下一个旧版本的试一试</p><p><a href="http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp" target="_blank" rel="noopener">TPC下载地址</a></p><p>之前用的是V 2.11的，现在下载一个V 2.10.1的试一下</p><p>执行完毕之后，首先报错</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g335c98sa0j20ns0g2tac.jpg" alt></p><p>权限不够，我重新使用hdfs用户来创建目录</p><p><code>hdfs</code>用户没有办法<code>git clone</code></p><p>我使用了<code>root</code>用户<code>clone</code>之后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown -R hdfs:hdfs hive-testbench/</span><br><span class="line">chmod -R 777 hive-testbench/</span><br></pre></td></tr></table></figure><p>将权限开放</p><p>其余操作使用HDFS完成</p><p>。。。</p><p>等了一段时间</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g336c9kby8j20rx05ydfz.jpg" alt></p><p>MR正常运行没有问题，可以MR运行完毕之后还是报错，不知道为什么</p><p>中间又做了很多尝试，失败的尝试在这不做记录</p><p>重点记录一下我在BUG日志中发现HiveServer2有一些问题</p><p>Google之后发现了是因为配置里面出现了问题<br>Java 8里面用原先的配置代码已经被舍弃了，更改完毕之后解决了这个问题，</p><p>但是<code>TPCDS</code>的问题还是没有解决，吐出一口老血</p><p>验证<code>HiveServer2</code>正确开启的代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> /usr/lib/hive/bin/beeline</span><br><span class="line"><span class="meta">beeline&gt;</span> !connect jdbc:hive2://localhost:10000 username password org.apache.hive.jdbc.HiveDriver</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt; SHOW TABLES;</span><br><span class="line">show tables;</span><br><span class="line">+-----------+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+</span><br><span class="line">+-----------+</span><br><span class="line">No rows selected (0.238 seconds)</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt;</span><br></pre></td></tr></table></figure><p>现在我解决问题的点还在于是不是<code>CDH</code>的配置还有一些问题</p><p>但是<code>TPCH</code>明明又能够生成数据的，难顶了</p><h4 id="3-数据从Hive转入Kudu速度过慢"><a href="#3-数据从Hive转入Kudu速度过慢" class="headerlink" title="3.数据从Hive转入Kudu速度过慢"></a>3.数据从Hive转入Kudu速度过慢</h4><p>从周五下班的点开始到周一上班，1000个Tasks，仅仅完成了210个，速度十分之慢。</p><hr><h3 id="二、解决办法"><a href="#二、解决办法" class="headerlink" title="二、解决办法"></a>二、解决办法</h3><h4 id="1-总结问题"><a href="#1-总结问题" class="headerlink" title="1.总结问题"></a>1.总结问题</h4><p>好好想了下我自己遇到的错误，有几个点，第一个点是<code>TPCH</code>是可以生成数据的，第二个点是我在编译<code>TPCDS</code>源码的过程中，报出了奇怪的提示，我一直怀疑可能是我编译的时候除了问题，但是重新编译了好几遍，一直没有找到解决办法。</p><p>这边在<a href="https://blog.csdn.net/sinat_36300982/article/details/89556220" target="_blank" rel="noopener">另一个技术博客上</a>找到了解决方案，可以在本地编译完成之后再上传到服务器，但是我看了一下这篇博客，他是用的官方原版的<code>hive-testbench</code>，里面会有一些错误，我直接下载了别人使用的版本hive14.zip(可以在TIM上下载)，然后<a href="http://dev.hortonworks.com.s3.amazonaws.com/hive-testbench/tpcds/TPCDS_Tools.zip" target="_blank" rel="noopener">下载TPCDS_Tools.zip</a>改名<code>tpcds_kit.zip</code>放进<code>tpcds</code>对应的文件夹就可以了，最后编译成功。</p><p>编译完成之后，数据在Hive上面，Hive上面生成了两个库，一个是<code>ORC</code>库，还有一个是TEXT库，<code>ORC</code>文件<code>impala</code>用不了就算了，迁移TEXT就行，代码可以在下面的<code>github</code>中找到，然后要注意的事情是最后<code>package</code>的代码，因为是<code>scala</code>，打包的代码和别的并不一样</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">finalName</span>&gt;</span>anlogSparkSQL<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 设置项目编译版本--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 用于编译scala代码到class --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>kuduimport.hiveToKudu<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用HUE里面的<code>Oozie</code>调用Spark程序的时候，如果想要在spark提交里面出现任务记录，应该添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.shuffle.memoryFraction=0.3</span><br><span class="line">--conf spark.yarn.historyServer.address=http://datanode127:18089</span><br><span class="line">--conf spark.eventLog.dir=hdfs://master126:8020/user/spark/spark2ApplicationHistory</span><br><span class="line">--conf spark.eventLog.enabled=true</span><br></pre></td></tr></table></figure><p><a href="https://github.com/YunKillerE/kudu-learning" target="_blank" rel="noopener">github/kudu-learning</a></p><hr><h4 id="2-自动生成Kudu表格脚本"><a href="#2-自动生成Kudu表格脚本" class="headerlink" title="2.自动生成Kudu表格脚本"></a>2.自动生成Kudu表格脚本</h4><p>脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists call_center;</span><br><span class="line"></span><br><span class="line">create table call_center(</span><br><span class="line">      cc_call_center_sk         bigint               </span><br><span class="line">,     cc_call_center_id         string              </span><br><span class="line">,     cc_rec_start_date        string                         </span><br><span class="line">,     cc_rec_end_date          string                         </span><br><span class="line">,     cc_closed_date_sk         bigint                       </span><br><span class="line">,     cc_open_date_sk           bigint                       </span><br><span class="line">,     cc_name                   string                   </span><br><span class="line">,     cc_class                  string                   </span><br><span class="line">,     cc_employees              int                       </span><br><span class="line">,     cc_sq_ft                  int                       </span><br><span class="line">,     cc_hours                  string                      </span><br><span class="line">,     cc_manager                string                   </span><br><span class="line">,     cc_mkt_id                 int                       </span><br><span class="line">,     cc_mkt_class              string                      </span><br><span class="line">,     cc_mkt_desc               string                  </span><br><span class="line">,     cc_market_manager         string                   </span><br><span class="line">,     cc_division               int                       </span><br><span class="line">,     cc_division_name          string                   </span><br><span class="line">,     cc_company                int                       </span><br><span class="line">,     cc_company_name           string                      </span><br><span class="line">,     cc_street_number          string                      </span><br><span class="line">,     cc_street_name            string                   </span><br><span class="line">,     cc_street_type            string                      </span><br><span class="line">,     cc_suite_number           string                      </span><br><span class="line">,     cc_city                   string                   </span><br><span class="line">,     cc_county                 string                   </span><br><span class="line">,     cc_state                  string                       </span><br><span class="line">,     cc_zip                    string                      </span><br><span class="line">,     cc_country                string                   </span><br><span class="line">,     cc_gmt_offset             double                  </span><br><span class="line">,     cc_tax_percentage         double</span><br><span class="line">,PRIMARY KEY(cc_call_center_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_page;</span><br><span class="line"></span><br><span class="line">create table catalog_page(</span><br><span class="line">      cp_catalog_page_sk        bigint               </span><br><span class="line">,     cp_catalog_page_id        string              </span><br><span class="line">,     cp_start_date_sk          bigint                       </span><br><span class="line">,     cp_end_date_sk            bigint                       </span><br><span class="line">,     cp_department             string                   </span><br><span class="line">,     cp_catalog_number         int                       </span><br><span class="line">,     cp_catalog_page_number    int                       </span><br><span class="line">,     cp_description            string                  </span><br><span class="line">,     cp_type                   string</span><br><span class="line">,PRIMARY KEY(cp_catalog_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_returns;</span><br><span class="line"></span><br><span class="line">create table catalog_returns</span><br><span class="line">(</span><br><span class="line">    cr_item_sk                bigint,</span><br><span class="line">cr_order_number           bigint,</span><br><span class="line">    cr_returned_date_sk       bigint,</span><br><span class="line">    cr_returned_time_sk       bigint,</span><br><span class="line">    cr_refunded_customer_sk   bigint,</span><br><span class="line">    cr_refunded_cdemo_sk      bigint,</span><br><span class="line">    cr_refunded_hdemo_sk      bigint,</span><br><span class="line">    cr_refunded_addr_sk       bigint,</span><br><span class="line">    cr_returning_customer_sk  bigint,</span><br><span class="line">    cr_returning_cdemo_sk     bigint,</span><br><span class="line">    cr_returning_hdemo_sk     bigint,</span><br><span class="line">    cr_returning_addr_sk      bigint,</span><br><span class="line">    cr_call_center_sk         bigint,</span><br><span class="line">    cr_catalog_page_sk        bigint,</span><br><span class="line">    cr_ship_mode_sk           bigint,</span><br><span class="line">    cr_warehouse_sk           bigint,</span><br><span class="line">    cr_reason_sk              bigint,</span><br><span class="line">    cr_return_quantity        int,</span><br><span class="line">    cr_return_amount          double,</span><br><span class="line">    cr_return_tax             double,</span><br><span class="line">    cr_return_amt_inc_tax     double,</span><br><span class="line">    cr_fee                    double,</span><br><span class="line">    cr_return_ship_cost       double,</span><br><span class="line">    cr_refunded_cash          double,</span><br><span class="line">    cr_reversed_charge        double,</span><br><span class="line">    cr_store_credit           double,</span><br><span class="line">    cr_net_loss               double</span><br><span class="line">,PRIMARY KEY(cr_item_sk,cr_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cr_item_sk) PARTITIONS 16</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_sales;</span><br><span class="line"></span><br><span class="line">create table catalog_sales</span><br><span class="line">(</span><br><span class="line">    cs_item_sk                bigint,</span><br><span class="line">    cs_order_number           bigint,</span><br><span class="line">    cs_sold_date_sk           bigint,</span><br><span class="line">    cs_sold_time_sk           bigint,</span><br><span class="line">    cs_ship_date_sk           bigint,</span><br><span class="line">    cs_bill_customer_sk       bigint,</span><br><span class="line">    cs_bill_cdemo_sk          bigint,</span><br><span class="line">    cs_bill_hdemo_sk          bigint,</span><br><span class="line">    cs_bill_addr_sk           bigint,</span><br><span class="line">    cs_ship_customer_sk       bigint,</span><br><span class="line">    cs_ship_cdemo_sk          bigint,</span><br><span class="line">    cs_ship_hdemo_sk          bigint,</span><br><span class="line">    cs_ship_addr_sk           bigint,</span><br><span class="line">    cs_call_center_sk         bigint,</span><br><span class="line">    cs_catalog_page_sk        bigint,</span><br><span class="line">    cs_ship_mode_sk           bigint,</span><br><span class="line">    cs_warehouse_sk           bigint,</span><br><span class="line">    cs_promo_sk               bigint,</span><br><span class="line">    cs_quantity               int,</span><br><span class="line">    cs_wholesale_cost         double,</span><br><span class="line">    cs_list_price             double,</span><br><span class="line">    cs_sales_price            double,</span><br><span class="line">    cs_ext_discount_amt       double,</span><br><span class="line">    cs_ext_sales_price        double,</span><br><span class="line">    cs_ext_wholesale_cost     double,</span><br><span class="line">    cs_ext_list_price         double,</span><br><span class="line">    cs_ext_tax                double,</span><br><span class="line">    cs_coupon_amt             double,</span><br><span class="line">    cs_ext_ship_cost          double,</span><br><span class="line">    cs_net_paid               double,</span><br><span class="line">    cs_net_paid_inc_tax       double,</span><br><span class="line">    cs_net_paid_inc_ship      double,</span><br><span class="line">    cs_net_paid_inc_ship_tax  double,</span><br><span class="line">    cs_net_profit             double</span><br><span class="line">,PRIMARY KEY(cs_item_sk,cs_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cs_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_address;</span><br><span class="line"></span><br><span class="line">create table customer_address</span><br><span class="line">(</span><br><span class="line">    ca_address_sk             bigint,</span><br><span class="line">    ca_address_id             string,</span><br><span class="line">    ca_street_number          string,</span><br><span class="line">    ca_street_name            string,</span><br><span class="line">    ca_street_type            string,</span><br><span class="line">    ca_suite_number           string,</span><br><span class="line">    ca_city                   string,</span><br><span class="line">    ca_county                 string,</span><br><span class="line">    ca_state                  string,</span><br><span class="line">    ca_zip                    string,</span><br><span class="line">    ca_country                string,</span><br><span class="line">    ca_gmt_offset             double,</span><br><span class="line">    ca_location_type          string</span><br><span class="line">,PRIMARY KEY(ca_address_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ca_address_sk) PARTITIONS 6</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_demographics;</span><br><span class="line"></span><br><span class="line">create table customer_demographics</span><br><span class="line">(</span><br><span class="line">    cd_demo_sk                bigint,</span><br><span class="line">    cd_gender                 string,</span><br><span class="line">    cd_marital_status         string,</span><br><span class="line">    cd_education_status       string,</span><br><span class="line">    cd_purchase_estimate      int,</span><br><span class="line">    cd_credit_rating          string,</span><br><span class="line">    cd_dep_count              int,</span><br><span class="line">    cd_dep_employed_count     int,</span><br><span class="line">    cd_dep_college_count      int </span><br><span class="line">,PRIMARY KEY(cd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer;</span><br><span class="line"></span><br><span class="line">create table customer</span><br><span class="line">(</span><br><span class="line">    c_customer_sk             bigint,</span><br><span class="line">    c_customer_id             string,</span><br><span class="line">    c_current_cdemo_sk        bigint,</span><br><span class="line">    c_current_hdemo_sk        bigint,</span><br><span class="line">    c_current_addr_sk         bigint,</span><br><span class="line">    c_first_shipto_date_sk    bigint,</span><br><span class="line">    c_first_sales_date_sk     bigint,</span><br><span class="line">    c_salutation              string,</span><br><span class="line">    c_first_name              string,</span><br><span class="line">    c_last_name               string,</span><br><span class="line">    c_preferred_cust_flag     string,</span><br><span class="line">    c_birth_day               int,</span><br><span class="line">    c_birth_month             int,</span><br><span class="line">    c_birth_year              int,</span><br><span class="line">    c_birth_country           string,</span><br><span class="line">    c_login                   string,</span><br><span class="line">    c_email_address           string,</span><br><span class="line">    c_last_review_date        string</span><br><span class="line">,PRIMARY KEY(c_customer_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (c_customer_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists date_dim;</span><br><span class="line"></span><br><span class="line">create table date_dim</span><br><span class="line">(</span><br><span class="line">    d_date_sk                 bigint,</span><br><span class="line">    d_date_id                 string,</span><br><span class="line">    d_date                    string,</span><br><span class="line">    d_month_seq               int,</span><br><span class="line">    d_week_seq                int,</span><br><span class="line">    d_quarter_seq             int,</span><br><span class="line">    d_year                    int,</span><br><span class="line">    d_dow                     int,</span><br><span class="line">    d_moy                     int,</span><br><span class="line">    d_dom                     int,</span><br><span class="line">    d_qoy                     int,</span><br><span class="line">    d_fy_year                 int,</span><br><span class="line">    d_fy_quarter_seq          int,</span><br><span class="line">    d_fy_week_seq             int,</span><br><span class="line">    d_day_name                string,</span><br><span class="line">    d_quarter_name            string,</span><br><span class="line">    d_holiday                 string,</span><br><span class="line">    d_weekend                 string,</span><br><span class="line">    d_following_holiday       string,</span><br><span class="line">    d_first_dom               int,</span><br><span class="line">    d_last_dom                int,</span><br><span class="line">    d_same_day_ly             int,</span><br><span class="line">    d_same_day_lq             int,</span><br><span class="line">    d_current_day             string,</span><br><span class="line">    d_current_week            string,</span><br><span class="line">    d_current_month           string,</span><br><span class="line">    d_current_quarter         string,</span><br><span class="line">    d_current_year            string </span><br><span class="line">,PRIMARY KEY(d_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (d_date_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists household_demographics;</span><br><span class="line"></span><br><span class="line">create table household_demographics</span><br><span class="line">(</span><br><span class="line">    hd_demo_sk                bigint,</span><br><span class="line">    hd_income_band_sk         bigint,</span><br><span class="line">    hd_buy_potential          string,</span><br><span class="line">    hd_dep_count              int,</span><br><span class="line">    hd_vehicle_count          int</span><br><span class="line">,PRIMARY KEY(hd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (hd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists income_band;</span><br><span class="line"></span><br><span class="line">create table income_band(</span><br><span class="line">      ib_income_band_sk         bigint               </span><br><span class="line">,     ib_lower_bound            int                       </span><br><span class="line">,     ib_upper_bound            int</span><br><span class="line">,PRIMARY KEY(ib_income_band_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ib_income_band_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists inventory;</span><br><span class="line"></span><br><span class="line">create table inventory</span><br><span class="line">(</span><br><span class="line">    inv_date_skbigint,</span><br><span class="line">    inv_item_skbigint,</span><br><span class="line">    inv_warehouse_skbigint,</span><br><span class="line">    inv_quantity_on_handint</span><br><span class="line">,PRIMARY KEY(inv_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (inv_date_sk) PARTITIONS 12</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists item;</span><br><span class="line"></span><br><span class="line">create table item</span><br><span class="line">(</span><br><span class="line">    i_item_sk                 bigint,</span><br><span class="line">    i_item_id                 string,</span><br><span class="line">    i_rec_start_date          string,</span><br><span class="line">    i_rec_end_date            string,</span><br><span class="line">    i_item_desc               string,</span><br><span class="line">    i_current_price           double,</span><br><span class="line">    i_wholesale_cost          double,</span><br><span class="line">    i_brand_id                int,</span><br><span class="line">    i_brand                   string,</span><br><span class="line">    i_class_id                int,</span><br><span class="line">    i_class                   string,</span><br><span class="line">    i_category_id             int,</span><br><span class="line">    i_category                string,</span><br><span class="line">    i_manufact_id             int,</span><br><span class="line">    i_manufact                string,</span><br><span class="line">    i_size                    string,</span><br><span class="line">    i_formulation             string,</span><br><span class="line">    i_color                   string,</span><br><span class="line">    i_units                   string,</span><br><span class="line">    i_container               string,</span><br><span class="line">    i_manager_id              int,</span><br><span class="line">    i_product_name            string</span><br><span class="line">,PRIMARY KEY(i_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (i_item_sk) PARTITIONS 4</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists promotion;</span><br><span class="line"></span><br><span class="line">create table promotion</span><br><span class="line">(</span><br><span class="line">    p_promo_sk                bigint,</span><br><span class="line">    p_promo_id                string,</span><br><span class="line">    p_start_date_sk           bigint,</span><br><span class="line">    p_end_date_sk             bigint,</span><br><span class="line">    p_item_sk                 bigint,</span><br><span class="line">    p_cost                    double,</span><br><span class="line">    p_response_target         int,</span><br><span class="line">    p_promo_name              string,</span><br><span class="line">    p_channel_dmail           string,</span><br><span class="line">    p_channel_email           string,</span><br><span class="line">    p_channel_catalog         string,</span><br><span class="line">    p_channel_tv              string,</span><br><span class="line">    p_channel_radio           string,</span><br><span class="line">    p_channel_press           string,</span><br><span class="line">    p_channel_event           string,</span><br><span class="line">    p_channel_demo            string,</span><br><span class="line">    p_channel_details         string,</span><br><span class="line">    p_purpose                 string,</span><br><span class="line">    p_discount_active         string </span><br><span class="line">,PRIMARY KEY(p_promo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (p_promo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists reason;</span><br><span class="line"></span><br><span class="line">create table reason(</span><br><span class="line">      r_reason_sk               bigint               </span><br><span class="line">,     r_reason_id               string              </span><br><span class="line">,     r_reason_desc             string                </span><br><span class="line">,PRIMARY KEY(r_reason_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (r_reason_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists ship_mode;</span><br><span class="line"></span><br><span class="line">create table ship_mode(</span><br><span class="line">      sm_ship_mode_sk           bigint               </span><br><span class="line">,     sm_ship_mode_id           string              </span><br><span class="line">,     sm_type                   string                      </span><br><span class="line">,     sm_code                   string                      </span><br><span class="line">,     sm_carrier                string                      </span><br><span class="line">,     sm_contract               string                      </span><br><span class="line">,PRIMARY KEY(sm_ship_mode_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (sm_ship_mode_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_returns;</span><br><span class="line"></span><br><span class="line">create table store_returns</span><br><span class="line">(</span><br><span class="line">    sr_item_sk                bigint,</span><br><span class="line">    sr_returned_date_sk       bigint,</span><br><span class="line">    sr_return_time_sk         bigint,</span><br><span class="line">    sr_customer_sk            bigint,</span><br><span class="line">    sr_cdemo_sk               bigint,</span><br><span class="line">    sr_hdemo_sk               bigint,</span><br><span class="line">    sr_addr_sk                bigint,</span><br><span class="line">    sr_store_sk               bigint,</span><br><span class="line">    sr_reason_sk              bigint,</span><br><span class="line">    sr_ticket_number          bigint,</span><br><span class="line">    sr_return_quantity        int,</span><br><span class="line">    sr_return_amt             double,</span><br><span class="line">    sr_return_tax             double,</span><br><span class="line">    sr_return_amt_inc_tax     double,</span><br><span class="line">    sr_fee                    double,</span><br><span class="line">    sr_return_ship_cost       double,</span><br><span class="line">    sr_refunded_cash          double,</span><br><span class="line">    sr_reversed_charge        double,</span><br><span class="line">    sr_store_credit           double,</span><br><span class="line">    sr_net_loss               double,</span><br><span class="line">PRIMARY KEY(sr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 32</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_sales;</span><br><span class="line"></span><br><span class="line">create table store_sales</span><br><span class="line">(</span><br><span class="line">    ss_item_sk                bigint,</span><br><span class="line">    ss_sold_date_sk           bigint,</span><br><span class="line">    ss_sold_time_sk           bigint,</span><br><span class="line">    ss_customer_sk            bigint,</span><br><span class="line">    ss_cdemo_sk               bigint,</span><br><span class="line">    ss_hdemo_sk               bigint,</span><br><span class="line">    ss_addr_sk                bigint,</span><br><span class="line">    ss_store_sk               bigint,</span><br><span class="line">    ss_promo_sk               bigint,</span><br><span class="line">    ss_ticket_number          bigint,</span><br><span class="line">    ss_quantity               int,</span><br><span class="line">    ss_wholesale_cost         double,</span><br><span class="line">    ss_list_price             double,</span><br><span class="line">    ss_sales_price            double,</span><br><span class="line">    ss_ext_discount_amt       double,</span><br><span class="line">    ss_ext_sales_price        double,</span><br><span class="line">    ss_ext_wholesale_cost     double,</span><br><span class="line">    ss_ext_list_price         double,</span><br><span class="line">    ss_ext_tax                double,</span><br><span class="line">    ss_coupon_amt             double,</span><br><span class="line">    ss_net_paid               double,</span><br><span class="line">    ss_net_paid_inc_tax       double,</span><br><span class="line">    ss_net_profit             double                  </span><br><span class="line">,PRIMARY KEY(ss_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ss_item_sk) PARTITIONS 96</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store;</span><br><span class="line"></span><br><span class="line">create table store</span><br><span class="line">(</span><br><span class="line">    s_store_sk                bigint,</span><br><span class="line">    s_store_id                string,</span><br><span class="line">    s_rec_start_date          string,</span><br><span class="line">    s_rec_end_date            string,</span><br><span class="line">    s_closed_date_sk          bigint,</span><br><span class="line">    s_store_name              string,</span><br><span class="line">    s_number_employees        int,</span><br><span class="line">    s_floor_space             int,</span><br><span class="line">    s_hours                   string,</span><br><span class="line">    s_manager                 string,</span><br><span class="line">    s_market_id               int,</span><br><span class="line">    s_geography_class         string,</span><br><span class="line">    s_market_desc             string,</span><br><span class="line">    s_market_manager          string,</span><br><span class="line">    s_division_id             int,</span><br><span class="line">    s_division_name           string,</span><br><span class="line">    s_company_id              int,</span><br><span class="line">    s_company_name            string,</span><br><span class="line">    s_street_number           string,</span><br><span class="line">    s_street_name             string,</span><br><span class="line">    s_street_type             string,</span><br><span class="line">    s_suite_number            string,</span><br><span class="line">    s_city                    string,</span><br><span class="line">    s_county                  string,</span><br><span class="line">    s_state                   string,</span><br><span class="line">    s_zip                     string,</span><br><span class="line">    s_country                 string,</span><br><span class="line">    s_gmt_offset              double,</span><br><span class="line">    s_tax_precentage          double                  </span><br><span class="line">,PRIMARY KEY(s_store_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (s_store_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists time_dim;</span><br><span class="line"></span><br><span class="line">create table time_dim</span><br><span class="line">(</span><br><span class="line">    t_time_sk                 bigint,</span><br><span class="line">    t_time_id                 string,</span><br><span class="line">    t_time                    int,</span><br><span class="line">    t_hour                    int,</span><br><span class="line">    t_minute                  int,</span><br><span class="line">    t_second                  int,</span><br><span class="line">    t_am_pm                   string,</span><br><span class="line">    t_shift                   string,</span><br><span class="line">    t_sub_shift               string,</span><br><span class="line">    t_meal_time               string</span><br><span class="line">,PRIMARY KEY(t_time_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (t_time_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists warehouse;</span><br><span class="line"></span><br><span class="line">create table warehouse(</span><br><span class="line">      w_warehouse_sk            bigint               </span><br><span class="line">,     w_warehouse_id            string              </span><br><span class="line">,     w_warehouse_name          string                   </span><br><span class="line">,     w_warehouse_sq_ft         int                       </span><br><span class="line">,     w_street_number           string                      </span><br><span class="line">,     w_street_name             string                   </span><br><span class="line">,     w_street_type             string                      </span><br><span class="line">,     w_suite_number            string                      </span><br><span class="line">,     w_city                    string                   </span><br><span class="line">,     w_county                  string                   </span><br><span class="line">,     w_state                   string                       </span><br><span class="line">,     w_zip                     string                      </span><br><span class="line">,     w_country                 string                   </span><br><span class="line">,     w_gmt_offset              double                  </span><br><span class="line">,PRIMARY KEY(w_warehouse_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (w_warehouse_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_page;</span><br><span class="line"></span><br><span class="line">create table web_page(</span><br><span class="line">      wp_web_page_sk            bigint               </span><br><span class="line">,     wp_web_page_id            string              </span><br><span class="line">,     wp_rec_start_date        string                         </span><br><span class="line">,     wp_rec_end_date          string                         </span><br><span class="line">,     wp_creation_date_sk       bigint                       </span><br><span class="line">,     wp_access_date_sk         bigint                       </span><br><span class="line">,     wp_autogen_flag           string                       </span><br><span class="line">,     wp_customer_sk            bigint                       </span><br><span class="line">,     wp_url                    string                  </span><br><span class="line">,     wp_type                   string                      </span><br><span class="line">,     wp_char_count             int                       </span><br><span class="line">,     wp_link_count             int                       </span><br><span class="line">,     wp_image_count            int                       </span><br><span class="line">,     wp_max_ad_count           int</span><br><span class="line">,PRIMARY KEY(wp_web_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wp_web_page_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_returns;</span><br><span class="line"></span><br><span class="line">create table web_returns</span><br><span class="line">(</span><br><span class="line">    wr_item_sk                bigint,</span><br><span class="line">    wr_returned_date_sk       bigint,</span><br><span class="line">    wr_returned_time_sk       bigint,</span><br><span class="line">    wr_refunded_customer_sk   bigint,</span><br><span class="line">    wr_refunded_cdemo_sk      bigint,</span><br><span class="line">    wr_refunded_hdemo_sk      bigint,</span><br><span class="line">    wr_refunded_addr_sk       bigint,</span><br><span class="line">    wr_returning_customer_sk  bigint,</span><br><span class="line">    wr_returning_cdemo_sk     bigint,</span><br><span class="line">    wr_returning_hdemo_sk     bigint,</span><br><span class="line">    wr_returning_addr_sk      bigint,</span><br><span class="line">    wr_web_page_sk            bigint,</span><br><span class="line">    wr_reason_sk              bigint,</span><br><span class="line">    wr_order_number           bigint,</span><br><span class="line">    wr_return_quantity        int,</span><br><span class="line">    wr_return_amt             double,</span><br><span class="line">    wr_return_tax             double,</span><br><span class="line">    wr_return_amt_inc_tax     double,</span><br><span class="line">    wr_fee                    double,</span><br><span class="line">    wr_return_ship_cost       double,</span><br><span class="line">    wr_refunded_cash          double,</span><br><span class="line">    wr_reversed_charge        double,</span><br><span class="line">    wr_account_credit         double,</span><br><span class="line">    wr_net_loss               double</span><br><span class="line">,PRIMARY KEY(wr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wr_item_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_sales;</span><br><span class="line"></span><br><span class="line">create table web_sales</span><br><span class="line">(</span><br><span class="line">    ws_item_sk                bigint,</span><br><span class="line">    ws_sold_date_sk           bigint,</span><br><span class="line">    ws_sold_time_sk           bigint,</span><br><span class="line">    ws_ship_date_sk           bigint,</span><br><span class="line">    ws_bill_customer_sk       bigint,</span><br><span class="line">    ws_bill_cdemo_sk          bigint,</span><br><span class="line">    ws_bill_hdemo_sk          bigint,</span><br><span class="line">    ws_bill_addr_sk           bigint,</span><br><span class="line">    ws_ship_customer_sk       bigint,</span><br><span class="line">    ws_ship_cdemo_sk          bigint,</span><br><span class="line">    ws_ship_hdemo_sk          bigint,</span><br><span class="line">    ws_ship_addr_sk           bigint,</span><br><span class="line">    ws_web_page_sk            bigint,</span><br><span class="line">    ws_web_site_sk            bigint,</span><br><span class="line">    ws_ship_mode_sk           bigint,</span><br><span class="line">    ws_warehouse_sk           bigint,</span><br><span class="line">    ws_promo_sk               bigint,</span><br><span class="line">    ws_order_number           bigint,</span><br><span class="line">    ws_quantity               int,</span><br><span class="line">    ws_wholesale_cost         double,</span><br><span class="line">    ws_list_price             double,</span><br><span class="line">    ws_sales_price            double,</span><br><span class="line">    ws_ext_discount_amt       double,</span><br><span class="line">    ws_ext_sales_price        double,</span><br><span class="line">    ws_ext_wholesale_cost     double,</span><br><span class="line">    ws_ext_list_price         double,</span><br><span class="line">    ws_ext_tax                double,</span><br><span class="line">    ws_coupon_amt             double,</span><br><span class="line">    ws_ext_ship_cost          double,</span><br><span class="line">    ws_net_paid               double,</span><br><span class="line">    ws_net_paid_inc_tax       double,</span><br><span class="line">    ws_net_paid_inc_ship      double,</span><br><span class="line">    ws_net_paid_inc_ship_tax  double,</span><br><span class="line">    ws_net_profit             double</span><br><span class="line">,PRIMARY KEY(ws_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ws_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_site;</span><br><span class="line"></span><br><span class="line">create table web_site</span><br><span class="line">(</span><br><span class="line">    web_site_sk           bigint,</span><br><span class="line">    web_site_id           string,</span><br><span class="line">    web_rec_start_date    string,</span><br><span class="line">    web_rec_end_date      string,</span><br><span class="line">    web_name              string,</span><br><span class="line">    web_open_date_sk      bigint,</span><br><span class="line">    web_close_date_sk     bigint,</span><br><span class="line">    web_class             string,</span><br><span class="line">    web_manager           string,</span><br><span class="line">    web_mkt_id            int,</span><br><span class="line">    web_mkt_class         string,</span><br><span class="line">    web_mkt_desc          string,</span><br><span class="line">    web_market_manager    string,</span><br><span class="line">    web_company_id        int,</span><br><span class="line">    web_company_name      string,</span><br><span class="line">    web_street_number     string,</span><br><span class="line">    web_street_name       string,</span><br><span class="line">    web_street_type       string,</span><br><span class="line">    web_suite_number      string,</span><br><span class="line">    web_city              string,</span><br><span class="line">    web_county            string,</span><br><span class="line">    web_state             string,</span><br><span class="line">    web_zip               string,</span><br><span class="line">    web_country           string,</span><br><span class="line">    web_gmt_offset        double,</span><br><span class="line">    web_tax_percentage    double</span><br><span class="line">,PRIMARY KEY(web_site_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (web_site_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br></pre></td></tr></table></figure><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">impala-shell -f impala-shell</span><br></pre></td></tr></table></figure><p>Tips：可能会遇到这样的错误</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR: ImpalaRuntimeException: Error creating Kudu table 'impala::kudu_spark_tpcds_2.catalog_sales'</span><br><span class="line">CAUSED BY: NonRecoverableException: The requested number of tablets is over the maximum permitted at creation time (60). Additional tablets may be added by adding range partitions to the table post-creation.</span><br></pre></td></tr></table></figure><p>原因：</p><p><code>Kudu</code>默认配置最多分区被限制了，需要配置</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3azpwvi62j21c10k9ta6.jpg" alt></p><p>如图栏目里，配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--max_create_tablets_per_ts=30</span><br></pre></td></tr></table></figure><p>生成日志后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail q* -n 1 &gt;&gt; kudu_time_2.log</span><br></pre></td></tr></table></figure><p>–</p><p>–</p><p>–</p><p>不知道为什么，<code>impala+kudu</code>对内存的管理存在一些问题，明明物理内存足够使用，却老是会用上交换内存。</p><p>测试性能下降，这里取消交换内存再试一次</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 取消交换内存：</span><br><span class="line">swapoff -a</span><br><span class="line">swapon -a</span><br></pre></td></tr></table></figure><p>parquet表格生成脚本  <code>alltables_parquet.sql</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> $&#123;<span class="keyword">VAR</span>:DB&#125; <span class="keyword">cascade</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">use</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">set</span> parquet_file_size=<span class="number">512</span>M;</span><br><span class="line"><span class="keyword">set</span> COMPRESSION_CODEC=snappy;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> call_center;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.call_center</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.call_center;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_page</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_page;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_address;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_address</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_address;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_demographics</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_demographics;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> date_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.date_dim</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.date_dim;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> household_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.household_demographics</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.household_demographics;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> income_band;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.income_band</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.income_band;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> inventory;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.inventory</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.inventory;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> item;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.item</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.item;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> promotion;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.promotion</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.promotion;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.reason</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.reason;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> ship_mode;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.ship_mode</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.ship_mode;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> <span class="keyword">store</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> time_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.time_dim</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.time_dim;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> warehouse;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.warehouse</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.warehouse;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_page</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_page;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_site;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_site</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_site;</span><br></pre></td></tr></table></figure><p>然后用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">impala-shell -i datanode127 --var=DB=tpcds_parquet_500 --var=HIVE_DB=tpcds_text_500 -f alltables_parquet.sql</span><br></pre></td></tr></table></figure><p>理论上也能这么生成<code>Kudu</code>表，只要用下面的语句。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> $&#123;<span class="keyword">VAR</span>:DB&#125; <span class="keyword">cascade</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">use</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> call_center;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.call_center</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cc_call_center_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.call_center;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_page</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cp_catalog_page_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_page;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cr_returned_date_sk,cr_returned_time_sk,cr_item_sk,cr_refunded_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(cr_returned_date_sk,cr_returned_time_sk,cr_item_sk,cr_refunded_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cs_sold_date_sk,cs_sold_time_sk,cs_ship_date_sk,cs_bill_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(cs_sold_date_sk,cs_sold_time_sk,cs_ship_date_sk,cs_bill_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_address;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_address</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ca_address_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_address;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_demographics</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cd_demo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_demographics;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (c_customer_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> date_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.date_dim</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (d_date_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.date_dim;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> household_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.household_demographics</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (hd_demo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.household_demographics;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> income_band;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.income_band</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ib_income_band_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.income_band;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> inventory;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.inventory</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (inv_date_sk,inv_item_sk,inv_warehouse_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(inv_date_sk,inv_item_sk,inv_warehouse_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.inventory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> item;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.item</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (i_item_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> promotion;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.promotion</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (p_promo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.promotion;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.reason</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (r_reason_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.reason;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> ship_mode;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.ship_mode</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (sm_ship_mode_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.ship_mode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (sr_returned_date_sk,sr_return_time_sk,sr_item_sk,sr_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(sr_returned_date_sk,sr_return_time_sk,sr_item_sk,sr_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> <span class="keyword">store</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (s_store_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> time_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.time_dim</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (t_time_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.time_dim;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> warehouse;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.warehouse</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (w_warehouse_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.warehouse;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_page</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (wp_web_page_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_page;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (wr_returned_date_sk,wr_returned_time_sk,wr_item_sk,wr_refunded_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(wr_returned_date_sk,wr_returned_time_sk,wr_item_sk,wr_refunded_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ws_sold_date_sk,ws_sold_time_sk,ws_ship_date_sk,ws_item_sk,ws_bill_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(ws_sold_date_sk,ws_sold_time_sk,ws_ship_date_sk,ws_item_sk,ws_bill_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_site;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_site</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (web_site_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_site;</span><br></pre></td></tr></table></figure><p>但是存在问题就是<code>Kudu</code>的表需要主键，并且主键需要放置在最前面，但是<code>tpcds</code>默认生成的表格无法把主键放在最前面，所以这样创建的表格主主键包含很多个key，所以还是用上面的方法。</p><hr><p><a href="https://blog.csdn.net/weixin_39478115/article/details/78469837" target="_blank" rel="noopener">kudu性能调优</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段&lt;br&gt;本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/categories/Apache/Kudu/"/>
    
    
      <category term="Analog Data" scheme="http://yoursite.com/tags/Analog-Data/"/>
    
  </entry>
  
  <entry>
    <title>Data Preprocessing | Day 1</title>
    <link href="http://yoursite.com/2020/03/09/day01/"/>
    <id>http://yoursite.com/2020/03/09/day01/</id>
    <published>2020-03-09T10:03:41.351Z</published>
    <updated>2019-05-09T01:43:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>针对英语文档阅读使用能力和ML知识点开的一个新坑<br>不定期更新<br>尽量使用英语</p></blockquote><a id="more"></a> <h2 id="Data-Preprocessing-Day-1"><a href="#Data-Preprocessing-Day-1" class="headerlink" title="Data Preprocessing | Day 1"></a>Data Preprocessing | Day 1</h2><h3 id="Step-1-Import-the-required-Libraries"><a href="#Step-1-Import-the-required-Libraries" class="headerlink" title="Step 1: Import the required Libraries"></a>Step 1: Import the required Libraries</h3><p>These Two are essential libraries which we will import every time.</p><p>NumPy: Library which contains Mathematical functions.</p><p>Pandas: Library used to import and manage the data sets.</p><h3 id="Step-2-Importing-the-Data-Set"><a href="#Step-2-Importing-the-Data-Set" class="headerlink" title="Step 2: Importing the Data Set"></a>Step 2: Importing the Data Set</h3><p>Data sets are generally available in .csv format. A CSV file stores tabular data in plain text(纯文本). Each lines of the file is a data record. We use the read_csv method of the pandas library to read a local CSV file as a dataframe. Then we make separate(分离) Matrix and Vector of independent and dependent variables from the dataframe.(然后我们从dataframe中制作自变量和因变量的矩阵和向量)</p><h3 id="Step-3-Handling-the-Missing-Data"><a href="#Step-3-Handling-the-Missing-Data" class="headerlink" title="Step 3: Handling the Missing Data"></a>Step 3: Handling the Missing Data</h3><p>The data we get is rarely homogeneous(同质的).Data can be missing due to various and needs to be handled so that it does not reduce the performance of our machine learning model. We can replace the missing data by the Mean or median of the entire column. We use <code>imputer</code> class of <code>sklearn.preprocessing</code> for this task.</p><h3 id="Step-4-Encoding-Categorical-Data"><a href="#Step-4-Encoding-Categorical-Data" class="headerlink" title="Step 4: Encoding Categorical Data"></a>Step 4: Encoding Categorical Data</h3><p>Categorical data are variables that contain label values(标签值) rather than numeric values(数值).The number of possible values is often limited to a fixed set. Example values such as “Yes” and “No” cannot be used in mathematical equations(数学方程) of the model so we need  to encode these variables into numbers. To achieve this we import <code>LabelEncoder</code> class from <code>Sklearn.preprocessing</code> library.</p><h3 id="Step-5-Splitting-the-dataset-into-test-set-and-training-set"><a href="#Step-5-Splitting-the-dataset-into-test-set-and-training-set" class="headerlink" title="Step 5: Splitting the dataset into test set and training set"></a>Step 5: Splitting the dataset into test set and training set</h3><p>We make two partitions of dataset one for training the model called training set and other for testing the performance of the trained model called test set. The split generally 80/20. We import <code>train_test_split()</code> method of <code>sklearn.crossvalidation</code> library.</p><h3 id="Step-6-Feature-Scaling-特征归一化"><a href="#Step-6-Feature-Scaling-特征归一化" class="headerlink" title="Step 6: Feature Scaling(特征归一化)"></a>Step 6: Feature Scaling(特征归一化)</h3><p>Most of the machine learning algorithms use the Euclidean distance(欧式距离) between two data points in their computations, features highly varying(变化) in magnitudes(大小), units and range(范围) pose(提出) problems. high magnitudes(幅度) features will weigh more in the distance calculations than features with low magnitudes. Done by Feature standardization or Z-score normalization(正常化). <code>StandardScalar</code> of <code>sklearn.preprocessing</code> is imported.</p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">"D:\\APP\\DataSet\\100-Days-Of-ML-Code-master\\datasets\\Data.csv"</span>)</span><br><span class="line">X = dataset.iloc[ : , :<span class="number">-1</span>].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">3</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">"NaN"</span>, strategy = <span class="string">"mean"</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[ : , <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelencoder_X = LabelEncoder()</span><br><span class="line">X[ : , <span class="number">0</span>] = labelencoder_X.fit_transform(X[ : , <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_Y = LabelEncoder()</span><br><span class="line">Y =  labelencoder_Y.fit_transform(Y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.transform(X_test)</span><br></pre></td></tr></table></figure><h4 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h4><p>代码详解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><p>首先我们使用一个简单的数据集。每一个数据集都会包括两部分，独立变量（independent variable）和依赖变量（dependent variable)。机器学习的目的就是需要通过独立变量来预测非独立变量（prediction）。<br>独立变量不会被影响而非独立变量可能被独立变量影响。</p><p>在以下数据集中Age和Salary就是独立变量，我们需要通过这两个独立变量预测是否会Purchase。所以Purchased就是非独立变量。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g2s67p6d2tj209706iq2w.jpg" alt></p><p>把np作为<code>numpy</code>的缩写，后面可以直接使用np来调用各种方法。</p><p>==&gt;</p><p><code>numpy</code>系统是<code>python</code>的一种开源的数值计算扩展。<br>这种工具可用来存储和处理大型矩阵，比<code>python</code>自身的嵌套列表结构要高效的多。<br>你可以理解为凡是和矩阵有关的都用<code>numpy</code>这个库。</p><p>==&gt;</p><p><code>pandas</code>该工具是为了解决数据分析任务而创建的。<code>pandas</code> 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。<br><code>pandas</code>提供了大量能使我们快速便捷地处理数据的函数和方法。它是使<code>python</code>成为强大而高效的数据分析环境的重要因素之一.</p><p>==&gt;</p><p>pandas导入语法：</p><ul><li>导入路径斜线问题</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_path1 = <span class="string">'D:/0Raw_data/ftm_p.csv'</span></span><br><span class="line">file_path2 = <span class="string">'D:\\0Raw_data\\ftm_p.csv'</span></span><br><span class="line">file_path3 = <span class="string">r'D:\0Raw_data\ftm_p.csv'</span></span><br></pre></td></tr></table></figure><ul><li>中文路径问题</li></ul><p>当错误类型如下，则一般是中文路径问题。</p><p>OSError: Initializing from file failed</p><p>不废话，解决方案就是先用open打开，而且一般用open先打开，能直接解决编码问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">path = open(file_path)</span><br><span class="line">data = pd.read_csv(path)</span><br></pre></td></tr></table></figure><ul><li>编码问题</li></ul><p>报错：UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xb9 in position 0: invalid start byte</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">f = open(file_path,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">data = pd.read_csv(f)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><p>解决方案2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">data = pd.read_csv(<span class="string">'D:/0Raw_data/ftm_p.csv'</span>,encoding=<span class="string">'gbk'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 独立变量vector</span></span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values  <span class="comment"># 第一个冒号是所有列（row），第二个是所有行（column）除了最后一个(Purchased)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 依赖变量vector</span></span><br><span class="line">Y = dataset.iloc[:, <span class="number">3</span> ].values <span class="comment"># 只取最后一个column作为依赖变量。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理丢失数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">'NaN'</span>, strategy = <span class="string">'mean'</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[:, <span class="number">1</span>:<span class="number">3</span>])   <span class="comment"># (inclusive column 1, exclusive column 3, means col 1 &amp; 2 逗号之前代表 所有行 ：,后面代表 [1,3)列])</span></span><br><span class="line">X[:, <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[:, <span class="number">1</span>:<span class="number">3</span>]) <span class="comment"># 将imputer 应用到数据</span></span><br></pre></td></tr></table></figure><h4 id="sklearn-preprocessing-Imputer解析"><a href="#sklearn-preprocessing-Imputer解析" class="headerlink" title="sklearn.preprocessing.Imputer解析:"></a>sklearn.preprocessing.Imputer解析:</h4><p>sklearn.preprocessing.Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)</p><p>missing_values：缺失值，可以为整数或NaN(缺失值numpy.nan用字符串‘NaN’表示)，默认为NaN</p><p>strategy：替换策略，字符串，默认用均值‘mean’替换</p><p>①若为mean时，用特征列的均值替换</p><p>②若为median时，用特征列的中位数替换</p><p>③若为most_frequent时，用特征列的众数替换</p><p>axis：指定轴数，默认axis=0代表列，axis=1代表行</p><p>copy：设置为True代表不在原数据集上修改，设置为False时，就地修改，存在如下情况时，即使设置为False时，也不会就地修改</p><p>①X不是浮点值数组</p><p>②X是稀疏且missing_values=0</p><p>③axis=0且X为CRS矩阵</p><p>④axis=1且X为CSC矩阵</p><p>statistics_属性：axis设置为0时，每个特征的填充值数组，axis=1时，报没有该属性错误</p><p>处理之前：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[&apos;France&apos;, 44.0, 72000.0],</span><br><span class="line">       [&apos;Spain&apos;, 27.0, 48000.0],</span><br><span class="line">       [&apos;Germany&apos;, 30.0, 54000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.0, 61000.0],</span><br><span class="line">       [&apos;Germany&apos;, 40.0, nan],</span><br><span class="line">       [&apos;France&apos;, 35.0, 58000.0],</span><br><span class="line">       [&apos;Spain&apos;, nan, 52000.0],</span><br><span class="line">       [&apos;France&apos;, 48.0, 79000.0],</span><br><span class="line">       [&apos;Germany&apos;, 50.0, 83000.0],</span><br><span class="line">       [&apos;France&apos;, 37.0, 67000.0]], dtype=object)</span><br></pre></td></tr></table></figure><p>处理之后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[&apos;France&apos;, 44.0, 72000.0],</span><br><span class="line">       [&apos;Spain&apos;, 27.0, 48000.0],</span><br><span class="line">       [&apos;Germany&apos;, 30.0, 54000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.0, 61000.0],</span><br><span class="line">       [&apos;Germany&apos;, 40.0, 63777.77777777778],</span><br><span class="line">       [&apos;France&apos;, 35.0, 58000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.77777777777778, 52000.0],</span><br><span class="line">       [&apos;France&apos;, 48.0, 79000.0],</span><br><span class="line">       [&apos;Germany&apos;, 50.0, 83000.0],</span><br><span class="line">       [&apos;France&apos;, 37.0, 67000.0]], dtype=object)</span><br></pre></td></tr></table></figure><h4 id="Sklearn数据预处理中fit-和transform-与fit-transform-的区别"><a href="#Sklearn数据预处理中fit-和transform-与fit-transform-的区别" class="headerlink" title="Sklearn数据预处理中fit()和transform()与fit_transform()的区别"></a>Sklearn数据预处理中fit()和transform()与fit_transform()的区别</h4><ul><li>fit():Method calculates the parameters μ and σ and saves them as internal objects.</li></ul><p>Imputer定义了规则，imputer指定训练范围，进行fit ，这里提到的模型都是非常简单的，无非平均数、方差这种。</p><ul><li>transform():Method using these calculated parameters apply the transformation to a particular dataset.</li></ul><p>transform，我理解是这样的，fit和transform的区别有点类似训练模型和训练数据，transform类似于训练数据这一块的</p><ul><li>fit_transform():joins the fit() and transform() method for transformation of dataset.</li></ul><p>将训练模型和训练数据放到一起的一个步骤。</p><p>Note</p><p>必须先用fit_transform(trainData)，之后再transform(testData)<br>如果直接transform(testData)，程序会报错<br>如果fit_transfrom(trainData)后，使用fit_transform(testData)而不transform(testData)，虽然也能归一化，但是两个结果不是在同一个“标准”下的，具有明显差异。(一定要避免这种情况)</p><p>==&gt;</p><h4 id="什么是独热编码？"><a href="#什么是独热编码？" class="headerlink" title="什么是独热编码？"></a>什么是独热编码？</h4><p> 独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。举例如下：</p><p>直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。举例如下：</p><p>假如有三种颜色特征：红、黄、蓝。在利用机器学习的算法时一般需要进行向量化或者数字化。那么你可能想令 红=1，黄=2，蓝=3。那么这样其实实现了标签编码，即给不同类别以标签。然而这意味着机器可能会学习到“红&lt;黄&lt;蓝”，但这并不是我们的让机器学习的本意，只是想让机器区分它们，并无大小比较之意。</p><p>所以这时标签编码是不够的，需要进一步转换。因为有三种颜色状态，所以就有3个比特。即红色：1 0 0 ，黄色: 0 1 0，蓝色：0 0 1 。</p><p>如此一来每两个向量之间的距离都是根号2，在向量空间距离都相等，所以这样不会出现偏序性，基本不会影响基于向量空间度量算法的效果。</p><h4 id="OneHotEncoder-和-LabelEncoder-独热编码和标签编码"><a href="#OneHotEncoder-和-LabelEncoder-独热编码和标签编码" class="headerlink" title="OneHotEncoder 和 LabelEncoder 独热编码和标签编码"></a>OneHotEncoder 和 LabelEncoder 独热编码和标签编码</h4><p>首先了解机器学习中的特征类别：<strong>连续型特征</strong>和<strong>离散型特征</strong></p><p>拿到获取的原始特征，必须对每一特征分别进行归一化，比如，特征A的取值范围是[-1000,1000]，特征B的取值范围是[-1,1]，如果使用logistic回归，w1<em>x1+w2</em>x2，因为x1取值太大了，所以x2基本起不了作用。所以，必须进行特征的归一化，每个特征都单独进行归一化。</p><p> 对于连续性特征：</p><ul><li><strong>Rescale bounded continuous features</strong>: All continuous input that are bounded, rescale them to [-1, 1] through x = (2x - max - min)/(max - min).    线性放缩到[-1,1]</li><li><strong>Standardize all continuous features</strong>: All continuous input should be standardized and by this I mean, for every continuous feature, compute its mean (u) and standard deviation (s) and do x = (x - u)/s.       放缩到均值为0，方差为1</li></ul><p>对于离散性特征：</p><ul><li><strong>Binarize categorical/discrete features</strong>: 对于离散的特征基本就是按照<strong>one-hot（独热）</strong>编码，该离散特征有多少取值，就用多少维来表示该特征。</li></ul><hr><p>1、方差是各个数据分别与其平均数之差的平方的和的平均数，用字母D表示。在概率论和数理统计中，方差（Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着重要意义。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2tw7h5xxuj208a00z0rz.jpg" alt></p><p>2、平方差公式（difference of two squares）是数学公式的一种，它属于乘法公式、因式分解及恒等式，被普遍使用。平方差指一个平方数或正方形，减去另一个平方数或正方形得来的乘法公式：a²-b²=(a+b)(a-b)</p><p>3、标准差（Standard Deviation） ，中文环境中又常称均方差，但不同于均方误差（mean squared error，均方误差是各数据偏离真实值的距离平方的平均数，也即误差平方和的平均数，计算公式形式上接近方差，它的开方叫均方根误差，均方根误差才和标准差形式上接近），标准差是离均差平方和平均后的方根，用σ表示。假设有一组数值X1,X2,X3,……XN（皆为实数），其平均值（算术平均值）为μ，公式如图。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2tw8bxdo1j204601o3y9.jpg" alt></p><p>概率论还是要慢慢补。。。</p><hr><p>Reference：</p><p><a href="https://blog.csdn.net/appleyuchi/article/details/73503282" target="_blank" rel="noopener">https://blog.csdn.net/appleyuchi/article/details/73503282</a></p><p><a href="https://scikit-learn.org/stable/_downloads/scikit-learn-docs.pdf" target="_blank" rel="noopener">scikit-learn user guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;针对英语文档阅读使用能力和ML知识点开的一个新坑&lt;br&gt;不定期更新&lt;br&gt;尽量使用英语&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="OneHotEncoder" scheme="http://yoursite.com/tags/OneHotEncoder/"/>
    
      <category term="LabelEncoder" scheme="http://yoursite.com/tags/LabelEncoder/"/>
    
  </entry>
  
  <entry>
    <title>Simple Linear Regression | Day 2</title>
    <link href="http://yoursite.com/2020/03/09/day02/"/>
    <id>http://yoursite.com/2020/03/09/day02/</id>
    <published>2020-03-09T10:03:41.335Z</published>
    <updated>2019-05-23T09:42:46.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这边还是直接贴上原图吧，手打实在是比较累，而且我按图打字的时候容易分心，很容易变成机械运动，不如直接上图。</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g39v8hb3uzj20m81jk4qp.jpg" alt></p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">'F:\\dataset\\100-Days-Of-ML-Code-master\\datasets\\studentscores.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : ,   : <span class="number">1</span> ].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">1</span> ].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = <span class="number">1</span>/<span class="number">4</span>, random_state = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">Y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_train , Y_train, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_train , regressor.predict(X_train), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3bbvz6ns6j20ac070mx2.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test , Y_test, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_test , regressor.predict(X_test), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3bbwiv3v2j20ac070q2u.jpg" alt></p><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>第二天的内容比较简单，没有需要特别结实的内容</p><p>值得注意的是，最后两个测试结果可视化和训练结果可视化内容里面的向量其实是一样的，<code>Y_pred = regressor.predict(X_test)</code>这一步其实类似于保存结果，但是后面不知道为什么没有直接使用起来，有点奇怪。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这边还是直接贴上原图吧，手打实在是比较累，而且我按图打字的时候容易分心，很容易变成机械运动，不如直接上图。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="Linear Regression" scheme="http://yoursite.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Multiple Linear Regression | Day 3</title>
    <link href="http://yoursite.com/2020/03/09/day03/"/>
    <id>http://yoursite.com/2020/03/09/day03/</id>
    <published>2020-03-09T10:03:40.983Z</published>
    <updated>2019-05-23T23:57:08.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>多元线性回归</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3beehr6fqj20m81jke81.jpg" alt></p><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><h4 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h4><p>For a successful regression analysis. It’s essential to validate these assumptions.</p><ol><li><p>Linearity: The relationship between dependent and independent variables should be Linear.</p></li><li><p>Homoscedasticity 方差齐性: (constant variance 恒定方差) of the errors should be maintained.  方差：离散程度的度量</p></li><li>Multivarivate Normality(多元正态性):  Multiple regression assumes that the residuals are normally distributed.</li><li>Lack of Multicollinearity(没有多重共线性，由于存在精确相关关系或者高度相关关系而使模型估计失真或难以估计准确): It is assumed that there is little or no multicollinearity in the data. Multicollinearity occurs when the features (or independent variables) are not independent of each other.</li></ol><h4 id="Dummy-Variables-虚变量、哑变量"><a href="#Dummy-Variables-虚变量、哑变量" class="headerlink" title="Dummy Variables(虚变量、哑变量)"></a>Dummy Variables(虚变量、哑变量)</h4><p>Using categorical data in Multiple Regression Models is a powerful method to include non-numeric data types into a regression model.</p><p>Categorical data refers to data values which represent categories - data values with a fixed and unordered number of values. for instance, gender(male/female). In a regression model, these values can be represented bu dummy variables - variables containing values such as 1 or 0 representing the presence or absence of the categorical.</p><h4 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h4><p>having too many variables could potentially cause our model to become less accurate. especially if certain variables have no effect on the outcome or have a significant effect on other variables. There are various methods to select the appropriate various methods to select the appropriate variable like -</p><ol><li>Forward Selection</li><li>Backward Elimination</li><li>Bi-directional Comparision</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;多元线性回归&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="Linear Regression" scheme="http://yoursite.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Grokking Algorithms</title>
    <link href="http://yoursite.com/2020/03/09/Grokking%20Algorithms/"/>
    <id>http://yoursite.com/2020/03/09/Grokking Algorithms/</id>
    <published>2020-03-09T10:03:40.785Z</published>
    <updated>2020-04-10T17:08:33.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。</p><p>这边值得一提的是作者推荐了一个网站，可汗学院，<code>khanacademy.org</code>  mark一下。</p><p>看完40%来总结一下，非常好，文盲也能看懂的算法入门。</p><p>这本书看完应该会扫一眼结城浩的《图解密码学》</p></blockquote><a id="more"></a> <h2 id="第一章-算法简介"><a href="#第一章-算法简介" class="headerlink" title="第一章 算法简介"></a>第一章 算法简介</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w8rt2sg9j20fk026mxh.jpg" alt></p><p>好的，我具备了</p><h3 id="1-2-二分查找"><a href="#1-2-二分查找" class="headerlink" title="1.2 二分查找"></a>1.2 二分查找</h3><p>二分查找(binary search)又叫折半搜索(half-interval search)、对数搜索(logarithmic search)，是一种在<strong>有序数组</strong>中查找某一特定元素的搜索算法。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。</p><p>对数：幂运算的逆运算</p><p>假设你要在字典中查找一个单词，而该字典包含240000个单词，<br>你认为每种查找最多需要多少步？</p><p>log<sub>2</sub> n步，本题中就是18步</p><p>给定一个有序数组和一个需要定位的数字，先创建两个变量 low 和 high，low和high一开始分别是数组的第一个和最后一个坐标，划定一个取中间元素的空间，然后取出中间元素和目标元素比较，如果不是的话，就更改low或者high中某一个的坐标为(low + high)/2，将查找空间缩小为原来的二分之一，然后继续。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(list, item)</span>:</span></span><br><span class="line">  low = <span class="number">0</span></span><br><span class="line">  high = len(list) - <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">    mid = (low + high) // <span class="number">2</span></span><br><span class="line">    guess = list[mid]</span><br><span class="line">    <span class="keyword">if</span> guess == item:</span><br><span class="line">      <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">if</span> guess &gt; item:</span><br><span class="line">      high = mid <span class="number">-1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      low = mid + <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">my_list = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/notebook#create=true&amp;language=python3" target="_blank" rel="noopener">运行环境</a></p><p>Tips：关于为什么更换搜索区域的时候没有直接用high = mid 或者low = mid</p><p>注意while的条件，如果没有这一条，范围缩小到两个数的时候，会无限循环</p><h4 id="运行时间"><a href="#运行时间" class="headerlink" title="运行时间"></a>运行时间</h4><p>最多猜测次数与列表长度相同被称为线性时间(linear time).</p><p>二分查找的运行时间为对数时间(log time).</p><h3 id="1-3-大-O-表示法"><a href="#1-3-大-O-表示法" class="headerlink" title="1.3 大 O 表示法"></a>1.3 大 O 表示法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">O(1):             常量时间，哈希</span><br><span class="line">O(log2(n)):       对数时间，二分，</span><br><span class="line">O(n):             线性时间，简单</span><br><span class="line">O(nlog2(n)):              快速排序</span><br><span class="line">O(n2):                    选择排序（冒泡）</span><br><span class="line">O(n!):                    旅行商问题</span><br></pre></td></tr></table></figure><p>算法的速度指的并非时间，而是操作数的增速。</p><p>谈论算法的速度时，我们说的是随着输入的增加，其运行时间将以什么样的速度增加。</p><p>算法的运行时间用大O表示法表示。</p><p>O(log n)比O(n)快，当需要搜索的元素越多时，前者比后者快得越多。</p><h4 id="旅行商问题"><a href="#旅行商问题" class="headerlink" title="旅行商问题"></a>旅行商问题</h4><p>行商问题（最短路径问题）（英语：travelling salesman problem, TSP）是这样一个问题：给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。它是组合优化中的一个NP困难问题，在运筹学和理论计算机科学中非常重要。</p><hr><h2 id="第二章-选择排序"><a href="#第二章-选择排序" class="headerlink" title="第二章 选择排序"></a>第二章 选择排序</h2><h3 id="2-1-内存工作原理"><a href="#2-1-内存工作原理" class="headerlink" title="2.1 内存工作原理"></a>2.1 内存工作原理</h3><h3 id="2-2-数组和链表"><a href="#2-2-数组和链表" class="headerlink" title="2.2 数组和链表"></a>2.2 数组和链表</h3><p><strong>链表</strong>：不需要移动元素，优势在插入元素</p><p>使用链表在中间插入元素只需要修改前面一个元素指向的地址，因此当需要在中间插入的时候，链表是更好的选择。</p><p>删除也是一样</p><p>数组和链表的运行时间：</p><table><thead><tr><th></th><th>数组</th><th>链表</th></tr></thead><tbody><tr><td>读取</td><td>O(1)</td><td>O(n)</td></tr><tr><td>插入</td><td>O(n)</td><td>O(1)</td></tr><tr><td>删除</td><td>O(n)</td><td>O(1)</td></tr></tbody></table><p>有两种访问方式：随机访问和顺序访问。</p><p>顺序访问意味着从第一个元素开始逐个读取元素，链表只能顺序访问，数组支持随机访问，所以数组在需要随机访问的情况下用得很多。</p><h3 id="2-3-选择排序"><a href="#2-3-选择排序" class="headerlink" title="2.3 选择排序"></a>2.3 选择排序</h3><p>时间复杂度的Tips</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g39qnoufddj20oz08a0v8.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findSmallest</span><span class="params">(arr)</span>:</span></span><br><span class="line">    smallest = arr[<span class="number">0</span>]</span><br><span class="line">    smallest_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(arr)):</span><br><span class="line">        <span class="keyword">if</span> arr[i] &lt; smallest:</span><br><span class="line">            smallest = arr[i]</span><br><span class="line">            smallest_index = i</span><br><span class="line">    <span class="keyword">return</span> smallest_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection</span><span class="params">(arr)</span>:</span></span><br><span class="line">    newArr = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        smallest = findSmallest(arr)</span><br><span class="line">        newArr.append(arr.pop(smallest))</span><br><span class="line">    <span class="keyword">return</span> newArr</span><br><span class="line"></span><br><span class="line">print(selection( [<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">10</span>] ))</span><br></pre></td></tr></table></figure><p>Tips：</p><p>python之间的语法不兼容是很蛋疼的事情</p><p>py2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Pyhon 2 can use print string without ()"</span>;</span><br></pre></td></tr></table></figure><p>py3:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Python3, print must use () to output string"</span>);</span><br></pre></td></tr></table></figure><p>py3中，print作为函数必须要带括号</p><h2 id="第三章-递归"><a href="#第三章-递归" class="headerlink" title="第三章 递归"></a>第三章 递归</h2><p>递归：优雅的问题解决办法</p><h3 id="3-1-递归"><a href="#3-1-递归" class="headerlink" title="3.1 递归"></a>3.1 递归</h3><p>“如果使用循环，程序的性能可能更高；如果使用递归，程序可能 更容易理解。如何选择要看什么对你来说更重要“</p><h3 id="3-2-基线条件和递归条件"><a href="#3-2-基线条件和递归条件" class="headerlink" title="3.2 基线条件和递归条件"></a>3.2 基线条件和递归条件</h3><p>递归条件(base case)是指函数调用自己，基线条件(recursive case)是指函数不再调用自己，从而表面形成无限循环。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countdown</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(i)</span><br><span class="line">    <span class="keyword">if</span> i &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        countdown(i<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">countdown(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="3-3-栈"><a href="#3-3-栈" class="headerlink" title="3.3 栈"></a>3.3 栈</h3><p>调用栈（call stack）</p><p>python虽然不是用的JVM 但是 对于栈内存的调用 好像都差不多</p><p>递归函数factorial(5)写作5!</p><p>意义是5! = 5 <em> 4 </em> 3 <em> 2 </em> 1</p><p>使用栈虽然很方便但是也要付出代价：占用大量内存</p><h2 id="第四章-快速排序"><a href="#第四章-快速排序" class="headerlink" title="第四章 快速排序"></a>第四章 快速排序</h2><h3 id="4-1-分而治之"><a href="#4-1-分而治之" class="headerlink" title="4.1 分而治之"></a>4.1 分而治之</h3><p>一种著名的递归式问题解决方法—divide and conquer,D&amp;C</p><p>重要的D&amp;C是算法：快排，优雅代码的典范</p><p>欧几里得算法(辗转相除法)：gcd(a.b) = gcd(b, a%b)</p><table><thead><tr><th>大的那个数</th><th>小的那个数</th><th>余数</th><th>商</th></tr></thead><tbody><tr><td>a</td><td>b</td><td>r0 = a%b</td><td>q0</td></tr><tr><td>b</td><td>r0</td><td>r1 = b% r0</td><td>q1</td></tr><tr><td>r0</td><td>r1</td><td>r2 = r0 % r1</td><td>q2</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>rN-4</td><td>rN-3</td><td>rN-2 = rN-4 % rN-3</td><td>qN-2</td></tr><tr><td>rN-3</td><td>rN-2</td><td>rN-1 = rN-3 % rN-2</td><td>qN-1</td></tr><tr><td>rN-2</td><td>rN-1</td><td>rN = rN-2 % rN-1</td><td>qN</td></tr><tr><td>rN-1</td><td>rN == 0</td><td>rN-1 = 1 <em> rN-1 - 0 </em> rN</td><td>0</td></tr></tbody></table><p>得到的最大公约数就是rN-1</p><p>欧几里得算法的证明：</p><p>我个人觉得反证法比较好理解：</p><p> 要证欧几里德算法成立，即证: gcd(a,b)=gcd(b,r),其中 gcd是取最大公约数的意思，r=a mod b<br>    下面证 gcd（a，b）=gcd（b，r）<br>    设  c是a，b的最大公约数，即c=gcd（a，b），则有 a=mc，b=nc，其中m，n为正整数，且m，n互为质数<br>    由 r= a mod b可知，r= a- qb 其中，q是正整数，<br>    则 r=a-qb=mc-qnc=（m-qn）c<br>    b=nc,r=(m-qn)c，且n，（m-qn）互质（假设n，m-qn不互质，则n=xd, m-qn=yd 其中x,y,d都是正整数，且d&gt;1</p><p>​    则a=mc=(qx+y)dc, b=xdc,这时a,b 的最大公约数变成dc，与前提矛盾，所以n ，m-qn一定互质）<br>​    则gcd（b,r）=c=gcd（a,b）<br>​    得证。</p><p>编写涉及数组的递归函数时，基线条件通常是数组为空或只包含一个元素。陷入困境时， 请检查基线条件是不是这样的。 </p><h3 id="4-2-快排"><a href="#4-2-快排" class="headerlink" title="4.2 快排"></a>4.2 快排</h3><p>快排使用了D&amp;C</p><p>思路：</p><p>基线条件：数组为空或者只包含一个元素。这种情况下，只需要原样返回。</p><p>对于两个元素的数组：如果第一个元素比第二个元素小，直接返回，如果不是，就交换位置。</p><p>三个元素的数组：</p><p>从数组中选择一个元素，这个元素被称为基准值(pivot)，</p><p>我们暂时先将数组的第一个元素作为基准值。</p><p>接下来找出比基准值小的元素以及比他大的元素。这个过程被称为分区（partition）</p><p>这里两个分区出来的数组时无需的，但是如果这两个数组是有序的，对整个数组进行排序将非常容易。</p><p>那么问题就转化成了如何对子数组进行排序，</p><p>这里我们讨论的是特定情况（三个元素），无论选用哪个元素作为pivot，剩下的情况总能用上面两个元素数组的排序方法代入。</p><p>于是就得到了解决办法</p><p>接下来四个元素的情况，类似的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="comment"># 基线条件</span></span><br><span class="line">    <span class="keyword">if</span> len(array) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 递归条件</span></span><br><span class="line">        pivot = array[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 分为两个数组</span></span><br><span class="line">        less = [ i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &lt;= pivot]</span><br><span class="line">        greater = [i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; pivot]</span><br><span class="line">        <span class="keyword">return</span> quicksort(less) + [pivot] + quicksort(greater)</span><br><span class="line"></span><br><span class="line">print(quicksort([<span class="number">1</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">45645</span>,<span class="number">34</span>,<span class="number">23</span>,<span class="number">65</span>,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这边可以对比一下时间复杂度</p><p>选择排序的时间复杂度是O(n<sup>2</sup>)</p><p>快速排序的时间复杂度最差是O(n<sup>2</sup>)，平均情况是O(n log n)</p><p>还有一种合并排序(merge sort)运行时间是O(n log n)</p><p>现在做出一个有趣的假设，假设简单查找每次需要10ms，二分查找的常量是1s，现在我们假设查找的元素个数是10个，简单查找需要100ms，二分查找却需要log 10 * 1s，可以二分查找的时间远大于简单查找，但是我们查找的元素很大时，比如40亿，这个时候我们使用简单查找需要463天，但是二分查找只要32s。</p><p>通过这个例子，我们可以看到常量的影响可能会很大。</p><p>我们再来看快排，快排的效率取决于选择的pivot，当pivot是最小值的时候，我们其实只用到的一个数组，要递归很多次才能递归结束，这种情况是最坏的情况</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3c25jnmi2j20jx0ga0v4.jpg" alt></p><p>如果我们选择的是中间值，是最佳情况，这种情况下根本不要这么多递归，因此调用栈就短得多。</p><p>需要注意的是，我们并不是如图这么简单的+n次调用，作为递归二次每次调用栈都设计O(n)，这是递归的性质决定的。</p><p>因此，实际上最佳情况是O(n log n)</p><p>最佳情况也是平均情况（和最佳情况在同一数量级所以忽略掉前面的参数，剩下的相同），快排是D&amp;G的典范。</p><h2 id="第五章-散列表-Hash-Table"><a href="#第五章-散列表-Hash-Table" class="headerlink" title="第五章 散列表 Hash Table"></a>第五章 散列表 Hash Table</h2><blockquote><p>散列表是足有用的基本数据结构之一。</p></blockquote><p>虽然二分法的效率已经可以了，但是能不能有一种查找方法的查找时间是O(1)呢——任意给出一个查找内容，都能立即给出答案。</p><h3 id="5-1-散列函数"><a href="#5-1-散列函数" class="headerlink" title="5.1 散列函数"></a>5.1 散列函数</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3eicbpqf2j20ar07y3z4.jpg" alt></p><p>散列函数应该满足的要求：</p><ul><li>他必须是一致的。例如：假设你输入apple时得到的是4，那么每次输入apple的时候，得到的都必须是4，如果不是这样，散列表将毫无用处。</li><li>它应该将不同的输入映射到不同的数字，如果一个散列函数不管输入是什么都返回1就不可以。最理想的情况是，将不同的输入映射到不同的数字。</li></ul><p>原理：</p><ul><li>散列函数总是将同样的输入映射到相同的索引。</li><li>不同输入映射到不同的索引。</li><li>散列函数知道数组有多大。</li></ul><p>散列表是一种包含额外逻辑的数据结构。</p><p>散列表又被称为散列映射、映射、字典和关联数组。（Hash Table）</p><p>python提供的散列表实现为字典，可以使用<code>dictt</code>来创建散列表。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ej8m95ydj20hm09omx2.jpg" alt></p><p>python语法中</p><p><code>book = dict()</code>和<code>book = {}</code>等价。</p><h3 id="5-2-应用案例"><a href="#5-2-应用案例" class="headerlink" title="5.2 应用案例"></a>5.2 应用案例</h3><p>电话簿</p><p>DNS解析（域名关联IP）DNS resolution</p><p>防止重复（比如抽奖、投票）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ejoaiscwj20gr0743yd.jpg" alt></p><p>将案列表用作缓存</p><p>Facebook将主页、about页面，Contact页面、Terms 和 Conditions页面等众多页面通过页面URL映射到页面数据。</p><h3 id="5-3-冲突-collision"><a href="#5-3-冲突-collision" class="headerlink" title="5.3 冲突 collision"></a>5.3 冲突 collision</h3><p>大多数语言都提供了散列实现，冲突是指，两个键分配的数组位置相同，这是个问题。</p><p>解决办法：如果两个键映射到了同一个位置，就在这个位置存储一个链表。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi55lqjgj20l606i74w.jpg" alt></p><p>但是，如果A开头的物品过多，散列表的效率将激素下降，然而：如果散列函数用的很好，这些列表就不会很长。</p><h3 id="5-4-性能"><a href="#5-4-性能" class="headerlink" title="5.4 性能"></a>5.4 性能</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi9iv10gj20d0071my4.jpg" alt></p><p>平均情况下，散列表的查找速度和数组一样快，而插入和删除速度与链表一样快，因此它兼具两者的优点！但是最糟的情况下，散列表的各种操作都很慢。</p><p>因此，为了避免冲突，需要有：</p><p>较低的填装因子。</p><p>良好的散列函数。</p><p>装填因子：</p><p>散列表包含的元素数目/位置总数</p><p>假设再散列表中存储100种商品的价格，散列表包含100个位置名最佳情况下，每个商品都将有自己的位置。</p><p>装填因子在大于1的情况下，需要在散列表中添加位置，这个操作被称为<strong>调整长度(resizing)</strong>。</p><p>一般操作是：<strong>数组增加一倍</strong>。</p><p>接下来，将所有元素用hash函数插入到新的散列表中。</p><p>平均而言，即便考虑到调整长度所需的时间，散列表操作所需的 时间也为O(1)。 </p><p>良好的散列函数让数组中的值呈均匀分布。 </p><p>糟糕的散列函数让值扎堆，导致大量的冲突。 </p><h2 id="第六章-广度优先搜索"><a href="#第六章-广度优先搜索" class="headerlink" title="第六章 广度优先搜索"></a>第六章 广度优先搜索</h2><blockquote><p>图算法之<em>广度优先搜索</em> (breadth-first search)</p></blockquote><h3 id="6-1-图简介"><a href="#6-1-图简介" class="headerlink" title="6.1 图简介"></a>6.1 图简介</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3finx3k1jj20jb07yq40.jpg" alt></p><p>如图用来解决从A点到B点最短路径问题的办法叫图计算方法。</p><p>这种最短路径既可能是最短路径，也有可能是国际象棋中将对方将死的最少步数。</p><p>解决最短路径问题的算法被称为<strong>广度优先搜索</strong>。</p><h3 id="6-2-图是什么"><a href="#6-2-图是什么" class="headerlink" title="6.2 图是什么"></a>6.2 图是什么</h3><p>图用于模拟不同的东西是如何相连的。</p><h3 id="6-3-广度优先搜索"><a href="#6-3-广度优先搜索" class="headerlink" title="6.3 广度优先搜索"></a>6.3 广度优先搜索</h3><p>书中的例计较简单，在朋友圈中找A，先遍历朋友，查找是否有A，有的话结束，没有的话，依次遍历朋友的朋友。（和之前找芒果经销商是一样的）</p><p>能够实现这种目的的数据结构叫做<strong>队列（queue）</strong></p><p>队列的工作原理：你不能随机访问队列中的元素。队列只支持两种操作：入队和出队。</p><p>队列是一种<strong>先进先出（First In First Out，FIFO）</strong>的数据结构，而栈是一种<strong>后进先出（Last In First Out，LIFO）</strong>的数据结构。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj0qapdpj20cd04ddg7.jpg" alt></p><h3 id="6-4-实现图"><a href="#6-4-实现图" class="headerlink" title="6.4 实现图"></a>6.4 实现图</h3><p>python实现一个简单的图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj7iusshj20f70azab9.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;&#125; </span><br><span class="line">graph[<span class="string">"you"</span>] = [<span class="string">"alice"</span>, <span class="string">"bob"</span>, <span class="string">"claire"</span>] </span><br><span class="line">graph[<span class="string">"bob"</span>] = [<span class="string">"anuj"</span>, <span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"alice"</span>] = [<span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"claire"</span>] = [<span class="string">"thom"</span>, <span class="string">"jonny"</span>] </span><br><span class="line">graph[<span class="string">"anuj"</span>] = [] </span><br><span class="line">graph[<span class="string">"peggy"</span>] = [] </span><br><span class="line">graph[<span class="string">"thom"</span>] = [] </span><br><span class="line">graph[<span class="string">"jonny"</span>] = []</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj9dzn13j20mt04pwf5.jpg" alt></p><p>上图中的有向图和无向图是等价的。</p><h3 id="6-5-实现算法"><a href="#6-5-实现算法" class="headerlink" title="6.5 实现算法"></a>6.5 实现算法</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fjb7w4rwj20jv0ls77b.jpg" alt></p><p>在Python中，可以使用函数deque来创建一个双端队列</p><p>这边需要考虑一个情况：就是朋友是朋友的朋友，循环调用会造成无限循环。</p><p>所以需要添加容错判断。用一个列表来记录检查过的人。</p><p>图的特殊情况：指针只往一个方向，比如说：族谱。</p><h2 id="第七章-狄克斯特拉算法-Dijkstra’s-Algorithm"><a href="#第七章-狄克斯特拉算法-Dijkstra’s-Algorithm" class="headerlink" title="第七章 狄克斯特拉算法 Dijkstra’s Algorithm"></a>第七章 狄克斯特拉算法 Dijkstra’s Algorithm</h2><h3 id="7-1-狄克斯特拉算法介绍"><a href="#7-1-狄克斯特拉算法介绍" class="headerlink" title="7.1 狄克斯特拉算法介绍"></a>7.1 狄克斯特拉算法介绍</h3><p>依旧图的讨论。</p><p>如果之前的路径有了权重（节点到节点之间花费的时间不等价），重新计算最短路径，就应该使用狄克斯特拉算法。</p><p>狄克斯特拉算法包含四个步骤：</p><ul><li>找出最便宜的节点，即可在最短时间内前往的节点。</li><li>对于该节点的邻居，检查是否有前往他们的最短路径，如果有，就更新其开销。</li><li>重复这个过程，知道对图中的每个节点都这样做了。</li><li>计算最终路径。</li></ul><h3 id="7-3-术语"><a href="#7-3-术语" class="headerlink" title="7.3 术语"></a>7.3 术语</h3><p>每条边关联的数字叫做权重（weight）。</p><p>带权重的图称为加权图（weighted graph），不带权重的图称为非加权图（unweighted graph）。</p><p>要计算非加权图中的最短路径，可以使用<strong>广度优先搜索</strong>。</p><p>如果是为了计算加权图中的最短路径，可以使用<strong>迪克斯特拉算法</strong>。</p><p>图还可能有环，这意味着你可以从一个节点出发，走一圈后又回到这个节点</p><p>在无向图中，每条边都是一个环，狄克斯特拉算法只使用于有向无环图（DAG）</p><h3 id="7-4-负权边"><a href="#7-4-负权边" class="headerlink" title="7.4 负权边"></a>7.4 负权边</h3><p>如果有负权边就不能用，就不能使用狄克斯特拉算法，因为负权边，就不能使用狄克斯特拉算法。</p><p>因为负权边会导致这种算法不管用。</p><p>因为：根据狄克斯特拉算法，没有比不支付任何费用获得海报更便宜的方式。</p><p>因此：不能将狄克斯特拉算法用于包含负权边的图。</p><p>要在包含负权边的图中，找出最短路径，可以使用另一种算法： 贝尔曼—福德算法（Bellman-Ford algorithm）.</p><h3 id="7-5-实现"><a href="#7-5-实现" class="headerlink" title="7.5 实现"></a>7.5 实现</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gntz3sx0j20aa06fq38.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gobfcl77j20fz0i4wek.jpg" alt></p><p>可以用以上代码表示上图的散列表。</p><p>上面代码表达的表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goeygswbj20b00a6dgw.jpg" alt></p><p>接下来需要一个散列表来粗春<strong>每个节点的开销</strong>。</p><p>节点的开销：从起点出发前往该节点需要的时间。</p><p>用表表示的话如图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gofc9opgj205805zdg6.jpg" alt></p><p>表中的无穷大可以这么表示：</p><p><a href="https://www.cnblogs.com/lvye-song/p/4029691.html" target="_blank" rel="noopener">python正负无穷</a></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goiypuf7j20e205z0sk.jpg" alt></p><p>除了上面两张表，还需要一个存储父节点的散列表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gox0oz11j209g096wf7.jpg" alt></p><p>创建这个散列表的代码如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gozpliz7j20aj05mjr7.jpg" alt></p><p>最后，需要一个数组用于记录处理过的节点，因为对于同一个节点，你不用处理多次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">processd = &#123;&#125;</span><br></pre></td></tr></table></figure><p>动图表示整个认证过程：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3hv4x3xeqg207v066jrc.gif" alt></p><p>整体过程：</p><p>本书介绍的python 迪克斯特拉算法：</p><p>使用了三个散列表和一个数组，三个散列表的作用分别是：</p><p>第一个：Graph散列表</p><p>用来记录每个节点到指向节点的权重</p><p>第二个：Costs散列表</p><p>指的起点到某个节点的消耗</p><p>第三个：Parents散列表</p><p>指的是父节点的散列表</p><p>数组的作用是记录用于处理过的节点。</p><p>处理过程是，</p><p>找出一个未处理的节点（规则定位开销最小的）</p><p>然后在表一获得该节点的开销和邻居。</p><p>遍历邻居，</p><p>接着计算从起点到X再到邻居节点的距离，然后在表一中对比这样的开销和原先的开销大小，如果这样效率更高，那么在表二中替换掉（或者更新掉原先的数字），然后在表三中改变其父节点为X</p><p>（表二记载的开销是经过父节点的最短开销）</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line"><span class="keyword">while</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">cost = costs[node]</span><br><span class="line">neighbors = graph[node]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> neighbors.keys():</span><br><span class="line">new_cost = cost + neighbors[n]</span><br><span class="line"><span class="keyword">if</span> costs[n] &gt; new_cost:</span><br><span class="line">costs[n] = new_cost</span><br><span class="line">parents[n] = node</span><br><span class="line">processed.append(node)</span><br><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_lowest_cost_node</span><span class="params">(costs)</span>:</span></span><br><span class="line">    lowest_cost = float(<span class="string">"inf"</span>)</span><br><span class="line">    lowest_cost_node = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> costs:</span><br><span class="line">        cost = costs[node]</span><br><span class="line">        <span class="keyword">if</span> cost &lt; lowest_cost <span class="keyword">and</span> node <span class="keyword">not</span> <span class="keyword">in</span> processed:</span><br><span class="line">        lowest_cost = cost</span><br><span class="line">            lowest_cost_node = node</span><br><span class="line"><span class="keyword">return</span> lowest_cost_node</span><br></pre></td></tr></table></figure><p>书上对这个过程的描述还可以，但是我觉得如果能增加一个循环就更好了。</p><h2 id="第八章-贪婪算法"><a href="#第八章-贪婪算法" class="headerlink" title="第八章 贪婪算法"></a>第八章 贪婪算法</h2><h3 id="8-1-教室调度问题"><a href="#8-1-教室调度问题" class="headerlink" title="8.1 教室调度问题"></a>8.1 教室调度问题</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iys53c86j20m50b140f.jpg" alt></p><p>解决方法：</p><p>(1) 选出结束最早的课，它就是要在这间教室上的第一堂课。 </p><p>(2) 接下来，必须选择第一堂课结束后才开始的课。同样，你选择结束最早的课，这将是要 在这间教室上的第二堂课。 </p><p>重读这样做就能找出答案。</p><p>即：每步都选择局部最优解，最终得到的就是全局最优解。</p><p>此方法并非万能！但是行之有效，并且<strong>简单</strong>！</p><h3 id="8-2-背包问题"><a href="#8-2-背包问题" class="headerlink" title="8.2 背包问题"></a>8.2 背包问题</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。&lt;/p&gt;
&lt;p&gt;这边值得一提的是作者推荐了一个网站，可汗学院，&lt;code&gt;khanacademy.org&lt;/code&gt;  mark一下。&lt;/p&gt;
&lt;p&gt;看完40%来总结一下，非常好，文盲也能看懂的算法入门。&lt;/p&gt;
&lt;p&gt;这本书看完应该会扫一眼结城浩的《图解密码学》&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading Note" scheme="http://yoursite.com/categories/Reading-Note/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/categories/Reading-Note/Grokking-Algorithms/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/tags/Grokking-Algorithms/"/>
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
      <category term="欧几里得算法" scheme="http://yoursite.com/tags/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/"/>
    
      <category term="快排" scheme="http://yoursite.com/tags/%E5%BF%AB%E6%8E%92/"/>
    
  </entry>
  
  <entry>
    <title>Hive Metastore Server发送元数据请求过多被拒绝</title>
    <link href="http://yoursite.com/2020/03/09/Hive%20Metastore%20Server%E5%8F%91%E9%80%81%E5%85%83%E6%95%B0%E6%8D%AE%E8%AF%B7%E6%B1%82%E8%BF%87%E5%A4%9A%E8%A2%AB%E6%8B%92%E7%BB%9D/"/>
    <id>http://yoursite.com/2020/03/09/Hive Metastore Server发送元数据请求过多被拒绝/</id>
    <published>2020-03-09T10:03:40.632Z</published>
    <updated>2020-04-10T17:08:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在TPCDS测试中Hive莫名其妙挂掉</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h4gok9evj20lx09paaq.jpg" alt></p><p>经查，是MySQL最大连接尝试数设置过低，连接失败10次就无法再连接</p><p>在设置的mysql数据库里面，调整次数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| amon               |</span><br><span class="line">| cm                 |</span><br><span class="line">| hive               |</span><br><span class="line">| hue                |</span><br><span class="line">| mysql              |</span><br><span class="line">| oozie              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test               |</span><br><span class="line">+--------------------+</span><br><span class="line">9 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show variables like &apos;%max_connect_errors%&apos;;</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| Variable_name      | Value |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| max_connect_errors | 10    |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; set global max_connect_errors = 1000;</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show variables like &apos;%max_connect_errors%&apos;;</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| Variable_name      | Value |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| max_connect_errors | 1000  |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>搞定</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在TPCDS测试中Hive莫名其妙挂掉&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>在HUE中整合Oozie和Spark2并验证</title>
    <link href="http://yoursite.com/2020/03/09/HUE%E4%B8%8AOozie%E5%92%8CSpark2%E6%95%B4%E5%90%88/"/>
    <id>http://yoursite.com/2020/03/09/HUE上Oozie和Spark2整合/</id>
    <published>2020-03-09T10:03:40.538Z</published>
    <updated>2020-04-10T17:08:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍</p></blockquote><a id="more"></a> <h3 id="查看sharelib文件夹的位置"><a href="#查看sharelib文件夹的位置" class="headerlink" title="查看sharelib文件夹的位置"></a>查看<code>sharelib</code>文件夹的位置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master126 ~]# oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">[ShareLib update status]</span><br><span class="line">sharelibDirOld = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">host = http://master126:11000/oozie</span><br><span class="line">sharelibDirNew = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">status = Successful</span><br></pre></td></tr></table></figure><h3 id="创建文件目录"><a href="#创建文件目录" class="headerlink" title="创建文件目录"></a>创建文件目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u oozie hdfs dfs -mkdir /user/oozie/share/lib/lib_20190521144826/spark2</span><br></pre></td></tr></table></figure><h3 id="向文件夹中添加Spark2需要的jar包"><a href="#向文件夹中添加Spark2需要的jar包" class="headerlink" title="向文件夹中添加Spark2需要的jar包"></a>向文件夹中添加Spark2需要的jar包</h3><p><code>/opt/cloudera/parcels/SPARK2/lib/spark2/jars</code>文件夹下的所有内容和</p><p><code>/opt/cloudera/parcels/CDH/lib/oozie/oozie-sharelib-yarn/lib/spark</code>下面的<code>oozie-sharelib-spark*.jar</code></p><p>在公司当前环境下，最终能凑齐的一共有293个jar文件，这边我下载下来打个包存在TIM里面，下次使用方便一些。</p><h3 id="修改目录的所有者和权限"><a href="#修改目录的所有者和权限" class="headerlink" title="修改目录的所有者和权限"></a>修改目录的所有者和权限</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u hdfs hadoop fs –chown -R oozie:oozie /user/oozie/share/lib/lib_20170921070424/spark2</span><br><span class="line">sudo -u hdfs hadoop fs –chmod -R 775 /user/oozie/share/lib/lib_20170921070424/spark2</span><br></pre></td></tr></table></figure><h3 id="更新并且确认"><a href="#更新并且确认" class="headerlink" title="更新并且确认"></a>更新并且确认</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">oozie admin -oozie http://master126:11000/oozie -shareliblist</span><br></pre></td></tr></table></figure><h3 id="用样例验证"><a href="#用样例验证" class="headerlink" title="用样例验证"></a>用样例验证</h3><p>测试的时候别的没什么</p><p>properties要注意修改为</p><table><thead><tr><th>–</th><th>–</th></tr></thead><tbody><tr><td>oozie.action.sharelib.for.spark</td><td>spark2</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
    
      <category term="HUE" scheme="http://yoursite.com/tags/HUE/"/>
    
  </entry>
  
  <entry>
    <title>Kerberos For CDH</title>
    <link href="http://yoursite.com/2020/03/09/Kerberos%20For%20CDH/"/>
    <id>http://yoursite.com/2020/03/09/Kerberos For CDH/</id>
    <published>2020-03-09T10:03:40.523Z</published>
    <updated>2020-04-10T17:08:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。</p><p>第一次更新：2019-4-29</p><p>第二次更新：2019-5-10</p><a id="more"></a> <h3 id="1-Kerberos-入门"><a href="#1-Kerberos-入门" class="headerlink" title="1.Kerberos 入门"></a>1.Kerberos 入门</h3><p>Kerberos是一种计算机网络授权协议，用来在非安全网络中，对个人通信以安全的手段进行身份认证。Hadoop集群中涉及的Kerberos一般是指MIT基于Kerberos协议开发的一套软件。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2fvwiwb03g20rs0fvaa2.gif" alt="ALT Kerberos"></p><p>Kerberos在希腊神话中是Hades的一条凶猛的三头保卫神犬。这三个头在Kerberos代表了Client、Server和KDC。</p><p><em>Kerberos的特点</em>：并不要求通信双方所在的网络环境安全，即使通信过程中数据被截取或者篡改，依然不会影响整套机制的正常工作。时间戳是Kerberos用来保证通信安全的重要手段。</p><p><em>Kerberos的基本思路</em>：基于对称加密，利用集中的认证服务器，实现用户和服务器之间的双向认证。(提供一种不能伪造、不能重放、已经鉴别的票据对象)。</p><hr><h3 id="2-Kerberos涉及名词"><a href="#2-Kerberos涉及名词" class="headerlink" title="2.Kerberos涉及名词"></a>2.Kerberos涉及名词</h3><p><strong>Principal</strong>：认证的主体，简单来说也就是 用户名</p><p><strong>Kinit</strong>：Kerberos认证(登录)命令，可以使用密码或者KEYTAB</p><p><strong>realm</strong>：有点类似namespace，一个principle只有在某个realm下才有意义</p><p><strong>Password</strong>：某个用户的密码，对应于Kerberos中的master key。password可以存在一个KEYTAB文件中。所以Kerberos中需要使用密码的场景都可以用一个KEYTAB作为输入</p><p><strong>credential</strong>：credential是“证明某个人确定是他自己/某一种行为的确可以发生”的凭据。在不同的使用场景下， credential的具体含义也略有不同：</p><ul><li><p>对于某个principal个体而言，他的credential就是他的password</p></li><li><p>在Kerberos认证的环节中，credential就意味着各种各样的ticket</p></li></ul><p><strong>TGT</strong>：Ticket Granting Ticket，要获得key还需要一个资格认证，获得这个资格认证的证明，叫做TGT</p><p><strong>Long-term Key/Master Key</strong>：在Security的领域中，有的Key可能长期内保持不变，比如你在密码，可能几年都不曾改变，这样的Key、以及由此派生的Key被称为Long-term Key。对于Long-term Key的使用有这样的原则：被Long-term Key加密的数据不应该在网络上传输。原因很简单，一旦这些被Long-term Key加密的数据包被恶意的网络监听者截获，在原则上，只要有充足的时间，他是可以通过计算获得你用于加密的Long-term Key的——任何加密算法都不可能做到绝对保密。</p><p><strong>Short-term Key/Session Key</strong>：由于被Long-term Key加密的数据包不能用于网络传送，所以我们使用另一种Short-term<br>Key来加密需要进行网络传输的数据。由于这种Key只在一段时间内有效，即使被加密的数据包被黑客截获，等他把Key计算出来的时候，这个Key早就已经过期了。</p><hr><h3 id="3-Kerberos原理"><a href="#3-Kerberos原理" class="headerlink" title="3.Kerberos原理"></a>3.Kerberos原理</h3><p>最简单的对称加密的思路：A发送信息给B，信息分为两段，一段是明文，一段是加密之后的密文，这个密钥只有A和B两个人知道，B收到信息后，用两个人都知道的密钥破解了这段密文，如果破解之后的内容和明文内容一致就说明 A的身份没有问题。</p><p><strong>Kerberos就是基于此基础之上的一套复杂的认证机制。</strong></p><p>下面对整个认证过程进行一个细致的分析，对于安装部署过程中纠错来说（尤其CDH集群集成的<code>Kerberos</code>有一些问题），这个过程是非常有必要的：</p><p>通过第二节介绍的两种Key，让被认证的一方提供一个仅限于他和认证方知晓的Key来鉴定对方的真实身份。而被这个<code>Key</code>加密的数据包需要在<code>Client</code>和<code>Server</code>之间传送，所以这个<code>Key</code>不能是一个<code>Long-term Key</code>，而只可能是<code>hort-term Key</code>，这个可以仅仅在<code>Client</code>和<code>Server</code>的一个<code>Session</code>中有效，所以我们称这个<code>Key</code>为<code>Client</code>和<code>Server</code>之间的<code>Session Key（Sserver-Client）</code>。</p><p>这个<code>Sserver-Client</code> 需要引入<code>Kerberos</code>中的一个十分重要的觉得：<code>Kerberos Distribution Center-KDC</code>。<code>KDC</code>在整个<code>Kerberos</code>认证流程中作为<code>Client</code>和<code>Server</code>共同信任的第三方起着重要的作用，而<code>Kerberos</code>的认证过程就是通过这三方协作完成。<code>KDC</code>中维护着一个存储着该<code>Domain</code>中所有账户的<code>Account Database</code>（一个轻量级的数据库），这个数据库汇中存储着每个<code>Account</code>的名称和派生于该<code>Account Password</code>的<code>Master Key</code>，派生手段一般来说是类似Hash这种，不可逆的，然后可以一一对应的方法。</p><p>稍微扩展一下上面的<code>key</code>的分发过程：</p><p>首先是<code>Client</code>向<code>KDC</code>发送一个对<code>SServer-Client</code>的申请。这个申请的内容可以简单概括为“我是某个<code>Client</code>，我需要一个<code>Session Key</code>用于访问某个<code>Server</code> ”。<code>KDC</code>在接受了这个请求以后，生成一个<code>Session</code>Key，为了保证这个<code>Session Key</code>仅仅限于发送请求的<code>Client</code>和他希望访问的<code>Server</code>知晓，<code>KDC</code>会为这个<code>Session Key</code>生成两个<code>Copy</code>，分别被<code>Client</code>和<code>Server</code>使用。然后从<code>Account database</code>中提取<code>Client</code>和<code>Server</code>的<code>Master Key</code>分别对这两个<code>Copy</code>进行对称加密。对于后者，和<code>Session Key</code>一起被加密的还包含关于<code>Client</code>的一些信息。（这里的<code>Client</code>可以理解为发送连接请求的节点，<code>Server</code>可以理解为<code>Client</code>发送请求的接受节点）。</p><p>现在KDC有了两个分别被<code>Master</code>和<code>Server</code>的<code>Master key</code>加密过的<code>Session Key</code>，下面介绍这两个<code>Session Key</code>的处理方式。</p><p>Kerberos 并不会直接把两个加密包分别发送给<code>Client</code>和<code>Server</code>，原因主要有两个：</p><p>第一：由于一个<code>Server</code>会面对若干不同的<code>Client</code>, 而每个<code>Client</code>都具有一个不同的<code>Session Key</code>。那么<code>Server</code>就会为所有的<code>Client</code>维护这样一个<code>Session Key</code>的列表，这样对于<code>Server</code>来说是比较麻烦而低效的。</p><p>第二：由于网络传输的不确定性，可能出现这样一种情况：<code>Client</code>很快获得<code>Session Key</code>，并将这个<code>Session Key</code>作为<code>Credential</code>随同访问请求发送到<code>Server</code>，但是用于<code>Server</code>的<code>Session Key</code>确还没有收到，并且很有可能承载这个<code>Session Key</code>的永远也到不了<code>Server</code>端，<code>Client</code>将永远得不到认证。</p><p>为了解决这个问题，<code>Kerberos</code>的做法是：<strong>将这两个被加密的<code>Copy</code>一并发送给<code>Client</code>，属于<code>Server</code>的那份由<code>Client</code>发送给<code>Server</code>。</strong></p><p><code>Client</code>实际上获得了两组信息：一个通过自己<code>Master Key</code>加密的<code>Session Key</code>，另一个被<code>Server</code>的<code>Master Key</code>加密的数据包，包含<code>Session Key</code>和关于自己的一些确认信息。通过一个双方知晓的<code>Key</code>就可以对对方进行有效的认证，但是在一个网络的环境中，这种简单的做法是具有安全漏洞，为此,<code>Client</code>需要提供更多的证明信息，我们把这种证明信息称为<code>Authenticator</code>，在<code>Kerberos</code>的<code>Authenticator</code>实际上就是关于<code>Client</code>的一些信息和当前时间的一个<code>Timestamp</code>。</p><p><code>Client</code>通过自己的<code>Master Key</code>对<code>KDC</code>加密的<code>Session Key</code>进行解密从而获得<code>Session Key</code>，随后创建<strong><code>Authenticator（Client Info + Timestamp）</code></strong>并用<code>Session Key</code>对其加密。最后连同从<code>KDC</code>获得的、被<code>Server</code>的<code>Master Key</code>加密过的数据包<strong><code>（Client Info + Session Key）</code></strong>一并发送到<code>Server</code>端。我们把通过<code>Server</code>的<code>Master Key</code>加密过的数据包称为<code>Session Ticket</code>。当<code>Server</code>接收到这两组数据后，先使用他自己的<code>Master Key</code>对<code>Session Ticket</code>进行解密，从而获得<code>Session Key</code>。随后使用该<code>Session Key</code>解密<code>Authenticator</code>，通过比较<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>从而实现对Client的认证。</p><p>这里涉及到了一个<code>Timestamp</code>，<code>Client</code>向<code>Server</code>发送的数据包如果被某个恶意网络监听者截获，该监听者随后将数据包作为自己的<code>Credential</code>冒充该<code>Client</code>对<code>Server</code>进行访问，在这种情况下，依然可以很顺利地获得<code>Server</code>的成功认证。为了解决这个问题，<code>Client</code>在<code>Authenticator</code>中会加入一个当前时间的<code>Timestamp</code>。</p><p>在<code>Server</code>对<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>进行比较之前，会先提取<code>Authenticator</code>中的<code>Timestamp</code>，并同当前的时间进行比较，如果他们之间的偏差超出一个可以<strong>接受的时间范围（一般是5mins）</strong>，<code>Server</code>会直接拒绝该<code>Client</code>的请求。在这里需要知道的是，<code>Server</code>维护着一个列表，这个列表记录着在这个可接受的时间范围内所有进行认证的Client和认证的时间。对于时间偏差在这个可接受的范围中的<code>Client</code>，<code>Server</code>会从这个列表中获得<strong>最近一个该<code>Client</code>的认证时间</strong>，只有当<code>Server</code>接收到<code>Authenticator</code>时，验证<code>Authenticator</code>中的<code>Timestamp</code>，确定传输时间小于接受范围后，<code>Server</code>才采用进行后续的认证流程。</p><hr><p><strong><code>Time Synchronization</code>的重要性</strong></p><p>上述基于<code>Timestamp</code>的认证机制只有在<code>Client</code>和<code>Server</code>端的时间保持同步的情况才有意义。所以保持<code>Time</code> <code>Synchronization</code>在整个认证过程中显得尤为重要。在一个<code>Domain</code>中，一般通过访问同一个<code>Time Service</code>获得当前时间的方式来实现时间的同步。</p><p><strong>双向认证（Mutual Authentication）</strong></p><p><code>Kerberos</code>一个重要的优势在于它能够提供双向认证：不但<code>Server</code>可以对<code>Client</code> 进行认证，<code>Client</code>也能对<code>Server</code>进行认证。</p><p>具体过程是这样的，如果<code>Client</code>需要对他访问的<code>Server</code>进行认证，会在它向<code>Server</code>发送的<code>Credential</code>中设置一个是否需要认证的<code>Flag</code>。<code>Server</code>在对<code>Client</code>认证成功之后，会把<code>Authenticator</code>中的<code>Timestamp</code>提出来，通过<code>Session Key</code>进行加密，当<code>Client</code>接收到并使用<code>Session Key</code>进行解密之后，如果确认<code>Timestamp</code>和原来的完全一致，那么他可以认定<code>Server</code>正试图访问的<code>Server</code>。</p><p>那么为什么<code>Server</code>不直接把通过Session Key进行加密的<code>Authenticator</code>原样发送给<code>Client</code>，而要把<code>Timestamp</code>提取出来加密发送给<code>Client</code>呢？原因在于防止恶意的监听者通过获取的<code>Client</code>发送的<code>Authenticator</code>冒充<code>Server</code>获得<code>Client</code>的认证。</p><p><strong>More</strong>：</p><p>通过上面的介绍，我们发现<code>Kerberos</code>实际上一个基于<code>Ticket</code>的认证方式。<code>Client</code>想要获取<code>Server</code>端的资源，先得通过<code>Server</code>的认证；而认证的先决条件是<code>Client</code>向<code>Server</code>提供从<code>KDC</code>获得的一个有<code>Server</code>的<code>Master Key</code>进行加密的<code>Session Ticket</code>（<code>Session Key + Client Info</code>）。可以这么说，<code>Session Ticket</code>是<code>Client</code>进入<code>Server</code>领域的一张门票。而这张门票必须从一个合法的<code>Ticket</code>颁发机构获得，这个颁发机构就是<code>Client</code>和<code>Server</code>双方信任的<code>KDC</code>， 同时这张<code>Ticket</code>具有超强的防伪标识：<strong>它是被<code>Server</code>的<code>Master Key</code>加密的。对<code>Client</code>来说， 获得<code>Session</code> <code>Ticket</code>是整个认证过程中最为关键的部分。</strong></p><hr><p>我了解到这儿感觉已经差不多了，然而这还只是Kerbeos的梗概  T_T</p><p><code>Client</code>要获得<code>Ticket</code>之前，还需要一个步骤，即获得<code>KDC</code>的权限确认，这个过程叫做<code>TGT：Ticket</code><br><code>Granting Ticket</code>。<code>TGT</code>的分发方仍然是<code>KDC</code>。首先<code>Client</code>向<code>KDC</code>发起对<code>TGT</code>的申请，申请的内容大致可以这样表示：“我需要一张<code>TGT</code>用以申请获取用以访问所有<code>Server</code>的<code>Ticket</code>”。<code>KDC</code>在收到该申请请求后，生成一个用于该<code>Client</code>和<code>KDC</code>进行安全通信的<code>Session Key（SKDC-Client）</code>。为了保证该<code>Session Key</code>仅供该<code>Client</code>和自己使用，<code>KDC</code>使用<code>Client</code>的<code>Master Key</code>和自己的<code>Master Key</code>对生成的<code>Session Key</code>进行加密，从而获得两个加密的<code>SKDC-Client</code>的<code>Copy</code>。对于后者，随<code>SKDC-Client</code>一起被加密的还包含以后用于鉴定<code>Client</code>身份的关于<code>Client</code>的一些信息。最后<code>KDC</code>将这两份<code>Copy</code>一起发送给<code>Client</code>。这里有一点需要注意的是：为了免去<code>KDC</code>对于基于不同<code>Client</code>的<code>Session Key</code>进行维护的麻烦，就像<code>Server</code>不会保存<code>Session Key（SServer-Client）</code>一样，<code>KDC</code>也不会去保存这个<code>Session Key（SKDC-Client）</code>，而选择完全靠<code>Client</code>自己提供的方式。</p><p>当<code>Client</code>收到<code>KDC</code>的两个加密数据包之后，先使用自己的<code>Master Key</code>对第一个<code>Copy</code>进行解密，从而获得<code>KDC</code>和<code>Client</code>的<code>Session</code><br><code>Key（SKDC-Client）</code>，并把该<code>Session</code> 和<code>TGT</code>进行缓存。有了<code>Session Key</code>和<code>TGT</code>，<code>Client</code>自己的<code>Master</code><br><code>Key</code>将不再需要，因为此后<code>Client</code>可以使用<code>SKDC-Client</code>向<code>KDC</code>申请用以访问每个<code>Server</code>的<code>Ticket</code>。同时需要注意的是<code>SKDC-Client</code>是一个<code>Session Key</code>，他具有自己的生命周期，同时<code>TGT</code>和<code>Session</code>相互关联，当<code>Session Key</code>过期，<code>TGT</code>也就宣告失效，此后<code>Client</code>不得不重新向<code>KDC</code>申请新的<code>TGT</code>，<code>KDC</code>将会生成一个不同<code>Session Key</code>和与之关联的<code>TGT</code>。同时，由于<code>Client Log off</code>也导致<code>SKDC-Client</code>的失效，所以<code>SKDC-Client</code>又被称为<code>Logon Session Key</code>。<strong><code>TGT</code>和<code>Ticket</code>有个区别就是<code>Ticket</code>是基于某个具体的<code>Server</code>的，而<code>TGT</code>则是和具体的<code>Server</code>无关的。</strong></p><p><code>Client</code>在获得自己和<code>KDC</code>的<code>Session Key（SKDC-Client）</code>之后，生成自己的<code>Authenticator</code>以及所要访问的<code>Server</code>名称的并使用<code>SKDC-Client</code>进行加密。随后连同<code>TGT</code>一起发送给<code>KDC</code>。<code>KDC</code>使用自己的<code>Master Key</code>对<code>TGT</code>进行解密，提取<code>Client Info</code>和<code>Session Key（SKDC-Client）</code>，然后使用这个<code>SKDC-Client</code>解密<code>Authenticator</code>获得<code>Client Info</code>，对两个<code>Client Info</code>进行比较进而验证对方的真实身份。验证成功，生成一份基于<code>Client</code>所要访问的<code>Server</code>的<code>Ticket</code>给<code>Client</code>，然后继续上面之说的过程。</p><p>介绍了这么多，重新把整个过程理一遍：</p><p>现在介绍的整个Authentication过程大概分为三个子过程</p><ul><li><p>Client向KDC申请TGT（Ticket Granting Ticket）。</p></li><li><p>Client通过获得TGT向DKC申请用于访问Server的Ticket。</p></li><li><p>Client最终向为了Server对自己的认证向其提交Ticket。</p></li></ul><p>整个Kerberos Authentication认证过程通过3个sub-protocol来完成：</p><ol><li>Authentication Service Exchange</li><li>Ticket Granting Service Exchange</li><li>Client/Server Exchange</li></ol><p>下面内容来自官方文档的翻译：</p><p>1.Authentication Service Exchange</p><p><code>Client</code>向<code>KDC</code>的<code>Authentication Service</code>发送<code>Authentication Service Request</code>（<code>KRB_AS_REQ</code>）, 为了确保<code>KRB_AS_REQ</code>仅限于自己和<code>KDC</code>知道，<code>Client</code>使用自己的<code>Master Key</code>对<code>KRB_AS_REQ</code>的主体部分进行加密（<code>KDC</code>可以通过<code>Domain</code> 的<code>Account Database</code>获得该<code>Client</code>的<code>Master Key</code>）。<code>KRB_AS_REQ</code>的大体包含以下的内容：</p><ul><li><code>Pre-authentication data</code>：包含用以证明自己身份的信息。说白了，就是证明自己知道自己声称的那个<code>account</code>的<code>Password</code>。一般地，它的内容是一个被<code>Client</code>的<code>Master key</code>加密过的<code>Timestamp</code>。</li><li><code>Client name</code> &amp; <code>realm</code>: 简单地说就是<code>Domain name\Client</code></li><li><code>Server Name</code>：注意这里的<code>Server Name</code>并不是<code>Client</code>真正要访问的<code>Server</code>的名称，而我们也说了<code>TGT</code>是和<code>Server</code>无关的（<code>Client</code>只能使用<code>Ticket</code>，而不是<code>TGT</code>去访问<code>Server</code>）。这里的<code>Server Name</code>实际上是<code>KDC</code>的<code>Ticket Granting Service</code>的<code>Server Name</code>。</li></ul><p><code>AS（Authentication Service）</code>通过它接收到的<code>KRB_AS_REQ</code>验证发送方的是否是在<code>Client name</code> &amp; <code>realm</code>中声称的那个人，也就是说要验证发送方是否知道<code>Client</code>的<code>Password</code>。所以<code>AS</code>只需从<code>Account Database</code>中提取<code>Client</code>对应的<code>Master Key</code>对<code>Pre-authentication data</code>进行解密，如果是一个合法的<code>Timestamp</code>，则可以证明发送方提供的是正确无误的密码。验证通过之后，<code>AS</code>将一份<code>Authentication Service</code> <code>Response（KRB_AS_REP）</code>发送给<code>Client</code>。<code>KRB_AS_REQ</code>主要包含两个部分：本<code>Client</code>的<code>Master Key</code>加密过的<code>Session Key（SKDC-Client：Logon Session Key）</code>和被自己（<code>KDC</code>）加密的<code>TGT</code>。而<code>TGT</code>大体又包含以下的内容：</p><ul><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>End time</code>: <code>TGT</code>到期的时间</p></li></ul><p>Client通过自己的Master Key对第一部分解密获得Session Key（SKDC-Client：Logon Session Key）之后，携带着TGT便可以进入下一步：TGS（Ticket Granting Service）Exchange。</p><p>2.Ticket Granting Service Exchange</p><p><code>TGS</code>（<code>Ticket Granting Service</code>）<code>Exchange</code>通过<code>Client</code>向<code>KDC</code>中的<code>TGS</code>（<code>Ticket Granting Service</code>）发送<code>Ticket Granting Service Request</code>（<code>KRB_TGS_REQ</code>）开始。<code>KRB_TGS_REQ</code>大体包含以下的内容：</p><ul><li><p>TGT：Client通过AS Exchange获得的Ticket Granting Ticket，TGT被KDC的Master Key进行加密。</p></li><li><p>Authenticator：用以证明当初TGT的拥有者是否就是自己，所以它必须以TGT的办法方和自己的Session Key（SKDC-Client：Logon Session Key）来进行加密。</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client。</p></li><li><p>Server name &amp; realm: 简单地说就是Domain name\Server，这回是Client试图访问的那个Server。</p></li></ul><p><code>TGS</code>收到<code>KRB_TGS_REQ</code>在发给<code>Client</code>真正的<code>Ticket</code>之前，先得整个<code>Client</code>提供的那个<code>TGT</code>是否是<code>AS</code>颁发给它的。于是它不得不通过<code>Client</code>提供的<code>Authenticator</code>来证明。但是<code>Authentication</code>是通过<code>Logon Session Key（SKDC-Client）</code>进行加密的，而自己并没有保存这个<code>Session Key</code>。所以TGS先得通过自己的<code>Master Key</code>对<code>Client</code>提供的<code>TGT</code>进行解密，从而获得这个<code>Logon Session Key（SKDC-Client）</code>，再通过这个<code>Logon Session Key（SKDC-Client）</code>解密<code>Authenticator</code>进行验证。验证通过向对方发送<code>Ticket Granting</code><br><code>Service Response（KRB_TGS_REP）</code>。这个<code>KRB_TGS_REP</code>有两部分组成：使用<code>Logon Session Key（SKDC-Client）</code>加密过用于<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>和使用<code>Server</code>的<code>Master Key</code>进行加密的<code>Ticket</code>。该<code>Ticket</code>大体包含以下一些内容：</p><ul><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>End time: Ticket的到期时间</p></li></ul><p><code>Client</code>收到<code>KRB_TGS_REP</code>，使用<code>Logon Session Key（SKDC-Client）</code>解密第一部分后获得<code>Session Key（SServer-Client）</code>。有了<code>Session Key</code>和<code>Ticket，Client</code>就可以之间和<code>Server</code>进行交互，而无须在通过<code>KDC</code>作中间人了。所以我们说<code>Kerberos</code>是一种高效的认证方式，它可以直接通过<code>Client</code>和<code>Server</code>双方来完成，不像Windows NT 4下的<code>NTLM</code>认证方式，每次认证都要通过一个双方信任的第3方来完成。</p><p>我们现在来看看 <code>Client</code>如果使用<code>Ticket</code>和<code>Server</code>怎样进行交互的，这个阶段通过我们的第3个<code>Sub-protocol</code>来完成：<code>CS（Client/Server ）Exchange</code>。</p><ol start="3"><li>CS（Client/Server ）Exchange</li></ol><p>这个已经经介绍过。<code>Client</code>通过<code>TGS Exchange</code>获得<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>，随后创建用于证明自己就是Ticket的真正所有者的<code>Authenticator</code>，并使用<code>Session Key（SServer-Client）</code>进行加密。最后将这个被加密过的<code>Authenticator</code>和<code>Ticket</code>作为<code>Application Service Request（KRB_AP_REQ）</code>发送给<code>Server</code>。除了上述两项内容之外，<code>KRB_AP_REQ</code>还包含一个<code>Flag</code>用于表示<code>Client</code>是否需要进行双向验证（<code>Mutual Authentication</code>）。</p><p><code>Server</code>接收到<code>KRB_AP_REQ</code>之后，通过自己的<code>Master Key</code>解密<code>Ticket</code>，从而获得<code>Session Key（SServer-Client）</code>。通过<code>Session Key（SServer-Client）</code>解密<code>Authenticator</code>，进而验证对方的身份。验证成功，让<code>Client</code>访问需要访问的资源，否则直接拒绝对方的请求。</p><p>对于需要进行双向验证，<code>Server</code>从<code>Authenticator</code>提取<code>Timestamp</code>，使用<code>Session Key（SServer-Client）</code>进行加密，并将其发送给<code>Client</code>用于<code>Client</code>验证<code>Server</code>的身份。</p><hr><p>以上是2000年的<code>Kerberos</code>技术，和今天我们使用的<code>Kerberos</code>是不太相同的，因为这样的一个认证过程有一个最大的隐患就是<strong>Long-term Key加密的数据在网络中传递</strong>。</p><p>解决办法也很简单：就是采用一个<code>Short-term</code>的<code>Session Key</code>，而不是<code>Server Master Key</code>对<code>Ticket</code>进行加密。这就是<code>Kerberos</code>的第四个<code>Sub-protocol</code>：<code>User2User Protocol</code>。</p><p>因为<code>KDC</code>是不是维护<code>Session Key</code>的，所以这个<code>Session key</code>只能靠申请<code>Ticket</code>的<code>Client</code>提供，所以在原先的第一步和第二步之间，<code>Client</code>还得对<code>Server</code>进行请求已获得<code>Server</code>和<code>KDC</code>之间的<code>Session Key</code>。而对于<code>Server</code>来说，他可以像<code>Client</code>一样通过<code>AS Exchange</code>获得他和<code>KDC</code>之间的<code>Session Key</code>（<code>SKDC-Server</code>）和一个封装了这个<code>Session Key</code>并被<code>KDC</code>的<code>Master Key</code>进行加密的<code>TGT</code>，一旦获得这个<code>TGT</code>，<code>Server</code>会缓存它，以待<code>Client</code>对它的请求。</p><p>所以现在添加完这个User2User的认证过程，这个过程有4个步骤组成，四个步骤如下：</p><ul><li><p>AS Exchange：Client通过此过程获得了属于自己的TGT，有了此TGT，Client可凭此向KDC申请用于访问某个Server的Ticket。</p></li><li><p>User2User：这一步的主要任务是获得封装了Server和KDC的Session Key（SKDC-Server）的属于Server的TGT。如果该TGT存在于Server的缓存中，则Server会直接将其返回给Client。否则通过AS Exchange从KDC获取。</p></li><li>TGS Exchange：Client通过向KDC提供自己的TGT，Server的TGT以及Authenticator向KDC申请用于访问Server的Ticket。KDC使用先用自己的Master Key解密Client的TGT获得SKDC-Client，通过SKDC-Client解密Authenticator验证发送者是否是TGT的真正拥有者，验证通过再用自己的Master Key解密Server的TGT获得KDC和Server 的Session Key（SKDC-Server），并用该Session Key加密Ticket返回给Client。</li><li>C/S Exchange：Client携带者通过KDC和Server 的Session Key（SKDC-Server）进行加密的Ticket和通过Client和Server的Session Key（SServer-Client）的Authenticator访问Server，Server通过SKDC-Server解密Ticket获得SServer-Client，通过SServer-Client解密Authenticator实现对Client的验证。</li></ul><hr><h3 id="4-Kerberos的安装和Apach原生HDFS的配置"><a href="#4-Kerberos的安装和Apach原生HDFS的配置" class="headerlink" title="4.Kerberos的安装和Apach原生HDFS的配置"></a>4.Kerberos的安装和Apach原生HDFS的配置</h3><p><strong>环境：</strong></p><ul><li>Linux版本：CentOS Linux release 7.2.1511 (Core)</li><li>CDH版本：5.13.3</li><li>JDK版本：jdk1.8.0_144</li><li>运行用户：root</li></ul><p><strong>准备工作：</strong></p><p>确认添加主机名解析到/etc/hosts 文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.16.0.3  master</span><br><span class="line">172.16.0.4  datanode0</span><br><span class="line">172.16.0.5  datanode1</span><br></pre></td></tr></table></figure><p>hostname 请使用小写，要不然在集成Kerberos 时会出现一些错误。</p><p><strong>安装Kerberos</strong>:</p><p>在<code>master</code>上安装包 <code>krb5</code>、<code>krb5-server</code> 和<code>krb5-client</code>。</p><p><code>yum install krb5-server -y</code></p><p>在所有节点上安装<code>krb5-devel</code>、<code>krb5-workstation</code>：</p><p><code>yum install krb5-devel krb5-workstation -y</code></p><p>修改配置文件</p><p>Kerberos的配置文件需要修改三个</p><p><code>/etc/krb5.conf</code></p><p><code>/var/kerberos/krb5kdc/kdc.conf</code></p><p><code>/var/kerberos/krb5kdc/kadm5.acl</code></p><p>配置Kerberos的krb5.conf</p><p>官网样例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[libdefaults]</span><br><span class="line">    default_realm = ATHENA.MIT.EDU</span><br><span class="line">    dns_lookup_kdc = true</span><br><span class="line">    dns_lookup_realm = false</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">        kdc = kerberos.mit.edu</span><br><span class="line">        kdc = kerberos-1.mit.edu</span><br><span class="line">        kdc = kerberos-2.mit.edu</span><br><span class="line">        admin_server = kerberos.mit.edu</span><br><span class="line">        master_kdc = kerberos.mit.edu</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">        kdc = kerberos.example.com</span><br><span class="line">        kdc = kerberos-1.example.com</span><br><span class="line">        admin_server = kerberos.example.com</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">    mit.edu = ATHENA.MIT.EDU</span><br><span class="line"></span><br><span class="line">[capaths]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">           EXAMPLE.COM = .</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">           ATHENA.MIT.EDU = .</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>说明：样例来自MIT官网，部分配置选项含义如下：</p><p><code>[logging]</code>：表示 server 端的日志的打印位置</p><p><code>[libdefaults]</code>：每种连接的默认配置，需要注意以下几个关键的小配置</p><p><code>default_realm = EXAMPLE.COM</code>：设置 Kerberos 应用程序的默认领域。如果您有多个领域，只需向 [realms] 节添加其他的语句。</p><p><code>ticket_lifetime</code>： 表明凭证生效的时限，一般为24小时。</p><p><code>renew_lifetime</code>： 表明凭证最长可以被延期的时限，一般为一个礼拜。当凭证过期之后，对安全认证的服务的后续访问则会失败。</p><p><code>clockskew</code>：时钟偏差是不完全符合主机系统时钟的票据时戳的容差，超过此容差将不接受此票据。通常，将时钟扭斜设置为 300 秒（5 分钟）。这意味着从服务器的角度看，票证的时间戳与它的偏差可以是在前后 5 分钟内。</p><p><code>udp_preference_limit= 1</code>：禁止使用 udp 可以防止一个 Hadoop 中的错误</p><p><code>[realms]</code>：列举使用的 realm。</p><p><code>kdc</code>：代表要 kdc 的位置。格式是 机器:端口</p><p><code>admin_server</code>：代表 admin 的位置。格式是 机器:端口</p><p><code>default_domain</code>：代表默认的域名</p><p><code>[appdefaults]</code>：可以设定一些针对特定应用的配置，覆盖默认配置。</p><p>经过一段时间的对比实验，最终配置文件设置为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[logging]</span><br><span class="line">     default = FILE:/var/log/krb5libs.log</span><br><span class="line">     kdc = FILE:/var/log/krb5kdc.log</span><br><span class="line">     admin_server = FILE:/var/log/kadmind.log</span><br><span class="line"></span><br><span class="line">[libdefaults]</span><br><span class="line">     dns_lookup_realm = false</span><br><span class="line">     dns_lookup_kdc = false</span><br><span class="line">     ticket_lifetime = 24h</span><br><span class="line">     renew_lifetime = 7d</span><br><span class="line">     forwardable = true</span><br><span class="line">     renewable = true</span><br><span class="line">     udp_preference_limit = 1</span><br><span class="line">     rdns = false</span><br><span class="line">     pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt</span><br><span class="line">     default_realm = JIMI.COM</span><br><span class="line">     default_tgs_enctypes = arcfour-hmac</span><br><span class="line">     default_tkt_enctypes = arcfour-hmac</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    JIMI.COM = &#123;</span><br><span class="line">      kdc = master126</span><br><span class="line">      admin_server = master126</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">     .jimi.com = JIMI.COM</span><br><span class="line">    jimi.com = JIMI.COM</span><br></pre></td></tr></table></figure><p>接下来是第二个配置文件<code>/var/kerberos/krb5kdc/kdc.conf</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[kdcdefaults]</span><br><span class="line"> kdc_ports = 88</span><br><span class="line"> kdc_tcp_ports = 88</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> JIMI.COM = &#123;</span><br><span class="line">  #master_key_type = aes256-cts</span><br><span class="line">  max_renewable_life= 7d 0h 0m 0s</span><br><span class="line">  acl_file = /var/kerberos/krb5kdc/kadm5.acl</span><br><span class="line">  dict_file = /usr/share/dict/words</span><br><span class="line">  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab</span><br><span class="line">  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>这边直接放上配置文件，官方的配置文件相当冗余，加上版本问题，我删除了很多选项，只留下了小部分，剩下的应该会以默认值运行。</p><p>贴上一点简单的选项说明：</p><p><code>EXAMPLE.COM</code>： 是设定的 <code>realms</code>。名字随意。<code>Kerberos</code> 可以支持多个 <code>realms</code>，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个 <code>realms</code> 跟机器的 <code>host</code> 没有大关系。</p><p><code>master_key_type</code>：和 <code>supported_enctypes</code> 默认使用 <code>aes256-cts</code>。JAVA 使用 <code>aes256-cts</code> 验证方式需要安装 JCE 包，见下面的说明。为了简便，你可以不使用 <code>aes256-cts</code> 算法，这样就不需要安装 <code>JCE</code> 。</p><p><code>acl_file</code>：标注了 admin 的用户权限，需要用户自己创建。文件格式是：<code>Kerberos_principal permissions</code> [target_principal] [restrictions]</p><p><code>supported_enctypes</code>：支持的校验方式。</p><p>admin_keytab：KDC 进行校验的 keytab。</p><p>这边要注意，如果系统是Centos5.6及以上系统，默认使用AES-256来加密，但是这个AES-256 JDK的安全包里默认不存在，所以要去<a href="https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html" target="_blank" rel="noopener">Oracle</a>下载，但是这个密码增强包貌似对JDK过高的版本支持有BUG，但是暂时好像还没遇到，下载下来之后放到这个目录里：$JAVA_HOME/jre/lib/security</p><p>还有一个文件<code>/var/kerberos/krb5kdc/kadm5.acl</code>：</p><p>这个是权限控制文件，修改为</p><p><a href="mailto:`*/admin@JIMI.COM" target="_blank" rel="noopener">`*/admin@JIMI.COM</a>        *`</p><p>这三个配置文件中只有krb5.conf需要拷贝到集群中其他服务器</p><p>别的两个配置文件不需要分发到别的节点。</p><p><strong>创建数据库</strong>：</p><p>原理里面已经讲过了KDC里面有一个小型的数据库，下面是对这个数据库的操作。</p><p>在master上运行初始化数据库命令，其中 -r 指定对应的realm</p><p><code>kdb5_util create -r JIMI.COM -s</code></p><p>出现 <code>loading random data</code> 的时候另开个终端执行点消耗CPU的命令如<code>cat /dev/sda &gt; /dev/urandom</code> 可以加快随机数采集。该命令会在<code>/var/kerberos/krb5kdc/</code> 目录下创建 <code>principal</code> 数据库。</p><p>如果遇到数据库已经存在的提示，可以把 <code>/var/kerberos/krb5kdc/</code> 目录下的 principal 的相关文件都删除掉。默认的数据库名字都是 <code>principal</code>。可以使用 -d 指定数据库名字。</p><p>这个数据库相当重要，后面还会介绍。</p><p><strong>启动服务</strong>：</p><p>在master节点上运行：</p><p>centos 6：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chkconfig krb5kdc on </span><br><span class="line">chkconfig kadmin on </span><br><span class="line">service krb5kdc start </span><br><span class="line">service kadmin start</span><br></pre></td></tr></table></figure><p>centos 7：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl start krb5kdc </span><br><span class="line">systemctl start kadmin </span><br><span class="line">systemctl status krb5kdc </span><br><span class="line">systemctl status kadmin</span><br></pre></td></tr></table></figure><hr><p><strong>创建Kerberos管理员</strong></p><p>Kerberos的管理，有两个方式，分别是kadmin.local 或 kadmin，至于使用哪个，取决于账户和权限访问。</p><ul><li><p>如果有访问 kdc 服务器的 root 权限，但是没有 kerberos admin 账户，使用 kadmin.local</p></li><li><p>如果没有访问 kdc 服务器的 root 权限，但是用 kerberos admin 账户，使用 kadmin</p></li></ul><p>在master上创建远程管理的程序员:</p><p>#手动输入两次密码，这里密码为 root</p><p><code>kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#也可以不用手动输入密码</p><p><code>echo -e &quot;root\nroot&quot; | kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#或者运行下面命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local &lt;&lt;eoj</span><br><span class="line">addprinc -pw root root/admin</span><br><span class="line">eoj</span><br></pre></td></tr></table></figure><p>系统时提示输入密码，密码不能为空，而且需要妥善保管。</p><p>测试Kerberos：</p><p>查看当前认证用户</p><p>#查看 principals</p><p><code>kadmin: list_principals</code></p><p>#添加一个新的principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  addprinc user1</span><br><span class="line">WARNING: no policy specified for user1@JIMI.COM; defaulting to no policy</span><br><span class="line">Enter password for principle</span><br><span class="line">Re-enter password for principal &quot;user1@JIMI.COM&quot;:</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; created.</span><br></pre></td></tr></table></figure><p>#删除 principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  delprinc user1</span><br><span class="line">Are you sure you want to delete the principal &quot;user1@JIMI.COM&quot;? (yes/no): yes</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; deleted.</span><br><span class="line">Make sure that you have removed this principal from all ACLs before reusing.</span><br><span class="line">kadmin:exit</span><br></pre></td></tr></table></figure><p>也可以直接通过下面的命令来执行</p><p>#提示需要输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;addprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#不用输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;list_principals&quot;</span><br><span class="line">kadmin.local -q &quot;addprinc user2&quot;</span><br><span class="line">kadmin.local -q &quot;delprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#创建一个测试用户test，密码设置为test：</p><p><code>echo -e &quot;test\ntest&quot; | kadmin.local -q &quot;addprinc test&quot;</code></p><p>#获取test用户的ticket 通过用户名和密码登录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kinit test</span><br><span class="line">Password for test@JIMI.COM</span><br><span class="line">klist -e </span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: test@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:29:02  11/08/14 15:29:02  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">  renew until 11/17/14 15:29:02, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>销毁test用户的ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kdestroy</span><br><span class="line">klist</span><br><span class="line">klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_0)</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>更新ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">kinit root/admin</span><br><span class="line">Password for root/admin@JIMI.COM</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:33:57  11/08/14 15:33:57  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br><span class="line">kinit -R</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:34:05  11/08/14 15:34:05  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>抽取密钥并将其储存在本地 keytab 文件 /etc/krb5.keytab 中。这个文件由超级用户拥有，所以您必须是 root 用户才能在 kadmin shell 中执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;ktadd kadmin/admin&quot;</span><br><span class="line">klist -k /etc/krb5.keytab</span><br><span class="line">Keytab name: FILE:/etc/krb5.keytab</span><br><span class="line">KVNO Principal</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br></pre></td></tr></table></figure><p>HDFS上配置kerberos</p><p>创建认证规则</p><p>在 <code>Kerberos</code> 安全机制里，一个 <code>principal</code> 就是 <code>realm</code> 里的一个对象，一个 <code>principal</code> 总是和一个密钥（<code>secret key</code>）成对出现的。</p><p>这个 <code>principal</code> 的对应物可以是 <code>service</code>，可以是 <code>host</code>，也可以是 <code>user</code>，对于 <code>Kerberos</code> 来说，都没有区别。</p><p><code>Kdc(Key distribute center)</code> 知道所有 <code>principal</code> 的 <code>secret key</code>，但每个 <code>principal</code> 对应的对象只知道自己的那个 <code>secret key</code> 。这也是“共享密钥“的由来。</p><p>对于 <code>hadoop</code>，<code>principals</code> 的格式为</p><p><a href="mailto:`username/fully.qualified.domain.name@YOUR-REALM.COM" target="_blank" rel="noopener">`username/fully.qualified.domain.name@YOUR-REALM.COM</a>`</p><p>通过 <code>yum</code> 源安装的 <code>cdh</code> 集群中，NameNode和 DataNode 是通过 hdfs 启动的，故为集群中每个服务器节点添加两个<code>principals：hdfs</code>、HTTP。</p><p>在 KCD server 上（这里是 cdh1）创建 hdfs principal：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/master@JIMI.COM"</span><br></pre></td></tr></table></figure><p>-randkey<br>标志没有为新 <code>principal</code> 设置密码，而是指示 <code>kadmin</code> 生成一个随机密钥。之所以在这里使用这个标志，是因为此 <code>principal</code> 不需要用户交互。它是计算机的一个服务器账户。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>创建完成后，查看：</p><p><code>kadmin.local -q &quot;listprincs&quot;</code></p><p>手动创建keytab文件</p><p><code>keytab</code>是包含 <code>principals</code> 和加密 <code>principal key</code> 的文件。<code>keytab</code> 文件对于每个 <code>host</code>是唯一的，因为 <code>key</code> 中包含 <code>hostname</code>。<code>keytab</code>文件用于不需要人工交互和保存纯文本密码，实现到 <code>kerberos</code> 上验证一个主机上的 <code>principal</code>。因为服务器上可以访问 <code>keytab</code> 文件即可以以 <code>principal</code> 的身份通过 <code>kerberos</code> 的认证，所以，<code>keytab</code> 文件应该被妥善保存，应该只有少数的用户可以访问。</p><p>创建包含 hdfs principal 和 host principal 的 hdfs keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k hdfs.keytab hdfs/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>创建包含 mapred principal 和 host principal 的 mapred keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k mapred.keytab mapred/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>注意：上面的方法使用了xst的norandkey参数，有些kerberos不支持该参数。<br>当不支持该参数时有这样的提示：<code>Principal -norandkey does not exist</code>.，需要使用下面的方法来生成keytab文件:</p><p>在master节点，即KDC server节点上执行下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/master@JIMI.COM"</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>这样，就会在 /var/kerberos/krb5kdc/ 目录下生成hdfs-unmerged.keytab 和 HTTP.keytab 两个文件，接下来使用 ktutil 合并者两个文件为 hdfs.keytab。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">ktutil</span><br><span class="line">ktutil: rkt hdfs-unmerged.keytab</span><br><span class="line">ktutil: rkt HTTP.keytab</span><br><span class="line">ktutil: wkt hdfs.keytab</span><br><span class="line">ktutil: exit</span><br></pre></td></tr></table></figure><p>使用 klist 即可查看 hdfs.keytab 文件列表：（省略）</p><p>验证是否正确合并了key，使用合并后的keytab，分别使用hdfs和host principals来获取证书。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kinit -k -t hdfs.keytab hdfs/master@JIMI.COM</span><br><span class="line">kinit -k -t hdfs.keytab HTTP/master@JIMI.COM</span><br></pre></td></tr></table></figure><p>如果出现错误：<code>kinit: Key table entry not found while getting initial credentials</code>，则上面的合并有问题，重新执行前面的操作。</p><p>部署kerberos keytab文件</p><p>拷贝 hdfs.keytab 文件到其他节点的 /etc/hadoop/conf 目录</p><p>并设置权限，分别在三个节点上运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab</span><br><span class="line">chmod 400 /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>原因：</p><p>由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改<code>kdc</code>中的<code>principal</code>的密码，则该<code>keytab</code>就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>修改hdfs配置文件，先停止集群</p><p>在集群总所有节点的<code>core-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authentication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;kerberos&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authorization&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>在集群总所有节点的<code>hdfs-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;  </span><br><span class="line">  &lt;value&gt;700&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1004&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.http.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1006&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果想开启 SSL，请添加（本文不对这部分做说明）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.http.policy&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTPS_ONLY&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果 HDFS 配置了 QJM HA，则需要添加（另外，你还要在 zookeeper 上配置 kerberos）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.internal.spnego.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果配置了WebHDFS，则添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置中需要注意的点：</p><p>1.<code>dfs.datanode.address</code>表示<code>data transceiver RPC server</code>所绑定的<code>hostname</code>或IP地址，如果开启 security，端口号必须小于 1024(privileged port)，否则的话启动 datanode 时候会报 <code>Cannot start secure cluster without privileged resources</code> 错误。</p><p>2.<code>principal</code> 中的 <code>instance</code> 部分可以使用 _HOST 标记，系统会自动替换它为全称域名。</p><p>3.如果开启了 <code>security, hadoop</code> 会对 <code>hdfs block data</code>(由 dfs.data.dir 指定)做 <code>permission check</code>，方式用户的代码不是调用<code>hdfs api</code>而是直接本地读<code>block data</code>，这样就绕过了<code>kerberos</code>和文件权限验证，管理员可以通过设置 <code>dfs.datanode.data.dir.perm</code> 来修改 <code>datanode</code> 文件权限，这里我们设置为700</p><p>CDH的权限管理（<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_sg_s1_install_cm_cdh.html" target="_blank" rel="noopener">来自cloudera官网</a>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs：NameNode, DataNodes, and Secondary NameNode</span><br><span class="line">mapred：JobTracker and TaskTrackers (MR1) and Job History Server (YARN)</span><br><span class="line">yarn：ResourceManager and NodeManagers (YARN)</span><br><span class="line">oozie：Oozie Server</span><br><span class="line">hue：Hue Server, Beeswax Server, Authorization Manager, and Job Designer</span><br></pre></td></tr></table></figure><p>目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dfs.name.dir/ hdfs:hadoop</span><br><span class="line">dfs.data.dir/ hdfs:hadoop</span><br><span class="line">mapred.local.dir/ mapred:hadoop</span><br><span class="line">mapred.system.dir in HDFS/ mapred:hadoop</span><br><span class="line">yarn.nodemanager.local-dirs/ yarn:yarn</span><br><span class="line">yarn.nodemanager.log-dirs/ yarn:yarn</span><br><span class="line">oozie.service.StoreService.jdbc.url (if using Derby)/ oozie:oozie</span><br><span class="line">[[database]] name/ hue:hue</span><br><span class="line">javax.jdo.option.ConnectionURL/ hue:hue</span><br></pre></td></tr></table></figure><p>启动NameNode</p><p>启动之前必须确保JCE jar已经替换，首先检查JSVC</p><p>首先master节点查看是否安装了JSVC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls /usr/lib/bigtop-utils/</span><br><span class="line">bigtop-detect-classpath  bigtop-detect-javahome  bigtop-detect-javalibs  jsvc</span><br></pre></td></tr></table></figure><p>然后编辑<code>/etc/default/hadoop-hdfs-datanode</code>，取消对下面注释并添加一行JSVC_HOME，修改如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">export HADOOP_SECURE_DN_PID_DIR=/var/run/hadoop-hdfs</span><br><span class="line">export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs</span><br><span class="line">export JSVC_HOME=/usr/lib/bigtop-utils</span><br></pre></td></tr></table></figure><p>hadoop-hdfs-datanode同步到其他节点</p><p>随后分别在CDH2、CDH3获取ticket然后启动服务</p><p>#root为root/admin密码</p><p><code>kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/master@JIMI.COM; service hadoop-hdfs-datanode start</code></p><p>（这仅仅为master节点上的操作，别的节点类似）</p><p>观察master上的Namenode日志，出现下面的日志表名Datanode启动成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">14/11/04 17:21:41 INFO security.UserGroupInformation:</span><br><span class="line">Login successful for user hdfs/cdh2@JAVACHEN.COM using keytab file /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>Tips:</p><ul><li><p>配置 hosts，hostname 请使用小写</p></li><li><p>确保 kerberos 客户端和服务端连通</p></li><li><p>替换 JRE 自带的 JCE jar 包</p></li><li><p>为 DataNode 设置运行用户并配置 JSVC_HOME</p></li><li><p>启动服务前，先获取 ticket 再运行相关命令</p></li></ul><p>Quote:</p><p><a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_kerberos_prin_keytab_deploy.html" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍1</a></p><p><a href="https://blog.cloudera.com/blog/2015/03/how-to-quickly-configure-kerberos-for-your-apache-hadoop-cluster/" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍2</a></p><p><a href="http://web.mit.edu/~kerberos/krb5-devel/doc/admin/conf_files/krb5_conf.html" target="_blank" rel="noopener">MIT官网的文档</a></p><p><a href="https://docs.oracle.com/cd/E24847_01/html/819-7061/setup-9.html" target="_blank" rel="noopener">Oracle官网对Kerberos的介绍</a></p><hr><h3 id="5-Apach-原生Zookeeper的Kerberos配置及其验证"><a href="#5-Apach-原生Zookeeper的Kerberos配置及其验证" class="headerlink" title="5.Apach 原生Zookeeper的Kerberos配置及其验证"></a>5.Apach 原生Zookeeper的Kerberos配置及其验证</h3><p>Zookeeper的配置分为两个步骤，先配置<code>Server</code>的<code>keytab</code>，再配置<code>Client</code>的<code>keytab</code>,如果<code>zookeeper-client</code> 和 <code>zookeeper-server</code> 安装在同一个节点上，则 <code>java.env</code> 中的 <code>java.security.auth.login.config</code> 参数会被覆盖，这一点从<code>zookeeper-client</code> 命令启动日志可以看出来。</p><p>首先是配置 <code>Zk Server</code></p><p>因为KDC已经配置了，所以KDC不用再配</p><p>第一步直接生成Keytab</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>将Keytab拷贝到目录<code>/etc/zookeeper/conf</code></p><p>在三个节点上分别执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>目的是为了控制权限。</p><p>然后修改配置文件</p><p>在三个节点上的zoo.cfg文件中添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>然后创建JAAS配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Server &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab="/etc/zookeeper/conf/zookeeper.keytab"</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal="zookeeper/master@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>三个节点都要有，每个节点里面的principal有点不同</p><p>然后是配置<strong>Zookeeper Client</strong></p><p>还是先生成<code>keytab</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>拷贝 zkcli.keytab 文件到其他节点的 <code>/etc/zookeeper/conf</code> 目录，并设置权限，分别在master、datanode0、datanode1 上执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;</span><br><span class="line">chown zookeeper:</span><br><span class="line">hadoop zkcli.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>由于 <code>keytab</code> 相当于有了永久凭证，不需要提供密码(如果修改 <code>kdc</code> 中的 <code>principal</code> 的密码，则该 <code>keytab</code> 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>创建 JAAS 配置文件</p><p>在<code>/etc/zookeeper/conf/</code>创建<code>client-jaas.conf</code>文件，内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Client &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab=&quot;/etc/zookeeper/conf/zkcli.keytab&quot;</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal=&quot;zkcli@JIMI.COM&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>同步到其他节点，然后在<code>/etc/zookeeper/conf/</code>目录创建或者修改<code>java.env</code>，内容如下</p><p><code>export CLIENT_JVMFLAGS=&quot;-Djava.security.auth.login.config=/etc/zookeeper/conf/client-jaas.conf&quot;</code></p><p>并且同步到别的节点上</p><p>接着是验证：</p><p>启动客户端：<code>zookeeper-client -server master:2181</code></p><p>创建一个<code>znode</code>节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 0] create /znode1 sasl:zkcli@JIMI.COM:cdwra</span><br><span class="line">Created /znode1</span><br></pre></td></tr></table></figure><p>验证该节点是否创建以及其ACL：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 1] getAcl /znode1</span><br><span class="line">    'world,'anyone</span><br><span class="line">    : cdrwa</span><br></pre></td></tr></table></figure><p>只要能够在一个节点上创建<code>znode</code>，在别的节点上能够显示出来，就说明<code>Zookeeper</code>的<code>kerberos</code>已经配置成功。</p><hr><h3 id="6-Apach原生Kafka的Kerberos配置及其验证"><a href="#6-Apach原生Kafka的Kerberos配置及其验证" class="headerlink" title="6.Apach原生Kafka的Kerberos配置及其验证"></a>6.Apach原生Kafka的Kerberos配置及其验证</h3><p>因为之前的操作已经搭建完了<code>KDC</code>，所以省略了自建<code>Kerberos</code>的步骤</p><p>首先还是为<code>broker</code>每台服务器在<code>Kerberos</code>服务器生成相应的<code>principal</code>和<code>Keytab</code>，将下列命令里生成的<code>kafka.keytab</code>文件分发到对应<code>broker</code>机器的统一位置，比如<code>/etc/kafka.keytab</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">addprinc -randkey kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost3@EXAMPLE.COM</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">xst -norandkey -k /opt/kafkahost1/kafka.keytab kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost2/kafka.keytab kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost3/kafka.keytab kafka/kafkahost3@EXAMPLE.COM</span><br></pre></td></tr></table></figure><p>配置kafka server文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">listeners=SASL_PLAINTEXT://:9092</span><br><span class="line">security.inter.broker.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism.inter.broker.protocol=GSSAPI</span><br><span class="line">sasl.enabled.mechanisms=GSSAPI</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">super.users=User:kafka</span><br><span class="line">authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</span><br></pre></td></tr></table></figure><p>KafkaClient模块是为了bin目录下kafka-console-consumer.sh之类的脚本使用的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">            com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">            useKeyTab=true</span><br><span class="line">            storeKey=true</span><br><span class="line">            keyTab="/etc/kafka.keytab"</span><br><span class="line">            principal="kafka/kafkahost1@EXAMPLE.COM";</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">KafkaClient &#123;</span><br><span class="line">        com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">        useKeyTab=true</span><br><span class="line">        storeKey=true</span><br><span class="line">        keyTab="/etc/kafka.keytab"</span><br><span class="line">        principal="kafka/kafkahost1@EXAMPLE.COM"</span><br><span class="line">        useTicketCache=true;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>修改bin目录下kafka-run-class.sh，在  exec $JAVA 后面增加kerberos启动参数,然后就可以用正常的脚本启动服务了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf</span><br></pre></td></tr></table></figure><p>或者用这个脚本启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">export KAFKA_HEAP_OPTS='-Xmx256M'</span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/zookeeper_jaas.conf'</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br><span class="line"></span><br><span class="line">sleep 5</span><br><span class="line"></span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/kafka_server_jaas.conf'</span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><p>最终的目的都是一样</p><p>客户端脚本使用</p><p>启用kerberos后，部分kafka管理脚本需要增加额外的参数才能使用</p><p>首先建立配置文件<code>client.properties</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">sasl.mechanism=GSSAPI</span><br></pre></td></tr></table></figure><p>涉及到的zookeeper.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line">requireClientAuthScheme=sasl</span><br><span class="line">jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>所以新命令的使用方式为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server kafkahost1:9092 --list --command-config client.properties</span><br><span class="line">bin/kafka-console-producer.sh --broker-list kafkahost1:9092 --topic test --producer.config client.properties</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kafkahost1:9092 --topic test --consumer.config client.properties</span><br></pre></td></tr></table></figure><p>如果之前JCE的包没有安装好的话会报如下错误，需要把JCE包安装到位即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WARN [Controller-60-to-broker-60-send-thread], Controller 60's connection to broker kafka60:9092 (id: 60 rack: null) was unsuccessful (kafka.controller.RequestSendThread)</span><br><span class="line">java.io.IOException: Connection to kafka60:9092 (id: 60 rack: null) failed</span><br></pre></td></tr></table></figure><p>能在命令行上运行成功命令行消费者和命令行生产者就说明ZK和Kafka的安装基本搞定，没有问题。</p><p>用Java连接集群上的Kerberos组件篇幅较长，涉及代码，另外开一篇文章说明。</p><hr><h3 id="7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装"><a href="#7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装" class="headerlink" title="7.Cloudera’s Distribution Including Apache Hadoop(CDH)上Kerberos的安装"></a>7.Cloudera’s Distribution Including Apache Hadoop(<em>CDH</em>)上Kerberos的安装</h3><p>CDH上面的Kerberos安装其实已经被简化了，简化的好处是安装方便，坏处是一旦出现问题不知从何处下手。所以对前面单独组件的了解是有必要的。实际操作下来，官网和各技术博客都有些许问题。在此记录。</p><p>首先配置KDC，和单独安装操作相同</p><p>首先KDC还是要配置，在CM服务器上配置KDC</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-serverkrb5-libs krb5-auth-dialog krb5-workstation</span><br></pre></td></tr></table></figure><p>修改<code>/etc/krb5.conf</code>、<code>/var/kerberos/krb5kdc/kadm5.acl</code>、<code>/var/kerberos/krb5kdc/kdc.conf</code>配置，配置内容相同，不再赘述。</p><p>然后在所有节点上（包括CM）安装Kerberos客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-libs krb5-workstation</span><br></pre></td></tr></table></figure><p>然后在CM节点上独立安装额外的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openldap-clients</span><br></pre></td></tr></table></figure><p>分发krb5.conf到另外两个节点</p><p>JCE包还是要记得替换，替换位置：<code>/usr/share/java/jdk1.8.0_144/jre/lib/security</code></p><p>KDC安装完成后，建立测试Kerberos的管理员账号。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5k45t4jj20et03u3yg.jpg" alt="alt admin"></p><hr><p>CM界面 -&gt;管理 -&gt; 安全</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5rgh6faj20o702ut8o.jpg" alt="CDH"></p><p>点击启用</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ar5zj20pe0o43zp.jpg" alt="CDH1"></p><p>全部勾选即可</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a7vuj20n00pmt9i.jpg" alt></p><p>这边别的都好理解，都是和host上面对应的，然后这个加密类型是和krb5.conf对应相同的</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v08hypj20k005dgll.jpg" alt></p><p>这上面有张图没截到，是选择是否要通过CM管理的，一般选择不通过</p><p>这边管理账户输入之前创建的管理账户即可。</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a1fxj20hz05d0ss.jpg" alt></p><p>这边CDH会帮助验证密码</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ayqyj20mx0oamxv.jpg" alt></p><p>接下来的这个步骤我标红了一块，我在这边spark2的部分，服务范围设置的是spark2，默认是spark，我修改了一下，避免以后的keytab名字出现歧义</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0e6e7j20oc0q1wfz.jpg" alt></p><p>随后，直接勾选重启选项即可</p><p>初步的配置就完成了</p><hr><p><a href="https://docs.huihoo.com/solaris/10/simplified-chinese/html/819-7061/aadmin-3.html#setup-304" target="_blank" rel="noopener">Oracle系统管理指南：安全性服务文档</a></p><p>（包括了备份和传播Kerberos、如何恢复Kerberos、如何在服务器升级后转换Kerberos数据库，如何重新配置主KDC服务器以使用增量传播，如何重新配置从KDC以使用增量传播，如何配置从 KDC 服务器以使用完全传播，如何验证KDC服务器已经同步，如何手动将Kerberos数据库传播到从KDC服务器，设计并行传播，设置并行传播的配置步骤，管理存储文件，如何删除存储文件等。）</p><hr><h3 id="8-对Kerberos的一点使用心得"><a href="#8-对Kerberos的一点使用心得" class="headerlink" title="8.对Kerberos的一点使用心得"></a>8.对Kerberos的一点使用心得</h3><p>CDH的Kerberos其实算是相对好管理的，最起码组件的principal都是CDH自动生成的。</p><p>在KDC上创建完成管理员开启功能之后，我常用的操作有这些：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 登录，通过创建的BDB管理账号管理</span><br><span class="line">kadmin.local</span><br><span class="line"><span class="meta">#</span> 查看现有的账号列表</span><br><span class="line">kadmin.local: listprincs</span><br><span class="line"><span class="meta">#</span> 这边CDH的Principal都是自动生成的，可以直接使用，在服务器上的文件夹里找到对应Keytab就可以登录</span><br><span class="line"><span class="meta">#</span> 找到Principal之后，可以查看Keytab的加密方式 过期时间等等</span><br><span class="line">klist -kt /root/hdfs.keytab</span><br><span class="line">Keytab name: FILE:/root/hdfs.keytab</span><br><span class="line">KVNO Timestamp           Principal</span><br><span class="line">---- ------------------- ------------------------------------------------------</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Keytab的作用就是获取KDC的ticket</span><br><span class="line">kinit -kt keytab/hdfs.keytab hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 还可以使用klist -e查看现在服务器里缓存的是哪一个ticket</span><br><span class="line">klist -e</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: hdfs/bigdata25@ZQYKJ.COM</span><br><span class="line"></span><br><span class="line">Valid starting       Expires              Service principal</span><br><span class="line">07/06/2018 11:24:46  07/07/2018 11:24:46  krbtgt/ZQYKJ.COM@ZQYKJ.COM</span><br><span class="line">renew until 07/11/2018 11:24:46, Etype (skey, tkt): aes128-cts-hmac-sha1-96, aes128-cts-hmac-sha1-96 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 退出</span><br><span class="line">kdestroy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 这边还拓展一个很实用的功能 keytab维护工具 ktutil</span><br><span class="line">ktutil</span><br><span class="line"><span class="meta">#</span> 打开之后即进入工具，工具里面我常用的功能包括：rkt（read keytab）</span><br><span class="line"><span class="meta">#</span> 可以将keytab读进来之后然后，生成新的keytab（合并）</span><br><span class="line"><span class="meta">#</span> 然后上面提到的read完成之后，也可以列出 分析 加密方式等等</span><br></pre></td></tr></table></figure><p><a href="https://www.freebsd.org/cgi/man.cgi?query=ktutil" target="_blank" rel="noopener">更多ktutilAPI</a></p><p><a href="https://www.ibm.com/support/knowledgecenter/zh/ssw_aix_71/com.ibm.aix.cmds3/klist.htm" target="_blank" rel="noopener">IBM对BDB数据库命令的介绍</a></p><hr><h3 id="9-如何验证Kerberos已经安装完成"><a href="#9-如何验证Kerberos已经安装完成" class="headerlink" title="9.如何验证Kerberos已经安装完成"></a>9.如何验证Kerberos已经安装完成</h3><p><strong>HDFS</strong><br>登录到某一个节点后，切换到hdfs用户，然后用kinit来获取credentials</p><p>现在用<code>hadoop hdfs -ls /</code>应该能正常输出结果<br>用kdestroy销毁credentials后，再使用<code>hadoop hdfs -ls /</code>会发现报错</p><p><strong>Kafka</strong></p><p>用新的一套API，消费者能正常消费，生产者能正常生产不报错误就算ok</p><p>注意：是新API，开启Kerberos之后老API无法再使用</p><p><strong>Zookeeper</strong></p><p>启动zookeeper：</p><p>Zookeeper-client -server master:2181</p><p>创建一个 znode 节点：</p><p>create /znode1 sasl:<a href="mailto:master@JIMI.com" target="_blank" rel="noopener">master@JIMI.com</a>:cdwra</p><p>在另外的节点上执行 </p><p>getAcl /znode1</p><p>如果能够获取在另外一个节点上输入的输入就证明没有问题</p><hr><h3 id="10-Kerberos遇到的坑"><a href="#10-Kerberos遇到的坑" class="headerlink" title="10.Kerberos遇到的坑"></a>10.Kerberos遇到的坑</h3><h4 id="Kerberos卸载BUG"><a href="#Kerberos卸载BUG" class="headerlink" title="Kerberos卸载BUG"></a>Kerberos卸载BUG</h4><p>Linux上的KDC存在严重的卸载BUG，使用<code>yum remove</code>卸载会出现严重的问题</p><p>因为之前<code>principal</code>的认证因为人为操作出现了一些问题，所以用yum remove卸载重新安装了一下，yum remove之后出现了大问题，凡是新连接外部的命令都无法使用，包括并不限于：yum、ssh、wgt等命令，FTP工具也无法使用，这就导致了无法连接外部下载kdc安装包</p><p>还好我卸载之前连接的SSH窗口没有关闭（新的连接无法建立， 但是已经建立的连接不会断开），用的XSHELL 6，我尝试用XFTP连接，失败，但是XSHELL 6 默认输入框里就有传输文件的功能，上传四个Kerberos文件之后重新安装之后才解决了问题。</p><h4 id="Zookeeper报错"><a href="#Zookeeper报错" class="headerlink" title="Zookeeper报错"></a>Zookeeper报错</h4><p>Zk这个组件启动的时候在互相连接的装一下会报Error，这个Error曾今困扰了我挺久</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-02-26 18:37:15,898 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception</span><br><span class="line">EndOfStreamException: Unable to read additional data from client sessionid 0x269290a81950073, likely client has closed socket</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><p>其实只要Zk验证了没有问题就行了，这个报错不用纠结</p><h4 id="Kafka启动-关闭Kerberos的坑"><a href="#Kafka启动-关闭Kerberos的坑" class="headerlink" title="Kafka启动/关闭Kerberos的坑"></a>Kafka启动/关闭Kerberos的坑</h4><p>CDH集群中为Kafka启用Kerberos需要些配置之外的操作，启用Kerberos的时候容易忽略这些细节，导致kafka开启不正确， 然后关闭的时候容易把这些操作忽略了，导致关闭不彻底，在有的环节仍然关闭了Kerberos。</p><p>Kafka在CDH中的配置需要先登录CM进入kafka服务，修改<code>ssl.client.auth</code>为none，这届两个Kerberos相关的配置设为开启，接下来还要修改security.inter.broker.protocol配置为SASL_PLAINTEXT，保存以上修改的配置后，回到主页根据提示重启kafka Server,接下来就是在客户端上的配置，本身CDH就会为了Kafka生成配置文件jaas.conf，对于这个配置文件真实一言难尽，里面的配置文件会有 不起眼的错误（是关于KafkaClient和KafkaServer混淆的错误），这一个改正完毕，还有一个配置文件client.properties文件，两个文件设置完毕后，在<code>/etc/profile</code>里面设置环境变量<code>exportKAFKA_OPTS=&quot;-Djava.security.krb5.conf=/etc/krb5.conf-Djava.security.auth.login.config=/opt/kafka/kafka_client.jaas&quot;</code>，配置完毕kafka这块，关闭的时候容易忘记，必须要记得从profile中删除才行。</p><h4 id="Flume配置文件导致文件无限传输"><a href="#Flume配置文件导致文件无限传输" class="headerlink" title="Flume配置文件导致文件无限传输"></a>Flume配置文件导致文件无限传输</h4><p>Flume的配置文件出了错误，因为对Flume的KafkaChannel的不熟悉</p><p>配置的时候把Channel的sink又连接到Source上导致了数据一致循环。。。</p><p>排查之后发现了这个问题，附上Flume的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">agent.sources = kafkaSource1</span><br><span class="line">agent.channels = kafkaChannel</span><br><span class="line">agent.sinks = hdfsSink</span><br><span class="line">agent.sources.kafkaSource1.channels = kafkaChannel</span><br><span class="line">agent.sinks.hdfsSink.channel = kafkaChannel</span><br><span class="line"></span><br><span class="line">agent.sources.kafkaSource1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent.sources.kafkaSource1.zookeeperConnect = master126:2181</span><br><span class="line">agent.sources.kafkaSource1.topic = report.alarm,report.distance,report.track,report.acc,report.stop</span><br><span class="line">agent.sources.kafkaSource1.consumer.group.id = cloudera_mirrormaker</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.timeout.ms = 100</span><br><span class="line">agent.sources.kafkaSource1.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.sources.kafkaSource1.batchSize = 100</span><br><span class="line">agent.sources.kafkaSource1.batchDurationMillis = 1000</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">agent.channels.kafkaChannel.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.channels.kafkaChannel.kafka.topic = source_from_kafka</span><br><span class="line">agent.channels.kafkaChannel.consumer.group.id = flume-consumer</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.sinks.hdfsSink.type = hdfs</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosKeytab= /hdfs-keytab/hdfs.keytab</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosPrincipal= hdfs@JIMI.COM</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path = hdfs://master126:8020/test/data/flume/kafka/%Y%m%d</span><br><span class="line"><span class="meta">#</span>上传文件的前缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix = %d_%&#123;topic&#125;</span><br><span class="line"><span class="meta">#</span>是否按照时间滚动文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.round = true</span><br><span class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundValue = 24</span><br><span class="line"><span class="meta">#</span>重新定义时间单位</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span>是否使用本地时间戳</span><br><span class="line">agent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span>积攒多少个Event才flush到HDFS一次</span><br><span class="line">agent.sinks.hdfsSink.hdfs.batchSize = 200</span><br><span class="line"><span class="meta">#</span>设置文件类型，可支持压缩</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span>多久生成一个新的文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval = 7200</span><br><span class="line"><span class="meta">#</span>设置每个文件的滚动大小</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize = 1073741824</span><br><span class="line"><span class="meta">#</span>文件的滚动与Event数量无关</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount = 0</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat = TEXT</span><br></pre></td></tr></table></figure><p>例子中的配置文件的最终效果是从kafka的多个topic report.stop等等读取数据之后通过kafka channel，然后根据不同的topic生成不同的文件。</p><h4 id="HUE中Oozie报错，时区错误"><a href="#HUE中Oozie报错，时区错误" class="headerlink" title="HUE中Oozie报错，时区错误"></a>HUE中Oozie报错，时区错误</h4><p>发现HUE的时间和实际时间有偏差，原因是HUE的时区默认是美国，要在配置里面修改，修改成东8区即可</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w01fhhuuj20wo05bjrg.jpg" alt></p><h4 id="系统自带的无法识别的配置文件"><a href="#系统自带的无法识别的配置文件" class="headerlink" title="系统自带的无法识别的配置文件"></a>系统自带的无法识别的配置文件</h4><p>值得一提的是这边有个错误我花了好久才发现，CDH因为是高度集成的，里面很多配置文件都是自己生成的，像Kafka的Keytab配置文件，文件里面的内容并不正确，里面指定的KafkaClient和Server根本无法识别，更改之后才可以识别。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   doNotPrompt=true</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr><h3 id="11-Windows下访问Kerberos-CDH集群的WebUI界面"><a href="#11-Windows下访问Kerberos-CDH集群的WebUI界面" class="headerlink" title="11.Windows下访问Kerberos CDH集群的WebUI界面"></a>11.Windows下访问Kerberos CDH集群的WebUI界面</h3><p><a href="http://web.mit.edu/kerberos/dist/" target="_blank" rel="noopener">MIT Kerberos下载地址</a></p><p>先安装windows下的Kerberos安装包，无脑安装就行了。</p><p>接着配置krb5..ini文件，将krb5.conf的内容拷贝进来，切忌不要直接更改后缀名就使用</p><p>接着启动MIT Kerberos软件</p><p>使用我们在linux KDC上注册的管理员账号登录即可。我们登录不同的服务使用到的不用的账号，这个软件貌似会通过我们这个管理员账号自己搞定。</p><p>还有一种方法，需要使用Keytab，还涉及到文件权限的问题，因为上面的方法我很轻易就成功访问了WebUI，所以第二种方法就没有尝试。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247483853&amp;idx=1&amp;sn=442a8ba87c922857253a437affe42506&amp;chksm=ec2ad1c4db5d58d2933ae5cde4ab1a7443c944e94aca85b51cbd8e9f4f3772162a39074da49d&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">更多内容访问阿里巨佬Fayson的公众号</a></p><hr><h3 id="12-禁用Kerberos需要调整的设置"><a href="#12-禁用Kerberos需要调整的设置" class="headerlink" title="12.禁用Kerberos需要调整的设置"></a>12.禁用Kerberos需要调整的设置</h3><p>说明：设置可能会随着版本变化有所变化</p><p><strong>Zookeeper</strong></p><ul><li><code>enableSecurity (Enable Kerberos Authentication)</code> : false</li><li><code>zoo.cfg</code> 的Server 高级配置代码段（安全阀）写入skipACL: yes</li></ul><p><strong>HDFS</strong></p><ul><li><code>hadoop.security.authentication</code> : Simple</li><li><code>hadoop.security.authorization</code> : false</li><li><code>dfs.datanode.address</code> : 1004 (for Kerberos) 改为 50010 (default)</li><li><code>dfs.datanode.http.address</code> : 1006 (for Kerberos) 改为 50075 (default)</li><li><code>dfs.datanode.data.dir.perm</code> : 700 改为 755</li></ul><p><strong>HBase</strong></p><ul><li><code>hbase.security.authentication</code> : Simple</li><li><code>hbase.security.authorization</code> : false</li><li><code>hbase.thrift.security.qop</code> : none</li></ul><p><strong>Hue</strong></p><ul><li><code>Kerberos Ticket Renewer</code>: 删除或停用角色</li></ul><p><strong>Kafka</strong></p><ul><li><code>kerberos.auth.enable</code>: false</li></ul><p><strong>SOLR</strong></p><ul><li><code>solr Secure Authentication</code> : Simple</li></ul><hr><h3 id="13-集群同步脚本"><a href="#13-集群同步脚本" class="headerlink" title="13.集群同步脚本"></a>13.集群同步脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">    #1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">    pcount=$#</span><br><span class="line">    if((pcount==0)); then</span><br><span class="line">    echo no args;</span><br><span class="line">    exit;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    #2 获取文件名称</span><br><span class="line">    p1=$1</span><br><span class="line">    fname=`basename $p1`</span><br><span class="line">    echo fname=$fname</span><br><span class="line"></span><br><span class="line">    #3 获取上级目录到绝对路径</span><br><span class="line">    pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">    echo pdir=$pdir</span><br><span class="line"></span><br><span class="line">    #4 获取当前用户名称</span><br><span class="line">    user=`whoami`</span><br><span class="line"></span><br><span class="line">    #5 循环</span><br><span class="line">    for((host=0; host&lt;2; host++)); do</span><br><span class="line">    echo ------------------- datanode$host --------------</span><br><span class="line">    rsync -rvl $pdir/$fname $user@datanode$host:$pdir</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><p>要先安装rsync</p><p>yum install rsync</p><p>安装成功之后才能用这个同步命令</p><hr><h3 id="14-Kerberos优化"><a href="#14-Kerberos优化" class="headerlink" title="14.Kerberos优化"></a>14.Kerberos优化</h3><h4 id="美团优化实战"><a href="#美团优化实战" class="headerlink" title="美团优化实战"></a>美团优化实战</h4><p><strong>为什么要优化：</strong></p><p>线上单台KDC服务器最大承受QPS是多少？哪台KDC的服务即将出现压力过大的问题？为什么机器的资源非常空闲，KDC的压力却会过大？如何优化？优化后瓶颈在哪儿？如何保证监控指标的全面性、可靠性和准确性？这都是本文需要回答的问题。从本次优化工作达成的最终结果上来看，单台服务器每秒的处理性能提升16倍左右，另外通过共享内存的方式设计了一个获取KDC各项核心指标的接口，使得服务的可用性进一步提升。</p><p>名词：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v66w9mt3j212m0hw75j.jpg" alt></p><p>下图是美团的架构，整个KDC服务都部署在同一个IDC</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v697svjwj21a40zc771.jpg" alt></p><p><strong>主要优化工作</strong></p><p>通过对KDC原理的分析，很容易判断只有前两部分才可能直接给KDC服务带来压力，因此本文涉及到的工作都将围绕上一部分的前两个环节展开分析。本次优化工作采用Grinder这一开源压测工具，分别对AS、TGS两个请求过程，采用相同机型（保证硬件的一致性）在不同场景下进行了压力测试。</p><p>优化之前，线上KDC服务启动的单进程；为最低风险的完成美团和点评数据的融合，KDC中keytab都开启了PREAUTH属性；承载KDC服务的部分服务器没有做RAID。KDC服务出现故障时，机器整体资源空闲，怀疑是单进程的处理能力达到上限；PREAUTH属性进一步保证提升了KDC服务的安全性，但可能带来一定的性能开销；如果线上服务器只加载了少量的keytab信息，那么没有被加载到内存的数据必然读取磁盘，从而带来一定的IO损耗。</p><p>因此本文中，对以下三个条件进行变动，分别进行了测试：</p><ol><li>对承载KDC服务的物理机型是否做RAID10；</li><li>请求的keytab在库中是否带有PRAUTH属性；</li><li>KDC是否启动多进程（多进程设置数目和物理机核数一致）。（实际测试工作中进行了多次测试）</li></ol><p><strong>Client和AS交互过程的压测</strong></p><p>下图为AS压测的一组平均水平的测试数据，使用的物理机有40核，因此多进程测试启动40个进程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6aqpeh0j212i0gumxq.jpg" alt></p><p>分析表中的数据，很容易提出如下问题从而需要进一步探索：</p><ol><li>比较表中第一行和第二行、第三行和第四行，主机做不做RAID为什么对结果几乎无影响？</li></ol><p>该四组（测试结果为49、53、100和104所在表2中的行）数据均在达到处理能力上限一段时间后产生认证失败，分析机器的性能数据，内存、网卡、磁盘资源均没有成为系统的瓶颈，CPU资源除了某个CPU偶尔被打满，其他均很空闲。分析客户端和服务端的认证日志，服务端未见明显异常，但是客户端发现大量的Socket Timeout错误（测试设置的Socket超时时间为30s）。由于测试过程中，客户端输出的压力始终大于KDC的最大处理能力，导致KDC端的AS始终处于满负荷状态，暂时处理不了的请求必然导致排队；当排队的请求等待时间超过设置的30s后便会开始超时从而认证出错，且伴随机器某一CPU被打满（如图3）。 显然KDC单进程服务的处理能力已经达到瓶颈且瓶颈存在单核CPU的处理能力，从而决定向多进程方向进行优化测试。</p><p>单进程KDC打满某一CPU：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6d08i8dj20xk0cewga.jpg" alt></p><p>下图为本次压力测试的一个通用模型，假设KDC单位时间内的最大处理能力是A，来自客户端的请求速率稳定为B且 B&gt;A ；图中黄色区域为排队的请求数，当某一请求排队超过30s，便会导致Socket Timedout错误。</p><p>AS处理能力和Client压力模型：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6dxwvacj21380pqwg3.jpg" alt></p><ol start="2"><li>比较上上一张表中第1和3行、第2和4行、第7和8行相比，为什么有PREAUTH属性的认证QPS大致是无该属性处理能力的一半？</li></ol><p>如果Client的keytab在KDC的库中不带有PREAUTH这一属性，Client发送请求，KDC的AS模块验证其合法性之后返回正确的结果；整个过程只需要两次建立链接进行交互便可完成。如果带有PREAUTH属性，意味着该keytab的认证启动了Kerberos 5协议中的 pre-authentication概念：当AS模块收到Client的请求信息后；故意给Client返回一个错误的请求包，Client会“领悟到”这是KDC的AS端需要进行提前认证；从而Client获取自己服务器的时间戳并用自己的密钥加密发送KDC，KDC解密后和自身所在服务器的时间进行对比，如果误差在能容忍的范围内；返回给Client正确的TGT响应包；过程如下图所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6fxvq1xj220o0d0juw.jpg" alt></p><ol start="3"><li>根据对问题2的分析，CPU占用表中第5和7行的值的比例应该近似为1:2，为什么第5行的值只有115，结果和理论差距如此之大？</li></ol><p>KDC的库中对客户端的keytab开启PREAUTH属性，客户端每认证一次，KDC需要将该次认证的时间戳等信息写到本次磁盘的BDB数据库的Log中；而关闭PREAUTH属性后，每次认证只需要从库中读取数据，只要给BDB数据库分配的内存足够大，就可以最大程度的减少和本次磁盘的交互。KDC40进程且开启PRAUTH，其AS处理能力的QPS只有115，分析机器性能的相关指标，发现瓶颈果然是单盘的IO，如图6所示。使用BDB提供的工具，查看美团数据平台KDC服务的BDB缓存命中率为99%，如下图所示：</p><p>无RAID多KDC进程服务器磁盘IO：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6i16k8hj21140ccgmj.jpg" alt></p><p>美团KDC缓存命中率：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jctl01j20pa0bw76t.jpg" alt></p><ol start="4"><li>KDC AS处理能力在多进程做RAID条件下，有无preauth属性，KDC服务是否有瓶颈？如果有在哪里？</li></ol><p>经多次实验，KDC的AS处理能力受目前物理机CPU处理能力的限制，图8为有PREAUTH属性的CPU使用情况截图，无PREAUTH结果一致。</p><p>40进程有PREAUTH，AS对CPU资源的使用情况：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jsf7r8j20z2154qk7.jpg" alt></p><p><strong>Client和TGS交互过程的压测：</strong></p><p>下表为TGS压测的一组平均水平的测试数据：</p><p>TGS压测：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6kt9fnvj212g0h2t9a.jpg" alt></p><p>可以发现KDC对TGS请求的处理能力和主机是否做RAID无关,结合KDC中TGS的请求原理，就较容易理解在BDB缓存命中率足够高的条件下，TGS的请求不需要和本次磁盘交互；进一步做实验，也充分验证了这一点，机器的磁盘IO在整个测试过程中，没有大的变化，如图所示，操作系统本身偶尔产生的IO完全构不成KDC的服务瓶颈。KDC单进程多进程的对比，其处理瓶颈和AS一致，均受到CPU处理能力的限制（单进程打满某一CPU，多进程几乎占用整台机器的CPU资源）。从Kerberos的设计原理分析，很容易理解，无论KDC库中的keytab是否带有PREAUTH属性，对TGS的处理逻辑几乎没有影响，压测的数据结果从实际角度验证了这一点。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6m4az4cj224a0tawhe.jpg" alt></p><p><strong>C：其它问题</strong></p><p>Client和KDC的交互，支持TCP和UDP两种协议。在网络环境良好的情况下，两种协议的KDC的测试结果理论上和实际中几乎一致。但是在原生代码中，使用TCP协议，在客户端给KDC造成一定压力持续6s左右，客户端开始认证出错，在远未达到超时时限的情况下，Client出现了<code>socket reset</code>类的错误。KDC查看内核日志，发现大量<code>possible SYN flooding on port 8089(KDC的服务端口). Sending cookies</code>，且通过<code>netstat -s</code>发现机器的<code>xxxx times the listen queue of a socket overflowed</code>异常增高，种种现象表明可能是服务端的半连接队列、全连接队列中的一个或者全部被打满。主要原理如图10所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6mjf0tvj20ws0wuq6q.jpg" alt></p><p>发现KDC服务所在服务器：半队列<code>/proc/sys/net/ipv4/tcp_max_syn_backlog为2048</code>。</p><p>全队列：1）系统参数<code>/proc/sys/net/core/somaxconn＝65535</code>，查看代码<code>listen()</code>函数的传入值为5。</p><p>故而判断TCP的瓶颈在于全队列，因此目标为将<code>listen</code>函数的第二个<code>backlog</code>参数变成可控可传入。</p><p><strong>KDC可监控的设计和实现</strong></p><p>开源社区对Kerberos实现的KDC完全没有对外暴露可监控的接口，最初线上的场景主要通过检索Log进行相关指标的监控，在统计服务QPS、各种错误的监控等方面，存在准确准确监控难的尴尬局面。为了实现对KDC准确、较全面的监控，对KDC进行了二次开发，设计一个获取监控指标的接口。对监控的设计，主要从以下三个方面进行了考虑和设计。</p><p><strong>A.设计上的权衡</strong></p><ol><li><p>监控的设计无论在什么场景下，都应该尽可能的不去或者最小程度的影响线上的服务，本文最终采用建立一块共享内存的方式，记录各个KDC进程的打点信息，实现的架构如图11所示。每个KDC进程对应共享内存中的一块区域，通过n个数组来存储KDC n个进程的服务指标：当某个KDC进程处理一个请求后，该请求对监控指标的影响会直接打点更新到其对应的Slot 数组中。更新的过程不受锁等待更新的影响，KDC对监控打点的调用仅仅是内存块中的更新，对服务的影响几乎可以忽略不计。相比其他方式，在实现上也更加简单、易理解。</p></li><li><p>纪录每个KDC进程的服务情况，便于准确查看每个进程的对请求的处理情况，有助于定位问题多种情况下出现的异常，缩短故障的定位时间。例如：能够准确的反应出每个进程的请求分布是否均匀、请求处理出现异常能够定位到具体是某个进程出现异常还是整体均有异常。</p><p>KDC监控设计的整体架构：</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6oip712j21v80y2wlc.jpg" alt></p><p><strong>B.程序的可拓展性</strong></p><p>任何指标的采集都是随着需求进行变更的，如果程序设计上不具有良好的扩展性，会后续的指标扩展带来很大的困扰。第一版KDC监控指标的采集只区分请求的成功与失败两种类型，美团数据平台KDC库中所有的keytab都具有PREAUTH属性。根据上文可知，去掉PREAUTH属性后，AS请求的QPS能够提升一倍。后续随着服务规模的进一步增长，如果AS请求的处理能力逐步成为瓶颈，会考虑去掉PREAUTH属性。为了准确监控去掉PREAUTH属性这一过程是否有、有多少请求出现错误，需要扩展一个监控指标，因此有了KDC监控的第二版。整个过程只需要修改三个地方，完成两个功能的实现：</p><ol><li>添加指标 ；</li><li>打点逻辑的添加。</li></ol><p>整个修改过程简单明了，因此，该KDC监控程序的设计具有非常好的扩展性。图12为监控指标的罗列和注释：</p><p>KDC监控指标及含义：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6pmygs4j21440ri7d2.jpg" alt></p><p><strong>C.接口工具kstat的设计</strong></p><p>获取KDC监控指标的接口工具主要分为两种：</p><ol><li>获取当前每个KDC进程对各个指标的累积值，该功能是为了和新美大的监控平台Falcon结合，方便实现指标的上报实现累加值和分钟级别速率值的处理；</li><li>获取制定次数在制定时间间隔内每个进程监控指标的瞬时速率，最小统计间隔可达秒级，方便运维人员登陆机器无延迟的查看当前KDC的服务情况，使其在公司监控系统不可用的情况下分析服务的当前问题。具体使用见下图。</li></ol><p>kstat的使用帮助和两种功能使用样例：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6qjmveyj21s60len2w.jpg" alt></p><p><strong>总结：</strong></p><p>通过本次对KDC服务的压测实验和分析，总结出KDC最优性能的调整方案为：</p><ol><li>KDC服务本身需要开启多进程和以充分利用多核机器的CPU资源，同时确保BDB的内存资源足够，保证其缓存命中率达到一定比例（越高越好，否则查询库会带来大量的磁盘读IO）；</li><li>选择的物理机要做RAID，否则在库中keytab带有PREAUTH属性的条件下，会带来大量的写，容易导致磁盘成为KDC的性能瓶颈。通过建立一块共享内存无锁的实现了KDC多进程指标的收集，加上其良好的扩展性和数据的精确性，极大的提高了KDC服务的可靠性。</li></ol><p>相比原来线上单进程的处理能力，目前单台服务器的处理性能提升10+倍以上。本次工作没有详细的论述TCP协议中半队列、全队列的相关参数应该如何设定才能达到最优，和服务本身结合到一起，每个参数的变更带来的影响具体是什么因为过于复杂，还没有介绍。</p><hr><p><a href="https://tech.meituan.com/2019/02/14/data-security-platform-construction-practice-jiangjunling.html" target="_blank" rel="noopener">美团数据安全平台建设实践</a>  介绍了权限模型和解决方案等</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。&lt;/p&gt;
&lt;p&gt;第一次更新：2019-4-29&lt;/p&gt;
&lt;p&gt;第二次更新：2019-5-10&lt;/p&gt;
    
    </summary>
    
      <category term="CDH" scheme="http://yoursite.com/categories/CDH/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/CDH/Security/"/>
    
    
      <category term="Kerberos" scheme="http://yoursite.com/tags/Kerberos/"/>
    
  </entry>
  
  <entry>
    <title>LabelEnconder 和 OneHotEncoder</title>
    <link href="http://yoursite.com/2020/03/09/LabelEnconder/"/>
    <id>http://yoursite.com/2020/03/09/LabelEnconder/</id>
    <published>2020-03-09T10:03:40.229Z</published>
    <updated>2019-05-09T06:42:45.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不得不说 我还是小看了ML的知识涉及的广度<br>光是ML 100days 的第一天其实涉及的内容就非常多<br>从Sklearn包到pycharm自带的各种BUG都搞的人头大</p><p>总算把这个整的有点明白了</p></blockquote><a id="more"></a> <h3 id="LabelEnconder"><a href="#LabelEnconder" class="headerlink" title="LabelEnconder"></a>LabelEnconder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEconder</span><br><span class="line">y = data.iloc[:.<span class="number">-1</span>] <span class="comment"># 索引所有的行 最后一列</span></span><br><span class="line"><span class="comment"># 三步</span></span><br><span class="line">le = LabelEnconder() <span class="comment"># 实例化</span></span><br><span class="line">le = le.fit(y) <span class="comment"># 导入数据</span></span><br><span class="line">label = le.transform(y) <span class="comment"># trasform就扣调取结果</span></span><br><span class="line"><span class="comment"># 这边fit了之后可以直接用le.classes_ 查看标签中有多少类别</span></span><br><span class="line">le.classes_</span><br><span class="line"><span class="comment"># 输出 array(['No','Unknown','Yes'],dtype=object)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以直接fit_transform()一步到位</span></span><br><span class="line">le.fit_transform(y)</span><br><span class="line"><span class="comment"># 这样看不到属性</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从上到下一步到位</span></span><br><span class="line">data.iloc[: , <span class="number">-1</span>] = LabelEncoder().fit_transform(data.iloc[: , <span class="number">-1</span>])</span><br><span class="line"><span class="comment"># 实例化、fit transform 全部完成</span></span><br></pre></td></tr></table></figure><h3 id="OneHotEncoder"><a href="#OneHotEncoder" class="headerlink" title="OneHotEncoder"></a>OneHotEncoder</h3><p>遇到互相不相关的属性，为了避免模型训练的时候把欧式距离计算进去，对结果造成影响<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任然可以一步到位</span></span><br><span class="line">OneHotEncoder(categories=<span class="string">'auto'</span>).fit_transform(X).toattay()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时这个数值还可以还原</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(enc.inverse_transform(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当被哑变量（Onehot）之后，需要一个借口来查看每列的意义</span></span><br><span class="line">enc.get_feature_names()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还有 concat 方法可以将两个表相连</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;不得不说 我还是小看了ML的知识涉及的广度&lt;br&gt;光是ML 100days 的第一天其实涉及的内容就非常多&lt;br&gt;从Sklearn包到pycharm自带的各种BUG都搞的人头大&lt;/p&gt;
&lt;p&gt;总算把这个整的有点明白了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="OneHotEncoder" scheme="http://yoursite.com/tags/OneHotEncoder/"/>
    
      <category term="LabelEncoder" scheme="http://yoursite.com/tags/LabelEncoder/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning</title>
    <link href="http://yoursite.com/2020/03/09/Machine%20Learning/"/>
    <id>http://yoursite.com/2020/03/09/Machine Learning/</id>
    <published>2020-03-09T10:03:40.075Z</published>
    <updated>2020-04-10T17:09:52.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>周志华的同名书籍的阅读笔记，入门读物</p></blockquote><a id="more"></a> <h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><h3 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h3><p>classification：预测结果是离散值（好瓜、坏瓜）</p><p>regression：预测结果是连续值（成熟度0.95 0.37）</p><p>binary classification：只涉及两个结果的分类，通常称之为一个为positive class 一个为 negative class</p><p>multi-class classification：多分类</p><p>预测任务是希望通过训练对训练集{(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>)…(x<sub>m</sub>,y<sub>m</sub>)}进行学习，建议一个从输入空间X到输出空间Y的映射，对于二分类任务，通常另Y = {-1,1}或者{0,1}，对多分类任务，|Y|&gt;2，回归任务，Y = R，R是实数集。</p><p>学习完毕模型后，使用其预测的过程叫做测试(testing)，被预测的样本称为“测试样本”(testing sample)，例如在学得f之后，对测视例x，可得到其预测目标y = f(x).</p><p>聚类（clustering），即将训练集中的西瓜分为若干组（cluster），在聚类学习中，分的类我们是事先不知道的，学习过程中使用的训练样本通常不配拥有标记信息。</p><p>根据训练数据是否用哦与标记信息，学习任务大致可以划分为两大类：“监督学习（supervised learning）和非监督学习（unsupervised learning），分类和回归是前者的代表，聚类是后者的代表”</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;周志华的同名书籍的阅读笔记，入门读物&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>将机器学习模型应用入生产的几种策略</title>
    <link href="http://yoursite.com/2020/03/09/Overview%20of%20the%20different%20approaches%20to%20putting%20Machine%20Learning%20(ML)%20models%20in%20production/"/>
    <id>http://yoursite.com/2020/03/09/Overview of the different approaches to putting Machine Learning (ML) models in production/</id>
    <published>2020-03-09T10:03:39.965Z</published>
    <updated>2020-04-10T17:10:05.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原文 <a href="https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86" target="_blank" rel="noopener"><overview of the different approaches to putting machine learning (ml) models in production></overview></a></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg" alt></p><a id="more"></a> <p>There are different approaches to putting models into productions, with benefits that can vary dependent on the specific use case. Take for example the use case of churn prediction, there is value in having a static value already that can easily be looked up when someone call a customer service, but there is some extra value that could be gained if for specific events, the model could be re-run with the newly acquired information.</p><p>There is generally different ways to both train and server models into production:</p><ul><li><strong>Train</strong>: one off, batch and real-time/online training</li><li><strong>Serve:</strong> Batch, Realtime (Database Trigger, Pub/Sub, web-service, inApp)</li></ul><p>Each approach having its own set of benefits and tradeoffs that need to be considered.</p><h3 id="One-off-Training"><a href="#One-off-Training" class="headerlink" title="One off Training"></a>One off Training</h3><p>Models don’t necessarily need to be continuously trained in order to be pushed to production. Quite often a model can be just trained ad-hoc by a data-scientist, and pushed to production until its performance deteriorates enough that they are called upon to refresh it.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wn761fzj209m09tq63.jpg" alt></p><blockquote><p>From Jupyter to Prod</p></blockquote><p>DataScientists prototyping and doing machine learning tend to operate in their environment of choice <a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter</a> Notebooks. Essentially an advanced GUI on a <a href="https://en.wikipedia.org/wiki/Read–eval–print_loop" target="_blank" rel="noopener">repl</a>, that allows you to save both code and command outputs.</p><p>Using that approach it is more than feasible to push an ad-hoc trained model from some piece of code in Jupyter to production. Different types of libraries and other notebook providers help further tie the link between the data-scientist workbench and production.</p><h4 id="Model-Format"><a href="#Model-Format" class="headerlink" title="Model Format"></a>Model Format</h4><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">Pickle</a> converts a python object to to a bitstream and allows it to be stored to disk and reloaded at a later time. It is provides a good format to store machine learning models provided that their intended applications is also built in python.</p><p><a href="https://github.com/onnx" target="_blank" rel="noopener">ONNX</a> the Open Neural Network Exchange format, is an open format that supports the storing and porting of predictive model across libraries and languages. Most deep learning libraries support it and sklearn also has a library extension to convert their model to <a href="https://github.com/onnx/sklearn-onnx/blob/master/docs/tutorial.rst" target="_blank" rel="noopener">ONNX’s format</a>.</p><p><a href="https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language" target="_blank" rel="noopener">PMML</a> or Predictive model markup language, is another interchange format for predictive models. Like for ONNX sklearn also has another library extension for converting the models to <a href="https://github.com/jpmml/sklearn2pmml" target="_blank" rel="noopener">PMML format</a>. It has the drawback however of only supporting certain type of prediction models.PMML has been around since 1997 and so has a large footprint of applications leveraging the format. Applications such as <a href="https://archive.sap.com/kmuuid2/a07faefd-61d7-2c10-bba6-89ac5ffc302c/Integrating Real-time Predictive Analytics into SAP Applications.pdf" target="_blank" rel="noopener">SAP</a> for instance is able to leverage certain versions of the PMML standard, likewise for CRM applications such as <a href="https://community.pega.com/knowledgebase/supported-pmml-model-types" target="_blank" rel="noopener">PEGA</a>.</p><p><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#about-pojos-and-mojos" target="_blank" rel="noopener">POJO and MOJO </a>are <a href="https://www.h2o.ai/" target="_blank" rel="noopener">H2O.ai</a>’s export format, that intendeds to offers an easily embeddable model into java application. They are however very specific to using the H2O’s platform.</p><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>For one off training of models, the model can either be trained and fine tune adhoc by a data-scientists or training through AutoML libraries. Having an easily reproducible setup, however helps pushing into the next stage of productionalization, ie: batch training.</p><h3 id="Batch-Training"><a href="#Batch-Training" class="headerlink" title="Batch Training"></a>Batch Training</h3><p>While not fully necessary to implement a model in production, batch training allows to have a constantly refreshed version of your model based on the latest train.</p><p>Batch training can benefit a-lot from AutoML type of frameworks, AutoML enables you to perform/automate activities such as feature processing, feature selection, model selections and parameter optimization. Their recent performance has been on par or bested the most diligent data-scientists.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wnpy6yjj20m80gntax.jpg" alt></p><p>Using them allows for a more comprehensive model training than what was typically done prior to their ascent: simply retraining the model weights.</p><p>Different technologies exists that are made to support this continuous batch training, these could for instance be setup through a mix of <a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">airflow</a> to manage the different workflow and an AutoML library such as <a href="https://epistasislab.github.io/tpot/" target="_blank" rel="noopener">tpot</a>, Different cloud providers offer their solutions for AutoML that can be put in a data workflow. Azure for instance integrates machine learning prediction and model training with their <a href="https://azure.microsoft.com/es-es/blog/retraining-and-updating-azure-machine-learning-models-with-azure-data-factory/" target="_blank" rel="noopener">data factory offering</a>.</p><h3 id="Real-time-training"><a href="#Real-time-training" class="headerlink" title="Real time training"></a>Real time training</h3><p>Real-time training is possible with ‘Online Machine Learning’ models, algorithms supporting this method of training includes K-means (through mini-batch), Linear and Logistic Regression (through Stochastic Gradient Descent) as well as Naive Bayes classifier.</p><p>Spark has StreamingLinearAlgorithm/StreamingLinearRegressionWithSGD to perform these operations, sklearn has SGDRegressor and SGDClassifier that can be incrementally trained. In sklearn, the incremental training is done through the partial_fit method as shown below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_0 = pd.DataFrame([[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">0</span>]] )</span><br><span class="line">y_0 = pd.DataFrame([[<span class="number">0</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">X_1 = pd.DataFrame([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">y_1 = pd.DataFrame([[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">clf = linear_model.SGDClassifier()</span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_0, y_0, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_1, y_1, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 1</span></span><br></pre></td></tr></table></figure><p>When deploying this type of models there needs to be serious operational support and monitoring as the model can be sensitive to new data and noise, and model performance needs to be monitored on the fly. In offline training, you can filter points of <a href="https://en.wikipedia.org/wiki/Leverage_(statistics" target="_blank" rel="noopener">high leverage</a>) and correct for this type of incoming data. This is much harder to do when you are constantly updating your model training based on a stream of new data points.</p><p>Another challenge that occurs with training online model is that they don’t decay historical information. This means that, on case there are structural changes in your datasets, the model will need to be anyway re-trained and that there will be a big onus in model lifecycle management.</p><h3 id="Batch-vs-Real-time-Prediction"><a href="#Batch-vs-Real-time-Prediction" class="headerlink" title="Batch vs. Real-time Prediction"></a>Batch vs. Real-time Prediction</h3><p>When looking at whether to setup a batch or real-time prediction, it is important to get an understanding of why doing real-time prediction would be important. It can potentially be for getting a new score when significant event happen, for instance what would be the churn score of customer when they call a contact center. These benefits needs to be weighted against the complexity and cost implications that arise from doing real-time predictions.</p><p><strong>Load implications</strong></p><p>Catering to real time prediction, requires a way to handle peak load. Depending on the approach taken and how the prediction ends up being used, choosing a real-time approach, might also require to have machine with extra computing power available in order to provide a prediction within a certain SLA. This contrasts with a batch approach where the predictions computing can be spread out throughout the day based on available capacity.</p><p><strong>Infrastructure Implications</strong></p><p>Going for real-time, put a much higher operational responsibility. People need to be able to monitor how the system is working, be alerted when there is issue as well as take some consideration with respect to failover responsibility. For batch prediction, the operational obligation is much lower, some monitoring is definitely needed, and altering is desired but the need to be able to know of issues arising directly is much lower.</p><p><strong>Cost Implications</strong></p><p>Going for real-time predictions also has costs implications, going for more computing power, not being able to spread the load throughout the day can force into purchasing more computing capacity than you would need or to pay for spot price increase. Depending on the approach and requirements taken there might also be extra cost due to needing more powerful compute capacity in order to meet SLAs. Furthermore, there would tend to be a higher infrastructure footprint when choosing for real time predictions. One potential caveat there is where the choice is made to rely on in app prediction, for that specific scenario the cost might actually end up being cheaper than going for a batch approach.</p><p><strong>Evaluation Implications</strong></p><p>Evaluating the prediction performance in real-time manner can be more challenging than for batch predictions. How do you evaluate performance when you are faced with a succession of actions in a short burst producing multiple predictions for a given customer for instance? Evaluating and debugging real-time prediction models are significantly more complex to manage. They also require a log collection mechanism that allows to both collect the different predictions and features that yielded the score for further evaluation.</p><h3 id="Batch-Prediction-Integration"><a href="#Batch-Prediction-Integration" class="headerlink" title="Batch Prediction Integration"></a>Batch Prediction Integration</h3><p>Batch predictions rely on two different set of information, one is the predictive model and the other one is the features that we will feed the model. In most type of batch prediction architecture, ETL is performed to either fetch pre-calculated features from a specific datastore (feature-store) or performing some type of transformation across multiple datasets to provide the input to the prediction model. The prediction model then iterates over all the rows in the datasets providing the different score.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wou2v4sj20ja08d74l.jpg" alt></p><p>Once all the predictions have been computed, we can then “serve” the score to the different systems wanting to consume the information. This can be done in different manner depending on thee use case for which we want to consume the score, for instance if we wanted to consume the score on a front-end application, we would most likely push the data to a “cache” or NoSQL database such as Redis so that we can offer milliseconds responses, while for certain use cases such as the creation of an email journey, we might just be relying on a CSV SFTP export or a data load to a more traditional RDBMS.</p><h3 id="Real-time-Prediction-integration"><a href="#Real-time-Prediction-integration" class="headerlink" title="Real-time Prediction integration"></a><strong>Real-time Prediction integration</strong></h3><p>Being able to push model into production for real-time applications require 3 base components. A customer/user profile, a set of triggers and predictive models.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wp3or7dj20em02k3yg.jpg" alt></p><p><strong>Profile:</strong> The customer profile contains all the related attribute to the customer as well as the different attributes (eg: counters) necessary in order to make a given prediction. This is required for customer level prediction in order to reduce the latency of pulling the information from multiple places as well as to simplify the integration of machine learning models in productions. In most cases a similar type of data store would be needed in order to effectively fetch the data needed to power the prediction model.</p><p><strong>Triggers:</strong> Triggers are events causing the initiation of process, they can be for churn for instance, call to a customer service center, checking information within your order history, etc …</p><p><strong>Models:</strong> models need to have been pre-trained and typically exported to one of the 3 formats previously mentioned (pickle, ONNX or PMML) to be something that we could easily port to production.</p><p>There are quite a few different approach to putting models for scoring purpose in production:</p><ul><li><em>Relying on in Database integration:</em> a lot of database vendors have made a significant effort to tie up advanced analytics use cases within the database. Be it by direct integration of Python or R code, to the import of PMML model.</li><li><em>Exploiting a Pub/Sub model</em>: The prediction model is essentially an application feeding of a data-stream and performing certain operations, such as pulling customer profile information.</li><li><em>Webservice:</em> Setting up an API wrapper around the model prediction and deploying it as a web-service. Depending on the way the web-service is setup it might or might not do the pull or data needed to power the model.</li><li><em>inApp:</em> it is also possible to deploy the model directly into a native or web application and have the model be run on local or external datasources.</li></ul><h4 id="Database-integrations"><a href="#Database-integrations" class="headerlink" title="Database integrations"></a><em>Database integrations</em></h4><p>If the overall size of your database is fairly small (&lt; 1M user profile) and the update frequency is occasional it can make sense to integrate some of the real-time update process directly within the database.</p><p>Postgres possess an integration that allows to run Python code as functions or stored procedure called <a href="http://pl/Python" target="_blank" rel="noopener">PL/Python</a>. This implementation has access to all the libraries that are part of the <strong>PYTHONPATH</strong>, and as such are able to use libraries such as Pandas and SKlearn to run some operations.</p><p>This can be coupled with Postgres’ <a href="https://www.tutorialspoint.com/postgresql/postgresql_triggers.htm" target="_blank" rel="noopener">Triggers</a> Mechanism to perform a run of the database and update the churn score. For instance if a new entry is made to a complaint table, it would be valuable to have the model be re-run in real-time.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpd8zz1j20f10b5q39.jpg" alt></p><p><strong>Sequence flow</strong></p><p>The flow could be setup in the following way:</p><p><em>New Event:</em> When a new row is inserted in the complain table, an event trigger is generated.</p><p><em>Trigger:</em> The trigger function would update the number of complaint made by this customer in the customer profile table and fetch the updated record for the customer.</p><p><em>Prediction Request:</em> Based on that it would re-run the churn model through PL/Python and retrieve the prediction.</p><p><em>Customer Profile Update:</em> It can then re-update the customer profile with the updated prediction. Downstream flows can then happen upon checking if the customer profile has been updated with new churn prediction value.</p><p><strong>Technologies</strong></p><p>Different databases are able to support the running of Python script, this is the case of PostGres which has a native Python integration as previosuly mentioned, but also of Ms SQL Server through its’ <a href="https://www.sqlshack.com/how-to-use-python-in-sql-server-2017-to-obtain-advanced-data-analytics/" target="_blank" rel="noopener">Machine Learning Service (in Database)</a>, other databases such as Teradata, are able to run R/Python script through an external script command. While Oracle supports <a href="https://docs.oracle.com/database/121/DMPRG/GUID-55C6ADBF-DA64-48B6-A424-5F0A59CD406D.htm#DMPRG701" target="_blank" rel="noopener">PMML model</a> through its data mining extension.</p><h4 id="Pub-Sub"><a href="#Pub-Sub" class="headerlink" title="Pub/Sub"></a>Pub/Sub</h4><p>Implementing real-time prediction through a pub/sub model allows to be able to properly handle the load through throttling. For engineers, it also means that they can just feed the event data through a single “logging” feed, to which different application can subscribe.</p><p>An example, of how this could be setup is shown below:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpl246sj20m8058aah.jpg" alt></p><p>The page view event is fired to a specific event topic, on which two application subscribe a page view counter, and a prediction. Both of these application filter out specific relevant event from the topic for their purpose and consume the different messages in the topics. The page view counter app, provides data to power a dashboard, while the prediction app, updates the customer profile.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpsew6sj20fg08x74h.jpg" alt></p><p><strong>Sequence flow:</strong></p><p>Event messages are pushed to the pub/sub topic as they occur, the prediction app poll the topic for new messages. When a new message is retrieved by the prediction app, it will request and retrieve the customer profile and use the message and the profile information to make a prediction. which it will ultimately push back to the customer profile for further use.</p><p>A slightly different flow can be setup where the data is first consumed by an “enrichment app” that adds the profile information to the message and then pushes it back to a new topic to finally be consumed by the prediction app and pushed onto the customer profile.</p><p><strong>Technologies:</strong></p><p>The typical open source combination that you would find that support this kind of use case in the data ecosystem is a combination of Kafka and Spark streaming, but a different setup is possible on the cloud. On google notably a google pub-sub/dataflow (Beam) provides a good alternative to that combination, on azure a combination of Azure-Service Bus or Eventhub and Azure Functions can serve as a good way to consume the mesages and generate these predictions.</p><p><em>Web Service</em></p><p>We can implement models into productions as web-services. Implementing predictions model as web-services are particularly useful in engineering teams that are fragmented and that need to handle multiple different interfaces such as web, desktop and mobile.</p><p>Interfacing with the web-service could be setup in different way:</p><ul><li>either providing an identifier and having the web-service pull the required information, compute the prediction and return its’ value</li><li>Or by accepting a payload, converting it to a data-frame, making the prediction and returning its’ value.</li></ul><p>The second approach is usually recommended in cases, when there is a lot of interaction happening and a local cache is used to essentially buffer the synchronization with the backend systems, or when needing to make prediction at a different grain than a customer id, for instance when doing session based predictions.</p><p>The systems making use of local storage, tend to have a reducer function, which role is to calculate what would be the customer profile, should the event in local storage be integrated back. As such it provides an approximation of the customer profile based on local data.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wq0vsywj20ix0b5mxt.jpg" alt></p><p><strong>Sequence Flow</strong></p><p>The flow for handling the prediction using a mobile app, with local storage can be described in 4 phases.</p><p><em>Application Initialization (1 to 3)**</em>:** The application initializes, and makes a request to the customer profile, and retrieve its initial value back, and initialize the profile in local storage.</p><p><em>Applications (4):</em> The application stores the different events happening with the application into an array in local storage.</p><p><em>Prediction Preparation (5 to 8)**</em>:*<em> The application wants to retrieve a new churn prediction, and therefore needs to prepare the information it needs to provide to the Churn Web-service. For that, it makes an initial request to local storage to retrieve the values of the profile and the array of events it has stored. Once they are retrieve, it makes a request to a reducer function providing these values as arguments, the reducer function outputs an updated</em> profile with the local events incorporated back into this profile.</p><p><em>Web-service Prediction (9 to 10):</em> The application makes a request to the churn prediction web-service, providing the different the updated*/reduced customer profile from step 8 as part of the payload. The web-service can then used the information provided by the payload to generate the prediction and output its value, back to the application.</p><p><strong>Technologies</strong></p><p>There are quite a few technologies that can be used to power a prediction web-service:</p><p><em>Functions</em></p><p>AWS Lambda functions, Google Cloud functions and Microsoft Azure Functions (although Python support is currently in Beta) offer an easy to setup interface to easily deploy scalable web-services.</p><p>For instance on Azure a prediction web-service could be implemented through a function looking roughly like this:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> azure.functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(req: func.HttpRequest)</span> -&gt; func.HttpResponse:</span></span><br><span class="line">    logging.info(<span class="string">'Python HTTP trigger function processed a request.'</span>)</span><br><span class="line">    <span class="keyword">if</span> req.body:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            logging.info(<span class="string">"Converting Request to DataFrame"</span>)</span><br><span class="line">            req_body = req.get_json()</span><br><span class="line">            df_body  = pd.DataFrame([req_body])</span><br><span class="line"></span><br><span class="line">            logging.info(<span class="string">"Loadding the Prediction Model"</span>)</span><br><span class="line">            filename = <span class="string">"model.pckl"</span></span><br><span class="line">            loaded_model = joblib.load(filename)</span><br><span class="line">            <span class="comment"># Features names need to have been added to the pickled model</span></span><br><span class="line">            feature_names = loaded_model.feature_names</span><br><span class="line">            <span class="comment"># subselect only the feature names </span></span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Subselecting the dataframe"</span>)</span><br><span class="line">            df_subselect = df_body[feature_names]</span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Predicting the Probability"</span>)</span><br><span class="line">            result = loaded_model.predict_proba(df_subselect)</span><br><span class="line">            <span class="comment"># We are looking at the probba prediction for class 1</span></span><br><span class="line">            prediction = result[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> func.HttpResponse(<span class="string">"&#123;prediction&#125;"</span>.format(prediction=prediction), status_code=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> func.HttpResponse(</span><br><span class="line">             <span class="string">"Please pass a name on the query string or in the request body"</span>,</span><br><span class="line">             status_code=<span class="number">400</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p><em>Container</em></p><p>An alternative to functions, is to deploy a flask or django application through a docker container (Amazon ECS, Azure Container Instance or Google Kubernetes Engine). Azure for instance provides an easy way to setup prediction containers through its’ <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where" target="_blank" rel="noopener">Azure Machine Learning service</a>.</p><p><em>Notebooks</em></p><p>Different notebooks providers such as <a href="https://docs.databricks.com/applications/mlflow/models.html" target="_blank" rel="noopener">databricks</a> and <a href="https://www.dataiku.com/dss/features/model-deployment/" target="_blank" rel="noopener">dataiku</a> have notably worked on simplifying the model deployment from their environments. These have the feature of setting up a webservice to a local environment or deploying to external systems such as Azure ML Service, Kubernetes engine etc…</p><h4 id="in-App"><a href="#in-App" class="headerlink" title="in App"></a>in App</h4><p>In certain situations when there are legal or privacy requirements that do not allow for data to be stored outside of an application, or there exists constraints such as having to upload a large amount of files, leveraging a model within the application tend to be the right approach.</p><p>Android-ML Kit or the likes of Caffe2 allows to leverage models within native applications, while <a href="https://www.tensorflow.org/js" target="_blank" rel="noopener">Tensorflow.js</a> and <a href="https://github.com/Microsoft/onnxjs" target="_blank" rel="noopener">ONNXJS</a> allow for running models directly in the browser or in apps leveraging javascripts.</p><h3 id="Considerations"><a href="#Considerations" class="headerlink" title="Considerations"></a>Considerations</h3><p>Beside the method of deployments of the models, they are quite a few important considerations to have when deploying to production.</p><p><strong>Model Complexity</strong></p><p>The complexity of the model itself, is the first considerations to have. Models such as a linear regressions and logistic regression are fairly easy to apply and do not usually take much space to store. Using more complex model such as a neural network or complex ensemble decision tree, will end up taking more time to compute, more time to load into memory on cold start and will prove more expensive to run</p><p><strong>Data Sources</strong></p><p>It is important to consider the difference that could occur between the datasource in productions and the one used for training. While it is important for the data used for the training to be in sync with the context it would be used for in production, it is often impractical to recalculate every value so that it becomes perfectly in-sync.</p><p><strong>Experimentation framework</strong></p><p>Setting up an experimentation framework, A/B testing the performance of different models versus objective metrics. And ensuring that there is sufficient tracking to accurately debug and evaluate models performance a posteriori.</p><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>Choosing how to deploy a predictive models into production is quite a complex affair, there are different way to handle the lifecycle management of the predictive models, different formats to stores them, multiple ways to deploy them and very vast technical landscape to pick from.</p><p>Understanding specific use cases, the team’s technical and analytics maturity, the overall organization structure and its’ interactions, help come to the the right approach for deploying predictive models to production.</p><hr><p>More from me on <a href="https://medium.com/analytics-and-data" target="_blank" rel="noopener">Hacking Analytics</a>:</p><ul><li><a href="https://medium.com/analytics-and-data/on-the-evolution-of-data-engineering-c5e56d273e37" target="_blank" rel="noopener">One the evolution of Data Engineering</a></li><li><a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">Airflow, the easy way</a></li><li><a href="https://medium.com/analytics-and-data/e-commerce-analysis-data-structures-and-applications-6420c4fa65e7" target="_blank" rel="noopener">E-commerce Analysis: Data-Structures and Applications</a></li><li><a href="https://medium.com/analytics-and-data/setting-up-airflow-on-azure-connecting-to-ms-sql-server-8c06784a7e2b" target="_blank" rel="noopener">Setting up Airflow on Azure &amp; connecting to MS SQL Server</a></li><li><a href="https://medium.com/analytics-and-data/3-simple-rules-to-build-machine-learning-models-that-add-value-61106db88461" target="_blank" rel="noopener">3 simple rules to build machine learning Models that add value</a></li></ul><blockquote><p>简单看了下，没有深入纠结，本文主要从离线和实时两个方面介绍了ML的应用，给了一些简单的例子。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;原文 &lt;a href=&quot;https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;overview of the different approaches to putting machine learning (ml) models in production&gt;&lt;/overview&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML</title>
    <link href="http://yoursite.com/2020/03/09/SparkML/"/>
    <id>http://yoursite.com/2020/03/09/SparkML/</id>
    <published>2020-03-09T10:03:39.674Z</published>
    <updated>2020-04-10T17:12:29.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SparkML也是个大坑，先在这里贴上pom文件</p></blockquote><a id="more"></a> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>Akka repository<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repo.akka.io/releases<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala/<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala/<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.11.4<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>reference.conf<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">Main-Class</span>&gt;</span><span class="tag">&lt;/<span class="name">Main-Class</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">        &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-common&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.37<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;2.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;SparkML也是个大坑，先在这里贴上pom文件&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Apache/Spark/"/>
    
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SQL积累</title>
    <link href="http://yoursite.com/2020/03/09/SQL/"/>
    <id>http://yoursite.com/2020/03/09/SQL/</id>
    <published>2020-03-09T10:03:39.532Z</published>
    <updated>2019-06-12T08:50:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>SQL看似简单其实也包含了相当多的内容</p><p>慢慢积累吧，最近状态不咋好，一点点来</p><a id="more"></a> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找最晚入职员工的所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">--有个答案是</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">0</span>,<span class="number">1</span></span><br><span class="line"><span class="comment">--但是这个答案有个问题，当一天由多个同事入职的时候会出现歧义</span></span><br><span class="line"><span class="comment">--所以用下面的方法是绝对正确的</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees <span class="keyword">where</span> hire_date = (<span class="keyword">select</span> <span class="keyword">max</span>(hire_date) <span class="keyword">from</span> employees)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找入职员工时间排名倒数第三的员工所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--LIMIT m,n : 表示从第m+1条开始，取n条数据；</span></span><br><span class="line"><span class="comment">--LIMIT n ： 表示从第0条开始，取n条数据，是limit(0,n)的缩写。</span></span><br><span class="line"><span class="comment">--考察点是limit的用法</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--与此同时还有一种想法觉得入职日期，只要是同一天的也就不分前后，也就是说，题目转换为了倒数三天前入职的所有同事。</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees </span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> hire_date </span><br><span class="line">     <span class="keyword">from</span> employees </span><br><span class="line">     <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">DESC</span> </span><br><span class="line">     <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">--同时考虑到distinct效率问题还可以改用group by</span></span><br><span class="line"><span class="comment">--经过测试，这种写法确实比上面的效率要高一点，同时应该要注意到这个应该和数据量也有关系</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">(<span class="keyword">select</span> hire_date</span><br><span class="line">    <span class="keyword">from</span> employees</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> hire_date</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找各个部门当前(to_date='9999-01-01')领导当前薪水详情以及其对应部门编号dept_no</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_manager`</span> (</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br><span class="line"><span class="comment">--要求输出格式：</span></span><br><span class="line"><span class="comment">--emp_nosalaryfrom_dateto_datedept_no</span></span><br><span class="line"><span class="comment">--答案一：先在两个表里用where过滤出现任的人选，然后用相等简单相等关联即可。</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.*, dm.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s,</span><br><span class="line">    dept_manager <span class="keyword">as</span> dm</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    dm.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    s.emp_no = dm.emp_no;</span><br><span class="line"><span class="comment">--答案二：</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.* , d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">    dept_manager <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    s.emp_no = d.emp_no</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    d.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="comment">--此题比较坑，限制了两个to_date，是因为薪水可能会变，人员也可能会变。</span></span><br><span class="line">然后两个表的前后位置不能动，否则和输出不符，姑且理解为必须小表<span class="keyword">join</span>大表吧。</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有已经分配部门的员工的last_name和first_name</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="comment">--我首先考虑的是没有使用join的情况</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d, employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    d.emp_no = e.emp_no</span><br><span class="line"><span class="comment">--其实从效率方面考虑，使用join会不会好一点，好像使用自然连接不用on就可以</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">natural</span> <span class="keyword">join</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--下面这道还是类似的</span></span><br><span class="line"><span class="comment">--查找所有员工的last_name和first_name以及对应部门编号dept_no，也包括展示没有分配具体部门的员工</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    e.emp_no = d.emp_no</span><br><span class="line"><span class="comment">--简单的left join</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有员工入职时候的薪水情况，给出emp_no以及salary， 并按照emp_no进行逆序</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL看似简单其实也包含了相当多的内容&lt;/p&gt;
&lt;p&gt;慢慢积累吧，最近状态不咋好，一点点来&lt;/p&gt;
    
    </summary>
    
      <category term="SQL" scheme="http://yoursite.com/categories/SQL/"/>
    
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>win10搜索栏失效</title>
    <link href="http://yoursite.com/2020/03/09/win10%E6%90%9C%E7%B4%A2%E6%A0%8F%E5%A4%B1%E6%95%88/"/>
    <id>http://yoursite.com/2020/03/09/win10搜索栏失效/</id>
    <published>2020-03-09T10:03:39.299Z</published>
    <updated>2020-04-10T17:13:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>打开电脑突然发现，win10菜单的快速搜索APP功能失效了</p></blockquote><a id="more"></a> <p>稍微研究了一下，很简单，两步解决<br>第一步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start powershell</span><br></pre></td></tr></table></figure><p>第二步，在弹出的新窗口中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-AppXPackage -Name Microsoft.Windows.Cortana | Foreach &#123;Add-AppxPackage -DisableDevelopmentMode -Register "$($_.InstallLocation)\AppXManifest.xml"&#125;</span><br></pre></td></tr></table></figure><p>bingo！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;打开电脑突然发现，win10菜单的快速搜索APP功能失效了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Picks" scheme="http://yoursite.com/categories/Picks/"/>
    
    
      <category term="Win10" scheme="http://yoursite.com/tags/Win10/"/>
    
  </entry>
  
</feed>
