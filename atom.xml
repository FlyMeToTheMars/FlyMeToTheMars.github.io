<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mars</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-20T14:03:54.347Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Fly Hugh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>朝花夕拾</title>
    <link href="http://yoursite.com/2020/04/20/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    <id>http://yoursite.com/2020/04/20/朝花夕拾/</id>
    <published>2020-04-20T10:28:03.000Z</published>
    <updated>2020-04-20T14:03:54.347Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>资质低下 三心二意 昨日知识 朝花夕拾</p></blockquote><a id="more"></a> <h3 id="更友好的创建对象方式"><a href="#更友好的创建对象方式" class="headerlink" title="更友好的创建对象方式"></a>更友好的创建对象方式</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gdnjra9kkrj20sk0c6myf.jpg" alt="3d5024b55687373af54fcb9ef4e0eb4.png"></p><p>上面的方式，对JVM来说是更友好的，因为堆内存的调用无法避免，所以从栈内存这边入手解决内存问题是一个不错的解决的方式</p><hr><h3 id="下面代码是否线程安全"><a href="#下面代码是否线程安全" class="headerlink" title="下面代码是否线程安全"></a>下面代码是否线程安全</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Singleton instance; </span><br><span class="line">    <span class="function"><span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span></span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span>(Singleton.class) &#123;</span><br><span class="line">                <span class="keyword">if</span> (instance == <span class="keyword">null</span>) instance = <span class="keyword">new</span> Singleton();</span><br><span class="line">            &#125; </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">return</span> instance; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>乍一看类似饿汉式的单例，线程安全，其实是有问题的</p><p>虽然只有一个线程能够获得锁，并且这个锁还是类锁，所有对象共享的</p><p>关键在于 jvm 对 new 的优化，这个变量没有声明 volatile，new 不是一个线程安全的操作，</p><p>对于 new 这个指令，一般的顺序是申请内存空间，初始化内存空间，然后把内存地址赋给 instance 对象，但是 jvm 会对这段指令进行优化，优化之后变成 申请内存空间，内存地址赋给 instance 对象，初始化内存空间，这就导致 第二层检查可能会出错，标准写法只需要在变量前声明 volatile 即可。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gdnkbsp9sij20pp0gy75i.jpg" alt="677701574e4f69f35e226ed6bc9a380.png"></p><hr><h3 id="volatile利用了什么协议来实现可见性"><a href="#volatile利用了什么协议来实现可见性" class="headerlink" title="volatile利用了什么协议来实现可见性"></a>volatile利用了什么协议来实现可见性</h3><p>volatile 是通过内存屏障实现的，MESI协议，缓存一致性协议</p><p>JVM推荐书《The Java Language Specification》<br>volatile 修饰的变量如果值发生变化 发现线程的高速缓存与主存数据不一致时候 由于缓存一致性协议 则总线将高速缓存中的值清空 其他线程只能通过访问主存来获取最新的值 并缓存到告诉缓存上。</p><hr><h3 id="Java-Trainsient-关键字"><a href="#Java-Trainsient-关键字" class="headerlink" title="Java Trainsient 关键字"></a>Java Trainsient 关键字</h3><p>1.一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 </p><p>2.transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口。 </p><p>3.一个静态变量不管是否被transient修饰，均不能被序列化。 </p><p>使用总结和场景：某个类的有些属性需要序列化，其他属性不需要被序列化，比如：敏感信息（如密码，银行卡号等），java 的transient关键字为我们提供了便利，你只需要实现Serilizable接口，将不需要序列化的属性前添加关键字transient，序列化对象的时候，这个属性就不会序列化到指定的目的地中。</p><h3 id="多线程中Random的使用"><a href="#多线程中Random的使用" class="headerlink" title="多线程中Random的使用"></a>多线程中Random的使用</h3><p>1.不要在多个线程间共享一个java.util.Random实例，而该把它放入ThreadLocal之中。</p><p>2.Java7以上我们更推荐使用java.util.concurrent.ThreadLocalRandom。</p><p>下面两条建议是 IDEA给的:</p><p>1.不要将将随机数放大10的若干倍然后取整，直接使用Random对象的nextInt或者nextLong方法</p><p>2.Math.random()应避免在多线程环境下使用</p><h3 id="为什么阿里禁止使用Executor创建线程池"><a href="#为什么阿里禁止使用Executor创建线程池" class="headerlink" title="为什么阿里禁止使用Executor创建线程池"></a>为什么阿里禁止使用Executor创建线程池</h3><p>阿里规约之所以强制要求手动创建线程池，也是和这些参数有关。具体为什么不允许，规约是这么说的：</p><p>线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。</p><p>Executor提供的四个静态方法创建线程池，但是阿里规约却并不建议使用它。</p><p>Executors各个方法的弊端：<br>1）newFixedThreadPool和newSingleThreadExecutor:<br>  主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至OOM。<br>2）newCachedThreadPool和newScheduledThreadPool:<br>  主要问题是线程数最大数是Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至OOM。</p><p>看一下这两种弊端怎么导致的。</p><p>第一种，newFixedThreadPool和newSingleThreadExecutor分别获得 FixedThreadPool 类型的线程池 和  SingleThreadExecutor 类型的线程池。　</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">new</span> ThreadPoolExecutor(nThreads, nThreads,</span><br><span class="line">                                     <span class="number">0L</span>, TimeUnit.MILLISECONDS,</span><br><span class="line">                                     <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public static ExecutorService newSingleThreadExecutor() &#123;</span><br><span class="line">       return new FinalizableDelegatedExecutorService</span><br><span class="line">           (new ThreadPoolExecutor(1, 1,</span><br><span class="line">                                   0L, TimeUnit.MILLISECONDS,</span><br><span class="line">                                   new LinkedBlockingQueue&lt;Runnable&gt;()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为，创建了一个无界队列LinkedBlockingQueuesize，是一个最大值为Integer.MAX_VALUE的线程阻塞队列，当添加任务的速度大于线程池处理任务的速度，可能会在队列堆积大量的请求，消耗很大的内存，甚至导致OOM。</p><h3 id="mac-清理maven仓库的脚本"><a href="#mac-清理maven仓库的脚本" class="headerlink" title="mac 清理maven仓库的脚本"></a>mac 清理maven仓库的脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 这里写你的仓库路径</span><br><span class="line">REPOSITORY_PATH=~/Documents/tools/apache-maven-3.0.3/repository</span><br><span class="line">echo 正在搜索...</span><br><span class="line">find $REPOSITORY_PATH -name &quot;*lastUpdated*&quot; | xargs rm -fr</span><br><span class="line">echo 删除完毕</span><br><span class="line"></span><br><span class="line">mac（linux）系统-创建.sh文件脚本执行（mac用.command终端也可以）</span><br></pre></td></tr></table></figure><h3 id="idea目录较多，文件名较长产生的错误"><a href="#idea目录较多，文件名较长产生的错误" class="headerlink" title="idea目录较多，文件名较长产生的错误"></a>idea目录较多，文件名较长产生的错误</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Error running &apos;ServiceStarter&apos;: Command line is too long. Shorten command line for ServiceStarter or also for Application default configuration.</span><br><span class="line"></span><br><span class="line">修改项目下 .idea\workspace.xml，找到标签 &lt;component name=&quot;PropertiesComponent&quot;&gt; ， 在标签里加一行 &lt;property name=&quot;dynamic.classpath&quot; value=&quot;true&quot; /&gt;</span><br></pre></td></tr></table></figure><h3 id="Log4J-指定屏蔽某些特定报警信息"><a href="#Log4J-指定屏蔽某些特定报警信息" class="headerlink" title="Log4J 指定屏蔽某些特定报警信息"></a>Log4J 指定屏蔽某些特定报警信息</h3><p>Logger.getLogger(“org.apache.library”).setLevel(Level.OFF)</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;资质低下 三心二意 昨日知识 朝花夕拾&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Interview" scheme="http://yoursite.com/categories/Interview/"/>
    
    
      <category term="PICKS" scheme="http://yoursite.com/tags/PICKS/"/>
    
  </entry>
  
  <entry>
    <title>Git详解</title>
    <link href="http://yoursite.com/2020/04/13/Git%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/04/13/Git详解/</id>
    <published>2020-04-13T08:24:45.000Z</published>
    <updated>2020-04-13T08:24:45.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>讨论了许多种git的情况，非常详细的git报告</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gds54g3odrj206402kmwz.jpg" alt="undefined"><br><a id="more"></a> </p><h1 id="Git超详细"><a href="#Git超详细" class="headerlink" title="Git超详细"></a>Git超详细</h1><h4 id="Git是什么"><a href="#Git是什么" class="headerlink" title="Git是什么"></a>Git是什么</h4><p>Git是目前世界上最先进的分布式版本控制系统。</p><h4 id="SVN与Git的最主要的区别"><a href="#SVN与Git的最主要的区别" class="headerlink" title="SVN与Git的最主要的区别"></a><strong>SVN与Git的最主要的区别</strong></h4><p>SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而干活的时候，用的都是自己的电脑，所以首先要从中央服务器哪里得到最新的版本，然后干活，干完后，需要把自己做完的活推送到中央服务器。集中式版本控制系统是必须联网才能工作，如果在局域网还可以，带宽够大，速度够快，如果在互联网下，如果网速慢的话，就纳闷了。</p><p>   Git是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。</p><h4 id="创建版本库"><a href="#创建版本库" class="headerlink" title="创建版本库"></a>创建版本库</h4><p>   什么是版本库？版本库又名仓库，英文名repository,你可以简单的理解一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改，删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻还可以将文件”还原”。</p><p>  所以创建一个版本库也非常简单，如下我是D盘 –&gt; www下 目录下新建一个testgit版本库。</p><p>pwd 命令是用于显示当前的目录。</p><p>   \1. 通过命令 git init 把这个目录变成git可以管理的仓库，如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyr2rpcnj20en025mx9.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyr2rpcnj20en025mx9.jpg" alt="img"></a></p><p>   这时候你当前testgit目录下会多了一个.git的目录，这个目录是Git来跟踪管理版本的，没事千万不要手动乱改这个目录里面的文件，否则，会把git仓库给破坏了。如下：</p><p>  <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyr1x3lzj20h004tgm1.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyr1x3lzj20h004tgm1.jpg" alt="img"></a></p><h4 id="把文件添加到版本库中"><a href="#把文件添加到版本库中" class="headerlink" title="把文件添加到版本库中"></a>把文件添加到版本库中</h4><p>​     首先要明确下，所有的版本控制系统，只能跟踪文本文件的改动，比如txt文件，网页，所有程序的代码等，Git也不列外，版本控制系统可以告诉你每次的改动，但是图片，视频这些二进制文件，虽能也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是知道图片从1kb变成2kb，但是到底改了啥，版本控制也不知道。</p><p>  <strong>下面先看下**</strong>demo<strong>**如下演示：</strong></p><p>   我在版本库testgit目录下新建一个记事本文件 readme.txt 内容如下：11111111</p><p>   第一步：使用命令 git add readme.txt添加到暂存区里面去。如下：</p><p>  <a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyr0wkxbj20ch028dfu.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyr0wkxbj20ch028dfu.jpg" alt="img"></a></p><p>  如果和上面一样，没有任何提示，说明已经添加成功了。</p><p>  第二步：用命令 git commit告诉Git，把文件提交到仓库。</p><p>  <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqz56axj20dp03djrr.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqz56axj20dp03djrr.jpg" alt="img"></a></p><p> 现在我们已经提交了一个readme.txt文件了，我们下面可以通过命令git status来查看是否还有文件未提交，如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqrg067j20d102zwen.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqrg067j20d102zwen.jpg" alt="img"></a></p><p> 说明没有任何文件未提交，但是我现在继续来改下readme.txt内容，比如我在下面添加一行2222222222内容，继续使用git status来查看下结果，如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqq7ts6j20h504r74x.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqq7ts6j20h504r74x.jpg" alt="img"></a></p><p>上面的命令告诉我们 readme.txt文件已被修改，但是未被提交的修改。</p><p>接下来我想看下readme.txt文件到底改了什么内容，如何查看呢？可以使用如下命令：</p><p>git diff readme.txt 如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqnrvxgj20ds05maal.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqnrvxgj20ds05maal.jpg" alt="img"></a></p><p>如上可以看到，readme.txt文件内容从一行11111111改成 二行 添加了一行22222222内容。</p><p>知道了对readme.txt文件做了什么修改后，我们可以放心的提交到仓库了，提交修改和提交文件是一样的2步(第一步是git add 第二步是：git commit)。</p><p>如下：</p><p> <a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqmcupsj20h609i402.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqmcupsj20h609i402.jpg" alt="img"></a></p><h4 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h4><p>   如上，我们已经学会了修改文件，现在我继续对readme.txt文件进行修改，再增加一行</p><p>内容为33333333333333.继续执行命令如下：</p><p>  <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyql1473j20cp03vdga.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyql1473j20cp03vdga.jpg" alt="img"></a></p><p>现在我已经对readme.txt文件做了三次修改了，那么我现在想查看下历史记录，如何查呢？我们现在可以使用命令 git log 演示如下所示：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyqd9m1dj20gt08ggn8.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyqd9m1dj20gt08ggn8.jpg" alt="img"></a></p><p>  git log命令显示从最近到最远的显示日志，我们可以看到最近三次提交，最近的一次是,增加内容为333333.上一次是添加内容222222，第一次默认是 111111.如果嫌上面显示的信息太多的话，我们可以使用命令 git log –pretty=oneline 演示如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqc3ziwj20gs02paai.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyqc3ziwj20gs02paai.jpg" alt="img"></a></p><p>  现在我想使用版本回退操作，我想把当前的版本回退到上一个版本，要使用什么命令呢？可以使用如下2种命令，第一种是：git reset –hard HEAD^ 那么如果要回退到上上个版本只需把HEAD^ 改成 HEAD^^ 以此类推。那如果要回退到前100个版本的话，使用上面的方法肯定不方便，我们可以使用下面的简便命令操作：git reset –hard HEAD~100 即可。未回退之前的readme.txt内容如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqavyf7j20ch04laap.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyqavyf7j20ch04laap.jpg" alt="img"></a></p><p>如果想回退到上一个版本的命令如下操作：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqa5xjfj20ct02xaad.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyqa5xjfj20ct02xaad.jpg" alt="img"></a></p><p>再来查看下 readme.txt内容如下：通过命令cat readme.txt查看</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq9fck2j20c402d74c.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq9fck2j20c402d74c.jpg" alt="img"></a></p><p>可以看到，内容已经回退到上一个版本了。我们可以继续使用git log 来查看下历史记录信息，如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq6bhrlj20dc063dgk.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq6bhrlj20dc063dgk.jpg" alt="img"></a></p><p>我们看到 增加333333 内容我们没有看到了，但是现在我想回退到最新的版本，如：有333333的内容要如何恢复呢？我们可以通过版本号回退，使用命令方法如下：</p><p>git reset –hard 版本号 ，但是现在的问题假如我已经关掉过一次命令行或者333内容的版本号我并不知道呢？要如何知道增加3333内容的版本号呢？可以通过如下命令即可获取到版本号：git reflog 演示如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyq5dtfrj20e603e0t5.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyq5dtfrj20e603e0t5.jpg" alt="img"></a></p><p>通过上面的显示我们可以知道，增加内容3333的版本号是 6fcfc89.我们现在可以命令</p><p>git reset –hard 6fcfc89来恢复了。演示如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq4m3oqj20e104974t.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq4m3oqj20e104974t.jpg" alt="img"></a></p><p>可以看到 目前已经是最新的版本了。</p><h4 id="理解工作区与暂存区的区别"><a href="#理解工作区与暂存区的区别" class="headerlink" title="理解工作区与暂存区的区别"></a>理解工作区与暂存区的区别</h4><p><strong>工作区：</strong>就是你在电脑上看到的目录，比如目录下testgit里的文件(.git隐藏目录版本库除外)。或者以后需要再新建的目录文件等等都属于工作区范畴。</p><p>   <strong>版本库**</strong>(Repository)<strong>**：</strong>工作区有一个隐藏目录.git,这个不属于工作区，这是版本库。其中版本库里面存了很多东西，其中最重要的就是stage(暂存区)，还有Git为我们自动创建了第一个分支master,以及指向master的一个指针HEAD。</p><p>我们前面说过使用Git提交文件到版本库有两步：</p><p> 第一步：是使用 git add 把文件添加进去，实际上就是把文件添加到暂存区。</p><p> 第二步：使用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支上。</p><p>我们继续使用demo来演示下：</p><p>我们在readme.txt再添加一行内容为4444444，接着在目录下新建一个文件为test.txt 内容为test，我们先用命令 git status来查看下状态，如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyq3ykzsj20hv06pwfi.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyq3ykzsj20hv06pwfi.jpg" alt="img"></a></p><p>现在我们先使用git add 命令把2个文件都添加到暂存区中，再使用git status来查看下状态，如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq2gn7sj20d206p0t8.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq2gn7sj20d206p0t8.jpg" alt="img"></a></p><p>接着我们可以使用git commit一次性提交到分支上，如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq1gpk0j20h704mdgm.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq1gpk0j20h704mdgm.jpg" alt="img"></a></p><h4 id="Git撤销修改和删除文件操作"><a href="#Git撤销修改和删除文件操作" class="headerlink" title="Git撤销修改和删除文件操作"></a>Git撤销修改和删除文件操作</h4><h4 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a><strong>撤销修改</strong></h4><p>  比如我现在在readme.txt文件里面增加一行 内容为555555555555，我们先通过命令查看如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq0rzrcj20ax03vaaa.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyq0rzrcj20ax03vaaa.jpg" alt="img"></a></p><p>在我未提交之前，我发现添加5555555555555内容有误，所以我得马上恢复以前的版本，现在我可以有如下几种方法可以做修改：</p><p>第一：如果我知道要删掉那些内容的话，直接手动更改去掉那些需要的文件，然后add添加到暂存区，最后commit掉。</p><p>第二：我可以按以前的方法直接恢复到上一个版本。使用 git reset –hard HEAD^</p><p>但是现在我不想使用上面的2种方法，我想直接想使用撤销命令该如何操作呢？首先在做撤销之前，我们可以先用 git status 查看下当前的状态。如下所示：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq034qhj20hs04oaam.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyq034qhj20hs04oaam.jpg" alt="img"></a></p><p>可以发现，Git会告诉你，git checkout — file 可以丢弃工作区的修改，如下命令：</p><p>git checkout — readme.txt,如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypz44y5j20eh03w0t4.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypz44y5j20eh03w0t4.jpg" alt="img"></a></p><p>命令 git checkout –readme.txt 意思就是，把readme.txt文件在工作区做的修改全部撤销，这里有2种情况，如下：</p><ol><li>readme.txt自动修改后，还没有放到暂存区，使用 撤销修改就回到和版本库一模一样的状态。</li><li>另外一种是readme.txt已经放入暂存区了，接着又作了修改，撤销修改就回到添加暂存区后的状态。</li></ol><p>对于第二种情况，我想我们继续做demo来看下，假如现在我对readme.txt添加一行 内容为6666666666666，我git add 增加到暂存区后，接着添加内容7777777，我想通过撤销命令让其回到暂存区后的状态。如下所示：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypybh8pj20h40deq52.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypybh8pj20h40deq52.jpg" alt="img"></a></p><p><strong>注意：</strong>命令git checkout — readme.txt 中的 — 很重要，如果没有 — 的话，那么命令变成创建分支了。</p><h4 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a><strong>删除文件</strong></h4><p> 假如我现在版本库testgit目录添加一个文件b.txt,然后提交。如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypxcttej20hr0awmzc.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypxcttej20hr0awmzc.jpg" alt="img"></a></p><p>如上：一般情况下，可以直接在文件目录中把文件删了，或者使用如上rm命令：rm b.txt ，如果我想彻底从版本库中删掉了此文件的话，可以再执行commit命令 提交掉，现在目录是这样的，</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypvtweyj20jj05cwf4.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypvtweyj20jj05cwf4.jpg" alt="img"></a></p><p>只要没有commit之前，如果我想在版本库中恢复此文件如何操作呢？</p><p>可以使用如下命令 git checkout — b.txt，如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyput1l8j20fh06s0tr.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyput1l8j20fh06s0tr.jpg" alt="img"></a></p><p>再来看看我们testgit目录，添加了3个文件了。如下所示：</p><h4 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h4><p> 在了解之前，先注册github账号，由于你的本地Git仓库和github仓库之间的传输是通过SSH加密的，所以需要一点设置：</p><p>   第一步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果有的话，直接跳过此如下命令，如果没有的话，打开命令行，输入如下命令：</p><p>ssh-keygen -t rsa –C “<a href="mailto:youremail@example.com" target="_blank" rel="noopener">youremail@example.com</a>”, 由于我本地此前运行过一次，所以本地有，如下所示：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypq7esij20kx04pt9c.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypq7esij20kx04pt9c.jpg" alt="img"></a></p><p>id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。</p><p>第二步：登录github,打开” settings”中的SSH Keys页面，然后点击“Add SSH Key”,填上任意title，在Key文本框里黏贴id_rsa.pub文件的内容。</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyppfdu3j20vh0nwdl0.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyppfdu3j20vh0nwdl0.jpg" alt="img"></a></p><p>点击 Add Key，你就应该可以看到已经添加的key。</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypnrj0cj20l60ad75p.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloypnrj0cj20l60ad75p.jpg" alt="img"></a></p><ol><li>如何添加远程库？</li></ol><p>​     现在的情景是：我们已经在本地创建了一个Git仓库后，又想在github创建一个Git仓库，并且希望这两个仓库进行远程同步，这样github的仓库可以作为备份，又可以其他人通过该仓库来协作。</p><p>  首先，登录github上，然后在右上角找到“create a new repo”创建一个新的仓库。如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypmocbsj20u40gttbc.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypmocbsj20u40gttbc.jpg" alt="img"></a></p><p>在Repository name填入<code>testgit</code>，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypm6o2gj20si0idwh4.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypm6o2gj20si0idwh4.jpg" alt="img"></a></p><p>  目前，在GitHub上的这个<code>testgit</code>仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。</p><p>现在，我们根据GitHub的提示，在本地的<code>testgit</code>仓库下运行命令：</p><p>git remote add origin <a href="https://github.com/tugenhua0707/testgit.git" target="_blank" rel="noopener">https://github.com/tugenhua0707/testgit.git</a></p><p>所有的如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypk8b34j20hk070764.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypk8b34j20hk070764.jpg" alt="img"></a></p><p>把本地库的内容推送到远程，使用 git push命令，实际上是把当前分支master推送到远程。</p><p>由于远程库是空的，我们第一次推送master分支时，加上了 –u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。推送成功后，可以立刻在github页面中看到远程库的内容已经和本地一模一样了，上面的要输入github的用户名和密码如下所示：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypjhn5ij20t40i7mzp.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypjhn5ij20t40i7mzp.jpg" alt="img"></a></p><p>从现在起，只要本地作了提交，就可以通过如下命令：</p><p>git push origin master</p><p>把本地master分支的最新修改推送到github上了，现在你就拥有了真正的分布式版本库了。</p><p>\2. 如何从远程库克隆？</p><p>上面我们了解了先有本地库，后有远程库时候，如何关联远程库。</p><p>现在我们想，假如远程库有新的内容了，我想克隆到本地来 如何克隆呢？</p><p>首先，登录github，创建一个新的仓库，名字叫testgit2.如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyphv15sj20t10gs775.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyphv15sj20t10gs775.jpg" alt="img"></a></p><p>如下，我们看到：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypexzvuj20ss0dgabs.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloypexzvuj20ss0dgabs.jpg" alt="img"></a></p><p>现在，远程库已经准备好了，下一步是使用命令git clone克隆一个本地库了。如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypec5t0j20hp03jwf6.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypec5t0j20hp03jwf6.jpg" alt="img"></a></p><p>接着在我本地目录下 生成testgit2目录了，如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypdbpwnj20jt05hmxr.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypdbpwnj20jt05hmxr.jpg" alt="img"></a></p><p>六：创建与合并分支。</p><p>在  版本回填退里，你已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。</p><p>首先，我们来创建dev分支，然后切换到dev分支上。如下操作：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypab36sj20bc04nweu.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloypab36sj20bc04nweu.jpg" alt="img"></a></p><p>git checkout 命令加上 –b参数表示创建并切换，相当于如下2条命令</p><p>git branch dev</p><p>git checkout dev</p><p>git branch查看分支，会列出所有的分支，当前分支前面会添加一个星号。然后我们在dev分支上继续做demo，比如我们现在在readme.txt再增加一行 7777777777777</p><p>首先我们先来查看下readme.txt内容，接着添加内容77777777，如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp9es90j20at0awjsq.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp9es90j20at0awjsq.jpg" alt="img"></a></p><p>现在dev分支工作已完成，现在我们切换到主分支master上，继续查看readme.txt内容如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp8mng3j20hm05qaav.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyp8mng3j20hm05qaav.jpg" alt="img"></a></p><p>现在我们可以把dev分支上的内容合并到分支master上了，可以在master分支上，使用如下命令 git merge dev 如下所示：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp83uksj20es073gmi.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp83uksj20es073gmi.jpg" alt="img"></a></p><p>git merge命令用于合并指定分支到当前分支上，合并后，再查看readme.txt内容，可以看到，和dev分支最新提交的是完全一样的。</p><p>注意到上面的<em>Fast-forward</em>信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。</p><p>合并完成后，我们可以接着删除dev分支了，操作如下：</p><p>总结创建与合并分支命令如下：</p><p>  查看分支：git branch</p><p>  创建分支：git branch name</p><p>  切换分支：git checkout name</p><p>创建+切换分支：git checkout –b name</p><p>合并某分支到当前分支：git merge name</p><p>删除分支：git branch –d name</p><h4 id="如何解决冲突"><a href="#如何解决冲突" class="headerlink" title="如何解决冲突"></a>如何解决冲突</h4><p>下面我们还是一步一步来，先新建一个新分支，比如名字叫fenzhi1，在readme.txt添加一行内容8888888，然后提交，如下所示：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp4jq8yj20ft0cu40a.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp4jq8yj20ft0cu40a.jpg" alt="img"></a></p><p>同样，我们现在切换到master分支上来，也在最后一行添加内容，内容为99999999，如下所示：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp3w0l1j20g80dwmz7.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp3w0l1j20g80dwmz7.jpg" alt="img"></a></p><p>现在我们需要在master分支上来合并fenzhi1，如下操作：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp1wo2ij20hm0gddi9.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyp1wo2ij20hm0gddi9.jpg" alt="img"></a></p><p>Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，其中&lt;&lt;&lt;HEAD是指主分支修改的内容，&gt;&gt;&gt;&gt;&gt;fenzhi1 是指fenzhi1上修改的内容，我们可以修改下如下后保存：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp11x4zj20g107e3zd.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp11x4zj20g107e3zd.jpg" alt="img"></a></p><p>如果我想查看分支合并的情况的话，需要使用命令 git log.命令行演示如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp0aj6uj20dt0o5gph.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyp0aj6uj20dt0o5gph.jpg" alt="img"></a></p><h4 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h4><p>通常合并分支时，git一般使用”Fast forward”模式，在这种模式下，删除分支后，会丢掉分支信息，现在我们来使用带参数 –no-ff来禁用”Fast forward”模式。首先我们来做demo演示下：</p><ol><li>创建一个dev分支。</li><li>修改readme.txt内容。</li><li>添加到暂存区。</li><li>切换回主分支(master)。</li><li>合并dev分支，使用命令 git merge –no-ff -m “注释” dev</li><li>查看历史记录</li></ol><p>截图如下：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoz5m31j20gr0lon0y.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoz5m31j20gr0lon0y.jpg" alt="img"></a></p><p><strong>分支策略：</strong>首先master主分支应该是非常稳定的，也就是用来发布新版本，一般情况下不允许在上面干活，干活一般情况下在新建的dev分支上干活，干完后，比如上要发布，或者说dev分支代码稳定后可以合并到主分支master上来。</p><p>七：bug分支：</p><p>   在开发中，会经常碰到bug问题，那么有了bug就需要修复，在Git中，分支是很强大的，每个bug都可以通过一个临时分支来修复，修复完成后，合并分支，然后将临时的分支删除掉。</p><p>比如我在开发中接到一个404 bug时候，我们可以创建一个404分支来修复它，但是，当前的dev分支上的工作还没有提交。比如如下：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoy0x5yj20he04m74v.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoy0x5yj20he04m74v.jpg" alt="img"></a></p><p>  并不是我不想提交，而是工作进行到一半时候，我们还无法提交，比如我这个分支bug要2天完成，但是我issue-404 bug需要5个小时内完成。怎么办呢？还好，Git还提供了一个stash功能，可以把当前工作现场 ”隐藏起来”，等以后恢复现场后继续工作。如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoxn4t8j20i3058dgo.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoxn4t8j20i3058dgo.jpg" alt="img"></a></p><p>  所以现在我可以通过创建issue-404分支来修复bug了。</p><p>首先我们要确定在那个分支上修复bug，比如我现在是在主分支master上来修复的，现在我要在master分支上创建一个临时分支，演示如下：</p><p> <a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyowmdooj20gp0etq55.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyowmdooj20gp0etq55.jpg" alt="img"></a></p><p>修复完成后，切换到master分支上，并完成合并，最后删除issue-404分支。演示如下：</p><p>现在，我们回到dev分支上干活了。</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyou5898j20bq03s0t6.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyou5898j20bq03s0t6.jpg" alt="img"></a></p><p>工作区是干净的，那么我们工作现场去哪里呢？我们可以使用命令 git stash list来查看下。如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyot6ny2j20c202lmxg.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyot6ny2j20c202lmxg.jpg" alt="img"></a></p><p>工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，可以使用如下2个方法：</p><ol><li>git stash apply恢复，恢复后，stash内容并不删除，你需要使用命令git stash drop来删除。</li><li>另一种方式是使用git stash pop,恢复的同时把stash内容也删除了。</li></ol><p>​     演示如下</p><p> <a href="https://images2015.cnblogs.com/blog/762349/201610/762349-20161026134059296-2019917854.png" target="_blank" rel="noopener"><img src="https://images2015.cnblogs.com/blog/762349/201610/762349-20161026134059296-2019917854.png" alt="img"></a></p><h4 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h4><p>当你从远程库克隆时候，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且远程库的默认名称是origin。</p><ol><li>要查看远程库的信息 使用 git remote</li><li>要查看远程库的详细信息 使用 git remote –v</li></ol><p>如下演示：</p><p> <a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyor8ayjj20h704pt9e.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyor8ayjj20h704pt9e.jpg" alt="img"></a></p><p><strong>一：推送分支：</strong></p><p>   推送分支就是把该分支上所有本地提交到远程库中，推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上：</p><p>   使用命令 git push origin master</p><p>比如我现在的github上的readme.txt代码如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyoolky9j20n00crt9x.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyoolky9j20n00crt9x.jpg" alt="img"></a></p><p>本地的readme.txt代码如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoljbdoj20bp05p74u.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoljbdoj20bp05p74u.jpg" alt="img"></a></p><p>现在我想把本地更新的readme.txt代码推送到远程库中，使用命令如下：</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoklccxj20f105nmy8.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoklccxj20f105nmy8.jpg" alt="img"></a></p><p>我们可以看到如上，推送成功，我们可以继续来截图github上的readme.txt内容 如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyojp2l7j20mi0dgdh4.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyojp2l7j20mi0dgdh4.jpg" alt="img"></a></p><p>可以看到 推送成功了，如果我们现在要推送到其他分支，比如dev分支上，我们还是那个命令 git push origin dev</p><p>那么一般情况下，那些分支要推送呢？</p><ol><li><p>master分支是主分支，因此要时刻与远程同步。</p></li><li><p>一些修复bug分支不需要推送到远程去，可以先合并到主分支上，然后把主分支master推送到远程去。</p></li><li><h4 id="抓取分支"><a href="#抓取分支" class="headerlink" title="抓取分支"></a>抓取分支</h4></li><li><p>多人协作时，大家都会往master分支上推送各自的修改。现在我们可以模拟另外一个同事，可以在另一台电脑上（注意要把SSH key添加到github上）或者同一台电脑上另外一个目录克隆，新建一个目录名字叫testgit2</p><p>但是我首先要把dev分支也要推送到远程去，如下</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoilae8j20dz047jrw.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoilae8j20dz047jrw.jpg" alt="img"></a></p><p>接着进入testgit2目录，进行克隆远程的库到本地来，如下：</p><p> <a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyofwtkzj20e404qdgn.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyofwtkzj20e404qdgn.jpg" alt="img"></a></p><p>现在目录下生成有如下所示：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoerppxj20jy07475a.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyoerppxj20jy07475a.jpg" alt="img"></a></p><p>现在我们的小伙伴要在dev分支上做开发，就必须把远程的origin的dev分支到本地来，于是可以使用命令创建本地dev分支：git checkout –b dev origin/dev</p><p>现在小伙伴们就可以在dev分支上做开发了，开发完成后把dev分支推送到远程库时。</p><p>如下：</p><p><a href="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyodj3j4j20gq0katc9.jpg" target="_blank" rel="noopener"><img src="http://ww1.sinaimg.cn/mw690/6941baebgw1eloyodj3j4j20gq0katc9.jpg" alt="img"></a></p><p>小伙伴们已经向origin/dev分支上推送了提交，而我在我的目录文件下也对同样的文件同个地方作了修改，也试图推送到远程库时，如下：</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyocm8nlj20hz0l3jvp.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyocm8nlj20hz0l3jvp.jpg" alt="img"></a></p><p>由上面可知：推送失败，因为我的小伙伴最新提交的和我试图推送的有冲突，解决的办法也很简单，上面已经提示我们，先用git pull把最新的提交从origin/dev抓下来，然后在本地合并，解决冲突，再推送。</p><p><a href="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoblpvij20gi07ugmx.jpg" target="_blank" rel="noopener"><img src="http://ww2.sinaimg.cn/mw690/6941baebgw1eloyoblpvij20gi07ugmx.jpg" alt="img"></a></p><p><em>git pull</em>也失败了，原因是没有指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接：如下：</p><p><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyoab9gfj20hy05j0tu.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyoab9gfj20hy05j0tu.jpg" alt="img"></a></p><p>这回<em>git pull</em>成功，但是合并有冲突，需要手动解决，解决的方法和分支管理中的 解决冲突完全一样。解决后，提交，再push：</p><p>我们可以先来看看readme.txt内容了。</p><p><a href="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyo7l3o6j20ef07p74y.jpg" target="_blank" rel="noopener"><img src="http://ww4.sinaimg.cn/mw690/6941baebgw1eloyo7l3o6j20ef07p74y.jpg" alt="img"></a></p><p>现在手动已经解决完了，我接在需要再提交，再push到远程库里面去。如下所示：<br><a href="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyo5em1aj20gt0dcwgv.jpg" target="_blank" rel="noopener"><img src="http://ww3.sinaimg.cn/mw690/6941baebgw1eloyo5em1aj20gt0dcwgv.jpg" alt="img"></a></p><p>因此：多人协作工作模式一般是这样的：</p><ol><li><p>首先，可以试图用git push origin branch-name推送自己的修改.</p></li><li><p>如果推送失败，则因为远程分支比你的本地更新早，需要先用git pull试图合并。</p></li><li><p>如果合并有冲突，则需要解决冲突，并在本地提交。再用git push origin branch-name推送。</p></li><li><h4 id="Git-基本常用命令如下"><a href="#Git-基本常用命令如下" class="headerlink" title="Git**基本常用命令如下**"></a><strong>Git**</strong>基本常用命令如下**</h4></li><li><p>mkdir：     XX (创建一个空目录 XX指目录名)</p><p>pwd：     显示当前目录的路径。</p><p>git init     把当前的目录变成可以管理的git仓库，生成隐藏.git文件。</p><p>git add XX    把xx文件添加到暂存区去。</p><p>git commit –m “XX” 提交文件 –m 后面的是注释。</p><p>git status    查看仓库状态</p><p>git diff XX   查看XX文件修改了那些内容</p><p>git log     查看历史记录</p><p>git reset –hard HEAD^ 或者 git reset –hard HEAD~ 回退到上一个版本</p><p>​            (如果想回退到100个版本，使用git reset –hard HEAD~100 )</p><p>cat XX     查看XX文件内容</p><p>git reflog    查看历史记录的版本号id</p><p>git checkout — XX 把XX文件在工作区的修改全部撤销。</p><p>git rm XX     删除XX文件</p><p>git remote add origin <a href="https://github.com/ev-power/XiaoYong" target="_blank" rel="noopener">https://github.com/ev-power/XiaoYong</a> 关联一个远程库</p><p>git push –u(第一次要用-u 以后不需要) origin master 把当前master分支推送到远程库</p><p>git clone <a href="https://github.com/ev-power/XiaoYong" target="_blank" rel="noopener">https://github.com/ev-power/XiaoYong</a> 从远程库中克隆</p><p>git checkout –b dev 创建dev分支 并切换到dev分支上</p><p>git branch 查看当前所有的分支</p><p>git checkout master 切换回master分支</p><p>git merge dev  在当前的分支上合并dev分支</p><p>git branch –d dev 删除dev分支</p><p>git branch name 创建分支</p><p>git stash 把当前的工作隐藏起来 等以后恢复现场后继续工作</p><p>git stash list 查看所有被隐藏的文件列表</p><p>git stash apply 恢复被隐藏的文件，但是内容不删除</p><p>git stash drop 删除文件</p><p>git stash pop 恢复文件的同时 也删除文件</p><p>git remote 查看远程库的信息</p><p>git remote –v 查看远程库的详细信息</p><p>git push origin master Git会把master分支推送到远程库对应的远程分支上   </p></li><li><p>本文非原创博客，部分内容有所更改，原文出自：<a href="http://www.cnblogs.com/tugenhua0707/p/4050072.html" target="_blank" rel="noopener">http://www.cnblogs.com/tugenhua0707/p/4050072.html</a></p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;讨论了许多种git的情况，非常详细的git报告&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2gy1gds54g3odrj206402kmwz.jpg&quot; alt=&quot;undefined&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Git" scheme="http://yoursite.com/categories/Git/"/>
    
    
      <category term="Git" scheme="http://yoursite.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>大数据实时计算引擎 Flink 实战与性能优化</title>
    <link href="http://yoursite.com/2020/04/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%20Flink%20%E5%AE%9E%E6%88%98%E4%B8%8E%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2020/04/09/大数据实时计算引擎 Flink 实战与性能优化/</id>
    <published>2020-04-08T16:42:16.000Z</published>
    <updated>2020-04-10T17:00:05.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据实时计算引擎-Flink-实战与性能优化"><a href="#大数据实时计算引擎-Flink-实战与性能优化" class="headerlink" title="大数据实时计算引擎 Flink 实战与性能优化"></a>大数据实时计算引擎 Flink 实战与性能优化</h1><blockquote><p>Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9a0z5j7i6j20bp04tdgk.jpg" alt="undefined"></p><a id="more"></a> <h2 id="一、公司到底需不需要引入实时计算引擎？"><a href="#一、公司到底需不需要引入实时计算引擎？" class="headerlink" title="一、公司到底需不需要引入实时计算引擎？"></a>一、公司到底需不需要引入实时计算引擎？</h2><h3 id="实时计算需求"><a href="#实时计算需求" class="headerlink" title="实时计算需求"></a>实时计算需求</h3><p>大数据发展至今，数据呈指数倍的增长，对实效性的要求也越来越高，所以你可能接触到下面这类需求会越来越多。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">小田，你看能不能做个监控大屏实时查看促销活动销售额（GMV）？</span><br><span class="line"></span><br><span class="line">小朱，搞促销活动的时候能不能实时统计下网站的 PV/UV 啊？</span><br><span class="line"></span><br><span class="line">小鹏，我们现在搞促销活动能不能实时统计销量 Top5 啊？</span><br><span class="line"></span><br><span class="line">小李，怎么回事啊？现在搞促销活动结果服务器宕机了都没告警，能不能加一个？</span><br><span class="line"></span><br><span class="line">小刘，服务器这会好卡，是不是出了什么问题啊，你看能不能做个监控大屏实时查看机器的运行情况？</span><br><span class="line"></span><br><span class="line">小赵，我们线上的应用频繁出现 Error 日志，但是只有靠人肉上机器查看才知道情况，能不能在出现错误的时候及时告警通知？</span><br><span class="line"></span><br><span class="line">小夏，我们 1 元秒杀促销活动中有件商品被某个用户薅了 100 件，怎么都没有风控啊？</span><br><span class="line"></span><br><span class="line">小宋，你看我们搞促销活动能不能根据每个顾客的浏览记录实时推荐不同的商品啊？</span><br><span class="line"></span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>那这些场景对应着什么业务需求呢？我们来总结下，大概如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plho80ksj20z50u042l.jpg" alt="undefined"></p><p>初看这些需求，是不是感觉很难？那么我们接下来来分析一下该怎么去实现？</p><p>从这些需求来看，最根本的业务都是需要<strong>实时查看数据信息</strong>，那么首先我们得想想如何去采集这些实时数据，然后将采集的实时数据进行实时的计算，最后将计算后的结果下发到第三方。</p><h3 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h3><p>就上面这些需求，我们需要采集些什么数据呢？</p><ol><li>买家搜索记录信息</li><li>买家浏览的商品信息</li><li>买家下单订单信息</li><li>网站的所有浏览记录</li><li>机器 CPU/MEM/IO 信息</li><li>应用日志信息</li></ol><h3 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h3><p>采集后的数据实时上报后，需要做实时的计算，那我们怎么实现计算呢？</p><ol><li>计算所有商品的总销售额</li><li>统计单个商品的销量，最后求 Top5</li><li>关联用户信息和浏览信息、下单信息</li><li>统计网站所有的请求 IP 并统计每个 IP 的请求数量</li><li>计算一分钟内机器 CPU/MEM/IO 的平均值、75 分位数值</li><li>过滤出 Error 级别的日志信息</li></ol><h3 id="数据实时下发"><a href="#数据实时下发" class="headerlink" title="数据实时下发"></a>数据实时下发</h3><p>实时计算后的数据，需要及时的下发到下游，这里说的下游代表可能是：</p><ol><li>告警方式（邮件、短信、钉钉、微信）</li></ol><p>在计算层会将计算结果与阈值进行比较，超过阈值触发告警，让运维提前收到通知，及时做好应对措施，减少故障的损失大小。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plhzw2umj210c0dsdh7.jpg" alt="undefined"></p><ol><li>存储（消息队列、DB、文件系统等）</li></ol><p>数据存储后，监控大盘（Dashboard）从存储（ElasticSearch、HBase 等）里面查询对应指标的数据就可以查看实时的监控信息，做到对促销活动的商品销量、销售额，机器 CPU、MEM 等有实时监控，运营、运维、开发、领导都可以实时查看并作出对应的措施。</p><ul><li>让运营知道哪些商品是爆款，哪些店铺成交额最多，哪些商品成交额最高，哪些商品浏览量最多；</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pli7o9s7j20gm12540a.jpg" alt="undefined"></p><ul><li>让运维可以时刻了解机器的运行状况，出现宕机或者其他不稳定情况可以及时处理；</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plijpowcj214o0u0jv0.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plio676aj21em0u0gqt.jpg" alt="undefined"></p><ul><li>让开发知道自己项目运行的情况，从 Error 日志知道出现了哪些 Bug；</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pliw2d21j217f0u0tdh.jpg" alt="undefined"></p><ul><li>让领导知道这次促销赚了多少 money。</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plj8y8wdj20p00goacz.jpg" alt="undefined"></p><p><strong>从数据采集到数据计算再到数据下发，整个流程在上面的场景对实时性要求还是很高的，任何一个地方出现问题都将影响最后的效果！</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pljh92jpj216l0u00wg.jpg" alt="undefined"></p><h3 id="实时计算场景"><a href="#实时计算场景" class="headerlink" title="实时计算场景"></a>实时计算场景</h3><p>前面说了这么多场景，这里我们总结一下实时计算常用的场景有哪些呢？</p><ol><li>交通信号灯数据</li><li>道路上车流量统计（拥堵状况）</li><li>公安视频监控</li><li>服务器运行状态监控</li><li>金融证券公司实时跟踪股市波动，计算风险价值</li><li>数据实时 ETL</li><li>银行或者支付公司涉及金融盗窃的预警</li></ol><p>……</p><p>另外自己还做过调研，实时计算框架的使用场景有如下这些：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkbpo1mj20u00ymq6n.jpg" alt="undefined"></p><p>总结一下大概有下面这四类：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkidal4j21as0ps0va.jpg" alt="undefined"></p><ol><li>实时数据存储</li></ol><p>实时数据存储的时候做一些微聚合、过滤某些字段、数据脱敏，组建数据仓库，实时 ETL。</p><ol start="2"><li>实时数据分析</li></ol><p>实时数据接入机器学习框架（TensorFlow）或者一些算法进行数据建模、分析，然后动态的给出商品推荐、广告推荐</p><ol start="3"><li>实时监控告警</li></ol><p>金融相关涉及交易、实时风控、车流量预警、服务器监控告警、应用日志告警</p><ol start="4"><li>实时数据报表</li></ol><p>活动营销时销售额/销售量大屏，TopN 商品</p><p>说到实时计算，这里不得不讲一下和传统的离线计算的区别！</p><h3 id="离线计算-vs-实时计算"><a href="#离线计算-vs-实时计算" class="headerlink" title="离线计算 vs 实时计算"></a>离线计算 vs 实时计算</h3><p>再讲这两个区别之前，我们先来看看流处理和批处理的区别：</p><h4 id="流处理与批处理"><a href="#流处理与批处理" class="headerlink" title="流处理与批处理"></a>流处理与批处理</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pllqzkewj219e0q0gnf.jpg" alt="undefined"></p><p>看完流处理与批处理这两者的区别之后，我们来抽象一下前面文章的场景需求（<strong>实时计算</strong>）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm1lerpj219s0e6759.jpg" alt="undefined"></p><p>实时计算需要不断的从 MQ 中读取采集的数据，然后处理计算后往 DB 里存储，在计算这层你无法感知到会有多少数据量过来、要做一些简单的操作（过滤、聚合等）、及时将数据下发。</p><p>相比传统的<strong>离线计算</strong>，它却是这样的：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm92vsuj219m0fudgu.jpg" alt="undefined"></p><p>在计算这层，它从 DB（不限 MySQL，还有其他的存储介质）里面读取数据，该数据一般就是固定的（前一天、前一星期、前一个月），然后再做一些复杂的计算或者统计分析，最后生成可供直观查看的报表（dashboard）。</p><h4 id="离线计算的特点"><a href="#离线计算的特点" class="headerlink" title="离线计算的特点"></a>离线计算的特点</h4><ol><li>数据量大且时间周期长（一天、一星期、一个月、半年、一年）</li><li>在大量数据上进行复杂的批量运算</li><li>数据在计算之前已经固定，不再会发生变化</li><li>能够方便的查询批量计算的结果</li></ol><h4 id="实时计算的特点"><a href="#实时计算的特点" class="headerlink" title="实时计算的特点"></a>实时计算的特点</h4><p>在大数据中与离线计算对应的则是实时计算，那么实时计算有什么特点呢？由于应用场景的各不相同，所以这两种计算引擎接收数据的方式也不太一样：离线计算的数据是固定的（不再会发生变化），通常离线计算的任务都是定时的，如：每天晚上 0 点的时候定时计算前一天的数据，生成报表；然而实时计算的数据源却是流式的。</p><p>这里我不得不讲讲什么是流式数据呢？我的理解是比如你在淘宝上下单了某个商品或者点击浏览了某件商品，你就会发现你的页面立马就会给你推荐这种商品的广告和类似商品的店铺，这种就是属于实时数据处理然后作出相关推荐，这类数据需要不断的从你在网页上的点击动作中获取数据，之后进行实时分析然后给出推荐。</p><h4 id="流式数据的特点"><a href="#流式数据的特点" class="headerlink" title="流式数据的特点"></a>流式数据的特点</h4><ol><li>数据实时到达</li><li>数据到达次序独立，不受应用系统所控制</li><li>数据规模大且无法预知容量</li><li>原始数据一经处理，除非特意保存，否则不能被再次取出处理，或者再次提取数据代价昂贵</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plmoolb2j219s0ts41e.jpg" alt="undefined"></p><h4 id="实时计算的优势"><a href="#实时计算的优势" class="headerlink" title="实时计算的优势"></a>实时计算的优势</h4><p><strong>实时计算一时爽，一直实时计算一直爽</strong>，对于持续生成最新数据的场景，采用流数据处理是非常有利的。例如，再监控服务器的一些运行指标的时候，能根据采集上来的实时数据进行判断，当超出一定阈值的时候发出警报，进行提醒作用。再如通过处理流数据生成简单的报告，如五分钟的窗口聚合数据平均值。复杂的事情还有在流数据中进行数据多维度关联、聚合、塞选，从而找到复杂事件中的根因。更为复杂的是做一些复杂的数据分析操作，如应用机器学习算法，然后根据算法处理后的数据结果提取出有效的信息，作出、给出不一样的推荐内容，让不同的人可以看见不同的网页（千人千面）。</p><h3 id="实时计算面临的挑战"><a href="#实时计算面临的挑战" class="headerlink" title="实时计算面临的挑战"></a>实时计算面临的挑战</h3><ol><li>数据处理唯一性（如何保证数据只处理一次？至少一次？最多一次？）</li><li>数据处理的及时性（采集的实时数据量太大的话可能会导致短时间内处理不过来，如何保证数据能够及时的处理，不出现数据堆积？）</li><li>数据处理层和存储层的可扩展性（如何根据采集的实时数据量的大小提供动态扩缩容？）</li><li>数据处理层和存储层的容错性（如何保证数据处理层和存储层高可用，出现故障时数据处理层和存储层服务依旧可用？）</li></ol><p>因为各种需求，也就造就了现在不断出现实时计算框架，在 1.2 节中将重磅介绍如今最火的实时计算框架 —— Flink，在 1.3 节中会对比介绍 Spark Streaming、Structured Streaming 和 Storm 之间的区别。</p><h3 id="小结与反思"><a href="#小结与反思" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节从实时计算的需求作为切入点，然后分析该如何去完成这种实时计算的需求，从而得知整个过程包括数据采集、数据计算、数据存储等，接着总结了实时计算场景的类型。最后开始介绍离线计算与实时计算的区别，并提出了实时计算可能带来的挑战。你们公司有文中所讲的类似需求吗？你是怎么解决的呢？</p><hr><h2 id="二、彻底了解大数据实时计算框架-Flink"><a href="#二、彻底了解大数据实时计算框架-Flink" class="headerlink" title="二、彻底了解大数据实时计算框架 Flink"></a>二、彻底了解大数据实时计算框架 Flink</h2><p>在 1.1 节中讲解了日常开发常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。</p><ul><li>第一代：Hadoop 承载的 MapReduce</li><li>第二代：支持 DAG（有向无环图）框架的计算引擎 Tez 和 Oozie，主要还是批处理任务</li><li>第三代：支持 Job 内部的 DAG（有向无环图），以 Spark 为代表</li><li>第四代：大数据统一计算引擎，包括流处理、批处理、AI、Machine Learning、图计算等，以 Flink 为代表</li></ul><p>或许会有人不同意以上的分类，笔者觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个。</p><p>本文将对 Flink 的整体架构和 Flink 的多种特性做个详细的介绍！在讲 Flink 之前的话，我们先来看看<strong>数据集类型</strong>和<strong>数据运算模型</strong>的种类。</p><h4 id="数据集类型"><a href="#数据集类型" class="headerlink" title="数据集类型"></a>数据集类型</h4><ul><li>无穷数据集：无穷的持续集成的数据集合</li><li>有界数据集：有限不会改变的数据集合</li></ul><p>那么那些常见的无穷数据集有哪些呢？</p><ul><li>用户与客户端的实时交互数据</li><li>应用实时产生的日志</li><li>金融市场的实时交易记录</li><li>…</li></ul><h4 id="数据运算模型"><a href="#数据运算模型" class="headerlink" title="数据运算模型"></a>数据运算模型</h4><ul><li>流式：只要数据一直在产生，计算就持续地进行</li><li>批处理：在预先定义的时间内运行计算，当计算完成时释放计算机资源</li></ul><p>那么我们再来看看 Flink 它是什么呢？</p><h3 id="Flink-是什么？"><a href="#Flink-是什么？" class="headerlink" title="Flink 是什么？"></a>Flink 是什么？</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr0788dj214a0gy755.jpg" alt="undefined"></p><p>Flink 是一个针对流数据和批数据的分布式处理引擎，代码主要是由 Java 实现，部分代码是 Scala。它可以处理有界的批量数据集、也可以处理无界的实时数据集。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已，所以 Flink 也是一款真正的流批统一的计算引擎。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr9ox1fj21gt0u0gp2.jpg" alt="undefined"></p><p>Flink 提供了 State、Checkpoint、Time、Window 等，它们为 Flink 提供了基石，本篇文章下面会稍作讲解，具体深度分析后面会有专门的文章来讲解。</p><h3 id="Flink-整体架构"><a href="#Flink-整体架构" class="headerlink" title="Flink 整体架构"></a>Flink 整体架构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plrjlud5j20lp0cuabw.jpg" alt="undefined"></p><p>从下至上：</p><ol><li>部署：Flink 支持本地运行（IDE 中直接运行程序）、能在独立集群（Standalone 模式）或者在被 YARN、Mesos、K8s 管理的集群上运行，也能部署在云上。</li><li>运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。</li><li>API：DataStream、DataSet、Table、SQL API。</li><li>扩展库：Flink 还包括用于 CEP（复杂事件处理）、机器学习、图形处理等场景。</li></ol><h3 id="Flink-支持多种方式部署"><a href="#Flink-支持多种方式部署" class="headerlink" title="Flink 支持多种方式部署"></a>Flink 支持多种方式部署</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pls0gqd0j22vw17qn0l.jpg" alt="undefined"></p><p>作为一个计算引擎，如果要做的足够完善，除了它自身的各种特点要包含，还得支持各种生态圈，比如部署的情况，Flink 是支持以 Standalone、YARN、Kubernetes、Mesos 等形式部署的。</p><ul><li>Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群</li><li>Standalone：在 Flink 目录下执行 <code>bin/start-cluster.sh</code> 脚本则会启动一个 Standalone 模式的集群</li><li>YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下：</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plt1pm8wj21880kot9g.jpg" alt="undefined"></p><ul><li>Kubernetes：Kubernetes 是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性，Flink 也支持部署在 Kubernetes 上，在 <a href="https://github.com/Aleksandr-Filichkin/flink-k8s/blob/master/flow.jpg" target="_blank" rel="noopener">GitHub</a> 看到有下面这种运行架构的。</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pltiz3r3j20ph0lbwfh.jpg" alt="undefined"></p><p>通常上面四种居多，另外还支持 AWS、MapR、Aliyun OSS 等。</p><h3 id="Flink-分布式运行"><a href="#Flink-分布式运行" class="headerlink" title="Flink 分布式运行"></a>Flink 分布式运行</h3><p>Flink 作业提交架构流程可见下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plu1ecy0j20op0fpad9.jpg" alt="undefined"></p><p>1、Program Code：我们编写的 Flink 应用程序代码</p><p>2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户</p><p>3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理 checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件</p><p>4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽（Slot 个数）决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plud1dkhj218i0iw78b.jpg" alt="undefined"></p><p>Flink 提供了不同的抽象级别的 API 以开发流式或批处理应用。</p><ul><li>最底层提供了有状态流。它将通过 Process Function 嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致性、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。</li><li>DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换或者计算。</li><li>Table API 是以表为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。 你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。</li><li>Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。</li></ul><p>Flink 除了 DataStream 和 DataSet API，它还支持 Table/SQL API，Flink 也将通过 SQL API 来构建统一的大数据流批处理引擎，因为在公司中通常会有那种每天定时生成报表的需求（批处理的场景，每晚定时跑一遍昨天的数据生成一个结果报表），但是也是会有流处理的场景（比如采用 Flink 来做实时性要求很高的需求），于是慢慢的整个公司的技术选型就变得越来越多了，这样开发人员也就要面临着学习两套不一样的技术框架，运维人员也需要对两种不一样的框架进行环境搭建和作业部署，平时还要维护作业的稳定性。</p><p>当我们的系统变得越来越复杂了，作业越来越多了，这对于开发人员和运维来说简直就是噩梦，没准哪天凌晨晚上就被生产环境的告警电话给叫醒。所以 Flink 系统能通过 SQL API 来解决批流统一的痛点，这样不管是开发还是运维，他们只需要关注一个计算框架就行，从而减少企业的用人成本和后期开发运维成本。</p><h3 id="Flink-程序与数据流结构"><a href="#Flink-程序与数据流结构" class="headerlink" title="Flink 程序与数据流结构"></a>Flink 程序与数据流结构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pluob9z7j21a00us11g.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plutdjgyj21q20h6q3c.jpg" alt="undefined"></p><p>一个完整的 Flink 应用程序结构就是如上两图所示：</p><p>1、Source：数据输入，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</p><p>2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</p><p>3、Sink：数据输出，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。</p><h3 id="Flink-支持丰富的-Connector"><a href="#Flink-支持丰富的-Connector" class="headerlink" title="Flink 支持丰富的 Connector"></a>Flink 支持丰富的 Connector</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plv10mhgj215218utau.jpg" alt="undefined"></p><p>通过源码可以发现不同版本的 Kafka、不同版本的 ElasticSearch、Cassandra、HBase、Hive、HDFS、RabbitMQ 都是支持的，除了流应用的 Connector 是支持的，另外还支持 SQL。</p><p>再就是要考虑计算的数据来源和数据最终存储，因为 Flink 在大数据领域的的定位就是实时计算，它不做存储（虽然 Flink 中也有 State 去存储状态数据，这里说的存储类似于 MySQL、ElasticSearch 等存储），所以在计算的时候其实你需要考虑的是数据源来自哪里，计算后的结果又存储到哪里去。庆幸的是 Flink 目前已经支持大部分常用的组件了，比如在 Flink 中已经支持了如下这些 Connector：</p><ul><li>不同版本的 Kafka</li><li>不同版本的 ElasticSearch</li><li>Redis</li><li>MySQL</li><li>Cassandra</li><li>RabbitMQ</li><li>HBase</li><li>HDFS</li><li>…</li></ul><p>这些 Connector 除了支持流作业外，目前还有还有支持 SQL 作业的，除了这些自带的 Connector 外，还可以通过 Flink 提供的接口做自定义 Source 和 Sink（在 3.8 节中）。</p><h3 id="Flink-提供事件时间-amp-处理时间语义"><a href="#Flink-提供事件时间-amp-处理时间语义" class="headerlink" title="Flink 提供事件时间&amp;处理时间语义"></a>Flink 提供事件时间&amp;处理时间语义</h3><p>Flink 支持多种 Time，比如 Event time、Ingestion Time、Processing Time，后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析</a> 中会很详细的讲解 Flink 中 Time 的概念。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plwkg73tj21li0u075w.jpg" alt="undefined"></p><h3 id="Flink-提供灵活的窗口机制"><a href="#Flink-提供灵活的窗口机制" class="headerlink" title="Flink 提供灵活的窗口机制"></a>Flink 提供灵活的窗口机制</h3><p>Flink 支持多种 Window，比如 Time Window、Count Window、Session Window，还支持自定义 Window。后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">如何使用 Flink Window 及 Window 基本概念与实现原理</a> 中会很详细的讲解 Flink 中 Window 的概念。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plx9wf35j22kg1bs0uc.jpg" alt="undefined"></p><h3 id="Flink-并行的执行任务"><a href="#Flink-并行的执行任务" class="headerlink" title="Flink 并行的执行任务"></a>Flink 并行的执行任务</h3><p>Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为 operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行； operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为 1：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plxlthw1j20jb0crace.jpg" alt="undefined"></p><h3 id="Flink-支持状态存储和容错"><a href="#Flink-支持状态存储和容错" class="headerlink" title="Flink 支持状态存储和容错"></a>Flink 支持状态存储和容错</h3><p>Flink 是一款有状态的流处理框架，它提供了丰富的状态访问接口，按照数据的划分方式，可以分为 Keyed State 和 Operator State，在 Keyed State 中又提供了多种数据结构：</p><ul><li>ValueState</li><li>MapState</li><li>ListState</li><li>ReducingState</li><li>AggregatingState</li></ul><p>另外状态存储也支持多种方式：</p><ul><li>MemoryStateBackend：存储在内存中</li><li>FsStateBackend：存储在文件中</li><li>RocksDBStateBackend：存储在 RocksDB 中</li></ul><p>Flink 中支持使用 Checkpoint 来提高程序的可靠性，开启了 Checkpoint 之后，Flink 会按照一定的时间间隔对程序的运行状态进行备份，当发生故障时，Flink 会将所有任务的状态恢复至最后一次发生 Checkpoint 中的状态，并从那里开始重新开始执行。</p><p>另外 Flink 还支持根据 Savepoint 从已停止作业的运行状态进行恢复，这种方式需要通过命令进行触发。</p><h3 id="Flink-实现了自己的内存管理机制"><a href="#Flink-实现了自己的内存管理机制" class="headerlink" title="Flink 实现了自己的内存管理机制"></a>Flink 实现了自己的内存管理机制</h3><p>//todo:深入内存到底要不要在第九章讲？ Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。我们在后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">深入探索 Flink 内存管理机制</a> 会深入讲解 Flink 里面的内存管理机制。</p><h3 id="Flink-支持多种扩展库"><a href="#Flink-支持多种扩展库" class="headerlink" title="Flink 支持多种扩展库"></a>Flink 支持多种扩展库</h3><p>Flink 扩展库中含有机器学习、Gelly 图形处理、CEP 复杂事件处理、State Processing API 等，关于这块内容可以在第六章查看。</p><h3 id="小结与反思-1"><a href="#小结与反思-1" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节在开始介绍 Flink 之前先讲解了下数据集类型和数据运算模型，接着开始介绍 Flink 的各种特性，</p><hr><h2 id="三、大数据框架-Flink、Blink、Spark-Streaming、Structured-Streaming和-Storm-的区别。"><a href="#三、大数据框架-Flink、Blink、Spark-Streaming、Structured-Streaming和-Storm-的区别。" class="headerlink" title="三、大数据框架 Flink、Blink、Spark Streaming、Structured Streaming和 Storm 的区别。"></a>三、大数据框架 Flink、Blink、Spark Streaming、Structured Streaming和 Storm 的区别。</h2><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>Flink 是一个针对流数据和批数据分布式处理的引擎，在某些对实时性要求非常高的场景，基本上都是采用 Flink 来作为计算引擎，它不仅可以处理有界的批数据，还可以处理无界的流数据，在 Flink 的设计愿想就是将批处理当成是流处理的一种特例。</p><p>在 Flink 的母公司 <a href="https://www.eu-startups.com/2019/01/alibaba-takes-over-berlin-based-streaming-analytics-startup-data-artisans/" target="_blank" rel="noopener">Data Artisans 被阿里收购</a>之后，阿里也在开始逐步将内部的 Blink 代码开源出来并合并在 Flink 主分支上。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plzy3s4ej215e1g045u.jpg" alt="undefined"></p><p>而 Blink 一个很强大的特点就是它的 SQL API 很强大，社区也在 Flink 1.9 版本将 Blink 开源版本大部分代码合进了 Flink 主分支。</p><h3 id="Blink"><a href="#Blink" class="headerlink" title="Blink"></a>Blink</h3><p>Blink 是早期阿里在 Flink 的基础上开始修改和完善后在内部创建的分支，然后 Blink 目前在阿里服务于阿里集团内部搜索、推荐、广告、菜鸟物流等大量核心实时业务。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm0elnw7j20mn0d7t9i.jpg" alt="undefined"></p><p>Blink 在阿里内部错综复杂的业务场景中锻炼成长着，经历了内部这么多用户的反馈（各种性能、资源使用率、易用性等诸多方面的问题），Blink 都做了针对性的改进。在 Flink Forward China 峰会上，阿里巴巴集团副总裁周靖人宣布 Blink 在 2019 年 1 月正式开源，同时阿里也希望 Blink 开源后能进一步加深与 Flink 社区的联动，</p><p>Blink 开源地址：<a href="https://github.com/apache/flink/tree/blink" target="_blank" rel="noopener">https://github.com/apache/flink/tree/blink</a></p><p>开源版本 Blink 的主要功能和优化点：</p><p>1、Runtime 层引入 Pluggable Shuffle Architecture，开发者可以根据不同的计算模型或者新硬件的需要实现不同的 shuffle 策略进行适配；为了性能优化，Blink 可以让算子更加灵活的 chain 在一起，避免了不必要的数据传输开销；在 BroadCast Shuffle 模式中，Blink 优化掉了大量的不必要的序列化和反序列化开销；Blink 提供了全新的 JM FailOver 机制，JM 发生错误之后，新的 JM 会重新接管整个 JOB 而不是重启 JOB，从而大大减少了 JM FailOver 对 JOB 的影响；Blink 支持运行在 Kubernetes 上。</p><p>2、SQL/Table API 架构上的重构和性能的优化是 Blink 开源版本的一个重大贡献。</p><p>3、Hive 的兼容性，可以直接用 Flink SQL 去查询 Hive 的数据，Blink 重构了 Flink catalog 的实现，并且增加了两种 catalog，一个是基于内存存储的 FlinkInMemoryCatalog，另外一个是能够桥接 Hive metaStore 的 HiveCatalog。</p><p>4、Zeppelin for Flink</p><p>5、Flink Web，更美观的 UI 界面，查看日志和监控 Job 都变得更加方便</p><p>对于开源那会看到一个对话让笔者感到很震撼：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Blink 开源后，两个开源项目之间的关系会是怎样的？未来 Flink 和 Blink 也会由不同的团队各自维护吗？</span><br><span class="line"></span><br><span class="line">Blink 永远不会成为另外一个项目，如果后续进入 Apache 一定是成为 Flink 的一部分</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm107pqzj217k104jw6.jpg" alt="undefined"></p><p>在 Blink 开源那会，笔者就将源码自己编译了一份，然后自己在本地一直运行着，感兴趣的可以看看文章 <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/" target="_blank" rel="noopener">阿里巴巴开源的 Blink 实时计算框架真香</a> ，你会发现 Blink 的 UI 还是比较美观和实用的。</p><p>如果你还对 Blink 有什么疑问，可以看看下面两篇文章：</p><p><a href="https://www.infoq.cn/article/wZ_b7Hw9polQWp3mTwVh" target="_blank" rel="noopener">阿里重磅开源 Blink：为什么我们等了这么久？</a></p><p><a href="https://www.infoq.cn/article/ZkOGAl6_vkZDTk8tfbbg" target="_blank" rel="noopener">重磅！阿里巴巴 Blink 正式开源，重要优化点解读</a></p><h3 id="1-3-3-Spark"><a href="#1-3-3-Spark" class="headerlink" title="1.3.3 Spark"></a>1.3.3 Spark</h3><p>Apache Spark 是一种包含流处理能力的下一代批处理框架。与 Hadoop 的 MapReduce 引擎基于各种相同原则开发而来的 Spark 主要侧重于通过完善的内存计算和处理优化机制加快批处理工作负载的运行速度。</p><p>Spark 可作为独立集群部署（需要相应存储层的配合），或可与 Hadoop 集成并取代 MapReduce 引擎。</p><h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2ghwbrj20sg0g0gmg.jpg" alt="undefined"></p><p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">Spark Streaming</a> 是 Spark API 核心的扩展，可实现实时数据的快速扩展，高吞吐量，容错处理。数据可以从很多来源（如 Kafka、Flume、Kinesis 等）中提取，并且可以通过很多函数来处理这些数据，处理完后的数据可以直接存入数据库或者 Dashboard 等。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2mmmyoj20wk0c6js2.jpg" alt="undefined"></p><p><strong>Spark Streaming 的内部实现原理</strong>是接收实时输入数据流并将数据分成批处理，然后由 Spark 引擎处理以批量生成最终结果流，也就是常说的 micro-batch 模式。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2t6k21j20tr06naad.jpg" alt="undefined"></p><p>DStreams 是 Spark Streaming 提供的基本的抽象，它代表一个连续的数据流。。它要么是从源中获取的输入流，要么是输入流通过转换算子生成的处理后的数据流。在内部实现上，DStream 由连续的序列化 RDD 来表示，每个 RDD 含有一段时间间隔内的数据：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm30f5cqj20ub06nweq.jpg" alt="undefined"></p><p>任何对 DStreams 的操作都转换成了对 DStreams 隐含的 RDD 的操作。例如 flatMap 操作应用于 lines 这个 DStreams 的每个 RDD，生成 words 这个 DStreams 的 RDD 过程如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3b0i1qj20ub0asmxp.jpg" alt="undefined"></p><p>通过 Spark 引擎计算这些隐含 RDD 的转换算子。DStreams 操作隐藏了大部分的细节，并且为了更便捷，为开发者提供了更高层的 API。</p><p><strong>Spark 支持的滑动窗口</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3imkjuj20rm0asdg9.jpg" alt="undefined"></p><p>它和 Flink 的滑动窗口类似，支持传入两个参数，一个代表窗口长度，一个代表滑动间隔。</p><p><strong>Spark 支持更多的 API</strong></p><p>因为 Spark 是使用 Scala 开发的居多，所以从官方文档就可以看得到对 Scala 的 API 支持的很好，而 Flink 源码实现主要以 Java 为主，因此也对 Java API 更友好，从两者目前支持的 API 友好程度，应该是 Spark 更好，它目前也支持 Python API，但是 Flink 新版本也在不断的支持 Python API。</p><p><strong>Spark 支持更多的 Machine Learning Lib</strong></p><p>你可以很轻松的使用 Spark MLlib 提供的机器学习算法，然后将这些这些机器学习算法模型应用在流数据中，目前 Flink Machine Learning 这块的内容还较少，不过阿里宣称会开源些 Flink Machine Learning 算法，保持和 Spark 目前已有的算法一致，我自己在 GitHub 上看到一个阿里开源的仓库，感兴趣的可以看看 <a href="https://github.com/alibaba/flink-ai-extended" target="_blank" rel="noopener">flink-ai-extended</a>。</p><p><strong>Spark Checkpoint</strong></p><p>Spark 和 Flink 一样都支持 Checkpoint，但是 Flink 还支持 Savepoint，你可以在停止 Flink 作业的时候使用 Savepoint 将作业的状态保存下来，当作业重启的时候再从 Savepoint 中将停止作业那个时刻的状态恢复起来，保持作业的状态和之前一致。</p><p><strong>Spark SQL</strong></p><p>Spark 除了 DataFrames 和 Datasets 外，也还有 SQL API，这样你就可以通过 SQL 查询数据，另外 Spark SQL 还可以用于从 Hive 中读取数据。</p><p>从 Spark 官网也可以看到很多比较好的特性，这里就不一一介绍了，如果对 Spark 感兴趣的话也可以去<a href="https://spark.apache.org/docs/latest/index.html" target="_blank" rel="noopener">官网</a>了解一下具体的使用方法和实现原理。</p><p><strong>Spark Streaming 优缺点</strong></p><p>1、优点</p><ul><li>Spark Streaming 内部的实现和调度方式高度依赖 Spark 的 DAG 调度器和 RDD，这就决定了 Spark Streaming 的设计初衷必须是粗粒度方式的，也就无法做到真正的实时处理</li><li>Spark Streaming 的粗粒度执行方式使其确保“处理且仅处理一次”的特性，同时也可以更方便地实现容错恢复机制。</li><li>由于 Spark Streaming 的 DStream 本质是 RDD 在流式数据上的抽象，因此基于 RDD 的各种操作也有相应的基于 DStream 的版本，这样就大大降低了用户对于新框架的学习成本，在了解 Spark 的情况下用户将很容易使用 Spark Streaming。</li></ul><p>2、缺点</p><ul><li>Spark Streaming 的粗粒度处理方式也造成了不可避免的数据延迟。在细粒度处理方式下，理想情况下每一条记录都会被实时处理，而在 Spark Streaming 中，数据需要汇总到一定的量后再一次性处理，这就增加了数据处理的延迟，这种延迟是由框架的设计引入的，并不是由网络或其他情况造成的。</li><li>使用的是 Processing Time 而不是 Event Time</li></ul><h3 id="Structured-Streaming"><a href="#Structured-Streaming" class="headerlink" title="Structured Streaming"></a>Structured Streaming</h3><p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">Structured Streaming</a> 是一种基于 Spark SQL 引擎的可扩展且容错的流处理引擎，它最关键的思想是将实时数据流视为一个不断增加的表，从而就可以像操作批的静态数据一样来操作流数据了。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3y2va0j214w0m03zf.jpg" alt="undefined"></p><p>会对输入的查询生成“结果表”，每个触发间隔（例如，每 1 秒）新行将附加到输入表，最终更新结果表，每当结果表更新时，我们希望能够将更改后的结果写入外部接收器去。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5gkz7aj21810r9dgp.jpg" alt="undefined"></p><p>终于支持事件时间的窗口操作：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5nafczj21980n940t.jpg" alt="undefined"></p><p>对比你会发现这个 Structured Streaming 怎么和 Flink 这么像，哈哈哈哈，不过这确实是未来的正确之路，两者的功能也会越来越相像的，期待它们出现更加令人兴奋的功能。</p><p>如果你对 Structured Streaming 感兴趣的话，可以去<a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">官网</a>做更深一步的了解，顺带附上 <a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">Structured Streaming</a> 的 Paper，同时也附上一位阿里小哥的 PPT —— <a href="https://www.slidestalk.com/s/FromSparkStreamingtoStructuredStreaming58639" target="_blank" rel="noopener">From Spark Streaming to Structured Streaming</a>。</p><h3 id="Flink-VS-Spark"><a href="#Flink-VS-Spark" class="headerlink" title="Flink VS Spark"></a>Flink VS Spark</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn93djtcj20g205umx6.jpg" alt="undefined"></p><p>通过上面你应该可以了解到 Flink 对比 Spark Streaming 的微批处理来说是有一定的优势，并且 Flink 还有一些特别的优点，比如灵活的时间语义、多种时间窗口、结合水印处理延迟数据等，但是 Spark 也有自己的一些优势，功能在早期来说是很完善的，并且新版本的 Spark 还添加了 Structured Streaming，它和 Flink 的功能很相近，两个还是值得更深入的对比，期待后面官方的测试对比报告。</p><h3 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h3><p>Storm 是一个开源的分布式实时计算系统，可以简单、可靠的处理大量的数据流。Storm 支持水平扩展，具有高容错性，保证每个消息都会得到处理，Strom 本身是无状态的，通过 ZooKeeper 管理分布式集群环境和集群状态。</p><h4 id="Storm-核心组件"><a href="#Storm-核心组件" class="headerlink" title="Storm 核心组件"></a>Storm 核心组件</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9bggknj21qo0yc407.jpg" alt="undefined"></p><p>Nimbus：负责资源分配和任务调度，Nimbus 对任务的分配信息会存储在 Zookeeper 上面的目录下。</p><p>Supervisor：负责去 Zookeeper 上的指定目录接受 Nimbus 分配的任务，启动和停止属于自己管理的 Worker 进程。它是当前物理机器上的管理者 —— 通过配置文件设置当前 Supervisor 上启动多少个 Worker。</p><p>Worker：运行具体处理组件逻辑的进程，Worker 运行的任务类型只有两种，一种是 Spout 任务，一种是 Bolt 任务。</p><p>Task：Worker 中每一个 Spout/Bolt 的线程称为一个 Task. 在 Storm0.8 之后，Task 不再与物理线程对应，不同 Spout/Bolt 的 Task 可能会共享一个物理线程，该线程称为 Executor。</p><p>Worker、Task、Executor 三者之间的关系:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9ibun2j21fe0u040j.jpg" alt="undefined"></p><h4 id="Storm-核心概念"><a href="#Storm-核心概念" class="headerlink" title="Storm 核心概念"></a>Storm 核心概念</h4><ul><li>Nimbus：Storm 集群主节点，负责资源分配和任务调度，任务的提交和停止都是在 Nimbus 上操作的，一个 Storm 集群只有一个 Nimbus 节点。</li><li>Supervisor：Storm 集群工作节点，接受 Nimbus 分配任务，管理所有 Worker。</li><li>Worker：工作进程，每个工作进程中都有多个 Task。</li><li>Executor：产生于 Worker 进程内部的线程，会执行同一个组件的一个或者多个 Task。</li><li>Task：任务，每个 Spout 和 Bolt 都是一个任务，每个任务都是一个线程。</li><li>Topology：计算拓扑，包含了应用程序的逻辑。</li><li>Stream：消息流，关键抽象，是没有边界的 Tuple 序列。</li><li>Spout：消息流的源头，Topology 的消息生产者。</li><li>Bolt：消息处理单元，可以过滤、聚合、查询数据库。</li><li>Tuple：数据单元，数据流中就是一个个 Tuple。</li><li>Stream grouping：消息分发策略，一共 6 种，控制 Tuple 的路由，定义 Tuple 在 Topology 中如何流动。</li><li>Reliability：可靠性，Storm 保证每个 Tuple 都会被处理。</li></ul><h4 id="Storm-数据处理流程图"><a href="#Storm-数据处理流程图" class="headerlink" title="Storm 数据处理流程图"></a>Storm 数据处理流程图</h4><p>Storm 处理数据的特点：数据源源不断，不断处理，数据都是 Tuple。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9pgxz7j21nm0qwq48.jpg" alt="undefined"></p><h3 id="Flink-VS-Storm"><a href="#Flink-VS-Storm" class="headerlink" title="Flink VS Storm"></a>Flink VS Storm</h3><p>可以参考的文章有：</p><p><a href="https://tech.meituan.com/2017/11/17/flink-benchmark.html" target="_blank" rel="noopener">流计算框架 Flink 与 Storm 的性能对比</a></p><p><a href="https://mp.weixin.qq.com/s/E7pM5XKb_QH225nl0JKFkg" target="_blank" rel="noopener">360 深度实践：Flink 与 Storm 协议级对比</a></p><p>两篇文章都从不同场景、不同数据压力下对比 Flink 和 Storm 两个实时计算框架的性能表现，最终结果都表明 Flink 比 Storm 的吞吐量和性能远超 Storm。</p><h3 id="全部对比结果"><a href="#全部对比结果" class="headerlink" title="全部对比结果"></a>全部对比结果</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9wb7zgj22y81eedne.jpg" alt="undefined"></p><p>如果对延迟要求不高的情况下，可以使用 Spark Streaming，它拥有丰富的高级 API，使用简单，并且 Spark 生态也比较成熟，吞吐量大，部署简单，社区活跃度较高，从 GitHub 的 star 数量也可以看得出来现在公司用 Spark 还是居多的，并且在新版本还引入了 Structured Streaming，这也会让 Spark 的体系更加完善。</p><p>如果对延迟性要求非常高的话，可以使用当下最火的流处理框架 Flink，采用原生的流处理系统，保证了低延迟性，在 API 和容错性方面做的也比较完善，使用和部署相对来说也是比较简单的，加上国内阿里贡献的 Blink，相信接下来 Flink 的功能将会更加完善，发展也会更加好，社区问题的响应速度也是非常快的，另外还有专门的钉钉大群和中文列表供大家提问，每周还会有专家进行直播讲解和答疑。</p><h3 id="小结与反思-2"><a href="#小结与反思-2" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>因在 1.2 节中已经对 Flink 的特性做了很详细的讲解，所以本篇主要介绍其他几种计算框架（Blink、Spark、Spark Streaming、Structured Streaming、Storm），并对比分析了这几种框架的特点与不同。你对这几种计算框架中的哪个最熟悉呢？了解过它们之间的差异吗？你有压测过它们的处理数据的性能吗？</p><hr><h2 id="四、Flink-环境准备"><a href="#四、Flink-环境准备" class="headerlink" title="四、Flink 环境准备"></a>四、Flink 环境准备</h2><p>通过前面几篇文章，相信你已经对 Flink 的基础概念等知识已经有一定了解，现在是不是迫切的想把 Flink 给用起来？先别急，我们先把电脑的准备环境给安装好，这样后面才能更愉快地玩耍。</p><p>废话不多说了，直奔主题。因为后面可能用到的有：Kafka、MySQL、ElasticSearch 等，另外像 Flink 编写程序还需要依赖 Java，还有就是我们项目是用 Maven 来管理依赖的，所以这篇文章我们先来安装下这个几个，准备好本地的环境，后面如果还要安装其他的组件我们到时在新文章中补充，如果你的操作系统已经中已经安装过 JDK、Maven、MySQL、IDEA 等，那么你可以跳过对应的内容，直接看你未安装过的。</p><p>这里我再说下我自己电脑的系统环境：macOS High Sierra 10.13.5，后面文章的演示环境不作特别说明的话就是都在这个系统环境中。</p><h3 id="JDK-安装与配置"><a href="#JDK-安装与配置" class="headerlink" title="JDK 安装与配置"></a>JDK 安装与配置</h3><p>虽然现在 JDK 已经更新到 12 了，但是为了稳定我们还是安装 JDK 8，如果没有安装过的话，可以去<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">官网</a> 的<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">下载页面</a>下载对应自己操作系统的最新 JDK8 就行。</p><p>Mac 系统的是 jdk-8u211-macosx-x64.dmg 格式、Linux 系统的是 jdk-8u211-linux-x64.tar.gz 格式。</p><p>Mac 系统安装的话直接双击然后一直按照提示就行了，最后 JDK 的安装目录在 <code>/Library/Java/JavaVirtualMachines/</code> ，然后在 <code>/etc/hosts</code> 中配置好环境变量（注意：替换你自己电脑本地的路径）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home</span><br><span class="line">export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure><p>Linux 系统的话就是在某个目录下直接解压就行了，然后在 <code>/etc/profile</code> 添加一下上面的环境变量（注意：替换你自己电脑的路径）。</p><p>然后执行 <code>java -version</code> 命令可以查看是否安装成功！</p><p> zhisheng@zhisheng ~  java -version<br>java version “1.8.0_152”<br>Java(TM) SE Runtime Environment (build 1.8.0_152-b16)<br>Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)</p><h3 id="Maven-安装与配置"><a href="#Maven-安装与配置" class="headerlink" title="Maven 安装与配置"></a>Maven 安装与配置</h3><p>安装好 JDK 后我们就可以安装 Maven 了，我们在<a href="http://maven.apache.org/download.cgi" target="_blank" rel="noopener">官网</a>下载二进制包就行，然后在自己本地软件安装目录解压压缩包就行。</p><p>接下来你需要配置一下环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export M2_HOME=/Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">export PATH=$PATH:$M2_HOME/bin</span><br></pre></td></tr></table></figure><p>然后执行命令 <code>mvn -v</code> 可以验证是否安装成功，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng ~ /Users  mvn -v</span><br><span class="line">Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T15:58:13+08:00)</span><br><span class="line">Maven home: /Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">Java version: 1.8.0_152, vendor: Oracle Corporation</span><br><span class="line">Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/jre</span><br><span class="line">Default locale: zh_CN, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;mac os x&quot;, version: &quot;10.13.5&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;</span><br></pre></td></tr></table></figure><h3 id="IDE-安装与配置"><a href="#IDE-安装与配置" class="headerlink" title="IDE 安装与配置"></a>IDE 安装与配置</h3><p>安装完 JDK 和 Maven 后，就可以安装 IDE 了，大家可以选择你熟练的 IDE 就行，我后面演示的代码都是在 IDEA 中运行的，如果想为了后面不出其他的 问题的话，建议尽量和我的环境保持一致。</p><p>IDEA 官网下载地址：<a href="https://www.jetbrains.com/idea/download/#section=mac" target="_blank" rel="noopener">下载页面的地址</a></p><p>下载后可以双击后然后按照提示一步步安装，安装完成后需要在 IDEA 中配置 JDK 路径和 Maven 的路径，后面我们开发也都是靠 Maven 来管理项目的依赖。</p><h3 id="MySQL-安装与配置"><a href="#MySQL-安装与配置" class="headerlink" title="MySQL 安装与配置"></a>MySQL 安装与配置</h3><p>因为后面文章有用到 MySQL，所以这里也讲一下如何安装与配置，首先去官网下载 MySQL 5.7，<a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">下载页面的地址</a>，根据你们到系统安装对应的版本，Mac 的话双击 dmg 安装包就可以按照提示一步步执行到安装成功。</p><p>启动 MySQL，如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pndeozfuj20u0130dll.jpg" alt="undefined"></p><p>出现绿色就证明 MySQL 服务启动成功了。后面我们操作数据库不会通过本地命令行来，而是会通过图形化软件，比如：Navicat、Sequel pro，这些图形化软件可比命令行的效率高太多，读者可以自行下载安装一下。</p><h3 id="Kafka-安装与配置"><a href="#Kafka-安装与配置" class="headerlink" title="Kafka 安装与配置"></a>Kafka 安装与配置</h3><p>后面我们文章中会大量用到 Kafka，所以 Kakfa 一定要安装好。官网下载地址：<a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">下载页面的地址</a></p><p>同样，我自己下载的版本是 1.1.0 （保持和我公司的生产环境一致），如果你对 Kafka 还不太熟悉，可以参考我以前写的一篇入门文章：<a href="http://www.54tianzhisheng.cn/2018/01/04/Kafka/" target="_blank" rel="noopener">Kafka 安装及快速入门</a>。</p><p>在这篇文章里面教大家怎么安装 Kafka、启动 Zookeeper、启动 Kafka 服务、创建 Topic、使用 producer 创建消息、使用 consumer 消费消息、查看 Topic 的信息，另外还有提供集群配置的方案。</p><h3 id="ElasticSearch-安装与配置"><a href="#ElasticSearch-安装与配置" class="headerlink" title="ElasticSearch 安装与配置"></a>ElasticSearch 安装与配置</h3><p>因为后面有文章介绍连接器 (connector) —— Elasticsearch 介绍和整和使用，并且最后面的案例文章也会把数据存储在 Elasticsearch 中的，所以这里就简单的讲解一下 Elasticsearch 的安装，在我以前的博客中写过一篇搭建 Elasticsearch 集群的：<a href="http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/" target="_blank" rel="noopener">Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程</a>。</p><p>这里我在本地安装个单机的 Elasticsearch 就行了，首先在官网 <a href="https://www.elastic.co/cn/downloads/past-releases" target="_blank" rel="noopener">下载页面</a> 找到 Elasticsearch 产品，我下载的版本是 elasticsearch-6.3.2 版本，同样和我们公司的线上环境版本保持一致，因为 Flink Elasticsearch connector 有分好几个版本：2.x、5.x、6.x 版本，不同版本到时候写数据存入到 Elasticsearch 的 Job 代码也是有点区别的，如果你们公司的 Elasticsearch 版本比较低的话，到时候后面版本的学习代码还得找官网的资料对比学习一下。</p><p>另外就是写这篇文章的时候 Elasticsearch 7.x 就早已经发布了，Flink 我暂时还没看到支持 Elasticsearch 7 的连接器，自己也没测试过，所以暂不清楚如果用 6.x 版本的 connector 去连接 7.x 的 Elasticsearch 会不会出现问题？建议还是跟着我的安装版本来操作！</p><p>除了这样下载 Elasticsearch 的话，你如果电脑安装了 Homebrew，也可以通过 Homebrew 来安装 Elasticsearch，都还挺方便的，包括你还可以通过 Docker 的方式快速启动一个 Elasticsearch 来。</p><p>下载好了 Elasticsearch 的压缩包，在你的安装目录下解压就行了，然后进入 Elasticsearch 的安装目录执行下面命令就可以启动 Elasticsearch 了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure><p>执行命令后的结果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pne4yctlj24m02d04qp.jpg" alt="undefined"></p><p>从浏览器端打开地址：<code>http://localhost:9200/</code> 即可验证是否安装成功：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnejkllpj20t60qsdhg.jpg" alt="undefined"></p><p>如果出现了如上图这样就代表 Elasticsearch 环境已经安装好了。</p><h3 id="小结与反思-3"><a href="#小结与反思-3" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲解了下 JDK、Maven、IDE、MySQL、Kafka、ElasticSearch 的安装与配置，因为这些都是后面要用的，所以这里单独抽一篇文章来讲解环境准备的安装步骤，当然这里还并不涉及全，因为后面我们还可能会涉及到 HBase、HDFS 等知识，后面我们用到再看，我们本系列的文章更多的还是讲解 Flink，所以更多的环境准备还是得靠大家自己独立完成。</p><p>这里我说下笔者自己一般安装环境的选择：</p><ol><li>组件尽量和公司的生产环境保持版本一致，不追求太新，够用就行，这样如果生产出现问题，本机还可以看是否可以复现出来</li><li>安装环境的时候先搜下类似的安装教程，提前知道要踩的坑，避免自己再次踩到</li></ol><p>下面文章我们就正式进入 Flink 专题了！</p><h2 id="五、Flink环境搭建"><a href="#五、Flink环境搭建" class="headerlink" title="五、Flink环境搭建"></a>五、Flink环境搭建</h2><p>在 2.1 节中已经将 Flink 的准备环境已经讲完了，本篇文章将带大家正式开始接触 Flink，那么我们得先安装一下 Flink。Flink 是可以在多个平台（Windows、Linux、Mac）上安装的。在开始写本书的时候最新版本是 1.8 版本，但是写到一半后更新到 1.9 了（合并了大量 Blink 的新特性），所以笔者又全部更新版本到 1.9，书籍后面也都是基于最新的版本讲解与演示。</p><p>Flink 的官网地址是：<a href="https://flink.apache.org/" target="_blank" rel="noopener">https://flink.apache.org/</a></p><h3 id="Flink-下载与安装"><a href="#Flink-下载与安装" class="headerlink" title="Flink 下载与安装"></a>Flink 下载与安装</h3><h4 id="Mac-amp-Linux-安装"><a href="#Mac-amp-Linux-安装" class="headerlink" title="Mac &amp; Linux 安装"></a>Mac &amp; Linux 安装</h4><p>你可以通过该地址 <a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">https://flink.apache.org/downloads.html</a> 下载到最新版本的 Flink。</p><p>这里我们选择 <code>Apache Flink 1.9.0 for Scala 2.11</code> 版本，点击跳转到了一个镜像下载选择的地址，随便选择哪个就行，只是下载速度不一致而已。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnl2lqdzj21y417i0wa.jpg" alt="undefined"></p><p>下载完后，你就可以直接解压下载的 Flink 压缩包了。</p><p>接下来我们可以启动一下 Flink，我们进入到 Flink 的安装目录下执行命令 <code>./bin/start-cluster.sh</code> 即可，产生的日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng /usr/local/flink-1.9.0  ./bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><p>如果你的电脑是 Mac 的话，那么你也可以通过 Homebrew 命令进行安装。先通过命令 <code>brew search flink</code> 查找一下包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng  ~  brew search flink</span><br><span class="line">==&gt; Formulae</span><br><span class="line">apache-flink ✔       homebrew/linuxbrew-core/apache-flink</span><br></pre></td></tr></table></figure><p>可以发现找得到 Flink 的安装包，但是这样安装的版本可能不是最新的，如果你要安装的话，则使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install apache-flink</span><br></pre></td></tr></table></figure><p>那么它就会开始进行下载并安装好，安装后的目录应该是在 <code>/usr/local/Cellar/apache-flink</code> 下。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnmiv3uoj21fi106q8b.jpg" alt="undefined"></p><p>你可以通过下面命令检查安装的 Flink 到底是什么版本的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink --version</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Version: 1.9.0, Commit ID: ff472b4</span><br></pre></td></tr></table></figure><p>这种的话运行是得进入 <code>/usr/local/Cellar/apache-flink/1.9.0/libexec/bin</code> 目录下执行命令 <code>./start-cluster.sh</code> 才可以启动 Flink 的。</p><p>启动后产生的日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><h4 id="Windows-安装"><a href="#Windows-安装" class="headerlink" title="Windows 安装"></a>Windows 安装</h4><p>如果你的电脑系统是 Windows 的话，那么你就直接双击 Flink 安装目录下面 bin 文件夹里面的 <code>start-cluster.bat</code> 就行，同样可以将 Flink 起动成功。</p><h3 id="Flink-启动与运行"><a href="#Flink-启动与运行" class="headerlink" title="Flink 启动与运行"></a>Flink 启动与运行</h3><p>启动成功后的话，我们可以通过访问地址<code>http://localhost:8081/</code> 查看 UI 长啥样了，如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnneh4l3j228o1j6myu.jpg" alt="undefined"></p><p>你在通过 jps 命令可以查看到运行的进程有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  jps</span><br><span class="line">73937 StandaloneSessionClusterEntrypoint</span><br><span class="line">74391 Jps</span><br><span class="line">520</span><br><span class="line">74362 TaskManagerRunner</span><br></pre></td></tr></table></figure><h3 id="Flink-目录配置文件解读"><a href="#Flink-目录配置文件解读" class="headerlink" title="Flink 目录配置文件解读"></a>Flink 目录配置文件解读</h3><p>Flink 安装好后，我们也运行启动看了效果了，接下来我们来看下它的目录结构吧：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> ✘ zhisheng@zhisheng  /usr/local/flink-1.9.0  ll</span><br><span class="line">total 1200</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff    11K  3  5 16:32 LICENSE</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff   582K  4  4 00:01 NOTICE</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff   1.3K  3  5 16:32 README.txt</span><br><span class="line">drwxr-xr-x@ 26 zhisheng  staff   832B  3  5 16:32 bin</span><br><span class="line">drwxr-xr-x@ 14 zhisheng  staff   448B  4  4 14:06 conf</span><br><span class="line">drwxr-xr-x@  6 zhisheng  staff   192B  4  4 14:06 examples</span><br><span class="line">drwxr-xr-x@  5 zhisheng  staff   160B  4  4 14:06 lib</span><br><span class="line">drwxr-xr-x@ 47 zhisheng  staff   1.5K  3  6 23:21 licenses</span><br><span class="line">drwxr-xr-x@  2 zhisheng  staff    64B  3  5 19:50 log</span><br><span class="line">drwxr-xr-x@ 22 zhisheng  staff   704B  4  4 14:06 opt</span><br></pre></td></tr></table></figure><p>上面目录：</p><ul><li><strong>bin</strong> 存放一些启动脚本</li><li><strong>conf</strong> 存放配置文件</li><li><strong>examples</strong> 存放一些案例的 Job Jar 包</li><li><strong>lib</strong> Flink 依赖的 Jar 包</li><li><strong>log</strong> 存放产生的日志文件</li><li><strong>opt</strong> 存放的是一些可选择的 Jar 包，后面可能会用到</li></ul><p>在 bin 目录里面有如下这些脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll bin</span><br><span class="line">total 256</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff    28K  3  5 16:32 config.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.2K  3  5 16:32 flink</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.7K  3  5 16:32 flink-console.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   6.2K  3  5 16:32 flink-daemon.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.2K  3  5 16:32 flink.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.5K  3  5 16:32 historyserver.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.8K  3  5 16:32 jobmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-appmaster-job.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-appmaster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-taskmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.2K  3  5 16:32 pyflink-stream.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.1K  3  5 16:32 pyflink.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.1K  3  5 16:32 pyflink.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.4K  3  5 16:32 sql-client.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.5K  3  5 16:32 standalone-job.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.3K  3  5 16:32 start-cluster.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 start-cluster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.3K  3  5 16:32 start-scala-shell.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 start-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.6K  3  5 16:32 stop-cluster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 stop-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.8K  3  5 16:32 taskmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.6K  3  5 16:32 yarn-session.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.2K  3  5 16:32 zookeeper.sh</span><br></pre></td></tr></table></figure><p>脚本包括了配置启动脚本、historyserver、Job Manager、Task Manager、启动集群和停止集群等脚本。</p><p>在 conf 目录下面有如下这些配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll conf</span><br><span class="line">total 112</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   9.8K  4  4 00:01 flink-conf.yaml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.1K  3  5 16:32 log4j-cli.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.8K  3  5 16:32 log4j-console.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.7K  3  5 16:32 log4j-yarn-session.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.9K  3  5 16:32 log4j.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.2K  3  5 16:32 logback-console.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.5K  3  5 16:32 logback-yarn.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.3K  3  5 16:32 logback.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff    15B  3  5 16:32 masters</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff    10B  3  5 16:32 slaves</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   3.8K  3  5 16:32 sql-client-defaults.yaml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.4K  3  5 16:32 zoo.cfg</span><br></pre></td></tr></table></figure><p>配置包含了 Flink 的自身配置、日志配置、masters、slaves、sql-client、zoo 等配置。</p><p>在 examples 目录里面可以看到有如下这些案例的目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll examples</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x@ 10 zhisheng  staff   320B  4  4 14:06 batch</span><br><span class="line">drwxr-xr-x@  3 zhisheng  staff    96B  4  4 14:06 gelly</span><br><span class="line">drwxr-xr-x@  4 zhisheng  staff   128B  4  4 14:06 python</span><br><span class="line">drwxr-xr-x@ 11 zhisheng  staff   352B  4  4 14:06 streaming</span><br></pre></td></tr></table></figure><p>这个目录下面有批、gelly、python、流的 demo，后面我们可以直接用上面的案例做些简单的测试。</p><p>在 log 目录里面存着 Task Manager &amp; Job manager 的日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll log</span><br><span class="line">total 144</span><br><span class="line">-rw-r--r--  1 zhisheng  staff    11K  4 25 20:10 flink-zhisheng-standalonesession-0-zhisheng.log</span><br><span class="line">-rw-r--r--  1 zhisheng  staff     0B  4 25 20:10 flink-zhisheng-standalonesession-0-zhisheng.out</span><br><span class="line">-rw-r--r--  1 zhisheng  staff    11K  4 25 20:10 flink-zhisheng-taskexecutor-0-zhisheng.log</span><br><span class="line">-rw-r--r--  1 zhisheng  staff     0B  4 25 20:10 flink-zhisheng-taskexecutor-0-zhisheng.out</span><br></pre></td></tr></table></figure><p>一般我们如果要深入了解一个知识点，最根本的方法就是看其源码实现，源码下面无秘密，所以我这里也讲一下如何将源码下载编译并运行，然后将代码工程导入到 IDEA 中去，方便自己查阅和 debug 代码。</p><h3 id="Flink-源码下载"><a href="#Flink-源码下载" class="headerlink" title="Flink 源码下载"></a>Flink 源码下载</h3><p>Flink GitHub 仓库地址：<a href="https://github.com/apache/flink" target="_blank" rel="noopener">https://github.com/apache/flink</a></p><p>执行下面命令将源码下载到本地：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:apache/flink.git</span><br></pre></td></tr></table></figure><p>拉取的时候找个网络好点的地方，这样速度可能会更快点。</p><p>然后你可以切换到项目的不同分支，比如 release-1.9、blink（阿里巴巴开源贡献的） ，执行下面命令将代码切换到 release-1.9 分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout release-1.9</span><br></pre></td></tr></table></figure><p>或者你也想去看看 Blink 的代码实现，你也可以执行下面命令切换到 blink 分支来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout blink</span><br></pre></td></tr></table></figure><h3 id="Flink-源码编译"><a href="#Flink-源码编译" class="headerlink" title="Flink 源码编译"></a>Flink 源码编译</h3><p>编译源码的话，你需要执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true</span><br></pre></td></tr></table></figure><ul><li>-Dmaven.test.skip：跳过测试代码</li><li>-Dmaven.javadoc.skip：跳过 javadoc 检查</li><li>-Dcheckstyle.skip：跳过代码风格检查</li></ul><p>maven 编译的时候跳过这些检查，这样可以减少很多时间，还可能会减少错误的发生。</p><p>注意：你的 maven 的 settings.xml 文件的 mirror 添加下面这个(这样才能下载到某些下载不了的依赖)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;*,!jeecg,!jeecg-snapshots,!mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br><span class="line"></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;mapr-public&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;mapr-releases&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;https://maven.aliyun.com/repository/mapr-public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><p>如果还遇到什么其他的问题的话，可以去看看我之前在我博客分享的一篇源码编译的文章（附视频）：<a href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/" target="_blank" rel="noopener">Flink 源码解析 —— 源码编译运行</a>。</p><h3 id="Flink-源码导入到-IDE"><a href="#Flink-源码导入到-IDE" class="headerlink" title="Flink 源码导入到 IDE"></a>Flink 源码导入到 IDE</h3><p>看下图，因为我们已经下载好了源码，直接在 IDEA 里面 open 这个 maven 项目就行了：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0un83lnzj21r20tqk0e.jpg" alt="undefined"></p><p>导入后大概就是下面这样子：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0unee0gmj21c00u0dn8.jpg" alt="undefined"></p><p>很顺利，没多少报错，这里我已经把一些代码风格检查相关的 Maven 插件给注释掉了。</p><h3 id="小结与反思-4"><a href="#小结与反思-4" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节主要讲了 FLink 在不同系统下的安装和运行方法，然后讲了下怎么去下载源码和将源码导入到 IDE 中。不知道你在将源码导入到 IDE 中是否有遇到什么问题呢？</p><h2 id="六、FlinkWordCount"><a href="#六、FlinkWordCount" class="headerlink" title="六、FlinkWordCount"></a>六、FlinkWordCount</h2><p>在 2.2 中带大家讲解了下 Flink 的环境安装，这篇文章就开始我们的第一个 Flink 案例实战，也方便大家快速开始自己的第一个 Flink 应用。大数据里学习一门技术一般都是从 WordCount 开始入门的，那么我还是不打破常规了，所以这篇文章我也将带大家通过 WordCount 程序来初步了解 Flink。</p><h3 id="Maven-创建项目"><a href="#Maven-创建项目" class="headerlink" title="Maven 创建项目"></a>Maven 创建项目</h3><p>Flink 支持 Maven 直接构建模版项目，你在终端使用该命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate                               \</span><br><span class="line">      -DarchetypeGroupId=org.apache.flink              \</span><br><span class="line">      -DarchetypeArtifactId=flink-quickstart-java      \</span><br><span class="line">      -DarchetypeVersion=1.9.0</span><br></pre></td></tr></table></figure><p>在执行的过程中它会提示你输入 groupId、artifactId、和 package 名，你按照要求输入就行，最后就可以成功创建一个项目。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uocicqoj21xw1aatf4.jpg" alt="undefined"></p><p>进入到目录你就可以看到已经创建了项目，里面结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng  ~/IdeaProjects/github/Flink-WordCount  tree</span><br><span class="line">.</span><br><span class="line">├── pom.xml</span><br><span class="line">└── src</span><br><span class="line">    └── main</span><br><span class="line">        ├── java</span><br><span class="line">        │   └── com</span><br><span class="line">        │       └── zhisheng</span><br><span class="line">        │           ├── BatchJob.java</span><br><span class="line">        │           └── StreamingJob.java</span><br><span class="line">        └── resources</span><br><span class="line">            └── log4j.properties</span><br><span class="line"></span><br><span class="line">6 directories, 4 files</span><br></pre></td></tr></table></figure><p>该项目中包含了两个类 BatchJob 和 StreamingJob，另外还有一个 log4j.properties 配置文件，然后你就可以将该项目导入到 IDEA 了。</p><p>你可以在该目录下执行 <code>mvn clean package</code> 就可以编译该项目，编译成功后在 target 目录下会生成一个 Job 的 Jar 包，但是这个 Job 还不能执行，因为 StreamingJob 这个类中的 main 方法里面只是简单的创建了 StreamExecutionEnvironment 环境，然后就执行 execute 方法，这在 Flink 中是不算一个可执行的 Job 的，因此如果你提交到 Flink UI 上也是会报错的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uokn3kbj227w0pawfm.jpg" alt="undefined"></p><p>运行报错：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uosbkccj22640mq3zx.jpg" alt="undefined"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Server Response Message:</span><br><span class="line">Internal server error.</span><br></pre></td></tr></table></figure><p>我们查看 Flink Job Manager 的日志可以看到：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uoz4xbmj226w12e0zj.jpg" alt="undefined"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-04-26 17:27:33,706 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler    - Unhandled exception.</span><br><span class="line">org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: No operators defined in streaming topology. Cannot execute.</span><br></pre></td></tr></table></figure><p>因为 execute 方法之前我们是需要补充我们 Job 的一些算子操作的，所以报错还是很正常的，本文下面将会提供完整代码。</p><h3 id="IDEA-创建项目"><a href="#IDEA-创建项目" class="headerlink" title="IDEA 创建项目"></a>IDEA 创建项目</h3><p>一般我们项目可能是由多个 Job 组成，并且代码也都是在同一个工程下面进行管理，上面那种适合单个 Job 执行，但如果多人合作的时候还是得在同一个工程下面进行项目的创建，每个 Flink Job 一个 module，下面我们将来讲解下如何利用 IDEA 创建 Flink 项目。</p><p>我们利用 IDEA 创建 Maven 项目，工程如下图这样，项目下面分很多模块，每个模块负责不同的业务</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0up8brt7j20p20k0dg6.jpg" alt="undefined"></p><p>接下来我们需要在父工程的 pom.xml 中加入如下属性（含编码、Flink 版本、JDK 版本、Scala 版本、Maven 编译版本）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Flink 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--JDK 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Scala 2.11 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后加入依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Apache Flink dependencies --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Add logging framework, to produce console output when running in the IDE. --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- These dependencies are excluded from the application JAR by default. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面依赖中 flink-java 和 flink-streaming-java 是我们 Flink 必备的核心依赖，为什么设置 scope 为 provided 呢（默认是 compile）？</p><p>是因为 Flink 其实在自己的安装目录中 lib 文件夹里的 <code>lib/flink-dist_2.11-1.9.0.jar</code> 已经包含了这些必备的 Jar 了，所以我们在给自己的 Flink Job 添加依赖的时候最后打成的 Jar 包可不希望又将这些重复的依赖打进去。有两个好处：</p><ul><li>减小了我们打的 Flink Job Jar 包容量大小</li><li>不会因为打入不同版本的 Flink 核心依赖而导致类加载冲突等问题</li></ul><p>但是问题又来了，我们需要在 IDEA 中调试运行我们的 Job，如果将 scope 设置为 provided 的话，是会报错的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Error: A JNI error has occurred, please check your installation and try again</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/api/common/ExecutionConfig$GlobalJobParameters</span><br><span class="line">    at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class="line">    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class="line">    at java.lang.Class.privateGetMethodRecursive(Class.java:3048)</span><br><span class="line">    at java.lang.Class.getMethod0(Class.java:3018)</span><br><span class="line">    at java.lang.Class.getMethod(Class.java:1784)</span><br><span class="line">    at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)</span><br><span class="line">    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.flink.api.common.ExecutionConfig$GlobalJobParameters</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">    ... 7 more</span><br></pre></td></tr></table></figure><p>默认 scope 为 compile 的话，本地调试的话就不会出错了。</p><p>另外测试到底能够减小多少 Jar 包的大小呢？我这里先写了个 Job 测试。</p><p>当 scope 为 compile 时，编译后的 target 目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  ~/Flink-WordCount/target   master ●✚  ll</span><br><span class="line">total 94384</span><br><span class="line">-rw-r--r--  1 zhisheng  staff    45M  4 26 21:23 Flink-WordCount-1.0-SNAPSHOT.jar</span><br><span class="line">drwxr-xr-x  4 zhisheng  staff   128B  4 26 21:23 classes</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 generated-sources</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 maven-archiver</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 maven-status</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.2K  4 26 21:23 original-Flink-WordCount-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>当 scope 为 provided 时，编译后的 target 目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng ~/Flink-WordCount/target   master ●✚  ll</span><br><span class="line">total 32</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.5K  4 26 21:27 Flink-WordCount-1.0-SNAPSHOT.jar</span><br><span class="line">drwxr-xr-x  4 zhisheng  staff   128B  4 26 21:27 classes</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 generated-sources</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 maven-archiver</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 maven-status</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.2K  4 26 21:27 original-Flink-WordCount-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>可以发现：当 scope 为 provided 时 Jar 包才 7.5k，而为 compile 时 Jar 包就 45M 了，你要想想这才只是一个简单的 WordCount 程序呢，差别就这么大。当我们把 Flink Job 打成一个 fat Jar 时，上传到 UI 的时间就能够很明显的对比出来（Jar 包越小上传的时间越短），所以把 scope 设置为 provided 还是很有必要的。</p><p>有人就会想了，那这不是和上面有冲突了吗？假如我既想打出来的 Jar 包要小，又想能够在本地 IDEA 中进行运行和调试 Job ？这里我提供一种方法：在父工程中的 pom.xml 引入如下 profiles。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>add-dependencies-for-IDEA<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">activation</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>idea.version<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">activation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br></pre></td></tr></table></figure><p>当你在 IDEA 中运行 Job 的时候，它会给你引入 flink-java、flink-streaming-java，且 scope 设置为 compile，但是你是打成 Jar 包的时候它又不起作用。如果你加了这个 profile 还是报错的话，那么可能是 IDEA 中没有识别到，你可以在 IDEA 的中查看下面两个配置确定一下（配置其中一个即可以起作用）。</p><p>1、查看 Maven 中的该 profile 是否已经默认勾选上了，如果没有勾选上，则手动勾选一下才会起作用</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0upjwdhnj20yw080q2z.jpg" alt="undefined"></p><p>2、Include dependencies with “Provided” scope 是否勾选，如果未勾选，则手动勾选后才起作用</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0upq3ibrj21n8188dia.jpg" alt="undefined"></p><h3 id="流计算-WordCount-应用程序代码"><a href="#流计算-WordCount-应用程序代码" class="headerlink" title="流计算 WordCount 应用程序代码"></a>流计算 WordCount 应用程序代码</h3><p>回到正题，利用 IDEA 创建好 WordCount 应用后，我们开始编写代码。</p><p><strong>Main 类</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//创建流运行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args));</span><br><span class="line">        env.fromElements(WORDS)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] splits = value.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split, <span class="number">1</span>));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(value1.f0, value1.f1 + value1.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print();</span><br><span class="line">        <span class="comment">//Streaming 程序必须加这个才能启动程序，否则不会有结果</span></span><br><span class="line">        env.execute(<span class="string">"zhisheng —— word count streaming demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] WORDS = <span class="keyword">new</span> String[]&#123;</span><br><span class="line">            <span class="string">"To be, or not to be,--that is the question:--"</span>,</span><br><span class="line">            <span class="string">"Whether 'tis nobler in the mind to suffer"</span></span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>pom.xml</strong> 文件中引入 build 插件并且要替换成你自己项目里面的 mainClass：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Java Compiler --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 使用 maven-shade 插件创建一个包含所有必要的依赖项的 fat Jar --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.apache.flink:force-shading<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!--注意：这里一定要换成你自己的 Job main 方法的启动类--&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.zhisheng.wordcount.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注意：上面这个 build 插件要记得加，否则打出来的 jar 包是不完整的，提交运行会报 ClassNotFoundException，该问题是初学者很容易遇到的问题，很多人咨询过笔者这个问题。</p><h3 id="WordCount-应用程序运行"><a href="#WordCount-应用程序运行" class="headerlink" title="WordCount 应用程序运行"></a>WordCount 应用程序运行</h3><h4 id="本地-IDE-运行"><a href="#本地-IDE-运行" class="headerlink" title="本地 IDE 运行"></a>本地 IDE 运行</h4><p>编译好 WordCount 程序后，我们在 IDEA 中右键 run main 方法就可以把 Job 运行起来，结果如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uq05u0wj21pc0vy74y.jpg" alt="undefined"></p><p>图中的就是将每个 word 和对应的个数一行一行打印出来，在本地 IDEA 中运行没有问题，我们接下来使用命令 <code>mvn clean package</code> 打包成一个 Jar (flink-learning-examples-1.0-SNAPSHOT.jar) 然后将其上传到 Flink UI 上运行一下看下效果。</p><h4 id="UI-运行-Job"><a href="#UI-运行-Job" class="headerlink" title="UI 运行 Job"></a>UI 运行 Job</h4><p>在 <code>http://localhost:8081/#/submit</code> 页面上传 flink-learning-examples-1.0-SNAPSHOT.jar 后，然后点击 Submit 后就可以运行了。</p><p>运行 Job 的 UI 如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uq6lppkj226o1awdhc.jpg" alt="undefined"></p><p>Job 的结果在 Task Manager 的 Stdout 中：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uqcmgspj224u1ewq50.jpg" alt="undefined"></p><h3 id="WordCount-应用程序代码分析"><a href="#WordCount-应用程序代码分析" class="headerlink" title="WordCount 应用程序代码分析"></a>WordCount 应用程序代码分析</h3><p>我们已经将 WordCount 程序代码写好了并且也在 IDEA 中和 Flink UI 上运行了 Job，并且程序运行的结果都是正常的。</p><p>那么我们来分析一下这个 WordCount 程序代码：</p><p>1、创建好 StreamExecutionEnvironment（流程序的运行环境）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure><p>2、给流程序的运行环境设置全局的配置（从参数 args 获取）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args));</span><br></pre></td></tr></table></figure><p>3、构建数据源，WORDS 是个字符串数组</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromElements(WORDS)</span><br></pre></td></tr></table></figure><p>4、将字符串进行分隔然后收集，组装后的数据格式是 (word、1)，1 代表 word 出现的次数为 1</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] splits = value.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">            <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>5、根据 word 关键字进行分组（0 代表对第一个字段分组，也就是对 word 进行分组）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyBy(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>6、对单个 word 进行计数操作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>7、打印所有的数据流，格式是 (word，count)，count 代表 word 出现的次数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print()</span><br></pre></td></tr></table></figure><p>8、开始执行 Job</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.execute(<span class="string">"zhisheng —— word count streaming demo"</span>);</span><br></pre></td></tr></table></figure><h3 id="小结与反思-5"><a href="#小结与反思-5" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节给大家介绍了 Maven 创建 Flink Job、IDEA 中创建 Flink 项目（详细描述了里面要注意的事情）、编写 WordCount 程序、IDEA 运行程序、在 Flink UI 运行程序、对 WordCount 程序每个步骤进行分析。</p><p>通过本小节，你接触了第一个 Flink 应用程序，也开启了 Flink 实战之旅。你有自己运行本节的代码去测试吗？动手测试的过程中有遇到什么问题吗？</p><p>本节涉及的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/wordcount" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/wordcount</a></p><h2 id="七、Flink-实时处理-Socket-数据"><a href="#七、Flink-实时处理-Socket-数据" class="headerlink" title="七、Flink 实时处理 Socket 数据"></a>七、Flink 实时处理 Socket 数据</h2><p>在 2.3 中讲解了 Flink 最简单的 WordCount 程序的创建、运行结果查看和代码分析，这篇文章继续带大家来看一个入门上手的程序：Flink 处理 Socket 数据。</p><h3 id="IDEA-创建项目-1"><a href="#IDEA-创建项目-1" class="headerlink" title="IDEA 创建项目"></a>IDEA 创建项目</h3><p>使用 IDEA 创建新的 module，结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">├── pom.xml</span><br><span class="line">└── src</span><br><span class="line">    ├── main</span><br><span class="line">    │   ├── java</span><br><span class="line">    │   │   └── com</span><br><span class="line">    │   │       └── zhisheng</span><br><span class="line">    │   │           └── socket</span><br><span class="line">    │   │               └── Main.java</span><br><span class="line">    │   └── resources</span><br><span class="line">    │       └── log4j.properties</span><br><span class="line">    └── test</span><br><span class="line">        └── java</span><br></pre></td></tr></table></figure><p>项目创建好了后，我们下一步开始编写 Flink Socket Job 的代码。</p><h3 id="Flink-Socket-应用程序代码"><a href="#Flink-Socket-应用程序代码" class="headerlink" title="Flink Socket 应用程序代码"></a>Flink Socket 应用程序代码</h3><p><strong>Main 类</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//参数检查</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String hostname = args[<span class="number">0</span>];</span><br><span class="line">        Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//获取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br><span class="line">        <span class="comment">//计数</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        sum.print();</span><br><span class="line">        env.execute(<span class="string">"Java WordCount from SocketText"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">            String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>pom.xml</strong> 添加 build：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.apache.flink:force-shading<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!--注意：这里一定要换成你自己的 Job main 方法的启动类--&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.zhisheng.socket.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Flink-Socket-应用程序运行"><a href="#Flink-Socket-应用程序运行" class="headerlink" title="Flink Socket 应用程序运行"></a>Flink Socket 应用程序运行</h3><h4 id="本地-IDE-运行-1"><a href="#本地-IDE-运行-1" class="headerlink" title="本地 IDE 运行"></a>本地 IDE 运行</h4><p>我们先在终端开启监听 9000 端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -l 9000</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ur4cqi4j21b60aeq36.jpg" alt="undefined"></p><p>然后右键运行 Main 类的 main 方法 (注意：需要传入运行参数 <code>127.0.0.1 9000</code>)：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urc5j0wj21ni14egno.jpg" alt="undefined"></p><p>运行结果如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urirdxgj21s20legma.jpg" alt="undefined"></p><p>我在终端一个个输入下面的字符串：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hello</span><br><span class="line">zhisheng</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">zhisheng</span><br><span class="line">zhisheng</span><br><span class="line">This is zhisheng‘s book</span><br></pre></td></tr></table></figure><p>然后在 IDEA 的运行结果会一个个输出来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2&gt; (hello,1)</span><br><span class="line">2&gt; (zhisheng,1)</span><br><span class="line">2&gt; (hello,2)</span><br><span class="line">2&gt; (hello,3)</span><br><span class="line">2&gt; (zhisheng,2)</span><br><span class="line">2&gt; (zhisheng,3)</span><br><span class="line">3&gt; (s,1)</span><br><span class="line">1&gt; (this,1)</span><br><span class="line">4&gt; (is,1)</span><br><span class="line">2&gt; (zhisheng,4)</span><br><span class="line">3&gt; (book,1)</span><br></pre></td></tr></table></figure><p>在本地 IDEA 中运行没有问题，我们接下来使用命令 <code>mvn clean package</code> 打包成一个 Jar (flink-learning-examples-1.0-SNAPSHOT.jar) 然后将其上传到 Flink UI 上运行一下看下效果。</p><h4 id="UI-运行-Job-1"><a href="#UI-运行-Job-1" class="headerlink" title="UI 运行 Job"></a>UI 运行 Job</h4><p>依旧和上面那样开启监听本地端口 9200，然后在 <code>http://localhost:8081/#/submit</code> 页面上传 flink-learning-examples-1.0-SNAPSHOT.jar 后，接着在 Main Class 填写运行的主函数，Program Arguments 填写参数 <code>127.0.0.1 9000</code>，最后点击 Submit 后就可以运行了。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urqumhzj22760mqgmr.jpg" alt="undefined"></p><p>UI 的运行详情如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ury0iq8j225w1au3zz.jpg" alt="undefined"></p><p>我在终端一个个输入下面的字符串：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  ~  nc -l 9000</span><br><span class="line">zhisheng</span><br><span class="line">zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">zhisheng</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">zhisheng</span><br></pre></td></tr></table></figure><p>查看 Task Manager 的 Stdout 可以查看到输出：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0us5qqsej227819wgnp.jpg" alt="undefined"></p><h3 id="Flink-Socket-应用程序代码分析"><a href="#Flink-Socket-应用程序代码分析" class="headerlink" title="Flink Socket 应用程序代码分析"></a>Flink Socket 应用程序代码分析</h3><p>1、参数检查，需要传入两个参数（hostname 和 port），符合条件就赋值给 hostname 和 port</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">    System.err.println(<span class="string">"USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">String hostname = args[<span class="number">0</span>];</span><br><span class="line">Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br></pre></td></tr></table></figure><p>2、创建好 StreamExecutionEnvironment（流程序的运行环境）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure><p>3、构建数据源，获取 Socket 数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br></pre></td></tr></table></figure><p>4、对 Socket 数据字符串分隔后收集在根据 word 分组后计数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将字符串进行分隔然后收集，组装后的数据格式是 (word、1)，1 代表 word 出现的次数为 1</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">        String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">            <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>5、打印所有的数据流，格式是 (word，count)，count 代表 word 出现的次数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum.print();</span><br></pre></td></tr></table></figure><p>6、开始执行 Job</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.execute(<span class="string">"Java WordCount from SocketText"</span>);</span><br></pre></td></tr></table></figure><h3 id="Flink-中使用-Lambda-表达式"><a href="#Flink-中使用-Lambda-表达式" class="headerlink" title="Flink 中使用 Lambda 表达式"></a>Flink 中使用 Lambda 表达式</h3><p>因为 Lambda 表达式看起来简洁，所以有时候也是希望在这些 Flink 作业中也可以使用上它，虽然 Flink 中是支持 Lambda，但是个人感觉不太友好。比如上面的应用程序如果将 LineSplitter 该类之间用 Lambda 表达式完成的话则要像下面这样写：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">stream.flatMap((s, collector) -&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (String token : s.toLowerCase().split(<span class="string">"\\W+"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure><p>但是这样写完后，运行作业报错如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.flink.api.common.functions.InvalidTypesException: The return type of function &apos;main(LambdaMain.java:34)&apos; could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the &apos;ResultTypeQueryable&apos; interface.</span><br><span class="line">    at org.apache.flink.api.dag.Transformation.getOutputType(Transformation.java:417)</span><br><span class="line">    at org.apache.flink.streaming.api.datastream.DataStream.getType(DataStream.java:175)</span><br><span class="line">    at org.apache.flink.streaming.api.datastream.DataStream.keyBy(DataStream.java:318)</span><br><span class="line">    at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:41)</span><br><span class="line">Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of &apos;Collector&apos; are missing. In many cases lambda methods don&apos;t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the &apos;org.apache.flink.api.common.functions.FlatMapFunction&apos; interface. Otherwise the type has to be specified explicitly using type information.</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractionUtils.validateLambdaType(TypeExtractionUtils.java:350)</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractionUtils.extractTypeFromLambda(TypeExtractionUtils.java:176)</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:571)</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:196)</span><br><span class="line">    at org.apache.flink.streaming.api.datastream.DataStream.flatMap(DataStream.java:611)</span><br><span class="line">    at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:34)</span><br></pre></td></tr></table></figure><p>根据上面的报错信息其实可以知道要怎么解决了，该错误是因为 Flink 在用户自定义的函数中会使用泛型来创建 serializer，当使用匿名函数时，类型信息会被保留。但 Lambda 表达式并不是匿名函数，所以 javac 编译的时候并不会把泛型保存到 class 文件里。</p><p>解决方法：使用 Flink 提供的 returns 方法来指定 flatMap 的返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用 TupleTypeInfo 来指定 Tuple 的参数类型</span></span><br><span class="line">.returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class))</span><br></pre></td></tr></table></figure><p>在 flatMap 后面加上上面这个 returns 就行了，但是如果算子多了的话，每个都去加一个 returns，其实会很痛苦的，所以通常使用匿名函数或者自定义函数居多。</p><h3 id="小结与反思-6"><a href="#小结与反思-6" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 的第二个应用程序 —— 读取 Socket 数据，希望通过两个简单的程序可以让你对 Flink 有个简单的认识，然后讲解了下 Flink 应用程序中使用 Lambda 表达式的问题。</p><p>本节涉及的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/socket" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/socket</a></p><h2 id="八、Flink多种时间语义对比"><a href="#八、Flink多种时间语义对比" class="headerlink" title="八、Flink多种时间语义对比"></a>八、Flink多种时间语义对比</h2><p>Flink 在流应用程序中支持不同的 <strong>Time</strong> 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。下面我们一起来看看这三个 Time。</p><h3 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h3><p>Processing Time 是指事件被处理时机器的系统时间。</p><p>如果我们 Flink Job 设置的时间策略是 Processing Time 的话，那么后面所有基于时间的操作（如时间窗口）都将会使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</p><p>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</p><p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p><h3 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h3><p>Event Time 是指事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。</p><p>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（事件产生的时间顺序）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</p><p>假设所有数据都已到达，Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，不管它们到达的顺序如何（是否按照事件产生的时间）。</p><h3 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h3><p>Ingestion Time 是事件进入 Flink 的时间。 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</p><p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，成本可能会高一点，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（只在进入 Flink 的时候分配一次），所以对事件的不同窗口操作将使用相同的时间戳（第一次分配的时间戳），而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p><p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序中不必指定如何生成水印。</p><p>在 Flink 中，Ingestion Time 与 Event Time 非常相似，唯一区别就是 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p><h3 id="三种-Time-对比结果"><a href="#三种-Time-对比结果" class="headerlink" title="三种 Time 对比结果"></a>三种 Time 对比结果</h3><p>一张图概括上面说的三种 Time：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uwc85x5j21li0u075w.jpg" alt="undefined"></p><ul><li>Processing Time：事件被处理时机器的系统时间</li><li>Event Time：事件自身的时间</li><li>Ingestion Time：事件进入 Flink 的时间</li></ul><p>一张图形象描述上面说的三种 Time：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uwqobe4j227414ogn1.jpg" alt="undefined"></p><h3 id="使用场景分析"><a href="#使用场景分析" class="headerlink" title="使用场景分析"></a>使用场景分析</h3><p>通过上面两个图相信大家已经对 Flink 中的这三个 Time 有所了解了，那么我们实际生产环境中通常该如何选择哪种 Time 呢？</p><p>一般来说在生产环境中将 Event Time 与 Processing Time 对比的比较多，这两个也是我们常用的策略，Ingestion Time 一般用的较少。</p><p>用 Processing Time 的场景大多是用户不关心事件时间，它只需要关心这个时间窗口要有数据进来，只要有数据进来了，我就可以对进来窗口中的数据进行一系列的计算操作，然后再将计算后的数据发往下游。</p><p>而用 Event Time 的场景一般是业务需求需要时间这个字段（比如购物时是要先有下单事件、再有支付事件；借贷事件的风控是需要依赖时间来做判断的；机器异常检测触发的告警也是要具体的异常事件的时间展示出来；商品广告及时精准推荐给用户依赖的就是用户在浏览商品的时间段/频率/时长等信息），只能根据事件时间来处理数据，而且还要从事件中获取到事件的时间。</p><p>但是使用事件时间的话，就可能有这样的情况：数据源采集的数据往消息队列中发送时可能因为网络抖动、服务可用性、消息队列的分区数据堆积的影响而导致数据到达的不一定及时，可能会出现数据出现一定的乱序、延迟几分钟等，庆幸的是 Flink 支持通过 WaterMark 机制来处理这种延迟的数据。关于 WaterMark 的机制我会在后面的文章讲解。</p><h3 id="如何设置-Time-策略？"><a href="#如何设置-Time-策略？" class="headerlink" title="如何设置 Time 策略？"></a>如何设置 Time 策略？</h3><p>在创建完流运行环境的时候，然后就可以通过 <code>env.setStreamTimeCharacteristic</code> 设置时间策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他两种:</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span></span><br></pre></td></tr></table></figure><h3 id="小结与反思-7"><a href="#小结与反思-7" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节介绍了 Flink 中的三种时间语义，相比较其他的流处理引擎来说支持的更多，你知道的流处理引擎支持哪些时间语义呢？</p><h2 id="九、Flink-Window-基础概念与实现原理"><a href="#九、Flink-Window-基础概念与实现原理" class="headerlink" title="九、Flink Window 基础概念与实现原理"></a>九、Flink Window 基础概念与实现原理</h2><p>目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语，例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” 。</p><p>对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。</p><p>在本节将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。</p><h3 id="什么是-Window？"><a href="#什么是-Window？" class="headerlink" title="什么是 Window？"></a>什么是 Window？</h3><p>下面我们结合一个现实的例子来说明。</p><p>就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？</p><p>假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uzyaa4aj21mm08mmx1.jpg" alt="undefined"></p><p>可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v0en8wuj21m80hudfw.jpg" alt="undefined"></p><p>这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？</p><p>这个问题，就相当于一个定义了一个 Window（窗口），Window 的界限是 1 分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v0wadrcj21my0gmaa6.jpg" alt="undefined"></p><p>第一分钟的数量为 18，第二分钟是 28，第三分钟是 24……这样，1 个小时内会有 60 个 Window。</p><p>再考虑一种情况，每 30 秒统计一次过去 1 分钟的汽车数量之和：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v1b9o6jj21mu0o2wet.jpg" alt="undefined"></p><p>此时，Window 出现了重合。这样，1 个小时内会有 120 个 Window。</p><h3 id="Window-有什么作用？"><a href="#Window-有什么作用？" class="headerlink" title="Window 有什么作用？"></a>Window 有什么作用？</h3><p>通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。</p><h3 id="Flink-自带的-Window"><a href="#Flink-自带的-Window" class="headerlink" title="Flink 自带的 Window"></a>Flink 自带的 Window</h3><p>Flink 在 KeyedStream（DataStream 的继承类） 中提供了下面几种 Window：</p><ul><li>以时间驱动的 Time Window</li><li>以事件数量驱动的 Count Window</li><li>以会话间隔驱动的 Session Window</li></ul><p>提供上面三种 Window 机制后，由于某些特殊的需要，DataStream API 也提供了定制化的 Window 操作，供用户自定义 Window。</p><p>下面将先围绕上面说的三种 Window 来进行分析并教大家如何使用，然后对其原理分析，最后在解析其源码实现。</p><h3 id="Time-Window-使用及源码分析"><a href="#Time-Window-使用及源码分析" class="headerlink" title="Time Window 使用及源码分析"></a>Time Window 使用及源码分析</h3><p>正如命名那样，Time Window 根据时间来聚合流数据。例如：一分钟的时间窗口就只会收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于下一个算子。</p><p>在 Flink 中使用 Time Window 非常简单，输入一个时间参数，这个时间参数可以利用 Time 这个类来控制，如果事前没指定 TimeCharacteristic 类型的话，则默认使用的是 ProcessingTime，如果对 Flink 中的 Time 还不了解的话，可以看前一篇文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db6a06bf6a6211cb96164ff" target="_blank" rel="noopener">Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析</a> 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>)) <span class="comment">//time Window 每分钟统计一次数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>时间窗口的数据窗口聚合流程如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v50qmevj21sc0w2wgo.jpg" alt="undefined"></p><p>在第一个窗口中（1 ～ 2 分钟）和为 7、第二个窗口中（2 ～ 3 分钟）和为 12、第三个窗口中（3 ～ 4 分钟）和为 7、第四个窗口中（4 ～ 5 分钟）和为 19。</p><p>该 timeWindow 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//时间窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, TimeWindow&gt; <span class="title">timeWindow</span><span class="params">(Time size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingProcessingTimeWindows.of(size));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingEventTimeWindows.of(size));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外在 Time Window 中还支持滑动的时间窗口，比如定义了一个每 30s 滑动一次的 1 分钟时间窗口，它会每隔 30s 去统计过去一分钟窗口内的数据，同样使用也很简单，输入两个时间参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>)) <span class="comment">//sliding time Window 每隔 30s 统计过去一分钟的数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>滑动时间窗口的数据聚合流程如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v9hcl5wj21sa0xq410.jpg" alt="undefined"></p><p>在该第一个时间窗口中（1 ～ 2 分钟）和为 7，第二个时间窗口中（1:30 ~ 2:30）和为 10，第三个时间窗口中（2 ~ 3 分钟）和为 12，第四个时间窗口中（2:30 ~ 3:30）和为 10，第五个时间窗口中（3 ~ 4 分钟）和为 7，第六个时间窗口中（3:30 ~ 4:30）和为 11，第七个时间窗口中（4 ~ 5 分钟）和为 19。</p><p>该 timeWindow 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动时间窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, TimeWindow&gt; <span class="title">timeWindow</span><span class="params">(Time size, Time slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        <span class="keyword">return</span> window(SlidingProcessingTimeWindows.of(size, slide));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> window(SlidingEventTimeWindows.of(size, slide));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Count-Window-使用及源码分析"><a href="#Count-Window-使用及源码分析" class="headerlink" title="Count Window 使用及源码分析"></a>Count Window 使用及源码分析</h3><p>Apache Flink 还提供计数窗口功能，如果计数窗口的值设置的为 3 ，那么将会在窗口中收集 3 个事件，并在添加第 3 个元素时才会计算窗口中所有事件的值。</p><p>在 Flink 中使用 Count Window 非常简单，输入一个 long 类型的参数，这个参数代表窗口中事件的数量，使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .countWindow(<span class="number">3</span>) <span class="comment">//统计每 3 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>计数窗口的数据窗口聚合流程如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vazxmyfj21ek0ny769.jpg" alt="undefined"></p><p>该 countWindow 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外在 Count Window 中还支持滑动的计数窗口，比如定义了一个每 3 个事件滑动一次的 4 个事件的计数窗口，它会每隔 3 个事件去统计过去 4 个事件计数窗口内的数据，使用也很简单，输入两个 long 类型的参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>) </span><br><span class="line">    .countWindow(<span class="number">4</span>, <span class="number">3</span>) <span class="comment">//每隔 3 个元素统计过去 4 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>滑动计数窗口的数据窗口聚合流程如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vbqi86ej225m0wmac7.jpg" alt="undefined"></p><p>该 countWindow 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Session-Window-使用及源码分析"><a href="#Session-Window-使用及源码分析" class="headerlink" title="Session Window 使用及源码分析"></a>Session Window 使用及源码分析</h3><p>Apache Flink 还提供了会话窗口，是什么意思呢？使用该窗口的时候你可以传入一个时间参数（表示某种数据维持的会话持续时长），如果超过这个时间，就代表着超出会话时长。</p><p>在 Flink 中使用 Session Window 非常简单，你该使用 Flink KeyedStream 中的 window 方法，然后使用 ProcessingTimeSessionWindows.withGap()（不一定就是只使用这个），在该方法里面你需要做的是传入一个时间参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">5</span>)))<span class="comment">//表示如果 5s 内没出现数据则认为超出会话时长，然后计算这个窗口的和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>会话窗口的数据窗口聚合流程如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vcsdq1kj22gu0xaq5w.jpg" alt="undefined"></p><p>该 Window 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供自定义 Window</span></span><br><span class="line"><span class="keyword">public</span> &lt;W extends Window&gt; <span class="function">WindowedStream&lt;T, KEY, W&gt; <span class="title">window</span><span class="params">(WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; assigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WindowedStream&lt;&gt;(<span class="keyword">this</span>, assigner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="如何自定义-Window？"><a href="#如何自定义-Window？" class="headerlink" title="如何自定义 Window？"></a>如何自定义 Window？</h3><p>当然除了上面几种自带的 Window 外，Apache Flink 还提供了用户可自定义的 Window，那么该如何操作呢？其实细心的同学可能已经发现了上面我写的每种 Window 的实现方式了，它们有 assigner、 evictor、trigger。如果你没发现的话，也不要紧，这里我们就来了解一下实现 Window 的机制，这样我们才能够更好的学会如何自定义 Window。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vdsn6xyj21nc12qq3x.jpg" alt="undefined"></p><h3 id="3-2-8-Window-源码定义"><a href="#3-2-8-Window-源码定义" class="headerlink" title="3.2.8 Window 源码定义"></a>3.2.8 Window 源码定义</h3><p>上面说了 Flink 中自带的 Window，主要利用了 KeyedStream 的 API 来实现，我们这里来看下 Window 的源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//获取属于此窗口的最大时间戳</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">maxTimestamp</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Window 这个抽象类有如下实现类：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vi0o960j20y20as3yg.jpg" alt="undefined"></p><p><strong>TimeWindow</strong> 源码定义如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//窗口开始时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> start;</span><br><span class="line">    <span class="comment">//窗口结束时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> end;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>GlobalWindow</strong> 源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> GlobalWindow INSTANCE = <span class="keyword">new</span> GlobalWindow();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">GlobalWindow</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">    <span class="comment">//对外提供 get() 方法返回 GlobalWindow 实例，并且是个全局单例</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> GlobalWindow <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Window-组件之-WindowAssigner-使用及源码分析"><a href="#Window-组件之-WindowAssigner-使用及源码分析" class="headerlink" title="Window 组件之 WindowAssigner 使用及源码分析"></a>Window 组件之 WindowAssigner 使用及源码分析</h3><p>到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。</p><p>窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。我们来看下 WindowAssigner 的代码的定义吧：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowAssigner</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//分配数据到窗口并返回窗口集合</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Collection&lt;W&gt; <span class="title">assignWindows</span><span class="params">(T element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 WindowAssigner 这个抽象类有如下实现类：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vlvjsgdj22ro0p0wey.jpg" alt="undefined"></p><p>这些 WindowAssigner 实现类的作用介绍：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vpz366lj20iv08gn0z.jpg" alt="TIM截图20191218145154.png"></p><p>如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，比如我拿 TumblingEventTimeWindows 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TumblingEventTimeWindows</span> <span class="keyword">extends</span> <span class="title">WindowAssigner</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> size;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">TumblingEventTimeWindows</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> offset)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (Math.abs(offset) &gt;= size) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"TumblingEventTimeWindows parameters must satisfy abs(offset) &lt; size"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.size = size;</span><br><span class="line">        <span class="keyword">this</span>.offset = offset;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 WindowAssigner 抽象类中的抽象方法 assignWindows</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title">assignWindows</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//实现该 TumblingEventTimeWindows 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他方法，对外提供静态方法，供其他类调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面你就会发现<strong>套路</strong>：</p><p>1、定义好实现类的属性</p><p>2、根据定义的属性添加构造方法</p><p>3、重写 WindowAssigner 中的 assignWindows 等方法</p><p>4、定义其他的方法供外部调用</p><h3 id="Window-组件之-Trigger-使用及源码分析"><a href="#Window-组件之-Trigger-使用及源码分析" class="headerlink" title="Window 组件之 Trigger 使用及源码分析"></a>Window 组件之 Trigger 使用及源码分析</h3><p>Trigger 表示触发器，每个窗口都拥有一个 Trigger（触发器），该 Trigger 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发，即清除（删除窗口并丢弃其内容），或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。</p><p>说了这么一大段，我们还是来看看 Trigger 的源码，定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Trigger</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//当有数据进入到 Window 运算符就会触发该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onElement</span><span class="params">(T element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的处理时间计时器触发时调用</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的事件时间计时器触发时调用该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当有数据流入 Window 运算符时就会触发 onElement 方法、当处理时间和事件时间生效时会触发 onProcessingTime 和 onEventTime 方法。每个触发动作的返回结果用 TriggerResult 定义。继续来看下 TriggerResult 的源码定义：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> TriggerResult &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//不做任何操作</span></span><br><span class="line">    CONTINUE(<span class="keyword">false</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理并移除窗口中的数据</span></span><br><span class="line">    FIRE_AND_PURGE(<span class="keyword">true</span>, <span class="keyword">true</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理窗口数据，窗口计算后不做清理</span></span><br><span class="line">    FIRE(<span class="keyword">true</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//清除窗口中的所有元素，并且在不计算窗口函数或不发出任何元素的情况下丢弃窗口</span></span><br><span class="line">    PURGE(<span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Trigger 这个抽象类有如下实现类：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vqk5hxnj22ua0is3ys.jpg" alt="undefined"></p><p>这些 Trigger 实现类的作用介绍：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vqqwlzej227017sdnt.jpg" alt="undefined"></p><p>如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，拿 CountTrigger 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountTrigger</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Trigger</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReducingStateDescriptor&lt;Long&gt; stateDesc = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(<span class="string">"count"</span>, <span class="keyword">new</span> Sum(), LongSerializer.INSTANCE);</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountTrigger</span><span class="params">(<span class="keyword">long</span> maxCount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = maxCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写抽象类 Trigger 中的抽象方法 </span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//实现 CountTrigger 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>套路</strong>：</p><ol><li>定义好实现类的属性</li><li>根据定义的属性添加构造方法</li><li>重写 Trigger 中的 onElement、onEventTime、onProcessingTime 等方法</li><li>定义其他的方法供外部调用</li></ol><h3 id="Window-组件之-Evictor-使用及源码分析"><a href="#Window-组件之-Evictor-使用及源码分析" class="headerlink" title="Window 组件之 Evictor 使用及源码分析"></a>Window 组件之 Evictor 使用及源码分析</h3><p>Evictor 表示驱逐者，它可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素，然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。</p><p>我们来看看 Evictor 的源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Evictor</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//在窗口函数之前调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">    <span class="comment">//在窗口函数之后调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Evictor 这个接口有如下实现类：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vrfwkxpj215m0f8q30.jpg" alt="undefined"></p><p>这些 Evictor 实现类的作用介绍：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vro7oi3j225y0ts0vk.jpg" alt="undefined"></p><p>如果你细看了上面三种中某个类的实现的话，你会发现一个规律，比如我就拿 CountEvictor 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountEvictor</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Evictor</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> doEvictAfter;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count, <span class="keyword">boolean</span> doEvictAfter)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = doEvictAfter;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictBefore 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictAfter 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">evict</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//内部的关键实现方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他的方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现<strong>套路</strong>：</p><ol><li>定义好实现类的属性</li><li>根据定义的属性添加构造方法</li><li>重写 Evictor 中的 evictBefore 和 evictAfter 方法</li><li>定义关键的内部实现方法 evict，处理具体的逻辑</li><li>定义其他的方法供外部调用</li></ol><p>上面我们详细讲解了 Window 中的组件 WindowAssigner、Trigger、Evictor，然后继续回到问题：如何自定义 Window？</p><p>上文讲解了 Flink 自带的 Window（Time Window、Count Window、Session Window），然后还分析了他们的源码实现，通过这几个源码，我们可以发现，它最后调用的都有一个方法，那就是 Window 方法，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供自定义 Window</span></span><br><span class="line"><span class="keyword">public</span> &lt;W extends Window&gt; <span class="function">WindowedStream&lt;T, KEY, W&gt; <span class="title">window</span><span class="params">(WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; assigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WindowedStream&lt;&gt;(<span class="keyword">this</span>, assigner);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//构造一个 WindowedStream 实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">WindowedStream</span><span class="params">(KeyedStream&lt;T, K&gt; input,</span></span></span><br><span class="line"><span class="function"><span class="params">        WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; windowAssigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.input = input;</span><br><span class="line">    <span class="keyword">this</span>.windowAssigner = windowAssigner;</span><br><span class="line">    <span class="comment">//获取一个默认的 Trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = windowAssigner.getDefaultTrigger(input.getExecutionEnvironment());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这个 Window 方法传入的参数是一个 WindowAssigner 对象（你可以利用 Flink 现有的 WindowAssigner，也可以根据上面的方法来自定义自己的 WindowAssigner），然后再通过构造一个 WindowedStream 实例（在构造实例的会传入 WindowAssigner 和获取默认的 Trigger）来创建一个 Window。</p><p>另外你可以看到滑动计数窗口，在调用 window 方法之后，还调用了 WindowedStream 的 evictor 和 trigger 方法，trigger 方法会覆盖掉你之前调用 Window 方法中默认的 trigger，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//trigger 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">trigger</span><span class="params">(Trigger&lt;? <span class="keyword">super</span> T, ? <span class="keyword">super</span> W&gt; trigger)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner &amp;&amp; !trigger.canMerge()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"A merging window assigner cannot be used with a trigger that does not support merging."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> BaseAlignedWindowAssigner) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Cannot use a "</span> + windowAssigner.getClass().getSimpleName() + <span class="string">" with a custom trigger."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//覆盖之前的 trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = trigger;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的各种窗口实现，你就会发现了：Evictor 是可选的，但是 WindowAssigner 和 Trigger 是必须会有的，这种创建 Window 的方法充分利用了 KeyedStream 和 WindowedStream 的 API，再加上现有的 WindowAssigner、Trigger、Evictor，你就可以创建 Window 了，另外你还可以自定义这三个窗口组件的实现类来满足你公司项目的需求。</p><h3 id="小结与反思-8"><a href="#小结与反思-8" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节从生活案例来分享关于 Window 方面的需求，进而开始介绍 Window 相关的知识，并把 Flink 中常使用的三种窗口都一一做了介绍，并告诉大家如何使用，还分析了其实现原理。最后还对 Window 的内部组件做了详细的分析，为自定义 Window 提供了方法。</p><p>不知道你看完本节后对 Window 还有什么疑问吗？你们是根据什么条件来选择使用哪种 Window 的？在使用的过程中有遇到什么问题吗？</p><h2 id="十、数据转换必须熟悉的算子（Operator）"><a href="#十、数据转换必须熟悉的算子（Operator）" class="headerlink" title="十、数据转换必须熟悉的算子（Operator）"></a>十、数据转换必须熟悉的算子（Operator）</h2><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vsgous8j21ri0dcmxj.jpg" alt="undefined"></p><p>在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是上图这种模型，有数据源（source），有数据下游（sink），我们写的应用程序多是对数据源过来的数据做一系列操作，总结如下。</p><ol><li><strong>Source</strong>: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</li><li><strong>Transformation</strong>: 数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</li><li><strong>Sink</strong>: 接收器，Sink 是指 Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来。Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 Socket 、自定义的 Sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 Sink。</li></ol><p>那么本文将给大家介绍的就是 Flink 中的批和流程序常用的算子（Operator）。</p><h3 id="DataStream-Operator"><a href="#DataStream-Operator" class="headerlink" title="DataStream Operator"></a>DataStream Operator</h3><p>我们先来看看流程序中常用的算子。</p><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><p>Map 算子的输入流是 DataStream，经过 Map 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成一个元素，举个例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; map = employeeStream.map(<span class="keyword">new</span> MapFunction&lt;Employee, Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Employee <span class="title">map</span><span class="params">(Employee employee)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        employee.salary = employee.salary + <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> employee;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">map.print();</span><br></pre></td></tr></table></figure><p>新的一年给每个员工的工资加 5000。</p><h4 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h4><p>FlatMap 算子的输入流是 DataStream，经过 FlatMap 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成零个、一个或多个元素，举个例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; flatMap = employeeStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Employee, Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Employee employee, Collector&lt;Employee&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (employee.salary &gt;= <span class="number">40000</span>) &#123;</span><br><span class="line">            out.collect(employee);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">flatMap.print();</span><br></pre></td></tr></table></figure><p>将工资大于 40000 的找出来。</p><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0xjwko8fj21200aqaa3.jpg" alt="undefined"></p><p>对每个元素都进行判断，返回为 true 的元素，如果为 false 则丢弃数据，上面找出工资大于 40000 的员工其实也可以用 Filter 来做：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; filter = employeeStream.filter(<span class="keyword">new</span> FilterFunction&lt;Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Employee employee)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (employee.salary &gt;= <span class="number">40000</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">filter.print();</span><br></pre></td></tr></table></figure><h4 id="KeyBy"><a href="#KeyBy" class="headerlink" title="KeyBy"></a>KeyBy</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0xkegqp2j213s0ju3yu.jpg" alt="undefined"></p><p>KeyBy 在逻辑上是基于 key 对流进行分区，相同的 Key 会被分到一个分区（这里分区指的就是下游算子多个并行节点的其中一个）。在内部，它使用 hash 函数对流进行分区。它返回 KeyedDataStream 数据流。举个例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream&lt;ProductEvent, Integer&gt; keyBy = productStream.keyBy(<span class="keyword">new</span> KeySelector&lt;ProductEvent, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(ProductEvent product)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> product.shopId;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">keyBy.print();</span><br></pre></td></tr></table></figure><p>根据商品的店铺 id 来进行分区。</p><h4 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h4><p>Reduce 返回单个的结果值，并且 reduce 操作每处理一个元素总是创建一个新值。常用的方法有 average、sum、min、max、count，使用 Reduce 方法都可实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; reduce = employeeStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Employee, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Employee employee)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> employee.shopId;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).reduce(<span class="keyword">new</span> ReduceFunction&lt;Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Employee <span class="title">reduce</span><span class="params">(Employee employee1, Employee employee2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        employee1.salary = (employee1.salary + employee2.salary) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">return</span> employee1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">reduce.print();</span><br></pre></td></tr></table></figure><p>上面先将数据流进行 keyby 操作，因为执行 Reduce 操作只能是 KeyedStream，然后将员工的工资做了一个求平均值的操作。</p><h4 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h4><p>DataStream API 支持各种聚合，例如 min、max、sum 等。 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream.sum(<span class="number">0</span>) </span><br><span class="line">KeyedStream.sum(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.min(<span class="number">0</span>) </span><br><span class="line">KeyedStream.min(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.max(<span class="number">0</span>) </span><br><span class="line">KeyedStream.max(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.minBy(<span class="number">0</span>) </span><br><span class="line">KeyedStream.minBy(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.maxBy(<span class="number">0</span>) </span><br><span class="line">KeyedStream.maxBy(<span class="string">"key"</span>)</span><br></pre></td></tr></table></figure><p>max 和 maxBy 之间的区别在于 max 返回流中的最大值，但 maxBy 返回具有最大值的键， min 和 minBy 同理。</p><h4 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h4><p>Window 函数允许按时间或其他条件对现有 KeyedStream 进行分组。 以下是以 10 秒的时间窗口聚合：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.keyBy(<span class="number">0</span>).window(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><p>有时候因为业务需求场景要求：聚合一分钟、一小时的数据做统计报表使用。</p><h4 id="WindowAll"><a href="#WindowAll" class="headerlink" title="WindowAll"></a>WindowAll</h4><p>WindowAll 将元素按照某种特性聚集在一起，该函数不支持并行操作，默认的并行度就是 1，所以如果使用这个算子的话需要注意一下性能问题，以下是使用例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.keyBy(<span class="number">0</span>).windowAll(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)));</span><br></pre></td></tr></table></figure><h4 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0y9pd8g3j210u0heq35.jpg" alt="undefined"></p><p>Union 函数将两个或多个数据流结合在一起。 这样后面在使用的时候就只需使用一个数据流就行了。 如果我们将一个流与自身组合，那么组合后的数据流会有两份同样的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.union(inputStream1, inputStream2, ...);</span><br></pre></td></tr></table></figure><h4 id="Window-Join"><a href="#Window-Join" class="headerlink" title="Window Join"></a>Window Join</h4><p>我们可以通过一些 key 将同一个 window 的两个数据流 join 起来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputStream.join(inputStream1)</span><br><span class="line">           .where(<span class="number">0</span>).equalTo(<span class="number">1</span>)</span><br><span class="line">           .window(Time.seconds(<span class="number">5</span>))     </span><br><span class="line">           .apply (<span class="keyword">new</span> JoinFunction () &#123;...&#125;);</span><br></pre></td></tr></table></figure><p>以上示例是在 5 秒的窗口中连接两个流，其中第一个流的第一个属性的连接条件等于另一个流的第二个属性。</p><h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ybr9m9lj212e0h4mxg.jpg" alt="undefined"></p><p>此功能根据条件将流拆分为两个或多个流。 当你获得混合流然后你可能希望单独处理每个数据流时，可以使用此方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split = inputStream.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;(); </span><br><span class="line">        <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            output.add(<span class="string">"even"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            output.add(<span class="string">"odd"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>上面就是将偶数数据流放在 even 中，将奇数数据流放在 odd 中。</p><h4 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yg0tundj21gg0ggq3e.jpg" alt="undefined"></p><p>上面用 Split 算子将数据流拆分成两个数据流（奇数、偶数），接下来你可能想从拆分流中选择特定流，那么就得搭配使用 Select 算子（一般这两者都是搭配在一起使用的），</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split;</span><br><span class="line">DataStream&lt;Integer&gt; even = split.select(<span class="string">"even"</span>); </span><br><span class="line">DataStream&lt;Integer&gt; odd = split.select(<span class="string">"odd"</span>); </span><br><span class="line">DataStream&lt;Integer&gt; all = split.select(<span class="string">"even"</span>,<span class="string">"odd"</span>);</span><br></pre></td></tr></table></figure><p>我们就介绍这么些常用的算子了，当然肯定也会有遗漏，具体还得查看官网 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/</a> 的介绍。</p><h3 id="DataSet-Operator"><a href="#DataSet-Operator" class="headerlink" title="DataSet Operator"></a>DataSet Operator</h3><p>上面介绍了 DataStream 的常用算子，其实上面也有一些算子也是同样适合于 DataSet 的，比如 Map、FlatMap、Filter 等（相同的我就不再重复了）；也有一些算子是 DataSet API 独有的，比如 DataStream 中分区使用的是 KeyBy，但是 DataSet 中使用的是 GroupBy。</p><h4 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = </span><br><span class="line"><span class="comment">// 返回 DataSet 中前 5 的元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out1 = in.first(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回分组后每个组的前 2 元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out2 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .first(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回分组后每个组的前 3 元素（按照上升排序）</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out3 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .sortGroup(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                          .first(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>还有一些，感兴趣的可以查看官网 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/dataset_transformations.html。" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/dataset_transformations.html。</a></p><h3 id="流批统一的思路"><a href="#流批统一的思路" class="headerlink" title="流批统一的思路"></a>流批统一的思路</h3><p>一般公司里的业务场景需求肯定不止是只有批计算，也不只是有流计算的。一般这两种需求是都存在的。比如每天凌晨 00:00 去算昨天一天商品的售卖情况，然后出报表给运营或者老板去分析；另外的就是处理实时的数据。</p><p>但是这样就会有一个问题，需要让开发掌握两套 API。有些数据工程师的开发能力可能并不高，他们会更擅长写一些 SQL 去分析，所以要是掌握两套 API 的话，对他们来说成本可能会很大。要是 Flink 能够提供一种高级的 API，上层做好完全封装，让开发无感知底层到底运行的是 DataSet 还是 DataStream API，这样不管是开发还是数据工程师只需要学习一套高级的 API 就行。</p><p>Flink 社区包括阿里巴巴实时计算团队也在大力推广这块，那就是我们的 Flink Table API &amp; SQL，在 Flink 1.9 版本，开源版本的 Blink 大部分代码已经合进去了，期待阿里实时计算团队为社区带来更多的贡献。</p><p>对于开发人员来说，流批统一的引擎（Table API &amp; SQL）在执行之前会根据运行的环境翻译成 DataSet 或者 DataStream API。因为这两种 API 底层的实现有很大的区别，所以在统一流和批的过程中遇到了不少挑战。</p><ul><li>理论基础：动态表</li><li>架构改进（统一的 Operator 框架、统一的查询处理）</li><li>优化器的统一</li><li>基础数据结构的统一</li><li>物理实现的共享</li></ul><p>关于 Table API &amp; SQL，在进阶篇第五章中有讲解！</p><h3 id="小结与反思-9"><a href="#小结与反思-9" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节介绍了在开发 Flink 作业中数据转换常使用的算子（包含流作业和批作业），DataStream API 和 DataSet API 中部分算子名字是一致的，也有不同的地方，最后讲解了下 Flink 社区后面流批统一的思路。</p><p>你们公司使用 Flink 是流作业居多还是批作业居多？</p><h2 id="十一、如何使用-DataStream-API-来处理数据？"><a href="#十一、如何使用-DataStream-API-来处理数据？" class="headerlink" title="十一、如何使用 DataStream API 来处理数据？"></a>十一、如何使用 DataStream API 来处理数据？</h2><p>在 3.3 节中讲了数据转换常用的 Operators（算子），然后在 3.2 节中也讲了 Flink 中窗口的概念和原理，那么我们这篇文章再来细讲一下 Flink 中的各种 DataStream API。</p><p>我们先来看下源码里面的 DataStream 大概有哪些类呢？</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yi4jstvj215m17ejss.jpg" alt="undefined"></p><p>可以发现其实还是有很多的类，只有熟练掌握了这些 API，我们才能在做数据转换和计算的时候足够灵活的运用开来（知道何时该选用哪种 DataStream？选用哪个 Function？）。那么我们先从 DataStream 开始吧！</p><h3 id="DataStream-如何使用及分析"><a href="#DataStream-如何使用及分析" class="headerlink" title="DataStream 如何使用及分析"></a>DataStream 如何使用及分析</h3><p>首先我们来看下 DataStream 这个类的定义吧：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A DataStream represents a stream of elements of the same type. A DataStreamcan be transformed into another DataStream by applying a transformation as</span><br><span class="line"> DataStream#map or DataStream#filter&#125;</span><br></pre></td></tr></table></figure><p>大概意思是：DataStream 表示相同类型的元素组成的数据流，一个数据流可以通过 map/filter 等算子转换成另一个数据流。</p><p>然后 DataStream 的类结构图如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yppinynj21cy0fodfz.jpg" alt="undefined"></p><p>它的继承类有 KeyedStream、SingleOutputStreamOperator 和 SplitStream。这几个类本文后面都会一一给大家讲清楚。下面我们来看看 DataStream 这个类中的属性和方法吧。</p><p>它的属性就只有两个：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> StreamExecutionEnvironment environment;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> StreamTransformation&lt;T&gt; transformation;</span><br></pre></td></tr></table></figure><p>但是它的方法却有很多，并且我们平时写的 Flink Job 几乎离不开这些方法，这也注定了这个类的重要性，所以得好好看下这些方法该如何使用，以及是如何实现的。</p><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>通过合并相同数据类型的数据流，然后创建一个新的数据流，union 方法代码实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> DataStream&lt;T&gt; <span class="title">union</span><span class="params">(DataStream&lt;T&gt;... streams)</span> </span>&#123;</span><br><span class="line">    List&lt;StreamTransformation&lt;T&gt;&gt; unionedTransforms = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    unionedTransforms.add(<span class="keyword">this</span>.transformation);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (DataStream&lt;T&gt; newStream : streams) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!getType().equals(newStream.getType())) &#123;   <span class="comment">//判断数据类型是否一致</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot union streams of different types: "</span> + getType() + <span class="string">" and "</span> + newStream.getType());</span><br><span class="line">        &#125;</span><br><span class="line">        unionedTransforms.add(newStream.getTransformation());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构建新的数据流</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DataStream&lt;&gt;(<span class="keyword">this</span>.environment, <span class="keyword">new</span> UnionTransformation&lt;&gt;(unionedTransforms));<span class="comment">//通过使用 UnionTransformation 将多个 StreamTransformation 合并起来</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么我们该如何去使用 union 呢（不止连接一个数据流，也可以连接多个数据流）？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//数据流 1 和 2</span></span><br><span class="line"><span class="keyword">final</span> DataStream&lt;Integer&gt; stream1 = env.addSource(...);</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;Integer&gt; stream2 = env.addSource(...);</span><br><span class="line"><span class="comment">//union</span></span><br><span class="line">stream1.union(stream2)</span><br></pre></td></tr></table></figure><h4 id="split"><a href="#split" class="headerlink" title="split"></a>split</h4><p>该方法可以将两个数据流进行拆分，拆分后的数据流变成了 SplitStream（在下文会详细介绍这个类的内部实现），该 split 方法通过传入一个 OutputSelector 参数进行数据选择，方法内部实现就是构造一个 SplitStream 对象然后返回：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SplitStream&lt;T&gt; <span class="title">split</span><span class="params">(OutputSelector&lt;T&gt; outputSelector)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> SplitStream&lt;&gt;(<span class="keyword">this</span>, clean(outputSelector));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后我们该如何使用这个方法呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">dataStream.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">8354166915727490130L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; s = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        <span class="keyword">if</span> (value &gt; <span class="number">4</span>) &#123;    <span class="comment">//大于 4 的数据放到 &gt; 这个 tag 里面去</span></span><br><span class="line">            s.add(<span class="string">"&gt;"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;    <span class="comment">//小于等于 4 的数据放到 &lt; 这个 tag 里面去</span></span><br><span class="line">            s.add(<span class="string">"&lt;"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>注意：该方法已经不推荐使用了！在 1.7 版本以后建议使用 Side Output 来实现分流操作。</p><h4 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h4><p>通过连接不同或相同数据类型的数据流，然后创建一个新的连接数据流，如果连接的数据流也是一个 DataStream 的话，那么连接后的数据流为 ConnectedStreams（会在下文介绍这个类的具体实现），它的具体实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">ConnectedStreams&lt;T, R&gt; <span class="title">connect</span><span class="params">(DataStream&lt;R&gt; dataStream)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ConnectedStreams&lt;&gt;(environment, <span class="keyword">this</span>, dataStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果连接的数据流是一个 BroadcastStream（广播数据流），那么连接后的数据流是一个 BroadcastConnectedStream（会在下文详细介绍该类的内部实现），它的具体实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">BroadcastConnectedStream&lt;T, R&gt; <span class="title">connect</span><span class="params">(BroadcastStream&lt;R&gt; broadcastStream)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BroadcastConnectedStream&lt;&gt;(</span><br><span class="line">            environment, <span class="keyword">this</span>, Preconditions.checkNotNull(broadcastStream), </span><br><span class="line">            broadcastStream.getBroadcastStateDescriptor());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、连接 DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、连接 BroadcastStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line"><span class="keyword">final</span> BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor);</span><br><span class="line">BroadcastConnectedStream&lt;Tuple2&lt;Long, Long&gt;, String&gt; connect = src1.connect(broadcast);</span><br></pre></td></tr></table></figure><h4 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h4><p>keyBy 方法是用来将数据进行分组的，通过该方法可以将具有相同 key 的数据划分在一起组成新的数据流，该方法有四种（它们的参数各不一样）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、参数是 KeySelector 对象</span></span><br><span class="line"><span class="keyword">public</span> &lt;K&gt; <span class="function">KeyedStream&lt;T, K&gt; <span class="title">keyBy</span><span class="params">(KeySelector&lt;T, K&gt; key)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(key));<span class="comment">//构造 KeyedStream 对象</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、参数是 KeySelector 对象和 TypeInformation 对象</span></span><br><span class="line"><span class="keyword">public</span> &lt;K&gt; <span class="function">KeyedStream&lt;T, K&gt; <span class="title">keyBy</span><span class="params">(KeySelector&lt;T, K&gt; key, TypeInformation&lt;K&gt; keyType)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(key), keyType);<span class="comment">//构造 KeyedStream 对象</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、参数是 1 至多个字段（用 0、1、2... 表示）</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(<span class="keyword">int</span>... fields)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (getType() <span class="keyword">instanceof</span> BasicArrayTypeInfo || getType() <span class="keyword">instanceof</span> PrimitiveArrayTypeInfo) &#123;</span><br><span class="line">        <span class="keyword">return</span> keyBy(KeySelectorUtil.getSelectorForArray(fields, getType()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> keyBy(<span class="keyword">new</span> Keys.ExpressionKeys&lt;&gt;(fields, getType()));<span class="comment">//调用 private 的 keyBy 方法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4、参数是 1 至多个字符串</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(String... fields)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> keyBy(<span class="keyword">new</span> Keys.ExpressionKeys&lt;&gt;(fields, getType()));<span class="comment">//调用 private 的 keyBy 方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//真正调用的方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(Keys&lt;T&gt; keys)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(KeySelectorUtil.getSelectorForKeys(keys,</span><br><span class="line">            getType(), getExecutionConfig())));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如何使用呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Event&gt; dataStream = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"zhisheng01"</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"zhisheng02"</span>, <span class="number">2.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"zhisheng03"</span>, <span class="number">2.1</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"zhisheng04"</span>, <span class="number">3.0</span>),</span><br><span class="line">    <span class="keyword">new</span> SubEvent(<span class="number">4</span>, <span class="string">"zhisheng05"</span>, <span class="number">4.0</span>, <span class="number">1.0</span>),</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第1种</span></span><br><span class="line">dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.getId();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第2种</span></span><br><span class="line">dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.getId();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;, Types.STRING);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第3种</span></span><br><span class="line">dataStream.keyBy(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第4种</span></span><br><span class="line">dataStream.keyBy(<span class="string">"zhisheng01"</span>, <span class="string">"zhisheng02"</span>);</span><br></pre></td></tr></table></figure><h4 id="partitionCustom"><a href="#partitionCustom" class="headerlink" title="partitionCustom"></a>partitionCustom</h4><p>使用自定义分区器在指定的 key 字段上将 DataStream 分区，这个 partitionCustom 有 3 个不同参数的方法，分别要传入的参数有自定义分区 Partitioner 对象、位置、字符和 KeySelector。它们内部也都是调用了私有的 partitionCustom 方法。</p><h4 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h4><p>broadcast 是将数据流进行广播，然后让下游的每个并行 Task 中都可以获取到这份数据流，通常这些数据是一些配置，一般这些配置数据的数据量不能太大，否则资源消耗会比较大。这个 broadcast 方法也有两个，一个是无参数，它返回的数据是 DataStream；另一种的参数是 MapStateDescriptor，它返回的参数是 BroadcastStream（这个也会在下文详细介绍）。</p><p>使用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、第一种</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; source = env.addSource(...).broadcast();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、第二种</span></span><br><span class="line"><span class="keyword">final</span> MapStateDescriptor&lt;Long, String&gt; utterDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">"broadcast-state"</span>, BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO</span><br><span class="line">);</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;String&gt; srcTwo = env.fromCollection(expected.values());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor);</span><br></pre></td></tr></table></figure><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>map 方法需要传入的参数是一个 MapFunction，当然传入 RichMapFunction 也是可以的，它返回的是 SingleOutputStreamOperator（这个类在会在下文详细介绍），该 map 方法里面的实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span><span class="params">(MapFunction&lt;T, R&gt; mapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;R&gt; outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(),</span><br><span class="line">            Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//调用 transform 方法</span></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Map"</span>, outType, <span class="keyword">new</span> StreamMap&lt;&gt;(clean(mapper)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法平时使用的非常频繁，然后我们该如何使用这个方法呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataStream.map(<span class="keyword">new</span> MapFunction&lt;Integer, String&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>flatMap 方法需要传入一个 FlatMapFunction 参数，当然传入 RichFlatMapFunction 也是可以的，如果你的 Flink Job 里面有连续的 filter 和 map 算子在一起，可以考虑使用 flatMap 一个算子来完成两个算子的工作，它返回的是 SingleOutputStreamOperator，该 flatMap 方法里面的实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">flatMap</span><span class="params">(FlatMapFunction&lt;T, R&gt; flatMapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;R&gt; outType = TypeExtractor.getFlatMapReturnTypes(clean(flatMapper),</span><br><span class="line">            getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//调用 transform 方法</span></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Flat Map"</span>, outType, <span class="keyword">new</span> StreamFlatMap&lt;&gt;(clean(flatMapper)));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法平时使用的非常频繁，使用方式如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Integer value, Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        out.collect(value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h4 id="process"><a href="#process" class="headerlink" title="process"></a>process</h4><p>在输入流上应用给定的 ProcessFunction，从而创建转换后的输出流，通过该方法返回的是 SingleOutputStreamOperator，具体代码实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">process</span><span class="params">(ProcessFunction&lt;T, R&gt; processFunction)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;R&gt; outType = TypeExtractor.getUnaryOperatorReturnType(</span><br><span class="line">        processFunction, ProcessFunction.class, <span class="number">0</span>, <span class="number">1</span>,</span><br><span class="line">        TypeExtractor.NO_INDEX, getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//调用下面的 process 方法</span></span><br><span class="line">    <span class="keyword">return</span> process(processFunction, outType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ProcessFunction&lt;T, R&gt; processFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">        TypeInformation&lt;R&gt; outputType)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    ProcessOperator&lt;T, R&gt; operator = <span class="keyword">new</span> ProcessOperator&lt;&gt;(clean(processFunction));</span><br><span class="line">    <span class="comment">//调用 transform 方法</span></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Process"</span>, outputType, operator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Long&gt; data = env.generateSequence(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义的 ProcessFunction</span></span><br><span class="line">ProcessFunction&lt;Long, Integer&gt; processFunction = <span class="keyword">new</span> ProcessFunction&lt;Long, Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Long value, Context ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">            Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">            Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; processed = data.keyBy(<span class="keyword">new</span> IdentityKeySelector&lt;Long&gt;()).process(processFunction);</span><br></pre></td></tr></table></figure><h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p>filter 用来过滤数据的，它需要传入一个 FilterFunction，然后返回的数据也是 SingleOutputStreamOperator，该方法的实现是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">filter</span><span class="params">(FilterFunction&lt;T&gt; filter)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Filter"</span>, getType(), <span class="keyword">new</span> StreamFilter&lt;&gt;(clean(filter)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法平时使用非常多：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; filter1 = src</span><br><span class="line">    .filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"zhisheng"</span>.equals(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><p>上面这些方法是平时写代码时用的非常多的方法，我们这里讲解了它们的实现原理和使用方式，当然还有其他方法，比如 assignTimestampsAndWatermarks、join、shuffle、forward、addSink、rebalance、iterate、coGroup、project、timeWindowAll、countWindowAll、windowAll、print 等，这里由于篇幅的问题就不一一展开来讲了。</p><h3 id="SingleOutputStreamOperator-如何使用及分析"><a href="#SingleOutputStreamOperator-如何使用及分析" class="headerlink" title="SingleOutputStreamOperator 如何使用及分析"></a>SingleOutputStreamOperator 如何使用及分析</h3><p>SingleOutputStreamOperator 这个类继承自 DataStream，所以 DataStream 中有的方法在这里也都有，那么这里就讲解下额外的方法的作用，如下。</p><ul><li>name()：该方法可以设置当前数据流的名称，如果设置了该值，则可以在 Flink UI 上看到该值；uid() 方法可以为算子设置一个指定的 ID，该 ID 有个作用就是如果想从 savepoint 恢复 Job 时是可以根据这个算子的 ID 来恢复到它之前的运行状态；</li><li>setParallelism() ：该方法是为每个算子单独设置并行度的，这个设置优先于你通过 env 设置的全局并行度；</li><li>setMaxParallelism() ：该为算子设置最大的并行度；</li><li>setResources()：该方法有两个（参数不同），设置算子的资源，但是这两个方法对外还没开放（是私有的，暂时功能性还不全）；</li><li>forceNonParallel()：该方法强行将并行度和最大并行度都设置为 1；</li><li>setChainingStrategy()：该方法对给定的算子设置 ChainingStrategy；</li><li>disableChaining()：该这个方法设置后将禁止该算子与其他的算子 chain 在一起；</li><li>getSideOutput()：该方法通过给定的 OutputTag 参数从 side output 中来筛选出对应的数据流。</li></ul><h3 id="KeyedStream-如何使用及分析"><a href="#KeyedStream-如何使用及分析" class="headerlink" title="KeyedStream 如何使用及分析"></a>KeyedStream 如何使用及分析</h3><p>KeyedStream 是 DataStream 在根据 KeySelector 分区后的数据流，DataStream 中常用的方法在 KeyedStream 后也可以用（除了 shuffle、forward 和 keyBy 等分区方法），在该类中的属性分别是 KeySelector 和 TypeInformation。</p><p>DataStream 中的窗口方法只有 timeWindowAll、countWindowAll 和 windowAll 这三种全局窗口方法，但是在 KeyedStream 类中的种类就稍微多了些，新增了 timeWindow、countWindow 方法，并且是还支持滑动窗口。</p><p>除了窗口方法的新增外，还支持大量的聚合操作方法，比如 reduce、fold、sum、min、max、minBy、maxBy、aggregate 等方法（列举的这几个方法都支持多种参数的）。</p><p>最后就是它还有 asQueryableState() 方法，能够将 KeyedStream 发布为可查询的 ValueState 实例。</p><h3 id="SplitStream-如何使用及分析"><a href="#SplitStream-如何使用及分析" class="headerlink" title="SplitStream 如何使用及分析"></a>SplitStream 如何使用及分析</h3><p>SplitStream 这个类比较简单，它代表着数据分流后的数据流了，它有一个 select 方法可以选择分流后的哪种数据流了，通常它是结合 split 使用的，对于单次分流来说还挺方便的。但是它是一个被废弃的类（Flink 1.7 后被废弃的，可以看下笔者之前写的一篇文章 <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/" target="_blank" rel="noopener">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ），其实可以用 side output 来代替这种 split，后面文章中我们也会讲通过简单的案例来讲解一下该如何使用 side output 做数据分流操作。</p><p>因为这个类的源码比较少，我们可以看下这个类的实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStream</span>&lt;<span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">DataStream</span>&lt;<span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">SplitStream</span><span class="params">(DataStream&lt;OUT&gt; dataStream, OutputSelector&lt;OUT&gt; outputSelector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(dataStream.getExecutionEnvironment(), <span class="keyword">new</span> SplitTransformation&lt;OUT&gt;(dataStream.getTransformation(), outputSelector));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//选择要输出哪种数据流</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataStream&lt;OUT&gt; <span class="title">select</span><span class="params">(String... outputNames)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> selectOutput(outputNames);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//上面那个 public 方法内部调用的就是这个方法，该方法是个 private 方法，对外隐藏了它是如何去找到特定的数据流。</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> DataStream&lt;OUT&gt; <span class="title">selectOutput</span><span class="params">(String[] outputNames)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String outName : outputNames) &#123;</span><br><span class="line">            <span class="keyword">if</span> (outName == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Selected names must not be null"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//构造了一个 SelectTransformation 对象</span></span><br><span class="line">        SelectTransformation&lt;OUT&gt; selectTransform = <span class="keyword">new</span> SelectTransformation&lt;OUT&gt;(<span class="keyword">this</span>.getTransformation(), Lists.newArrayList(outputNames));</span><br><span class="line">        <span class="comment">//构造了一个 DataStream 对象</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DataStream&lt;OUT&gt;(<span class="keyword">this</span>.getExecutionEnvironment(), selectTransform);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="WindowedStream-如何使用及分析"><a href="#WindowedStream-如何使用及分析" class="headerlink" title="WindowedStream 如何使用及分析"></a>WindowedStream 如何使用及分析</h3><p>虽然 WindowedStream 不是继承自 DataStream，并且我们在 3.1 节中也做了一定的讲解，但是当时没讲里面的 Function，所以在这里刚好一起做一个补充。</p><p>在 WindowedStream 类中定义的属性有 KeyedStream、WindowAssigner、Trigger、Evictor、allowedLateness 和 lateDataOutputTag。</p><ul><li>KeyedStream：代表着数据流，数据分组后再开 Window</li><li>WindowAssigner：Window 的组件之一</li><li>Trigger：Window 的组件之一</li><li>Evictor：Window 的组件之一（可选）</li><li>allowedLateness：用户指定的允许迟到时间长</li><li>lateDataOutputTag：数据延迟到达的 Side output，如果延迟数据没有设置任何标记，则会被丢弃</li></ul><p>在 3.1 节中我们讲了上面的三个窗口组件 WindowAssigner、Trigger、Evictor，并教大家该如何使用，那么在这篇文章我就不再重复，那么接下来就来分析下其他几个的使用方式和其实现原理。</p><p>先来看下 allowedLateness 这个它可以在窗口后指定允许迟到的时间长，使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>)</span><br><span class="line">    .timeWindow(Time.milliseconds(<span class="number">20</span>))</span><br><span class="line">    .allowedLateness(Time.milliseconds(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>lateDataOutputTag 这个它将延迟到达的数据发送到由给定 OutputTag 标识的 side output（侧输出），当水印经过窗口末尾（并加上了允许的延迟后），数据就被认为是延迟了。</p><p>对于 keyed windows 有五个不同参数的 reduce 方法可以使用，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、参数为 ReduceFunction</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> reduce(function, <span class="keyword">new</span> PassThroughWindowFunction&lt;K, W, T&gt;());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、参数为 ReduceFunction 和 WindowFunction</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> reduce(reduceFunction, function, resultType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、参数为 ReduceFunction、WindowFunction 和 TypeInformation</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> input.transform(opName, resultType, operator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4、参数为 ReduceFunction 和 ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> reduce(reduceFunction, function, resultType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//5、参数为 ReduceFunction、ProcessWindowFunction 和 TypeInformation</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType)</span> </span>&#123;</span><br><span class="line">    ... </span><br><span class="line">    <span class="keyword">return</span> input.transform(opName, resultType, operator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>除了 reduce 方法，还有六个不同参数的 fold 方法、aggregate 方法；两个不同参数的 apply 方法、process 方法（其中你会发现这两个 apply 方法和 process 方法内部其实都隐式的调用了一个私有的 apply 方法）；其实除了前面说的两个不同参数的 apply 方法外，还有四个其他的 apply 方法，这四个方法也是参数不同，但是其实最终的是利用了 transform 方法；还有的就是一些预定义的聚合方法比如 sum、min、minBy、max、maxBy，它们的方法参数的个数不一致，这些预聚合的方法内部调用的其实都是私有的 aggregate 方法，该方法允许你传入一个 AggregationFunction 参数。我们来看一个具体的实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//max</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">max</span><span class="params">(String field)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//内部调用私有的的 aggregate 方法</span></span><br><span class="line">    <span class="keyword">return</span> aggregate(<span class="keyword">new</span> ComparableAggregator&lt;&gt;(field, input.getType(), AggregationFunction.AggregationType.MAX, <span class="keyword">false</span>, input.getExecutionConfig()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//私有的 aggregate 方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">aggregate</span><span class="params">(AggregationFunction&lt;T&gt; aggregator)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//继续调用的是 reduce 方法</span></span><br><span class="line">    <span class="keyword">return</span> reduce(aggregator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//该 reduce 方法内部其实又是调用了其他多个参数的 reduce 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    function = input.getExecutionEnvironment().clean(function);</span><br><span class="line">    <span class="keyword">return</span> reduce(function, <span class="keyword">new</span> PassThroughWindowFunction&lt;K, W, T&gt;());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的方法调用过程，你会发现代码封装的很深，得需要你自己好好跟一下源码才可以了解更深些。</p><p>上面讲了这么多方法，你会发现 reduce 方法其实是用的蛮多的之一，那么就来看看该如何使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span>  </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .print();</span><br></pre></td></tr></table></figure><h3 id="AllWindowedStream-如何使用及分析"><a href="#AllWindowedStream-如何使用及分析" class="headerlink" title="AllWindowedStream 如何使用及分析"></a>AllWindowedStream 如何使用及分析</h3><p>前面讲完了 WindowedStream，再来看看这个 AllWindowedStream 你会发现它的实现其实无太大区别，该类中的属性和方法都和前面 WindowedStream 是一样的，然后我们就不再做过多的介绍，直接来看看该如何使用呢？</p><p>AllWindowedStream 这种场景下是不需要让数据流做 keyBy 分组操作，直接就进行 windowAll 操作，然后在 windowAll 方法中传入 WindowAssigner 参数对象即可，然后返回的数据结果就是 AllWindowedStream 了，下面使用方式继续执行了 AllWindowedStream 中的 reduce 方法来返回数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dataStream.windowAll(SlidingEventTimeWindows.of(Time.of(<span class="number">1</span>, TimeUnit.SECONDS), Time.of(<span class="number">100</span>, TimeUnit.MILLISECONDS)))</span><br><span class="line">    .reduce(<span class="keyword">new</span> RichReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">6448847205314995812L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1,</span></span></span><br><span class="line"><span class="function"><span class="params">                Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h3 id="ConnectedStreams-如何使用及分析"><a href="#ConnectedStreams-如何使用及分析" class="headerlink" title="ConnectedStreams 如何使用及分析"></a>ConnectedStreams 如何使用及分析</h3><p>ConnectedStreams 这个类定义是表示（可能）两个不同数据类型的数据连接流，该场景如果对一个数据流进行操作会直接影响另一个数据流，因此可以通过流连接来共享状态。比较常见的一个例子就是一个数据流（随时间变化的规则数据流）通过连接其他的数据流，这样另一个数据流就可以利用这些连接的规则数据流。</p><p>ConnectedStreams 在概念上可以认为和 Union 数据流是一样的。</p><p>在 ConnectedStreams 类中有三个属性：environment、inputStream1 和 inputStream2，该类中的方法如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yvb90dej21em0rygnv.jpg" alt="undefined"></p><p>在 ConnectedStreams 中可以通过 getFirstInput 获取连接的第一个流、通过 getSecondInput 获取连接的第二个流，同时它还含有六个 keyBy 方法来将连接后的数据流进行分组，这六个 keyBy 方法的参数各有不同。另外它还含有 map、flatMap、process 方法来处理数据（其中 map 和 flatMap 方法的参数分别使用的是 CoMapFunction 和 CoFlatMapFunction），其实如果你细看其方法里面的实现就会发现都是调用的 transform 方法。</p><p>上面讲完了 ConnectedStreams 类的基础定义，接下来我们来看下该类如何使用呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));    <span class="comment">//流 1</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));    <span class="comment">//流 2</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2);    <span class="comment">//连接流 1 和流 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的六种 keyBy 方法</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup1 = connected.keyBy(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup2 = connected.keyBy(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>&#125;, <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>&#125;);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup3 = connected.keyBy(<span class="string">"f0"</span>, <span class="string">"f0"</span>);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup4 = connected.keyBy(<span class="keyword">new</span> String[]&#123;<span class="string">"f0"</span>&#125;, <span class="keyword">new</span> String[]&#123;<span class="string">"f0"</span>&#125;);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(<span class="keyword">new</span> FirstSelector(), <span class="keyword">new</span> FirstSelector());</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(<span class="keyword">new</span> FirstSelector(), <span class="keyword">new</span> FirstSelector(), Types.STRING);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 map 方法</span></span><br><span class="line">connected.map(<span class="keyword">new</span> CoMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Object&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map1</span><span class="params">(Tuple2&lt;Long, Long&gt; value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map2</span><span class="params">(Tuple2&lt;Long, Long&gt; value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 flatMap 方法</span></span><br><span class="line">connected.flatMap(<span class="keyword">new</span> CoFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap1</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap2</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;).name(<span class="string">"testCoFlatMap"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 process 方法</span></span><br><span class="line">connected.process(<span class="keyword">new</span> CoProcessFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.f0 &lt; <span class="number">3</span>) &#123;</span><br><span class="line">            out.collect(value);</span><br><span class="line">            ctx.output(sideOutputTag, <span class="string">"sideout1-"</span> + String.valueOf(value));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.f0 &gt;= <span class="number">3</span>) &#123;</span><br><span class="line">            out.collect(value);</span><br><span class="line">            ctx.output(sideOutputTag, <span class="string">"sideout2-"</span> + String.valueOf(value));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="BroadcastStream-如何使用及分析"><a href="#BroadcastStream-如何使用及分析" class="headerlink" title="BroadcastStream 如何使用及分析"></a>BroadcastStream 如何使用及分析</h3><p>BroadcastStream 这个类定义是表示 broadcast state（广播状态）组成的数据流。通常这个 BroadcastStream 数据流是通过调用 DataStream 中的 broadcast 方法才返回的，注意 BroadcastStream 后面不能使用算子去操作这些流，唯一可以做的就是使用 KeyedStream/DataStream 的 connect 方法去连接 BroadcastStream，连接之后的话就会返回一个 BroadcastConnectedStream 数据流。</p><p>在 BroadcastStream 中我们该如何使用呢？通常是在 DataStream 中使用 broadcast 方法，该方法需要传入一个 MapStateDescriptor 对象，可以看下该方法的实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> BroadcastStream&lt;T&gt; <span class="title">broadcast</span><span class="params">(<span class="keyword">final</span> MapStateDescriptor&lt;?, ?&gt;... broadcastStateDescriptors)</span> </span>&#123;</span><br><span class="line">    Preconditions.checkNotNull(broadcastStateDescriptors);  <span class="comment">//检查是否为空</span></span><br><span class="line">    <span class="keyword">final</span> DataStream&lt;T&gt; broadcastStream = setConnectionType(<span class="keyword">new</span> BroadcastPartitioner&lt;&gt;());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BroadcastStream&lt;&gt;(environment, broadcastStream, broadcastStateDescriptors);  <span class="comment">//构建 BroadcastStream 对象，传入 env 环境、broadcastStream 和 broadcastStateDescriptors</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面方法传入的参数 broadcastStateDescriptors，我们可以像下面这样去定义一个 MapStateDescriptor 对象：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> MapStateDescriptor&lt;Long, String&gt; utterDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">    <span class="string">"broadcast-state"</span>, BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="BroadcastConnectedStream-如何使用及分析"><a href="#BroadcastConnectedStream-如何使用及分析" class="headerlink" title="BroadcastConnectedStream 如何使用及分析"></a>BroadcastConnectedStream 如何使用及分析</h3><p>BroadcastConnectedStream 这个类定义是表示 keyed 或者 non-keyed 数据流和 BroadcastStream 数据流进行连接后组成的数据流。比如在 DataStream 中执行 connect 方法就可以连接两个数据流了，那么在 DataStream 中 connect 方法实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">BroadcastConnectedStream&lt;T, R&gt; <span class="title">connect</span><span class="params">(BroadcastStream&lt;R&gt; broadcastStream)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BroadcastConnectedStream&lt;&gt;( <span class="comment">//构造 BroadcastConnectedStream 对象</span></span><br><span class="line">            environment,</span><br><span class="line">            <span class="keyword">this</span>,</span><br><span class="line">            Preconditions.checkNotNull(broadcastStream),</span><br><span class="line">            broadcastStream.getBroadcastStateDescriptor());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个 BroadcastConnectedStream 类中主要的方法有：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yvm1xntj21hq0hujsj.jpg" alt="undefined"></p><p>从图中可以看到四个 process 方法和一个 transform 私有方法，其中四个 process 方法也是参数不同，最后实际调用的方法就是这个私有的 transform 方法。</p><h3 id="QueryableStateStream-如何使用及分析"><a href="#QueryableStateStream-如何使用及分析" class="headerlink" title="QueryableStateStream 如何使用及分析"></a>QueryableStateStream 如何使用及分析</h3><p>QueryableStateStream 该类代表着可查询的状态流。该类的定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryableStateStream</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//要查询的状态名称</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String queryableStateName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//状态的 Key 序列化器</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TypeSerializer&lt;K&gt; keySerializer;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//状态的 descriptor </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> StateDescriptor&lt;?, V&gt; stateDescriptor;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造器</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">QueryableStateStream</span><span class="params">(String queryableStateName, StateDescriptor&lt;?, V&gt; stateDescriptor, TypeSerializer&lt;K&gt; keySerializer)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//返回可以查询状态的名称</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getQueryableStateName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> queryableStateName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//返回 key 序列化器</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeSerializer&lt;K&gt; <span class="title">getKeySerializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> keySerializer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//返回状态的 descriptor </span></span><br><span class="line">    <span class="keyword">public</span> StateDescriptor&lt;?, V&gt; getStateDescriptor() &#123;</span><br><span class="line">        <span class="keyword">return</span> stateDescriptor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 KeyedStream 你可以通过 asQueryableState() 方法返回一个 QueryableStateStream 数据流，这个方法可以通过传入不同的参数来实现，主要的参数就是 queryableStateName 和 StateDescriptor（这个参数你可以传入 ValueStateDescriptor、FoldingStateDescriptor 和 ReducingStateDescriptor 三种）。</p><p>具体如何使用呢，我们来看个 demo：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ValueStateDescriptor&lt;Tuple2&lt;Integer, Long&gt;&gt; valueState = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">    <span class="string">"any"</span>, source.getType(),    <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">QueryableStateStream&lt;Integer, Tuple2&lt;Integer, Long&gt;&gt; queryableState =</span><br><span class="line">    source.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Integer, Long&gt;, Integer&gt;() &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">7480503339992214681L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Tuple2&lt;Integer, Long&gt; value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).asQueryableState(<span class="string">"zhisheng"</span>, valueState);</span><br></pre></td></tr></table></figure><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本节算是对 Flink DataStream 包下的所有常用的 Stream 做了个讲解，不仅从使用方式来介绍这些 Stream API 该如何使用，而且还给出了部分 demo，此外还剖析了部分 Stream 的代码结构及其内部部分方法的源码实现，从而能够让大家不仅仅是从表面上去使用这些 DataStream API，还能够对它们的实现原理有了解，这样就可以做到活学活用，并且还可以自己去做扩展。</p><h2 id="十二、Flink-WaterMark-详解及结合-WaterMark-处理延迟数据"><a href="#十二、Flink-WaterMark-详解及结合-WaterMark-处理延迟数据" class="headerlink" title="十二、Flink WaterMark 详解及结合 WaterMark 处理延迟数据"></a>十二、Flink WaterMark 详解及结合 WaterMark 处理延迟数据</h2><p>在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。</p><p>下图表示选择 Event Time 与 Process Time 的实际效果图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zf4f9lvj217e0om74r.jpg" alt="undefined"></p><p>在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实却仍然这么骨感，会因为各种各样的问题（网络的抖动、设备的故障、应用的异常等原因）从而导致如图中曲线一样，Process Time 总是会与 Event Time 有一些延迟。所谓乱序，其实是指 Flink 接收到的事件的先后顺序并不是严格的按照事件的 Event Time 顺序排列的。比如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zfc2xczj20s20hsaaj.jpg" alt="undefined"></p><p>然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如：</p><ul><li>错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</li><li>设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</li></ul><p>这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。庆幸的是 Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。</p><h3 id="Watermark-是什么？"><a href="#Watermark-是什么？" class="headerlink" title="Watermark 是什么？"></a>Watermark 是什么？</h3><p>举个例子：</p><p>统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。</p><p>Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制（下文会讲）去处理。</p><p>下面通过几个图来了解一下 Watermark 是如何工作的！</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zfu71ysj21e20p0dgv.jpg" alt="undefined"></p><p>上图中的数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，<code>W(4)</code> 和 <code>W(9)</code> 是水印。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqks7btj21ik0pqdgr.jpg" alt="undefined"></p><p>经过 Flink 的消费，数据 <code>1</code>、<code>3</code>、<code>2</code> 进入了第一个窗口，然后 <code>7</code> 会进入第二个窗口，接着 <code>3</code> 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 <code>4</code> 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqrrk6kj21bi0nkgmc.jpg" alt="undefined"></p><p>那么接着后面的数据 <code>5</code> 和 <code>6</code> 会进入到第二个窗口里面，数据 <code>9</code> 会进入在第三个窗口里面。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqyn66nj21cm0lsq46.jpg" alt="undefined"></p><p>那么当遇到水印 <code>9</code> 时，发现水印比第二个窗口的结束时间 <code>8</code> 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去。</p><p>相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。</p><h3 id="Flink-中-Watermark-的设置"><a href="#Flink-中-Watermark-的设置" class="headerlink" title="Flink 中 Watermark 的设置"></a>Flink 中 Watermark 的设置</h3><p>在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPunctuatedWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPunctuatedWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPunctuatedWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPunctuatedWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以设置 Watermark 是有如下两种方式：</p><ul><li>AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。</li></ul><p>在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。</p><ul><li>AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</li></ul><p>在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。</p><p>下面再分别详细讲下这两种的实现方式。</p><h3 id="Punctuated-Watermark"><a href="#Punctuated-Watermark" class="headerlink" title="Punctuated Watermark"></a>Punctuated Watermark</h3><p>AssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。</p><p>那么该怎么利用这个来定义水印生成器呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPunctuatedWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(Word lastElement, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> extractedTimestamp % <span class="number">3</span> == <span class="number">0</span> ? <span class="keyword">new</span> Watermark(extractedTimestamp) : <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。</p><h3 id="3-5-4-Periodic-Watermark"><a href="#3-5-4-Periodic-Watermark" class="headerlink" title="3.5.4 Periodic Watermark"></a>3.5.4 Periodic Watermark</h3><p>通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentTimestamp = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word word, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (word.getTimestamp() &gt; currentTimestamp) &#123;</span><br><span class="line">            <span class="keyword">this</span>.currentTimestamp = word.getTimestamp();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> currentTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> maxTimeLag = <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，该方法会返回当前时间戳与事件时间进行比较，如果事件的时间戳比 currentTimestamp 大的话，那么就将当前事件的时间戳赋值给 currentTimestamp。getCurrentWatermark() 方法是获取当前的水位线，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 <code>long maxTimeLag = 5000;</code> 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。</p><p>通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。</p><p>AssignerWithPeriodicWatermarks 这个接口有四个实现类，分别如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zrpl1ivj21xk0i674j.jpg" alt="undefined"></p><p>BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zrx0blbj21hs1gsaci.jpg" alt="undefined"></p><p>你可以像下面一样使用该类来分配时间和生成水印：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Time.seconds(10) 代表允许延迟的时间大小</span></span><br><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">    <span class="comment">//重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ul><li>CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。</li><li>AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> elementPrevTimestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = extractAscendingTimestamp(element);</span><br><span class="line">    <span class="keyword">if</span> (newTimestamp &gt;= <span class="keyword">this</span>.currentTimestamp) &#123;</span><br><span class="line">        <span class="keyword">this</span>.currentTimestamp = ne∏wTimestamp;</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        violationHandler.handleViolation(newTimestamp, <span class="keyword">this</span>.currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 <code>System.currentTimeMillis()</code> 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。</li></ul><p><strong>注意</strong>：</p><p>1、使用这种方式周期性生成水印的话，你可以通过 <code>env.getConfig().setAutoWatermarkInterval(...);</code> 来设置生成水印的间隔（每隔 n 毫秒）。</p><p>2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;MyType&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="comment">/* condition */</span>) &#123;</span><br><span class="line">        MyType next = getNext();</span><br><span class="line">        ctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class="line">            ctx.emitWatermark(<span class="keyword">new</span> Watermark(next.getWatermarkTime()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="每个-Kafka-分区的时间戳"><a href="#每个-Kafka-分区的时间戳" class="headerlink" title="每个 Kafka 分区的时间戳"></a>每个 Kafka 分区的时间戳</h3><p>当以 Kafka 来作为数据源的时候，通常每个 Kafka 分区的数据时间戳是递增的（事件是有序的），但是当你作业设置多个并行度的时候，Flink 去消费 Kafka 数据流是并行的，那么并行的去消费 Kafka 分区的数据就会导致打乱原每个分区的数据时间戳的顺序。在这种情况下，你可以使用 Flink 中的 <code>Kafka-partition-aware</code> 特性来生成水印，使用该特性后，水印会在 Kafka 消费端生成，然后每个 Kafka 分区和每个分区上的水印最后的合并方式和水印在数据流 shuffle 过程中的合并方式一致。</p><p>如果事件时间戳严格按照每个 Kafka 分区升序，则可以使用前面提到的 AscendingTimestampExtractor 水印生成器来为每个分区生成水印。下面代码教大家如何使用 <code>per-Kafka-partition</code> 来生成水印。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FlinkKafkaConsumer011&lt;Event&gt; kafkaSource = <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">"zhisheng"</span>, schema, props);</span><br><span class="line">kafkaSource.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Event&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.eventTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; stream = env.addSource(kafkaSource);</span><br></pre></td></tr></table></figure><p>下图表示水印在 Kafka 分区后如何通过流数据流传播：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zs9dj5oj20r10a8t9f.jpg" alt="undefined"></p><p>其实在上文中已经提到的一点是在设置 Periodic Watermark 时，是允许提供一个参数，表示数据最大的延迟时间。其实这个值要结合自己的业务以及数据的情况来设置，如果该值设置的太小会导致数据因为网络或者其他的原因从而导致乱序或者延迟的数据太多，那么最后窗口触发的时候，可能窗口里面的数据量很少，那么这样计算的结果很可能误差会很大，对于有的场景（要求正确性比较高）是不太符合需求的。但是如果该值设置的太大，那么就会导致很多窗口一直在等待延迟的数据，从而一直不触发，这样首先就会导致数据的实时性降低，另外将这么多窗口的数据存在内存中，也会增加作业的内存消耗，从而可能会导致作业发生 OOM 的问题。</p><p>综上建议：</p><ul><li>合理设置允许数据最大延迟时间</li><li>不太依赖事件时间的场景就不要设置时间策略为 EventTime</li></ul><h3 id="延迟数据该如何处理-三种方法"><a href="#延迟数据该如何处理-三种方法" class="headerlink" title="延迟数据该如何处理(三种方法)"></a>延迟数据该如何处理(三种方法)</h3><h4 id="丢弃（默认）"><a href="#丢弃（默认）" class="headerlink" title="丢弃（默认）"></a>丢弃（默认）</h4><p>在 Flink 中，对这么延迟数据的默认处理方式是丢弃。</p><h4 id="allowedLateness-再次指定允许数据延迟的时间"><a href="#allowedLateness-再次指定允许数据延迟的时间" class="headerlink" title="allowedLateness 再次指定允许数据延迟的时间"></a>allowedLateness 再次指定允许数据延迟的时间</h4><p>allowedLateness 表示允许数据延迟的时间，这个方法是在 WindowedStream 中的，用来设置允许窗口数据延迟的时间，超过这个时间的元素就会被丢弃，这个的默认值是 0，该设置仅针对于以事件时间开的窗口，它的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">allowedLateness</span><span class="params">(Time lateness)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> millis = lateness.toMilliseconds();</span><br><span class="line">    checkArgument(millis &gt;= <span class="number">0</span>, <span class="string">"The allowed lateness cannot be negative."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.allowedLateness = millis;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之前有多个小伙伴问过我 Watermark 中允许的数据延迟和这个数据延迟的区别是啥？我的回复是该允许延迟的时间是在 Watermark 允许延迟的基础上增加的时间。那么具体该如何使用 allowedLateness 呢。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">    .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">    .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .allowedLateness(Time.milliseconds(<span class="number">2</span>))  <span class="comment">//表示允许再次延迟 2 毫秒</span></span><br><span class="line">    .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">        <span class="comment">//计算逻辑</span></span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h4 id="sideOutputLateData-收集迟到的数据"><a href="#sideOutputLateData-收集迟到的数据" class="headerlink" title="sideOutputLateData 收集迟到的数据"></a>sideOutputLateData 收集迟到的数据</h4><p>sideOutputLateData 这个方法同样是 WindowedStream 中的方法，该方法会将延迟的数据发送到给定 OutputTag 的 side output 中去，然后你可以通过 <code>SingleOutputStreamOperator.getSideOutput(OutputTag)</code> 来获取这些延迟的数据。具体的操作方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义 OutputTag</span></span><br><span class="line">OutputTag&lt;Integer&gt; lateDataTag = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"late"</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; windowOperator = dataStream</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">        .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">        .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">        .allowedLateness(Time.milliseconds(<span class="number">2</span>))</span><br><span class="line">        .sideOutputLateData(lateDataTag)    <span class="comment">//指定 OutputTag</span></span><br><span class="line">        .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="comment">//计算逻辑</span></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">windowOperator.addSink(resultSink);</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定的 OutputTag 从 Side Output 中获取到延迟的数据之后，你可以通过 addSink() 方法存储下来，这样可以方便你后面去排查哪些数据是延迟的。</span></span><br><span class="line">windowOperator.getSideOutput(lateDataTag)</span><br><span class="line">        .addSink(lateResultSink);</span><br></pre></td></tr></table></figure><h3 id="小结与反思-10"><a href="#小结与反思-10" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Watermark 的概念，并讲解了 Flink 中自带的 Watermark，然后还教大家如何设置 Watermark 以及如何自定义 Watermark，最后通过结合 Window 与 Watermark 去处理延迟数据，还讲解了三种常见的处理延迟数据的方法。</p><p>关于 Watermark 你有遇到什么问题吗？对于延迟数据你通常是怎么处理的？</p><p>本节相关的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/watermark" target="_blank" rel="noopener">Watermark</a></p><h2 id="十三、Flink-常用的-Source-和-Sink-Connectors-介绍"><a href="#十三、Flink-常用的-Source-和-Sink-Connectors-介绍" class="headerlink" title="十三、Flink 常用的 Source 和 Sink Connectors 介绍"></a>十三、Flink 常用的 Source 和 Sink Connectors 介绍</h2><p>通过前面我们可以知道 Flink Job 的大致结构就是 <code>Source ——&gt; Transformation ——&gt; Sink</code>。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1164fu1kj21ri0dcmxj.jpg" alt="undefined"></p><p>那么这个 Source 是什么意思呢？我们下面来看看。</p><h3 id="Data-Source-介绍"><a href="#Data-Source-介绍" class="headerlink" title="Data Source 介绍"></a>Data Source 介绍</h3><p>Data Source 是什么呢？就字面意思其实就可以知道：数据来源。</p><p>Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即处理实时的数据流（做计算操作），然后将处理后的数据实时下发，只要数据源源不断过来，Flink 就能够一直计算下去。</p><p>Flink 中你可以使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 来为你的程序添加数据来源。</p><p>Flink 已经提供了若干实现好了的 source function，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。</p><p>那么常用的 Data Source 有哪些呢？</p><h3 id="常用的-Data-Source"><a href="#常用的-Data-Source" class="headerlink" title="常用的 Data Source"></a>常用的 Data Source</h3><p>StreamExecutionEnvironment 中可以使用以下这些已实现的 stream source。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga116diy51j21ji16aads.jpg" alt="undefined"></p><p>总的来说可以分为下面几大类：</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><ol><li>fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。</li><li>fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。</li><li>fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; input = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"barfoo"</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"start"</span>, <span class="number">2.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"foobar"</span>, <span class="number">3.0</span>),</span><br><span class="line">    ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ol><li>fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。</li><li>generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。</li></ol><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text = env.readTextFile(<span class="string">"file:///path/to/file"</span>);</span><br></pre></td></tr></table></figure><p>2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。</p><p>3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS<em>CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS</em>ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class="line">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">100</span>,</span><br><span class="line">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br></pre></td></tr></table></figure><p><strong>实现:</strong></p><p>在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。</p><p><strong>重要注意：</strong></p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。</p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。</p><h4 id="基于-Socket"><a href="#基于-Socket" class="headerlink" title="基于 Socket"></a>基于 Socket</h4><p>socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">        .socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>) <span class="comment">// 监听 localhost 的 9999 端口过来的数据</span></span><br><span class="line">        .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="自定义"><a href="#自定义" class="headerlink" title="自定义"></a>自定义</h4><p>addSource - 添加一个新的 source function。例如，你可以用 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 从 Apache Kafka 读取数据。</p><p><strong>说说上面几种的特点</strong></p><ol><li>基于集合：有界数据集，更偏向于本地测试用</li><li>基于文件：适合监听文件修改并读取其内容</li><li>基于 Socket：监听主机的 host port，从 Socket 中获取数据</li><li>自定义 addSource：大多数的场景数据都是无界的，会源源不断过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;KafkaEvent&gt; input = env</span><br><span class="line">        .addSource(</span><br><span class="line">            <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.getRequired(<span class="string">"input-topic"</span>), <span class="comment">//从参数中获取传进来的 topic </span></span><br><span class="line">                <span class="keyword">new</span> KafkaEventSchema(),</span><br><span class="line">                parameterTool.getProperties())</span><br><span class="line">            .assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkExtractor()));</span><br></pre></td></tr></table></figure><p>Flink 目前支持如下面常见的 Source：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga117rgkt8j21kw0yeakr.jpg" alt="undefined"></p><p>如果你想自定义自己的 Source 呢？在后面 3.8 节会讲解。</p><h3 id="Data-Sink-介绍"><a href="#Data-Sink-介绍" class="headerlink" title="Data Sink 介绍"></a>Data Sink 介绍</h3><p>首先 Sink 的意思是：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1181fv12j20w20ri42r.jpg" alt="undefined"></p><p>大概可以猜到了吧！Data sink 有点把数据存储下来（落库）的意思。Flink 在拿到数据后做一系列的计算后，最后要将计算的结果往下游发送。比如将数据存储到 MySQL、ElasticSearch、Cassandra，或者继续发往 Kafka、 RabbitMQ 等消息队列，更或者直接调用其他的第三方应用服务（比如告警）。</p><h3 id="常用的-Data-Sink"><a href="#常用的-Data-Sink" class="headerlink" title="常用的 Data Sink"></a>常用的 Data Sink</h3><p>上面介绍了 Flink Data Source 有哪些，这里也看看 Flink Data Sink 支持的有哪些。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga118a05luj21kw0xgtk0.jpg" alt="undefined"></p><p>再看下源码有哪些呢？</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga118gxfh0j22tc0mijrw.jpg" alt="undefined"></p><p>可以看到有 Kafka、ElasticSearch、Socket、RabbitMQ、JDBC、Cassandra POJO、File、Print 等 Sink 的方式。</p><p>可能自带的这些 Sink 不支持你的业务场景，那么你也可以自定义符合自己公司业务需求的 Sink，同样在后面 3.8 节将教会大家。</p><h3 id="小结与反思-11"><a href="#小结与反思-11" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中常用的 Connector，包括 Source 和 Sink 的，其中每种都有很多 Connector，大家可以根据实际场景去使用合适的 Connector。</p><h2 id="十四、Flink-Connector-Kafka-使用和剖析"><a href="#十四、Flink-Connector-Kafka-使用和剖析" class="headerlink" title="十四、Flink Connector Kafka 使用和剖析"></a>十四、Flink Connector Kafka 使用和剖析</h2><p>在前面 3.6 节中介绍了 Flink 中的 Data Source 和 Data Sink，然后还讲诉了自带的一些 Source 和 Sink 的 Connector。本篇文章将讲解一下用的最多的 Connector —— Kafka，带大家利用 Kafka Connector 读取 Kafka 数据，做一些计算操作后然后又通过 Kafka Connector 写入到 kafka 消息队列去。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11hb4lnmj229g0l43z0.jpg" alt="undefined"></p><h3 id="准备环境和依赖"><a href="#准备环境和依赖" class="headerlink" title="准备环境和依赖"></a>准备环境和依赖</h3><h4 id="环境安装和启动"><a href="#环境安装和启动" class="headerlink" title="环境安装和启动"></a>环境安装和启动</h4><p>如果你已经安装好了 Flink 和 Kafka，那么接下来使用命令运行启动 Flink、Zookepeer、Kafka 就行了。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11hipsabj218s0kq40p.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11ho1mg2j21di090754.jpg" alt="undefined"></p><p>执行命令都启动好了后就可以添加依赖了。</p><h4 id="添加-maven-依赖"><a href="#添加-maven-依赖" class="headerlink" title="添加 maven 依赖"></a>添加 maven 依赖</h4><p>Flink 里面支持 Kafka 0.8.x 以上的版本，具体采用哪个版本的 Maven 依赖需要根据安装的 Kafka 版本来确定。因为之前我们安装的 Kafka 是 1.1.0 版本，所以这里我们选择的 Kafka Connector 为 <code>flink-connector-kafka-0.11_2.11</code> （支持 Kafka 0.11.x 版本及以上，该 Connector 支持 Kafka 事务消息传递，所以能保证 Exactly Once）。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Flink、Kafka、Flink Kafka Connector 三者对应的版本可以根据 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html" target="_blank" rel="noopener">官网</a> 的对比来选择。需要注意的是 <code>flink-connector-kafka_2.11</code> 这个版本支持的 Kafka 版本要大于 1.0.0，从 Flink 1.9 版本开始，它使用的是 Kafka 2.2.0 版本的客户端，虽然这些客户端会做向后兼容，但是建议还是按照官网约定的来规范使用 Connector 版本。另外你还要添加的依赖有：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--flink java--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--log--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--alibaba fastjson--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.51<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="测试数据发到-Kafka-Topic"><a href="#测试数据发到-Kafka-Topic" class="headerlink" title="测试数据发到 Kafka Topic"></a>测试数据发到 Kafka Topic</h3><p>实体类，Metric.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Metric</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, Object&gt; fields;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, String&gt; tags;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>往 kafka 中写数据工具类：KafkaUtils.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"metric"</span>;  <span class="comment">// kafka topic，Flink 程序中需要和这个统一 </span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//key 序列化</span></span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//value 序列化</span></span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        Metric metric = <span class="keyword">new</span> Metric();</span><br><span class="line">        metric.setTimestamp(System.currentTimeMillis());</span><br><span class="line">        metric.setName(<span class="string">"mem"</span>);</span><br><span class="line">        Map&lt;String, String&gt; tags = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;String, Object&gt; fields = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        tags.put(<span class="string">"cluster"</span>, <span class="string">"zhisheng"</span>);</span><br><span class="line">        tags.put(<span class="string">"host_ip"</span>, <span class="string">"101.147.022.106"</span>);</span><br><span class="line"></span><br><span class="line">        fields.put(<span class="string">"used_percent"</span>, <span class="number">90</span>d);</span><br><span class="line">        fields.put(<span class="string">"max"</span>, <span class="number">27244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"used"</span>, <span class="number">17244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"init"</span>, <span class="number">27244873</span>d);</span><br><span class="line"></span><br><span class="line">        metric.setTags(tags);</span><br><span class="line">        metric.setFields(fields);</span><br><span class="line"></span><br><span class="line">        ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(metric));</span><br><span class="line">        producer.send(record);</span><br><span class="line">        System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(metric));</span><br><span class="line"></span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">300</span>);</span><br><span class="line">            writeToKafka();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11id4k1dj22iy166q6l.jpg" alt="undefined"></p><p>如果出现如上图标记的，即代表能够不断往 kafka 发送数据的。</p><h3 id="Flink-如何消费-Kafka-数据？"><a href="#Flink-如何消费-Kafka-数据？" class="headerlink" title="Flink 如何消费 Kafka 数据？"></a>Flink 如何消费 Kafka 数据？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);  <span class="comment">//key 反序列化</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>); <span class="comment">//value 反序列化</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"metric"</span>,  <span class="comment">//kafka topic</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),  <span class="comment">// String 序列化</span></span><br><span class="line">                props)).setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        dataStreamSource.print(); <span class="comment">//把从 kafka 读取到的数据打印在控制台</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data source"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行起来：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11j9hshgj22gq1acte2.jpg" alt="undefined"></p><p>看到没程序，Flink 程序控制台能够源源不断地打印数据呢。</p><h4 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h4><p>使用 FlinkKafkaConsumer011 时传入了三个参数。</p><ul><li>Kafka topic：这个代表了 Flink 要消费的是 Kafka 哪个 Topic，如果你要同时消费多个 Topic 的话，那么你可以传入一个 Topic List 进去，另外也支持正则表达式匹配 Topic</li><li>序列化：上面代码我们使用的是 SimpleStringSchema</li><li>配置属性：将 Kafka 等的一些配置传入</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11jkx2dlj21po15y76t.jpg" alt="undefined"></p><p>前面演示了 Flink 如何消费 Kafak 数据，接下来演示如何把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。</p><h3 id="Flink-如何将计算后的数据发到-Kafka？"><a href="#Flink-如何将计算后的数据发到-Kafka？" class="headerlink" title="Flink 如何将计算后的数据发到 Kafka？"></a>Flink 如何将计算后的数据发到 Kafka？</h3><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//其他 Kafka 集群配置</span><br><span class="line">kafka.brokers=xxx:9092,xxx:9092,xxx:9092</span><br><span class="line">kafka.group.id=metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=xxx:2181</span><br><span class="line">metrics.topic=xxx</span><br><span class="line">stream.parallelism=5</span><br><span class="line">kafka.sink.brokers=localhost:9092</span><br><span class="line">kafka.sink.topic=metric-test</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><p>目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11jv4exfj22660qm470.jpg" alt="undefined"></p><h4 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h4><p>Main.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        data.addSink(<span class="keyword">new</span> FlinkKafkaProducer011&lt;Metrics&gt;(</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.brokers"</span>),</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.topic"</span>),</span><br><span class="line">                <span class="keyword">new</span> MetricSchema()</span><br><span class="line">                )).name(<span class="string">"flink-connectors-kafka"</span>)</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"stream.sink.parallelism"</span>));</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h4><p>启动程序，查看运行结果，不段执行上面命令，查看是否有新的 topic 出来：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11k5j2xkj21i90u0alj.jpg" alt="undefined"></p><p>执行命令可以查看该 topic 的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kelpl3j221a0lyguc.jpg" alt="undefined"></p><p>前面代码使用的 FlinkKafkaProducer011 只传了三个参数：brokerList、topicId、serializationSchema（序列化），其实是支持传入多个参数的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kmeontj22ie14oad2.jpg" alt="undefined"></p><h3 id="FlinkKafkaConsumer-源码剖析"><a href="#FlinkKafkaConsumer-源码剖析" class="headerlink" title="FlinkKafkaConsumer 源码剖析"></a>FlinkKafkaConsumer 源码剖析</h3><p>FlinkKafkaConsumer 的继承关系如下图所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kuirzfj21ha0lk74e.jpg" alt="undefined"></p><p>可以发现几个版本的 FlinkKafkaConsumer 都继承自 FlinkKafkaConsumerBase 抽象类，所以可知 FlinkKafkaConsumerBase 是最核心的类了。FlinkKafkaConsumerBase 实现了 CheckpointedFunction、CheckpointListener 接口，继承了 RichParallelSourceFunction 抽象类来读取 Kafka 数据。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11l1xtxpj21gw0cimx4.jpg" alt="undefined"></p><p>在 FlinkKafkaConsumerBase 中的 open 方法中做了大量的配置初始化工作，然后在 run 方法里面是由 AbstractFetcher 来获取数据的，在 AbstractFetcher 中有用 List&gt; 来存储着所有订阅分区的状态信息，包括了下面这些字段：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> KafkaTopicPartition partition;    <span class="comment">//分区</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> KPH kafkaPartitionHandle;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> offset;   <span class="comment">//消费到的 offset</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> committedOffset;  <span class="comment">//提交的 offset</span></span><br></pre></td></tr></table></figure><p>在 FlinkKafkaConsumerBase 中还有字段定义 Flink 自动发现 Kafka 主题和分区个数的时间，默认是不开启的（时间为 Long.MIN_VALUE），像如果传入的是正则表达式参数，那么动态的发现主题还是有意义的，如果配置的已经是固定的 Topic，那么完全就没有开启这个的必要，另外就是 Kafka 的分区个数的自动发现，像高峰流量的时期，如果 Kafka 的分区扩容了，但是在 Flink 这边没有配置这个参数那就会导致 Kafka 新分区中的数据不会被消费到，这个参数由 <code>flink.partition-discovery.interval-millis</code> 控制。</p><h3 id="FlinkKafkaProducer-源码剖析"><a href="#FlinkKafkaProducer-源码剖析" class="headerlink" title="FlinkKafkaProducer 源码剖析"></a>FlinkKafkaProducer 源码剖析</h3><p>FlinkKafkaProducer 这个有些特殊，不同版本的类结构有些不一样，如 FlinkKafkaProducer011 是继承的 TwoPhaseCommitSinkFunction 抽象类，而 FlinkKafkaProducer010 和 FlinkKafkaProducer09 是基于 FlinkKafkaProducerBase 类来实现的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11la4kjqj217m0g0gln.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11lhi3j5j21as0jq0su.jpg" alt="undefined"></p><p>在 Kafka 0.11.x 版本后支持了事务，这让 Flink 与 Kafka 的事务相结合从而实现端到端的 Exactly once 才有了可能，在 9.5 节中会详细讲解如何利用 TwoPhaseCommitSinkFunction 来实现 Exactly once 的。</p><p>数据 Sink 到下游的 Kafka，可你能会关心数据的分区策略，在 Flink 中自带了一种就是 FlinkFixedPartitioner，它使用的是 round-robin 策略进行下发到下游 Kafka Topic 的分区上的，当然也提供了 FlinkKafkaPartitioner 接口供你去实现自定义的分区策略。</p><h3 id="使用-Flink-connector-kafka-可能会遇到的问题"><a href="#使用-Flink-connector-kafka-可能会遇到的问题" class="headerlink" title="使用 Flink-connector-kafka 可能会遇到的问题"></a>使用 Flink-connector-kafka 可能会遇到的问题</h3><h4 id="如何消费多个-Kafka-Topic"><a href="#如何消费多个-Kafka-Topic" class="headerlink" title="如何消费多个 Kafka Topic"></a>如何消费多个 Kafka Topic</h4><p>通常可能会有很多类型的数据全部发到 Kafka，但是发送的数据却不是在同一个 Topic 里面，然后在 Flink 处消费的时候，又要去同时消费这些多个 Topic，在 Flink 中除了支持可以消费单个 Topic 的数据，还支持传入多个 Topic，另外还支持 Topic 的正则表达式（因为有时候可能会事先不确定到底会有多少个 Topic，所以使用正则来处理会比较好，只要在 Kafka 建立的 Topic 名是有规律的就行），如下几种构造器可以传入不同参数来创建 FlinkKafkaConsumer 对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//多个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//正则表达式 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="想要获取数据的元数据信息"><a href="#想要获取数据的元数据信息" class="headerlink" title="想要获取数据的元数据信息"></a>想要获取数据的元数据信息</h4><p>在消费 Kafka 数据的时候，有时候想获取到数据是从哪个 Topic、哪个分区里面过来的，这条数据的 offset 值是多少。这些元数据信息在有的场景真的需要，那么这种场景下该如何获取呢？其实在获取数据进行反序列化的时候使用 KafkaDeserializationSchema 就行。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KafkaDeserializationSchema</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span>, <span class="title">ResultTypeQueryable</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isEndOfStream</span><span class="params">(T nextElement)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">T <span class="title">deserialize</span><span class="params">(ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 KafkaDeserializationSchema 接口中的 deserialize 方法里面的 ConsumerRecord 类中是包含了数据的元数据信息。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> checksum;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedKeySize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedValueSize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所在在使用 FlinkKafkaConsumer011 构造对象的的时候可以传入实现 KafkaDeserializationSchema 接口后的参数对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//多个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(topics, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//正则表达式 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="多种数据类型"><a href="#多种数据类型" class="headerlink" title="多种数据类型"></a>多种数据类型</h4><p>因为在 Kafka 的数据的类型可能会有很多种类型，比如是纯 String、String 类型的 JSON、Avro、Protobuf。那么源数据类型不同，在消费 Kafka 的时候反序列化也是会有一定的不同，但最终还是依赖前面的 KafkaDeserializationSchema 或者 DeserializationSchema （反序列化的 Schema），数据经过处理后的结果再次发到 Kafka 数据类型也是会有多种，它依赖的是 SerializationSchema（序列化的 Schema）。</p><h4 id="序列化失败"><a href="#序列化失败" class="headerlink" title="序列化失败"></a>序列化失败</h4><p>因为数据是从 Kafka 过来的，难以避免的是 Kafka 中的数据可能会出现 null 或者不符合预期规范的数据，然后在反序列化的时候如果作业里面没有做异常处理的话，就会导致作业失败重启，这样情况可以在反序列化处做异常处理，保证作业的健壮性。</p><h4 id="Kafka-消费-Offset-的选择"><a href="#Kafka-消费-Offset-的选择" class="headerlink" title="Kafka 消费 Offset 的选择"></a>Kafka 消费 Offset 的选择</h4><p>因为在 Flink Kafka Consumer 中是支持配置如何确定从 Kafka 分区开始消费的起始位置的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FlinkKafkaConsumer011&lt;String&gt; myConsumer = <span class="keyword">new</span> FlinkKafkaConsumer0111&lt;&gt;(...);</span><br><span class="line">consumer.setStartFromEarliest();     <span class="comment">//从最早的数据开始消费</span></span><br><span class="line">consumer.setStartFromLatest();       <span class="comment">//从最新的数据开始消费</span></span><br><span class="line">consumer.setStartFromTimestamp(...); <span class="comment">//从根据指定的时间戳（ms）处开始消费</span></span><br><span class="line">consumer.setStartFromGroupOffsets(); <span class="comment">//默认从提交的 offset 开始消费</span></span><br></pre></td></tr></table></figure><p>另外还支持根据分区指定的 offset 去消费 Topic 数据，示例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">specificStartOffsets.put(<span class="keyword">new</span> KafkaTopicPartition(<span class="string">"zhisheng"</span>, <span class="number">0</span>), <span class="number">23L</span>);</span><br><span class="line">specificStartOffsets.put(<span class="keyword">new</span> KafkaTopicPartition(<span class="string">"zhisheng"</span>, <span class="number">1</span>), <span class="number">31L</span>);</span><br><span class="line">specificStartOffsets.put(<span class="keyword">new</span> KafkaTopicPartition(<span class="string">"zhisheng"</span>, <span class="number">2</span>), <span class="number">43L</span>);</span><br><span class="line"></span><br><span class="line">myConsumer.setStartFromSpecificOffsets(specificStartOffsets);</span><br></pre></td></tr></table></figure><p>注意：这种情况下如果该分区中不存在指定的 Offset 了，则会使用默认的 setStartFromGroupOffsets 来消费分区中的数据。如果作业是从 Checkpoint 或者 Savepoint 还原的，那么上面这些配置无效，作业会根据状态中存储的 Offset 为准，然后开始消费。</p><p>上面这几种策略是支持可以配置的，需要在作业中指定，具体选择哪种是需要根据作业的业务需求来判断的。</p><h3 id="小结与反思-12"><a href="#小结与反思-12" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中最常使用的 Connector —— Kafka，该 Connector 不仅可以作为 Source，还可以作为 Sink。通过了完成的案例讲解从 Kafka 读取数据和写入数据到 Kafka，并分析了这两个的主要类的结构。最后讲解了使用该 Connector 可能会遇到的一些问题，该如何去解决这些问题。</p><p>你在公司使用该 Connector 的过程中有遇到什么问题吗？是怎么解决的呢？还有什么问题要补充？</p><p>本节涉及代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-kafka" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-kafka</a></p><h2 id="十五、如何自定义-Flink-Connectors（Source-和-Sink）？"><a href="#十五、如何自定义-Flink-Connectors（Source-和-Sink）？" class="headerlink" title="十五、如何自定义 Flink Connectors（Source 和 Sink）？"></a>十五、如何自定义 Flink Connectors（Source 和 Sink）？</h2><p>在前面文章 3.6 节中讲解了 Flink 中的 Data Source 和 Data Sink，然后介绍了 Flink 中自带的一些 Source 和 Sink 的 Connector，接着我们还有几篇实战会讲解了如何从 Kafka 处理数据写入到 Kafka、ElasticSearch 等，当然 Flink 还有一些其他的 Connector，我们这里就不一一介绍了，大家如果感兴趣的话可以去官网查看一下，如果对其代码实现比较感兴趣的话，也可以去看看其源码的实现。我们这篇文章来讲解一下如何自定义 Source 和 Sink Connector？这样我们后面再遇到什么样的需求都难不倒我们了。</p><h3 id="如何自定义-Source-Connector？"><a href="#如何自定义-Source-Connector？" class="headerlink" title="如何自定义 Source Connector？"></a>如何自定义 Source Connector？</h3><p>这里就演示一下如何自定义 Source 从 MySQL 中读取数据。</p><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加 MySQL 依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="数据库建表"><a href="#数据库建表" class="headerlink" title="数据库建表"></a>数据库建表</h4><p>数据库建表如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`student`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">unsigned</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`password`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`age`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">5</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COLLATE</span>=utf8_bin;</span><br></pre></td></tr></table></figure><h4 id="数据库插入数据"><a href="#数据库插入数据" class="headerlink" title="数据库插入数据"></a>数据库插入数据</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`student`</span> <span class="keyword">VALUES</span> (<span class="string">'1'</span>, <span class="string">'zhisheng01'</span>, <span class="string">'123456'</span>, <span class="string">'18'</span>), (<span class="string">'2'</span>, <span class="string">'zhisheng02'</span>, <span class="string">'123'</span>, <span class="string">'17'</span>), (<span class="string">'3'</span>, <span class="string">'zhisheng03'</span>, <span class="string">'1234'</span>, <span class="string">'18'</span>), (<span class="string">'4'</span>, <span class="string">'zhisheng04'</span>, <span class="string">'12345'</span>, <span class="string">'16'</span>);</span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure><h4 id="新建实体类"><a href="#新建实体类" class="headerlink" title="新建实体类"></a>新建实体类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> String password;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="自定义-Source-类"><a href="#自定义-Source-类" class="headerlink" title="自定义 Source 类"></a>自定义 Source 类</h4><p>SourceFromMySQL 是自定义的 Source 类，该类继承 RichSourceFunction，实现里面的 open、close、run、cancel 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceFromMySQL</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"select * from Student;"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 程序执行完毕就可以进行，关闭连接和释放资源的动作了</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123; <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DataStream 调用一次 run() 方法用来获取数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Student&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ResultSet resultSet = ps.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(</span><br><span class="line">                    resultSet.getInt(<span class="string">"id"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"name"</span>).trim(),</span><br><span class="line">                    resultSet.getString(<span class="string">"password"</span>).trim(),</span><br><span class="line">                    resultSet.getInt(<span class="string">"age"</span>));</span><br><span class="line">            ctx.collect(student);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">                con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"123456"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                System.out.println(<span class="string">"mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Flink-应用程序代码"><a href="#Flink-应用程序代码" class="headerlink" title="Flink 应用程序代码"></a>Flink 应用程序代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> SourceFromMySQL()).print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data sourc"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 Flink 程序，控制台日志中可以看见打印的 student 信息。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1svkzhh7j21kw0zknbx.jpg" alt="undefined"></p><h3 id="RichSourceFunction-使用及源码分析"><a href="#RichSourceFunction-使用及源码分析" class="headerlink" title="RichSourceFunction 使用及源码分析"></a>RichSourceFunction 使用及源码分析</h3><p>从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，其实也是可以使用 SourceFunction 函数来自定义 Source。 RichSourceFunction 函数比 SourceFunction 多了 open 方法（可以用来初始化）和获取应用上下文的方法，那么来了解一下该类。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1svvcw3jj21h811ct9j.jpg" alt="undefined"></p><p>它是一个抽象类，继承自 AbstractRichFunction，实现了 SourceFunction 接口，其子类有三个，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sw3v5pzj22a011gtao.jpg" alt="undefined"></p><ul><li>MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。</li><li>MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。</li><li>ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。</li></ul><p>除了上面使用 RichSourceFunction 和 SourceFunction 来自定义 Source，还可以继承 RichParallelSourceFunction 抽象类或实现 ParallelSourceFunction 接口来实现自定义 Source 函数。</p><h3 id="如何自定义-Sink-Connector？"><a href="#如何自定义-Sink-Connector？" class="headerlink" title="如何自定义 Sink Connector？"></a>如何自定义 Sink Connector？</h3><p>下面将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去</p><h4 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h4><p>写了一个工具类往 Kafka 的 topic 中发送数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(student));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="SinkToMySQL"><a href="#SinkToMySQL" class="headerlink" title="SinkToMySQL"></a>SinkToMySQL</h4><p>该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法，在 invoke 方法中将数据插入到 MySQL 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Student value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//组装数据，执行插入操作</span></span><br><span class="line">        ps.setInt(<span class="number">1</span>, value.getId());</span><br><span class="line">        ps.setString(<span class="number">2</span>, value.getName());</span><br><span class="line">        ps.setString(<span class="number">3</span>, value.getPassword());</span><br><span class="line">        ps.setInt(<span class="number">4</span>, value.getAge());</span><br><span class="line">        ps.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">            con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"root123456"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span>+ e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Flink-程序"><a href="#Flink-程序" class="headerlink" title="Flink 程序"></a>Flink 程序</h4><p>这里的 source 是从 Kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 Student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; JSON.parseObject(string, Student.class)); <span class="comment">//Fastjson 解析字符串成 student 对象</span></span><br><span class="line"></span><br><span class="line">        student.addSink(<span class="keyword">new</span> SinkToMySQL()); <span class="comment">//数据 sink 到 mysql</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add sink"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>运行 Flink 程序，然后再运行 KafkaUtils2.java 工具类，这样就可以了。</p><p>如果数据插入成功了，那么查看下我们的数据库：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sy3run1j21kw0zkti9.jpg" alt="undefined"></p><p>数据库中已经插入了 100 条我们从 Kafka 发送的数据了。证明我们的 SinkToMySQL 起作用了。</p><h3 id="RichSinkFunction-使用及源码分析"><a href="#RichSinkFunction-使用及源码分析" class="headerlink" title="RichSinkFunction 使用及源码分析"></a>RichSinkFunction 使用及源码分析</h3><p>通过上面的 demo 可以发现继承 RichSinkFunction 类，然后实现内部的 open、close、invoke 方法就可以实现自定义 Sink 了，RichSinkFunction 的类图如下。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sypiutnj21ka10o0tl.jpg" alt="undefined"></p><p>该类继承了 AbstractRichFunction 抽象类，实现了 SinkFunction 接口，同样该类也是一个 Rich 函数，它比 SinkFunction 多了 open（可以初始化数据） 和 getRuntimeContext（可以获取上下文）方法，如果不需要这两个方法，同样也是可以实现 SinkFunction 接口来自定义 Sink 的。</p><h3 id="小结与反思-13"><a href="#小结与反思-13" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中该如何去自定义 Connector，包括 Source 和 Sink，每种也都有提供样例去教大家如何操作。</p><p>本节相关代码链接：</p><ul><li><a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sources" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sources</a></li><li><a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sinks" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sinks</a></li></ul><h2 id="十六、如何使用-Flink-Connectors-——-ElasticSearch？"><a href="#十六、如何使用-Flink-Connectors-——-ElasticSearch？" class="headerlink" title="十六、如何使用 Flink Connectors —— ElasticSearch？"></a>十六、如何使用 Flink Connectors —— ElasticSearch？</h2><h3 id="准备环境和依赖-1"><a href="#准备环境和依赖-1" class="headerlink" title="准备环境和依赖"></a>准备环境和依赖</h3><h4 id="ElasticSearch-安装"><a href="#ElasticSearch-安装" class="headerlink" title="ElasticSearch 安装"></a>ElasticSearch 安装</h4><p>因为在 2.1 节中已经讲过 ElasticSearch 的安装，这里就不做过多的重复，需要注意的一点就是 Flink 的 ElasticSearch Connector 是区分版本号的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tmjwgh3j213s05wmx9.jpg" alt="undefined"></p><p>所以添加依赖的时候要区分一下，根据你安装的 ElasticSearch 来选择不一样的版本依赖，另外就是不同版本的 ElasticSearch 还会导致下面的数据写入到 ElasticSearch 中出现一些不同，我们这里使用的版本是 ElasticSearch6，如果你使用的是其他的版本可以参考官网的实现。</p><h4 id="添加依赖-1"><a href="#添加依赖-1" class="headerlink" title="添加依赖"></a>添加依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面这依赖版本号请自己根据使用的版本对应改变下。</p><h3 id="Flink-写入数据到-ElasticSearch-应用程序"><a href="#Flink-写入数据到-ElasticSearch-应用程序" class="headerlink" title="Flink 写入数据到 ElasticSearch 应用程序"></a>Flink 写入数据到 ElasticSearch 应用程序</h3><h4 id="ESSinkUtil-工具类"><a href="#ESSinkUtil-工具类" class="headerlink" title="ESSinkUtil 工具类"></a>ESSinkUtil 工具类</h4><p>这个工具类是自己封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面章节还会再讲些其他的配置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ESSinkUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * es sink</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts es hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bulkFlushMaxActions bulk flush size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism 并行数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data 数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> func</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSink</span><span class="params">(List&lt;HttpHost&gt; hosts, <span class="keyword">int</span> bulkFlushMaxActions, <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func)</span> </span>&#123;</span><br><span class="line">        ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(hosts, func);</span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions);</span><br><span class="line">        data.addSink(esSinkBuilder.build()).setParallelism(parallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析配置文件的 es hosts</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> MalformedURLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;HttpHost&gt; <span class="title">getEsAddresses</span><span class="params">(String hosts)</span> <span class="keyword">throws</span> MalformedURLException </span>&#123;</span><br><span class="line">        String[] hostList = hosts.split(<span class="string">","</span>);</span><br><span class="line">        List&lt;HttpHost&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String host : hostList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (host.startsWith(<span class="string">"http"</span>)) &#123;</span><br><span class="line">                URL url = <span class="keyword">new</span> URL(host);</span><br><span class="line">                addresses.add(<span class="keyword">new</span> HttpHost(url.getHost(), url.getPort()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String[] parts = host.split(<span class="string">":"</span>, <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span> (parts.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    addresses.add(<span class="keyword">new</span> HttpHost(parts[<span class="number">0</span>], Integer.parseInt(parts[<span class="number">1</span>])));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> MalformedURLException(<span class="string">"invalid elasticsearch hosts format"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> addresses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Main-启动类"><a href="#Main-启动类" class="headerlink" title="Main 启动类"></a>Main 启动类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sink2ES6Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取所有参数</span></span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        <span class="comment">//准备好环境</span></span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        <span class="comment">//从kafka读取数据</span></span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从配置文件中读取 es 的地址</span></span><br><span class="line">        List&lt;HttpHost&gt; esAddresses = ESSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS));</span><br><span class="line">        <span class="comment">//从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒</span></span><br><span class="line">        <span class="keyword">int</span> bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, <span class="number">40</span>);</span><br><span class="line">        <span class="comment">//从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积</span></span><br><span class="line">        <span class="keyword">int</span> sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自己再自带的 es sink 上一层封装了下</span></span><br><span class="line">        ESSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data,</span><br><span class="line">                (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123;</span><br><span class="line">                    requestIndexer.add(Requests.indexRequest()</span><br><span class="line">                            .index(ZHISHENG + <span class="string">"_"</span> + metric.getName())  <span class="comment">//es 索引名</span></span><br><span class="line">                            .type(ZHISHENG) <span class="comment">//es type</span></span><br><span class="line">                            .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); </span><br><span class="line">                &#125;);</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors es6"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="配置文件-1"><a href="#配置文件-1" class="headerlink" title="配置文件"></a>配置文件</h4><p>配置都支持集群模式填写，注意用 <code>,</code> 分隔！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng-metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng-metrics</span><br><span class="line">stream.parallelism=5</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">elasticsearch.hosts=localhost:9200</span><br><span class="line">elasticsearch.bulk.flush.max.actions=40</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><h3 id="验证数据是否写入-ElasticSearch？"><a href="#验证数据是否写入-ElasticSearch？" class="headerlink" title="验证数据是否写入 ElasticSearch？"></a>验证数据是否写入 ElasticSearch？</h3><p>执行 Main 类的 main 方法，我们的程序是只打印 Flink 的日志，没有打印存入的日志（因为我们这里没有打日志）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tmzgh4vj232e1jyjyg.jpg" alt="undefined"></p><p>所以看起来不知道我们的 Sink 是否有用，数据是否从 Kafka 读取出来后存入到 ES 了。你可以查看下本地起的 ES 终端或者服务器的 ES 日志就可以看到效果了。ES 日志如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1toe3bzdj21et0u0wqw.jpg" alt="undefined"></p><p>上图是我本地 Mac 电脑终端的 ES 日志，可以看到我们的索引了。如果还不放心，你也可以在你的电脑装个 Kibana，然后更加的直观查看下 ES 的索引情况（或者直接敲 ES 的命令）。我们用 Kibana 查看存入 ES 的索引如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1toma7ioj21en0u0agv.jpg" alt="undefined"></p><p>程序执行了一会，存入 ES 的数据量就很大了。</p><h3 id="如何保证在海量数据实时写入下-ElasticSearch-的稳定性？"><a href="#如何保证在海量数据实时写入下-ElasticSearch-的稳定性？" class="headerlink" title="如何保证在海量数据实时写入下 ElasticSearch 的稳定性？"></a>如何保证在海量数据实时写入下 ElasticSearch 的稳定性？</h3><p>上面代码已经可以实现你的大部分场景了，但是如果你的业务场景需要保证数据的完整性（不能出现丢数据的情况），那么就需要添加一些重试策略，因为在我们的生产环境中，很有可能会因为某些组件不稳定性导致各种问题，所以这里我们就要在数据存入失败的时候做重试操作，这个 Flink 自带的 es sink 就支持了，常用的失败重试配置有:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. bulk.flush.backoff.enable 用来表示是否开启重试机制</span><br><span class="line"></span><br><span class="line">2. bulk.flush.backoff.type 重试策略，有两种：EXPONENTIAL 指数型（表示多次重试之间的时间间隔按照指数方式进行增长）、CONSTANT 常数型（表示多次重试之间的时间间隔为固定常数）</span><br><span class="line"></span><br><span class="line">3. bulk.flush.backoff.delay 进行重试的时间间隔</span><br><span class="line"></span><br><span class="line">4. bulk.flush.backoff.retries 失败重试的次数</span><br><span class="line"></span><br><span class="line">5. bulk.flush.max.actions: 批量写入时的最大写入条数</span><br><span class="line"></span><br><span class="line">6. bulk.flush.max.size.mb: 批量写入时的最大数据量</span><br><span class="line"></span><br><span class="line">7. bulk.flush.interval.ms: 批量写入的时间间隔，配置后则会按照该时间间隔严格执行，无视上面的两个批量写入配置</span><br></pre></td></tr></table></figure><p>看下，就是如下这些配置了，如果你需要的话，可以在这个地方配置扩充。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tp0gu4uj21ec10utb6.jpg" alt="undefined"></p><h3 id="使用-Flink-connector-elasticsearch-可能会遇到的问题"><a href="#使用-Flink-connector-elasticsearch-可能会遇到的问题" class="headerlink" title="使用 Flink-connector-elasticsearch 可能会遇到的问题"></a>使用 Flink-connector-elasticsearch 可能会遇到的问题</h3><p>写入 ES 的时候会有这些情况会导致写入 ES 失败。</p><p>1、ES 集群队列满了，报如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12:08:07.326 [I/O dispatcher 13] ERROR o.a.f.s.c.e.ElasticsearchSinkBase - Failed Elasticsearch item request: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@566c9379 on EsThreadPoolExecutor[name = node-1/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@f00b373[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 6277]]]]</span><br></pre></td></tr></table></figure><p>是这样的，我电脑安装的 ES 队列容量默认应该是 200，我没有修改过。我这里如果配置的 bulk flush size * 并发 Sink 数量 这个值如果大于这个 queue capacity ，那么就很容易导致出现这种因为 ES 队列满了而写入失败。</p><p>当然这里你也可以通过调大点 es 的队列。参考：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html</a></p><p>2、ES 集群某个节点挂了</p><p>这个就不用说了，肯定写入失败的。跟过源码可以发现 RestClient 类里的 performRequestAsync 方法一开始会随机的从集群中的某个节点进行写入数据，如果这台机器掉线，会进行重试在其他的机器上写入，那么当时写入的这台机器的请求就需要进行失败重试，否则就会把数据丢失！</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpbszamj22h628igqp.jpg" alt="undefined"></p><p>3、ES 集群某个节点的磁盘满了</p><p>这里说的磁盘满了，并不是磁盘真的就没有一点剩余空间的，是 ES 会在写入的时候检查磁盘的使用情况，在 85% 的时候会打印日志警告。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpnrm0zj21yh0k6qqj.jpg" alt="undefined"></p><p>这里我看了下源码如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpvtgmwj21tm18gtcn.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tq2ra8kj21hu0o4abf.jpg" alt="undefined"></p><p>如果你想继续让 ES 写入的话就需要去重新配一下 ES 让它继续写入，或者你也可以清空些不必要的数据腾出磁盘空间来。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.addSink(<span class="keyword">new</span> ElasticsearchSink&lt;&gt;(</span><br><span class="line">    config, transportAddresses,</span><br><span class="line">    <span class="keyword">new</span> ElasticsearchSinkFunction&lt;String&gt;() &#123;...&#125;,</span><br><span class="line">    <span class="keyword">new</span> ActionRequestFailureHandler() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(ActionRequest action,</span></span></span><br><span class="line"><span class="function"><span class="params">                Throwable failure,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> restStatusCode,</span></span></span><br><span class="line"><span class="function"><span class="params">                RequestIndexer indexer)</span> throw Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) &#123;</span><br><span class="line">                <span class="comment">//队列满了，重新添加用于索引的 document</span></span><br><span class="line">                indexer.add(action);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, ElasticsearchParseException.class)) &#123;</span><br><span class="line">                <span class="comment">// 对于有问题的 document，删除该请求，没有额外的错误处理逻辑</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//对于抛出其他的异常错误，直接就当成 sink 失败，向外抛出异常，你也可以抛出自定义的异常</span></span><br><span class="line">                <span class="keyword">throw</span> failure;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure><p>如果仅仅只是想做失败重试，也可以直接使用官方提供的默认的 RetryRejectedExecutionFailureHandler ，该处理器会对 EsRejectedExecutionException 导致到失败写入做重试处理。如果你没有设置失败处理器（failure handler），那么就会使用默认的 NoOpFailureHandler 来简单处理所有的异常。</p><h3 id="小结与反思-14"><a href="#小结与反思-14" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中的 ElasticSearch Connector 的使用，通过一个案例教大家如何将读取到的 Kafka 数据写入到 ElasticSearch，最后讲解了 Flink 写入 ElasticSearch 的时候的各种配置和可能遇到的问题及其解决方法。</p><p>本节涉及的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6</a></p><h2 id="十七、如何使用-Flink-Connectors-——-HBase？"><a href="#十七、如何使用-Flink-Connectors-——-HBase？" class="headerlink" title="十七、如何使用 Flink Connectors —— HBase？"></a>十七、如何使用 Flink Connectors —— HBase？</h2><h3 id="准备环境和依赖-2"><a href="#准备环境和依赖-2" class="headerlink" title="准备环境和依赖"></a>准备环境和依赖</h3><h4 id="HBase-安装"><a href="#HBase-安装" class="headerlink" title="HBase 安装"></a>HBase 安装</h4><p>如果是苹果系统，可以使用 HomeBrew 命令安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install hbase</span><br></pre></td></tr></table></figure><p>HBase 最终会安装在路径 <code>/usr/local/Cellar/hbase/</code> 下面，安装版本不同，文件名也不同。</p><h4 id="配置-HBase"><a href="#配置-HBase" class="headerlink" title="配置 HBase"></a>配置 HBase</h4><p>打开 <code>libexec/conf/hbase-env.sh</code> 修改里面的 JAVA_HOME：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line">export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home&quot;</span><br></pre></td></tr></table></figure><p>根据你自己的 JAVA_HOME 来配置这个变量。</p><p>打开 <code>libexec/conf/hbase-site.xml</code> 配置 HBase 文件存储目录:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/var/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储内建zookeeper文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/var/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="运行-HBase"><a href="#运行-HBase" class="headerlink" title="运行 HBase"></a>运行 HBase</h4><p>执行启动的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-hbase.sh</span><br></pre></td></tr></table></figure><p>执行后打印出来的日志如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">starting master, logging to /usr/local/var/log/hbase/hbase-zhisheng-master-zhisheng.out</span><br></pre></td></tr></table></figure><h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><p>使用 jps 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/Cellar/hbase/1.2.9/libexec  jps</span><br><span class="line">91302 HMaster</span><br><span class="line">62535 RemoteMavenServer</span><br><span class="line">1100</span><br><span class="line">91471 Jps</span><br></pre></td></tr></table></figure><p>出现 HMaster 说明安装运行成功。</p><h4 id="启动-HBase-Shell"><a href="#启动-HBase-Shell" class="headerlink" title="启动 HBase Shell"></a>启动 HBase Shell</h4><p>执行下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hbase shell</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tsa2sfwj22800a4abv.jpg" alt="undefined"></p><h4 id="停止-HBase"><a href="#停止-HBase" class="headerlink" title="停止 HBase"></a>停止 HBase</h4><p>执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/stop-hbase.sh</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1ttq07q5j21xk05sjrz.jpg" alt="undefined"></p><h4 id="HBase-常用命令"><a href="#HBase-常用命令" class="headerlink" title="HBase 常用命令"></a>HBase 常用命令</h4><p>HBase 中常用的命令有：list（列出已存在的表）、create（创建表）、put（写数据）、get（读数据）、scan（读数据，读全表）、describe（显示表详情）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tu0f06kj216m0f4wfe.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tu7vqa2j227y10cdke.jpg" alt="undefined"></p><h4 id="添加依赖-2"><a href="#添加依赖-2" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加 HBase 相关的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hbase_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Flink HBase Connector 中，HBase 不仅可以作为数据源，也还可以写入数据到 HBase 中去，我们先来看看如何从 HBase 中读取数据。</p><h3 id="Flink-使用-TableInputFormat-读取-HBase-批量数据"><a href="#Flink-使用-TableInputFormat-读取-HBase-批量数据" class="headerlink" title="Flink 使用 TableInputFormat 读取 HBase 批量数据"></a>Flink 使用 TableInputFormat 读取 HBase 批量数据</h3><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>先往 HBase 中插入五条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">put &apos;zhisheng&apos;, &apos;first&apos;, &apos;info:bar&apos;, &apos;hello&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;second&apos;, &apos;info:bar&apos;, &apos;zhisheng001&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;third&apos;, &apos;info:bar&apos;, &apos;zhisheng002&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;four&apos;, &apos;info:bar&apos;, &apos;zhisheng003&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;five&apos;, &apos;info:bar&apos;, &apos;zhisheng004&apos;</span><br></pre></td></tr></table></figure><p>scan 整个 <code>zhisheng</code> 表的话，有五条数据：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tuhmykdj21qu0co75v.jpg" alt="undefined"></p><h4 id="Flink-Job-代码"><a href="#Flink-Job-代码" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 读取 HBase 数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReadMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//表名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HBASE_TABLE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line">    <span class="comment">// 列族</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] INFO = <span class="string">"info"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line">    <span class="comment">//列名</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] BAR = <span class="string">"bar"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.createInput(<span class="keyword">new</span> TableInputFormat&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> Tuple2&lt;String, String&gt; reuse = <span class="keyword">new</span> Tuple2&lt;String, String&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Scan <span class="title">getScanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">                scan.addColumn(INFO, BAR);</span><br><span class="line">                <span class="keyword">return</span> scan;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> String <span class="title">getTableName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> HBASE_TABLE_NAME;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Tuple2&lt;String, String&gt; <span class="title">mapResultToTuple</span><span class="params">(Result result)</span> </span>&#123;</span><br><span class="line">                String key = Bytes.toString(result.getRow());</span><br><span class="line">                String val = Bytes.toString(result.getValue(INFO, BAR));</span><br><span class="line">                reuse.setField(key, <span class="number">0</span>);</span><br><span class="line">                reuse.setField(val, <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">return</span> reuse;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple2&lt;String, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f1.startsWith(<span class="string">"zhisheng"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中将 HBase 中的读取全部读取出来后然后过滤以 <code>zhisheng</code> 开头的 value 数据。读取结果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tuqtg8pj224k172gud.jpg" alt="undefined"></p><p>可以看到输出的结果中已经将以 <code>zhisheng</code> 开头的四条数据都打印出来了。</p><h3 id="Flink-使用-TableOutputFormat-向-HBase-写入数据"><a href="#Flink-使用-TableOutputFormat-向-HBase-写入数据" class="headerlink" title="Flink 使用 TableOutputFormat 向 HBase 写入数据"></a>Flink 使用 TableOutputFormat 向 HBase 写入数据</h3><h4 id="添加依赖-3"><a href="#添加依赖-3" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hadoop-compatibility_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>要在 HBase 中提交创建 <code>zhisheng_sink</code> 表，并且 Column 为 <code>info_sink</code> （如果先运行程序的话是会报错说该表不存在的）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &apos;zhisheng_sink&apos;, &apos;info_sink&apos;</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tv440c3j227k0okn03.jpg" alt="undefined"></p><h4 id="Flink-Job-代码-1"><a href="#Flink-Job-代码-1" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><p>接着写 Flink Job 的代码，这里我们将 WordCount 的结果 KV 数据写入到 HBase 中去，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 写入数据到 HBase</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseWriteMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//表名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HBASE_TABLE_NAME = <span class="string">"zhisheng_sink"</span>;</span><br><span class="line">    <span class="comment">// 列族</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] INFO = <span class="string">"info_sink"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line">    <span class="comment">//列名</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] BAR = <span class="string">"bar_sink"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        Job job = Job.getInstance();</span><br><span class="line">        job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, HBASE_TABLE_NAME);</span><br><span class="line">        env.fromElements(WORDS)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] splits = value.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split, <span class="number">1</span>));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;Text, Mutation&gt;&gt;() &#123;</span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">transient</span> Tuple2&lt;Text, Mutation&gt; reuse;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        reuse = <span class="keyword">new</span> Tuple2&lt;Text, Mutation&gt;();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;Text, Mutation&gt; <span class="title">map</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        reuse.f0 = <span class="keyword">new</span> Text(value.f0);</span><br><span class="line">                        Put put = <span class="keyword">new</span> Put(value.f0.getBytes(ConfigConstants.DEFAULT_CHARSET));</span><br><span class="line">                        put.addColumn(INFO, BAR, Bytes.toBytes(value.f1.toString()));</span><br><span class="line">                        reuse.f1 = put;</span><br><span class="line">                        <span class="keyword">return</span> reuse;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).output(<span class="keyword">new</span> HadoopOutputFormat&lt;Text, Mutation&gt;(<span class="keyword">new</span> TableOutputFormat&lt;Text&gt;(), job));</span><br><span class="line">        env.execute(<span class="string">"Flink Connector HBase sink Example"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] WORDS = <span class="keyword">new</span> String[]&#123;</span><br><span class="line">            <span class="string">"To be, or not to be,--that is the question:--"</span>,</span><br><span class="line">            <span class="string">"The fair is be in that orisons"</span></span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行该 Job 的话，然后再用 HBase shell 命令去验证数据是否插入成功了：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tvgngwkj21v40fw41q.jpg" alt="undefined"></p><p>可以看见数据已经成功写入了 11 条，然后我们验证一下数据的条数是不是一样的呢？我们在上面的代码中将 map 和 output 算子给注释掉，然后用上 print 打印出来的话，结果如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(be,<span class="number">3</span>)</span><br><span class="line">(is,<span class="number">2</span>)</span><br><span class="line">(in,<span class="number">1</span>)</span><br><span class="line">(or,<span class="number">1</span>)</span><br><span class="line">(orisons,<span class="number">1</span>)</span><br><span class="line">(not,<span class="number">1</span>)</span><br><span class="line">(the,<span class="number">2</span>)</span><br><span class="line">(fair,<span class="number">1</span>)</span><br><span class="line">(question,<span class="number">1</span>)</span><br><span class="line">(that,<span class="number">2</span>)</span><br><span class="line">(to,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>统计的结果刚好也是 11 条数据，说明我们的写入过程中没有丢失数据。但是运行 Job 的话你会看到日志中报了一条这样的错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.IllegalArgumentException: Can not create a Path from a null string</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tvrecrwj22xc0p4n5q.jpg" alt="undefined"></p><p>这个问题是因为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Path partitionsPath = <span class="keyword">new</span> Path(conf.get(<span class="string">"mapred.output.dir"</span>), <span class="string">"partitions_"</span> + UUID.randomUUID());</span><br></pre></td></tr></table></figure><p>当配置项 mapred.output.dir 不存在时，conf.get() 将返回 null，从而导致上述异常。那么该如何解决这个问题呢？</p><blockquote><p>需要在代码中或配置文件中添加配置项 mapred.output.dir。</p></blockquote><p>比如在代码里加上这行代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.getConfiguration().set(<span class="string">"mapred.output.dir"</span>, <span class="string">"/tmp"</span>);</span><br></pre></td></tr></table></figure><p>再次运行这个 Job 你就不会发现报错了。</p><h3 id="Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据"><a href="#Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据" class="headerlink" title="Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据"></a>Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据</h3><p>从上面两个程序中你可以发现两个都是批程序（从 HBase 读取批量的数据、写入批量的数据进 HBase），下面跟着笔者来演示一个流程序。</p><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><p>本来是打算演示从 Kafka 读取 String 类型的数据，但是为了好演示，我这里直接在代码里面造一些数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; dataStream = env.addSource(<span class="keyword">new</span> SourceFunction&lt;String&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            out.collect(String.valueOf(Math.floor(Math.random() * <span class="number">100</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>如果是读取 Kafka 数据请对应替换成：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">    parameterTool.get(METRICS_TOPIC),   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">    <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">    props));</span><br></pre></td></tr></table></figure><h4 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h4><p>获取到数据后需要将数据写入到 HBase，这里使用的实现 HBaseOutputFormat 接口，然后重写里面的 configure、open、writeRecord、close 方法，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseOutputFormat</span> <span class="keyword">implements</span> <span class="title">OutputFormat</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> org.apache.hadoop.conf.Configuration configuration;</span><br><span class="line">    <span class="keyword">private</span> Connection connection = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> String taskNumber = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> Table table = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rowNumber = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Configuration parameters)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//设置配置信息</span></span><br><span class="line">        configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(HBASE_ZOOKEEPER_QUORUM, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_ZOOKEEPER_QUORUM));</span><br><span class="line">        configuration.set(HBASE_ZOOKEEPER_PROPERTY_CLIENTPORT, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_ZOOKEEPER_PROPERTY_CLIENTPORT));</span><br><span class="line">        configuration.set(HBASE_RPC_TIMEOUT, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_RPC_TIMEOUT));</span><br><span class="line">        configuration.set(HBASE_CLIENT_OPERATION_TIMEOUT, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_CLIENT_OPERATION_TIMEOUT));</span><br><span class="line">        configuration.set(HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(<span class="keyword">int</span> taskNumber, <span class="keyword">int</span> numTasks)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        TableName tableName = TableName.valueOf(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_TABLE_NAME));</span><br><span class="line">        Admin admin = connection.getAdmin();</span><br><span class="line">        <span class="keyword">if</span> (!admin.tableExists(tableName)) &#123; <span class="comment">//检查是否有该表，如果没有，创建</span></span><br><span class="line">            log.info(<span class="string">"==============不存在表 = &#123;&#125;"</span>, tableName);</span><br><span class="line">                admin.createTable(<span class="keyword">new</span> HTableDescriptor(TableName.valueOf(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_TABLE_NAME)))</span><br><span class="line">                        .addFamily(<span class="keyword">new</span> HColumnDescriptor(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_COLUMN_NAME))));</span><br><span class="line">        &#125;</span><br><span class="line">        table = connection.getTable(tableName);</span><br><span class="line">        <span class="keyword">this</span>.taskNumber = String.valueOf(taskNumber);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeRecord</span><span class="params">(String record)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Put put = <span class="keyword">new</span> Put(Bytes.toBytes(taskNumber + rowNumber));</span><br><span class="line">        put.addColumn(Bytes.toBytes(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_COLUMN_NAME)), Bytes.toBytes(<span class="string">"zhisheng"</span>),</span><br><span class="line">                Bytes.toBytes(String.valueOf(rowNumber)));</span><br><span class="line">        rowNumber++;</span><br><span class="line">        table.put(put);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        table.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="配置文件-2"><a href="#配置文件-2" class="headerlink" title="配置文件"></a>配置文件</h4><p>配置文件中的一些配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng</span><br><span class="line">stream.parallelism=4</span><br><span class="line">stream.sink.parallelism=4</span><br><span class="line">stream.default.parallelism=4</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line"></span><br><span class="line"># HBase</span><br><span class="line">hbase.zookeeper.quorum=localhost:2181</span><br><span class="line">hbase.client.retries.number=1</span><br><span class="line">hbase.master.info.port=-1</span><br><span class="line">hbase.zookeeper.property.clientPort=2081</span><br><span class="line">hbase.rpc.timeout=30000</span><br><span class="line">hbase.client.operation.timeout=30000</span><br><span class="line">hbase.client.scanner.timeout.period=30000</span><br><span class="line"></span><br><span class="line"># HBase table name</span><br><span class="line">hbase.table.name=zhisheng_stream</span><br><span class="line">hbase.column.name=info_stream</span><br></pre></td></tr></table></figure><h3 id="项目运行及验证"><a href="#项目运行及验证" class="headerlink" title="项目运行及验证"></a>项目运行及验证</h3><p>运行项目后然后你再去用 HBase shell 命令查看你会发现该 <code>zhisheng_stream</code> 表之前没有建立，现在建立了，再通过 scan 命令查看的话，你会发现数据一直在更新，不断增加数据条数。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tw2katwj2230114wmp.jpg" alt="undefined"></p><h3 id="小结与反思-15"><a href="#小结与反思-15" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节开始讲解了 HBase 相关的环境安装和基础命令，接着讲解了如何去读取 HBase 数据和写入数据到 HBase。</p><p>本节涉及的完整代码地址在：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-hbase" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-hbase</a></p><h2 id="十八、如何使用-Flink-Connectors-——-Redis？"><a href="#十八、如何使用-Flink-Connectors-——-Redis？" class="headerlink" title="十八、如何使用 Flink Connectors —— Redis？"></a>十八、如何使用 Flink Connectors —— Redis？</h2><p>在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。</p><h3 id="安装-Redis"><a href="#安装-Redis" class="headerlink" title="安装 Redis"></a>安装 Redis</h3><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><p>先在 <a href="https://redis.io/download" target="_blank" rel="noopener">https://redis.io/download</a> 下载到 Redis。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-5.0.4.tar.gz</span><br><span class="line">tar xzf redis-5.0.4.tar.gz</span><br><span class="line">cd redis-5.0.4</span><br><span class="line">make</span><br></pre></td></tr></table></figure><h4 id="通过-HomeBrew-安装"><a href="#通过-HomeBrew-安装" class="headerlink" title="通过 HomeBrew 安装"></a>通过 HomeBrew 安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install redis</span><br></pre></td></tr></table></figure><p>如果需要后台运行 Redis 服务，使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew services start redis</span><br></pre></td></tr></table></figure><p>要运行命令，可以直接到 /usr/local/bin 目录下，有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br><span class="line">redis-cli</span><br></pre></td></tr></table></figure><p>两个命令，执行 <code>redis-server</code> 可以打开服务端：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u105kjbj21980em75o.jpg" alt="undefined"></p><p>然后另外开一个终端，运行 <code>redis-cli</code> 命令可以运行客户端：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u1cqotwj20r605tzk9.jpg" alt="undefined"></p><h3 id="准备商品数据发送至-Kafka"><a href="#准备商品数据发送至-Kafka" class="headerlink" title="准备商品数据发送至 Kafka"></a>准备商品数据发送至 Kafka</h3><p>这里我打算将从 Kafka 读取到所有到商品的信息，然后将商品信息中的 <strong>商品ID</strong> 和 <strong>商品价格</strong> 提取出来，然后写入到 Redis 中，供第三方服务根据商品 ID 查询到其对应的商品价格。</p><p>首先定义我们的商品类 （其中 id 和 price 字段是我们最后要提取的）为：</p><p>ProductEvent.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 商品</span></span><br><span class="line"><span class="comment"> * blog：http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> * 微信公众号：zhisheng</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Builder</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类目 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long categoryId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 编码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String code;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long shopId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String shopName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long brandId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String brandName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 图片地址</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String imageUrl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 状态（1(上架),-1(下架),-2(冻结),-3(删除)）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> status;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> type;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 标签</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;String&gt; tags;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 价格（以分为单位）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long price;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后写个工具类不断的模拟商品数据发往 Kafka，工具类 <code>ProductUtil.java</code> ：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"zhisheng"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10000</span>; i++) &#123;</span><br><span class="line">            ProductEvent product = ProductEvent.builder().id((<span class="keyword">long</span>) i)  <span class="comment">//商品的 id</span></span><br><span class="line">                    .name(<span class="string">"product"</span> + i)    <span class="comment">//商品 name</span></span><br><span class="line">                    .price(random.nextLong() / <span class="number">10000000000000L</span>) <span class="comment">//商品价格（以分为单位）</span></span><br><span class="line">                    .code(<span class="string">"code"</span> + i).build();  <span class="comment">//商品编码</span></span><br><span class="line"></span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(product));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(product));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-消费-Kafka-中商品数据"><a href="#Flink-消费-Kafka-中商品数据" class="headerlink" title="Flink 消费 Kafka 中商品数据"></a>Flink 消费 Kafka 中商品数据</h3><p>我们需要在 Flink 中消费 Kafka 数据，然后将商品中的两个数据（商品 id 和 price）取出来。先来看下这段 Flink Job 代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line">        Properties props = KafkaConfigUtil.buildKafkaProps(parameterTool);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; product = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.get(METRICS_TOPIC),   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props))</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, ProductEvent.class)) <span class="comment">//反序列化 JSON</span></span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ProductEvent, Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(ProductEvent value, Collector&lt;Tuple2&lt;String, String&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//收集商品 id 和 price 两个属性</span></span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(value.getId().toString(), value.getPrice().toString()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        product.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink redis connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后 IDEA 中启动运行 Job，再运行上面的 ProductUtil 发送 Kafka 数据的工具类，注意：也得提前启动 Kafka。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u1psyytj225g0pstcr.jpg" alt="undefined"></p><p>上图左半部分是工具类发送数据到 Kafka 打印的日志，右半部分是 Job 执行的结果，可以看到它已经将商品的 id 和 price 数据获取到了。</p><p>那么接下来我们需要的就是将这种 <code>Tuple2</code> 格式的 KV 数据写入到 Redis 中去。要将数据写入到 Redis 的话是需要先添加依赖的。</p><h3 id="Redis-Connector-简介"><a href="#Redis-Connector-简介" class="headerlink" title="Redis Connector 简介"></a>Redis Connector 简介</h3><p>Redis Connector 提供用于向 Redis 发送数据的接口的类。接收器可以使用三种不同的方法与不同类型的 Redis 环境进行通信：</p><ul><li>单 Redis 服务器</li><li>Redis 集群</li><li>Redis Sentinel</li></ul><h3 id="添加依赖-4"><a href="#添加依赖-4" class="headerlink" title="添加依赖"></a>添加依赖</h3><p>需要添加 Flink Redis Sink 的 Connector，这个 Redis Connector 官方只有老的版本，后面也一直没有更新，所以可以看到网上有些文章都是添加老的版本的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>包括该部分的文档都是很早之前的啦，可以查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/streaming/connectors/redis.html。" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/streaming/connectors/redis.html。</a></p><p>另外在 <a href="https://bahir.apache.org/docs/flink/current/flink-streaming-redis/" target="_blank" rel="noopener">https://bahir.apache.org/docs/flink/current/flink-streaming-redis/</a> 也看到一个 Flink Redis Connector 的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>两个依赖功能都是一样的，我们还是就用官方的那个 Maven 依赖来进行演示。</p><h3 id="Flink-写入数据到-Redis"><a href="#Flink-写入数据到-Redis" class="headerlink" title="Flink 写入数据到 Redis"></a>Flink 写入数据到 Redis</h3><p>像写入到 Redis，那么肯定要配置 Redis 服务的地址（不管是单机的还是集群）。</p><p><strong>单机的 Redis</strong> 你可以这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder().setHost(<span class="string">"127.0.0.1"</span>).build();</span><br></pre></td></tr></table></figure><p>这个 FlinkJedisPoolConfig 源码中有四个属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String host;  <span class="comment">//hostname or IP</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> port;     <span class="comment">//端口，默认 6379</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> database; <span class="comment">//database index</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String password;  <span class="comment">//password</span></span><br></pre></td></tr></table></figure><p>另外你还可以通过 FlinkJedisPoolConfig 设置其他的的几个属性（因为 FlinkJedisPoolConfig 继承自 FlinkJedisConfigBase，这几个属性在 FlinkJedisConfigBase 抽象类的）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> maxTotal;   <span class="comment">//池可分配的对象最大数量，默认是 8</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> maxIdle;    <span class="comment">//池中空闲的对象最大数量，默认是 8</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> minIdle;    <span class="comment">//池中空闲的对象最小数量，默认是 0</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> connectionTimeout;  <span class="comment">//socket 或者连接超时时间，默认是 2000ms</span></span><br></pre></td></tr></table></figure><p><strong>Redis 集群</strong> 你可以这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisClusterConfig config = <span class="keyword">new</span> FlinkJedisClusterConfig.Builder()</span><br><span class="line">                .setNodes(<span class="keyword">new</span> HashSet&lt;InetSocketAddress&gt;(</span><br><span class="line">                        Arrays.asList(<span class="keyword">new</span> InetSocketAddress(<span class="string">"redis1"</span>, <span class="number">6379</span>)))).build();</span><br></pre></td></tr></table></figure><p><strong>Redis Sentinels</strong> 你可以这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisSentinelConfig sentinelConfig = <span class="keyword">new</span> FlinkJedisSentinelConfig.Builder()</span><br><span class="line">        .setMasterName(<span class="string">"master"</span>)</span><br><span class="line">        .setSentinels(<span class="keyword">new</span> HashSet&lt;&gt;(Arrays.asList(<span class="string">"sentinel1"</span>, <span class="string">"sentinel2"</span>)))</span><br><span class="line">        .setPassword(<span class="string">""</span>)</span><br><span class="line">        .setDatabase(<span class="number">1</span>).build();</span><br></pre></td></tr></table></figure><p>另外就是 Redis Sink 了，Redis Sink 核心类是 RedisMapper，它是一个接口，里面有三个方法，使用时我们需要重写这三个方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RedisMapper</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//设置使用 Redis 的数据结构类型，和 key 的名词，RedisCommandDescription 中有两个属性 RedisCommand、key</span></span><br><span class="line">    <span class="function">RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">//获取 key 值</span></span><br><span class="line">    <span class="function">String <span class="title">getKeyFromData</span><span class="params">(T var1)</span></span>;</span><br><span class="line">    <span class="comment">//获取 value 值</span></span><br><span class="line">    <span class="function">String <span class="title">getValueFromData</span><span class="params">(T var1)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面 RedisCommandDescription 中有两个属性 RedisCommand、key。RedisCommand 可以设置 Redis 的数据结果类型，下面是 Redis 数据结构的类型对应着的 Redis Command 的类型：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2170ijj214e0mcjsd.jpg" alt="undefined"></p><p>其对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> RedisCommand &#123;</span><br><span class="line">    LPUSH(RedisDataType.LIST),</span><br><span class="line">    RPUSH(RedisDataType.LIST),</span><br><span class="line">    SADD(RedisDataType.SET),</span><br><span class="line">    SET(RedisDataType.STRING),</span><br><span class="line">    PFADD(RedisDataType.HYPER_LOG_LOG),</span><br><span class="line">    PUBLISH(RedisDataType.PUBSUB),</span><br><span class="line">    ZADD(RedisDataType.SORTED_SET),</span><br><span class="line">    HSET(RedisDataType.HASH);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> RedisDataType redisDataType;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">RedisCommand</span><span class="params">(RedisDataType redisDataType)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.redisDataType = redisDataType;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisDataType <span class="title">getRedisDataType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.redisDataType;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们实现这个 RedisMapper 接口如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisSinkMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//指定 RedisCommand 的类型是 HSET，对应 Redis 中的数据结构是 HASH，另外设置 key = zhisheng</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.HSET, <span class="string">"zhisheng"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.f0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在 Flink Job 中加入下面这行，将数据通过 RedisSinkMapper 写入到 Redis 中去：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">product.addSink(<span class="keyword">new</span> RedisSink&lt;Tuple2&lt;String, String&gt;&gt;(conf, <span class="keyword">new</span> RedisSinkMapper()));</span><br></pre></td></tr></table></figure><h3 id="验证写入结果"><a href="#验证写入结果" class="headerlink" title="验证写入结果"></a>验证写入结果</h3><p>运行 Job 的话，就是把数据已经插入进 Redis 了，那么如何验证我们的结果是否正确呢？</p><p>1、我们去终端 Cli 执行命令查看这个 zhisheng 的 key，然后查找某个商品 id (1 ~ 10000) 对应的商品价格，超过这个 id 则为 nil。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2a7aouj215o0pg75r.jpg" alt="undefined"></p><p>2、另外一种验证的方式就是通过 Java 代码来操作 Redis 查询数据了。</p><p>我们先引入 Redis 的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>连接 Redis 查询数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Jedis jedis = <span class="keyword">new</span> Jedis(<span class="string">"127.0.0.1"</span>);</span><br><span class="line">        System.out.println(<span class="string">"Server is running: "</span> + jedis.ping());</span><br><span class="line">        System.out.println(<span class="string">"result:"</span> + jedis.hgetAll(<span class="string">"zhisheng"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2jipebj224g12owj7.jpg" alt="undefined"></p><p>这一行把所有的数据都打印出来了，所以我们的数据确实成功地插入到 Redis 中去了。</p><h3 id="小结与反思-16"><a href="#小结与反思-16" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本文先讲解了 Redis 的安装，然后讲了 Flink 如何消费 Kafka 的数据并将数据写入到 Redis 中去。在实战的过程中还分析了 Flink Redis Connector 中的原理，只要我们懂得了这些原理，后面再去做这块的需求就难不倒大家了。</p><p>本节涉及的代码地址在：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-redis" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-redis</a></p><h2 id="十九、如何使用-Side-Output-来分流"><a href="#十九、如何使用-Side-Output-来分流" class="headerlink" title="十九、如何使用 Side Output 来分流?"></a>十九、如何使用 Side Output 来分流?</h2><p>通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。</p><h3 id="使用-Filter-分流"><a href="#使用-Filter-分流" class="headerlink" title="使用 Filter 分流"></a>使用 Filter 分流</h3><p>使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; <span class="string">"machine"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出机器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; <span class="string">"docker"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出容器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; <span class="string">"application"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出应用的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; <span class="string">"middleware"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出中间件的数据</span></span><br></pre></td></tr></table></figure><h3 id="使用-Split-分流"><a href="#使用-Split-分流" class="headerlink" title="使用 Split 分流"></a>使用 Split 分流</h3><p>先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SplitStream&lt;MetricEvent&gt; splitData = data.split(<span class="keyword">new</span> OutputSelector&lt;MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(MetricEvent metricEvent)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                tags.add(<span class="string">"machine"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                tags.add(<span class="string">"docker"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                tags.add(<span class="string">"application"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                tags.add(<span class="string">"middleware"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;MetricEvent&gt; machine = splitData.select(<span class="string">"machine"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = splitData.select(<span class="string">"docker"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = splitData.select(<span class="string">"application"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = splitData.select(<span class="string">"middleware"</span>);</span><br></pre></td></tr></table></figure><p>上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/" target="_blank" rel="noopener">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ，当时排查这个问题还查到两个相关的 Flink Issue。</p><ul><li><a href="https://issues.apache.org/jira/browse/FLINK-5031" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-5031</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11084" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-11084</a></li></ul><p>这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。</p><h3 id="使用-Side-Output-分流"><a href="#使用-Side-Output-分流" class="headerlink" title="使用 Side Output 分流"></a>使用 Side Output 分流</h3><p>要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 output tag</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; machineTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"machine"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; dockerTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"docker"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; applicationTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"application"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; middlewareTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"middleware"</span>) &#123;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>定义好 OutputTag 后，可以使用下面几种函数来处理数据：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><p>在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(<span class="keyword">new</span> ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                context.output(machineTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                context.output(dockerTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                context.output(applicationTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                context.output(middlewareTag, metricEvent);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                collector.collect(metricEvent);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag);</span><br></pre></td></tr></table></figure><p>这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。</p><h3 id="小结与反思-17"><a href="#小结与反思-17" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了下 Flink 中将数据分流的三种方式，涉及的完整代码 GitHub 地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput</a></p><h2 id="二十、Flink-State-深度讲解"><a href="#二十、Flink-State-深度讲解" class="headerlink" title="二十、Flink State 深度讲解"></a>二十、Flink State 深度讲解</h2><p>在基础篇中的 1.2 节中介绍了 Flink 是一款有状态的流处理框架。那么大家可能有点疑问，这个状态是什么意思？拿 Flink 最简单的 Word Count 程序来说，它需要不断的对 word 出现的个数进行结果统计，那么后一个结果就需要利用前一个的结果然后再做 +1 的操作，这样前一个计算就需要将 word 出现的次数 count 进行存着（这个 count 那么就是一个状态）然后后面才可以进行累加。</p><h3 id="为什么需要-state？"><a href="#为什么需要-state？" class="headerlink" title="为什么需要 state？"></a>为什么需要 state？</h3><p>对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）：</p><ul><li>Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费</li><li>Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1vh26in0j21k20qg752.jpg" alt="undefined"></p><p>为了解决上面两种情况（数据重复消费或者数据没有消费）的发生，那么是不是就得需要个什么东西做个记录将这种数据消费状态，Flink state 就这样诞生了，state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态），当 Job 因为某种错误或者其他原因导致重启时，就能够从 checkpoint（定时将 state 做一个全局快照，在 Flink 中，为了能够让 Job 在运行的过程中保证容错性，才会对这些 state 做一个快照，在 4.3 节中会详细讲） 中的 state 数据进行恢复。</p><h3 id="State-的种类"><a href="#State-的种类" class="headerlink" title="State 的种类"></a>State 的种类</h3><p>在 Flink 中有两个基本的 state：Keyed state 和 Operator state，下面来分别介绍一下这两种 State。</p><h3 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h3><p>Keyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。你可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。逻辑上，Keyed State 总是对应着一个 二元组，在某种程度上，因为每个具体的 key 总是属于唯一一个具体的 parallel-operator-instance（并行操作实例），这种情况下，那么就可以简化认为是 。Keyed State 可以进一步组织成 Key Group，Key Group 是 Flink 重新分配 Keyed State 的最小单元，所以有多少个并行，就会有多少个 Key Group。在执行过程中，每个 keyed operator 的并行实例会处理来自不同 key 的不同 Key Group。</p><h3 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h3><p>对 Operator State 而言，每个 operator state 都对应着一个并行实例。Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。</p><p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p><p>在 Flink 源码中，在 flink-core module 下的 org.apache.flink.api.common.state 中可以看到 Flink 中所有和 State 相关的类。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w0k0t1xj216a1kyq4n.jpg" alt="undefined"></p><h3 id="Raw-and-Managed-State"><a href="#Raw-and-Managed-State" class="headerlink" title="Raw and Managed State"></a>Raw and Managed State</h3><p>Keyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。</p><p>原始状态是 Operator（算子）保存它们自己的数据结构中的 state，当 checkpoint 时，原始状态会以字节流的形式写入进 checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。</p><p>托管状态可以使用 Flink runtime 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink runtime 会对这些状态进行编码然后将它们写入到 checkpoint 中。</p><p>DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在实现 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。</p><p>注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。</p><h3 id="如何使用托管-Keyed-State"><a href="#如何使用托管-Keyed-State" class="headerlink" title="如何使用托管 Keyed State"></a>如何使用托管 Keyed State</h3><p>托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。</p><p>我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的：</p><ul><li>ValueState: 保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。</li><li>ListState: 保存一个值的列表，用 add(T) 或者 addAll(List) 来添加，用 Iterable get() 来获取。</li><li>ReducingState: 保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。</li><li>AggregatingState: 保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。</li><li>FoldingState: 与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。</li><li>MapState: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map) 添加，或者使用 get(UK) 获取。</li></ul><p>所有类型的状态都有一个 clear() 方法来清除当前的状态。</p><p>注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。</p><p>需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。</p><p>要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。</p><p>状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态：</p><ul><li>ValueState getState(ValueStateDescriptor)</li><li>ReducingState getReducingState(ReducingStateDescriptor)</li><li>ListState getListState(ListStateDescriptor)</li><li>AggregatingState getAggregatingState(AggregatingState)</li><li>FoldingState getFoldingState(FoldingStateDescriptor)</li><li>MapState getMapState(MapStateDescriptor)</li></ul><p>上面讲了这么多概念，那么来一个例子来看看如何使用状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowAverage</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//访问状态的 value 值</span></span><br><span class="line">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 count</span></span><br><span class="line">        currentSum.f0 += <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 sum</span></span><br><span class="line">        currentSum.f1 += input.f1;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        sum.update(currentSum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果 count 等于 2, 发出平均值并清除状态</span></span><br><span class="line">        <span class="keyword">if</span> (currentSum.f0 &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class="line">            sum.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">                        <span class="string">"average"</span>, <span class="comment">//状态名称</span></span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class="comment">//类型信息</span></span><br><span class="line">                        Tuple2.of(<span class="number">0L</span>, <span class="number">0L</span>)); <span class="comment">//状态的默认值</span></span><br><span class="line">        sum = getRuntimeContext().getState(descriptor);<span class="comment">//获取状态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">env.fromElements(Tuple2.of(<span class="number">1L</span>, <span class="number">3L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">5L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">7L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">4L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">2L</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> CountWindowAverage())</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果会打印出 (1,4) 和 (1,5)</span></span><br></pre></td></tr></table></figure><p>这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。</p><h3 id="State-TTL-存活时间"><a href="#State-TTL-存活时间" class="headerlink" title="State TTL(存活时间)"></a>State TTL(存活时间)</h3><h4 id="State-TTL-介绍"><a href="#State-TTL-介绍" class="headerlink" title="State TTL 介绍"></a>State TTL 介绍</h4><p>TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"zhisheng"</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);    <span class="comment">//开启 ttl</span></span><br></pre></td></tr></table></figure><p>上面配置中有几个选项需要注意：</p><p>1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。</p><p>2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）：</p><ul><li>StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新</li><li>StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新</li></ul><p>3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired：</p><ul><li>StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值</li><li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回</li></ul><p>在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据.</p><p>另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。</p><p>注意：</p><ul><li>状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。</li><li>目前仅支持参考 processing time 的 TTL</li><li>使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。</li><li>TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。</li><li>只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。</li></ul><h4 id="清除过期-state"><a href="#清除过期-state" class="headerlink" title="清除过期 state"></a>清除过期 state</h4><p>默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。</p><p>注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。</p><p>此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupFullSnapshot()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>此配置不适用于 RocksDB 状态后端中的增量 checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。</p><p>除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupInBackground()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。</p><p>我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。</p><p>在该类中的属性有如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w14qx84j20pg0bw3ys.jpg" alt="undefined"></p><ul><li>DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig</li><li>UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选</li><li>StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired</li><li>TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选</li><li>Time：设置 TTL 的时间，这里有两个参数 unit 和 size</li><li>CleanupStrategies：TTL 清理策略，在该类中又有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL<em>STATE</em>SCAN<em>SNAPSHOT、INCREMENTAL</em>CLEANUP 和 ROCKSDB<em>COMPACTION</em>FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器）。</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w1dvbnrj21l40e4jrv.jpg" alt="undefined"></p><p>如果对 State TTL 还有不清楚的可以看看 Flink 源码 flink-runtime module 中的 state ttl 相关的实现：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w1nxwk9j215w0z8gmm.jpg" alt="undefined"></p><h3 id="如何使用托管-Operator-State"><a href="#如何使用托管-Operator-State" class="headerlink" title="如何使用托管 Operator State"></a>如何使用托管 Operator State</h3><p>为了使用托管的 Operator State，必须要有一个有状态的函数，这个函数可以实现 CheckpointedFunction 或者 ListCheckpointed 接口。</p><p>下面分别讲一下如何使用：</p><h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><p>如果是实现 CheckpointedFunction 接口的话，那么我们先来看下这个接口里面有什么方法呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//当请求 checkpoint 快照时，将调用此方法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//在分布式执行期间创建并行功能实例时，将调用此方法。 函数通常在此方法中设置其状态存储数据结构</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p>当有请求执行 checkpoint 的时候，snapshotState() 方法就会被调用，initializeState() 方法会在每次初始化用户定义的函数时或者从更早的 checkpoint 恢复的时候被调用，因此 initializeState() 不仅是不同类型的状态被初始化的地方，而且还是 state 恢复逻辑的地方。</p><p>目前，List 类型的托管状态是支持的，状态被期望是一个可序列化的对象的 List，彼此独立，这样便于重分配，换句话说，这些对象是可以重新分配的 non-keyed state 的最小粒度，根据状态的访问方法，定义了重新分配的方案：</p><ul><li>Even-split redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或者恢复的时候，这个状态元素列表会被按照并行度分为子列表，每个算子会得到一个子列表。这个子列表可能为空，或包含一个或多个元素。举个例子，如果使用并行性 1，算子的检查点状态包含元素 element1 和 element2，当将并行性增加到 2 时，element1 可能最终在算子实例 0 中，而 element2 将转到算子实例 1 中。</li><li>Union redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或恢复的时候，每个算子都会获得完整的状态元素列表。</li></ul><p>如下示例是一个有状态的 SinkFunction 使用 CheckpointedFunction 来发送到外部之前缓存数据，使用了Even-split策略。</p><p>下面是一个有状态的 SinkFunction 的示例，它使用 CheckpointedFunction 来缓存数据，然后再将这些数据发送到外部系统，使用了 Even-split 策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BufferingSink</span> <span class="keyword">implements</span> <span class="title">SinkFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;, <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threshold;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BufferingSink</span><span class="params">(<span class="keyword">int</span> threshold)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">        <span class="keyword">this</span>.bufferedElements = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context contex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        bufferedElements.add(value);</span><br><span class="line">        <span class="keyword">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class="line">                <span class="comment">//将数据发到外部系统</span></span><br><span class="line">            &#125;</span><br><span class="line">            bufferedElements.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        checkpointedState.clear();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123;</span><br><span class="line">            checkpointedState.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">            <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">"buffered-elements"</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class="line">                bufferedElements.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>initializeState 方法将 FunctionInitializationContext 作为参数，它用来初始化 non-keyed 状态。注意状态是如何初始化的，类似于 Keyed state，StateDescriptor 包含状态名称和有关状态值的类型的信息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">    <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">"buffered-elements"</span>,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br></pre></td></tr></table></figure><h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><p>是一种受限的 CheckpointedFunction，只支持 List 风格的状态和 even-spit 的重分配策略。该接口里面的方法有：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w20ksxwj21160aoq2y.jpg" alt="undefined"></p><ul><li>snapshotState(): 获取函数的当前状态。状态必须返回此函数先前所有的调用结果。</li><li>restoreState(): 将函数或算子的状态恢复到先前 checkpoint 的状态。此方法在故障恢复后执行函数时调用。如果函数的特定并行实例无法恢复到任何状态，则状态列表可能为空。</li></ul><h3 id="Stateful-Source-Functions"><a href="#Stateful-Source-Functions" class="headerlink" title="Stateful Source Functions"></a>Stateful Source Functions</h3><p>与其他算子相比，有状态的 source 函数需要注意的地方更多，比如为了保证状态的更新和结果的输出原子性，用户必须在 source 的 context 上加锁。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Long</span>&gt; <span class="keyword">implements</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次语义的当前偏移量</span></span><br><span class="line">    <span class="keyword">private</span> Long offset = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//作业取消标志</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Object lock = ctx.getCheckpointLock();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            <span class="comment">//输出和状态更新是原子性的</span></span><br><span class="line">            <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">                ctx.collect(offset);</span><br><span class="line">                offset += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> checkpointTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(offset);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;Long&gt; state)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Long s : state)</span><br><span class="line">            offset = s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或许有些算子想知道什么时候 checkpoint 全部做完了，可以参考使用 org.apache.flink.runtime.state.CheckpointListener 接口来实现，在该接口里面有 notifyCheckpointComplete 方法。</p><h3 id="Broadcast-State"><a href="#Broadcast-State" class="headerlink" title="Broadcast State"></a>Broadcast State</h3><h4 id="Broadcast-State-如何使用"><a href="#Broadcast-State-如何使用" class="headerlink" title="Broadcast State 如何使用"></a>Broadcast State 如何使用</h4><p>前面提到了两种 Operator state 支持的动态扩展方法：even-split redistribution 和 union redistribution。Broadcast State 是 Flink 支持的另一种扩展方式，它用来支持将某一个流的数据广播到下游所有的 Task 中，数据都会存储在下游 Task 内存中，接收到广播的数据流后就可以在操作中利用这些数据，一般我们会将一些规则数据进行这样广播下去，然后其他的 Task 也都能根据这些规则数据做配置，更常见的就是规则动态的更新，然后下游还能够动态的感知。</p><p>Broadcast state 的特点是：</p><ul><li>使用 Map 类型的数据结构</li><li>仅适用于同时具有广播流和非广播流作为数据输入的特定算子</li><li>可以具有多个不同名称的 Broadcast state</li></ul><p>那么我们该如何使用 Broadcast State 呢？下面通过一个例子来讲解一下，在这个例子中，我要广播的数据是监控告警的通知策略规则，然后下游拿到我这个告警通知策略去判断哪种类型的告警发到哪里去，该使用哪种方式来发，静默时间多长等。</p><p>第一个数据流是要处理的数据源，流中的对象具有告警或者恢复的事件，其中用一个 type 字段来标识哪个事件是告警，哪个事件是恢复，然后还有其他的字段标明是哪个集群的或者哪个项目的，简单代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;AlertEvent&gt; alertData = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">"alert"</span>,</span><br><span class="line">        <span class="keyword">new</span> AlertEventSchema(),</span><br><span class="line">        parameterTool.getProperties()));</span><br></pre></td></tr></table></figure><p>然后第二个数据流是要广播的数据流，它是告警通知策略数据（定时从 MySQL 中读取的规则表），简单代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Rule&gt; alarmdata = env.addSource(<span class="keyword">new</span> GetAlarmNotifyData());</span><br><span class="line"></span><br><span class="line"><span class="comment">// MapState 中保存 (RuleName, Rule) ，在描述类中指定 State name</span></span><br><span class="line">MapStateDescriptor&lt;String, Rule&gt; ruleStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">            <span class="string">"RulesBroadcastState"</span>,</span><br><span class="line">            BasicTypeInfo.STRING_TYPE_INFO,</span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Rule&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line"><span class="comment">// alarmdata 使用 MapStateDescriptor 作为参数广播，得到广播流</span></span><br><span class="line">BroadcastStream&lt;Rule&gt; ruleBroadcastStream = alarmdata.broadcast(ruleStateDescriptor);</span><br></pre></td></tr></table></figure><p>然后你要做的是将两个数据流进行连接，连接后再根据告警规则数据流的规则数据进行处理（这个告警的逻辑很复杂，我们这里就不再深入讲），伪代码大概如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alertData.connect(ruleBroadcastStream)</span><br><span class="line">    .process(</span><br><span class="line">        <span class="keyword">new</span> KeyedBroadcastProcessFunction&lt;AlertEvent, Rule&gt;() &#123;</span><br><span class="line">            <span class="comment">//根据告警规则的数据进行处理告警事件</span></span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//可能还有更多的操作</span></span><br></pre></td></tr></table></figure><p><code>alertData.connect(ruleBroadcastStream)</code> 该 connect 方法将两个流连接起来后返回一个 BroadcastConnectedStream 对象，如果对 BroadcastConnectedStream 不太清楚的可以回看下文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db6a754f6a6211cb9616526" target="_blank" rel="noopener">4如何使用 DataStream API 来处理数据？</a> 再次复习一下。BroadcastConnectedStream 调用 process() 方法执行处理逻辑，需要指定一个逻辑实现类作为参数，具体是哪种实现类取决于非广播流的类型：</p><ul><li>如果非广播流是 keyed stream，需要实现 KeyedBroadcastProcessFunction</li><li>如果非广播流是 non-keyed stream，需要实现 BroadcastProcessFunction</li></ul><p>那么该怎么获取这个 Broadcast state 呢，它需要通过上下文来获取:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctx.getBroadcastState(ruleStateDescriptor)</span><br></pre></td></tr></table></figure><h4 id="BroadcastProcessFunction-和-KeyedBroadcastProcessFunction"><a href="#BroadcastProcessFunction-和-KeyedBroadcastProcessFunction" class="headerlink" title="BroadcastProcessFunction 和 KeyedBroadcastProcessFunction"></a>BroadcastProcessFunction 和 KeyedBroadcastProcessFunction</h4><p>这两个抽象函数有两个相同的需要实现的接口:</p><ul><li>processBroadcastElement()：处理广播流中接收的数据元</li><li>processElement()：处理非广播流数据的方法</li></ul><p>用于处理非广播流是 non-keyed stream 的情况:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastProcessFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">BaseBroadcastProcessFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用于处理非广播流是 keyed stream 的情况</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedBroadcastProcessFunction</span>&lt;<span class="title">KS</span>, <span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这两个接口提供的上下文对象有所不同。非广播方（processElement）使用 ReadOnlyContext，而广播方（processBroadcastElement）使用 Context。这两个上下文对象（简称 ctx）通用的方法接口有：</p><ul><li>访问 Broadcast state：ctx.getBroadcastState(MapStateDescriptor stateDescriptor)</li><li>查询数据元的时间戳：ctx.timestamp()</li><li>获取当前水印：ctx.currentWatermark()</li><li>获取当前处理时间：ctx.currentProcessingTime()</li><li>向旁侧输出（side-outputs）发送数据：ctx.output(OutputTag outputTag, X value)</li></ul><p>这两者不同之处在于对 Broadcast state 的访问限制：广播方对其具有读和写的权限（read-write），非广播方只有读的权限（read-only），为什么要这么设计呢，主要是为了保证 Broadcast state 在算子的所有并行实例中是相同的。由于 Flink 中没有跨任务的通信机制，在一个任务实例中的修改不能在并行任务间传递，而广播端在所有并行任务中都能看到相同的数据元，只对广播端提供可写的权限。同时要求在广播端的每个并行任务中，对接收数据的处理是相同的。如果忽略此规则会破坏 State 的一致性保证，从而导致不一致且难以诊断的结果。也就是说，processBroadcast() 的实现逻辑必须在所有并行实例中具有相同的确定性行为。</p><h4 id="使用-Broadcast-state-需要注意"><a href="#使用-Broadcast-state-需要注意" class="headerlink" title="使用 Broadcast state 需要注意"></a>使用 Broadcast state 需要注意</h4><p>前面介绍了 Broadcast state，并将 BroadcastProcessFunction 和 KeyedBroadcastProcessFunction 做了个对比，那么接下来强调一下使用 Broadcast state 时需要注意的事项：</p><ul><li>没有跨任务的通信，这就是为什么只有广播方可以修改 Broadcast state 的原因。</li><li>用户必须确保所有任务以相同的方式为每个传入的数据元更新 Broadcast state，否则可能导致结果不一致。</li><li>跨任务的 Broadcast state 中的事件顺序可能不同，虽然广播的元素可以保证所有元素都将转到所有下游任务，但元素到达的顺序可能不一致。因此，Broadcast state 更新不能依赖于传入事件的顺序。</li><li>所有任务都会把 Broadcast state 存入 checkpoint，虽然 checkpoint 发生时所有任务都具有相同的 Broadcast state。这是为了避免在恢复期间所有任务从同一文件中进行恢复（避免热点），然而代价是 state 在 checkpoint 时的大小成倍数（并行度数量）增加。</li><li>Flink 确保在恢复或改变并行度时不会有重复数据，也不会丢失数据。在具有相同或改小并行度后恢复的情况下，每个任务读取其状态 checkpoint。在并行度增大时，原先的每个任务都会读取自己的状态，新增的任务以循环方式读取前面任务的检查点。</li><li>不支持 RocksDB state backend，Broadcast state 在运行时保存在内存中。</li></ul><h3 id="Queryable-State"><a href="#Queryable-State" class="headerlink" title="Queryable State"></a>Queryable State</h3><p>Queryable State，顾名思义，就是可查询的状态。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w2cze3oj20pl0e1jrv.jpg" alt="undefined"></p><p>传统管理这些状态的方式是通过将计算后的状态结果存储在第三方 KV 存储中，然后由第三方应用去获取这些 KV 状态，但是在 Flink 种，现在有了 Queryable State，意味着允许用户对流的内部状态进行实时查询。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w2kwwogj20mn0bvjrw.jpg" alt="undefined"></p><p>那么就不再像其他流计算框架，需要将结果存储到其他外部存储系统才能够被查询到，这样我们就可以不再需要等待状态写入外部存储（这块可能是其他系统的主要瓶颈之一），甚至可以做到无需任何数据库就可以让用户直接查询到数据，这使得数据获取到的时间会更短，更及时，如果你有这块的需求（需要将某些状态数据进行展示，比如数字大屏），那么就强烈推荐使用 Queryable State。目前可查询的 state 主要针对可分区的 state，如 keyed state 等。</p><p>在 Flink 源码中，为此还专门有一个 module 来讲 Queryable State 呢！</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3663xoj216a08qt8v.jpg" alt="undefined"></p><p>那么我们该如何使用 Queryable State 呢？有如下两种方式 ：</p><ul><li>QueryableStateStream, 将 KeyedStream 转换为 QueryableStateStream，类似于 Sink，后续不能进行任何转换操作</li><li>StateDescriptor#setQueryable(String queryableStateName)，将 Keyed State 设置为可查询的 （不支持 Operator State）</li></ul><p>外部应用在查询 Flink 应用程序内部状态的时候要使用 QueryableStateClient, 提交异步查询请求来获取状态。如何使状态可查询呢，假如已经创建了一个状态可查询的 Job，并通过 JobClient 提交 Job，那么它在 Flink 内部的具体实现如下图（图片来自 <a href="http://vishnuviswanath.com/flink_queryable_state1.html" target="_blank" rel="noopener">Queryable States in ApacheFlink - How it works</a>）所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3h8lofj212w0x776i.jpg" alt="undefined"></p><p>上面讲解了让 State 可查询的原理，如果要在 Flink 集群中使用的话，首先得将 Flink 安装目录下 opt 里面的 <code>flink-queryable-state-runtime_2.11-1.9.0.jar</code> 复制到 lib 目录下，默认 lib 目录是不包含这个 jar 的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3pegglj21tg1aaqbx.jpg" alt="undefined"></p><p>然后你可以像下面这样操作让状态可查询：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reducing state</span></span><br><span class="line">ReducingStateDescriptor&lt;Tuple2&lt;Integer, Long&gt;&gt; reducingState = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">"zhisheng"</span>,</span><br><span class="line">        <span class="keyword">new</span> SumReduce(),</span><br><span class="line">        source.getType());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> String queryName = <span class="string">"zhisheng"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> QueryableStateStream&lt;Integer, Tuple2&lt;Integer, Long&gt;&gt; queryableState =</span><br><span class="line">        dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Integer, Long&gt;, Integer&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4126824763829132959L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Tuple2&lt;Integer, Long&gt; value)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).asQueryableState(queryName, reducingState);</span><br></pre></td></tr></table></figure><p>除了上面的 Reducing，你还可以使用 ValueState、FoldingState，还可以直接通过asQueryableState(queryName），注意不支持 ListState，调用 asQueryableState 方法后会返回 QueryableStateStream，接着无需再做其他操作。</p><p>那么用户如果定义了 Queryable State 的话，该怎么来查询对应的状态呢？下面来看看具体逻辑：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w44qz7bj212w0x7wgo.jpg" alt="undefined"></p><p>简单来说，当用户在 Job 中定义了 queryable state 之后，就可以在外部通过QueryableStateClient 来查询对应的状态实时值，你可以创建如下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 Queryable State Client</span></span><br><span class="line">QueryableStateClient client = <span class="keyword">new</span> QueryableStateClient(host, port);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">QueryableStateClient</span><span class="params">(<span class="keyword">final</span> InetAddress remoteAddress, <span class="keyword">final</span> <span class="keyword">int</span> remotePort)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.client = <span class="keyword">new</span> Client&lt;&gt;(</span><br><span class="line">            <span class="string">"Queryable State Client"</span>, <span class="number">1</span>,</span><br><span class="line">            messageSerializer, <span class="keyword">new</span> DisabledKvStateRequestStats());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 QueryableStateClient 中有几个不同参数的 getKvState 方法，参数可有 JobID、queryableStateName、key、namespace、keyTypeInfo、namespaceTypeInfo、StateDescriptor，其实内部最后调用的是一个私有的 getKvState 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;KvStateResponse&gt; <span class="title">getKvState</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobID jobId, <span class="keyword">final</span> String queryableStateName,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">int</span> keyHashCode, <span class="keyword">final</span> <span class="keyword">byte</span>[] serializedKeyAndNamespace)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//构造 KV state 查询的请求</span></span><br><span class="line">    KvStateRequest request = <span class="keyword">new</span> KvStateRequest(jobId, queryableStateName, keyHashCode, serializedKeyAndNamespace);</span><br><span class="line">    <span class="comment">//这个 client 是在构造 QueryableStateClient 中赋值的，这个 client 是 Client&lt;KvStateRequest, KvStateResponse&gt;，发送请求后会返回 CompletableFuture&lt;KvStateResponse&gt;</span></span><br><span class="line">    <span class="keyword">return</span> client.sendRequest(remoteAddress, request);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 Flink 源码中专门有一个 QueryableStateOptions 类来设置可查询状态相关的配置，有如下这些配置。</p><p>服务器端：</p><ul><li>queryable-state.proxy.ports：可查询状态代理的服务器端口范围的配置参数，默认是 9069</li><li>queryable-state.proxy.network-threads：客户端代理的网络线程数，默认是 0</li><li>queryable-state.proxy.query-threads：客户端代理的异步查询线程数，默认是 0</li><li>queryable-state.server.ports：可查询状态服务器的端口范围，默认是 9067</li><li>queryable-state.server.network-threads：KvState 服务器的网络线程数</li><li>queryable-state.server.query-threads：KvStateServerHandler 的异步查询线程数</li><li>queryable-state.enable：是否启用可查询状态代理和服务器</li></ul><p>客户端：</p><ul><li>queryable-state.client.network-threads：KvState 客户端的网络线程数</li></ul><p><strong>注意</strong>：</p><p>可查询状态的生命周期受限于 Job 的生命周期，例如，任务在启动时注册可查询状态，在清理的时候会注销它。在未来的版本中，可能会将其解耦，以便在任务完成后仍可以允许查询到任务的状态。</p><h3 id="小结与反思-18"><a href="#小结与反思-18" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节一开始讲解了 State 出现的原因，接着讲解了 Flink 中的 State 分类，然后对 Flink 中的每种 State 做了详细的讲解，希望可以好好消化这节的内容。你对本节的内容有什么不理解的地方吗？在使用 State 的过程中有遇到什么问题吗？</p><h2 id="二十一、如何选择-Flink-状态后端存储"><a href="#二十一、如何选择-Flink-状态后端存储" class="headerlink" title="二十一、如何选择 Flink 状态后端存储?"></a>二十一、如何选择 Flink 状态后端存储?</h2><h3 id="State-Backends"><a href="#State-Backends" class="headerlink" title="State Backends"></a>State Backends</h3><p>当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，刚好 Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外，在 Flink 安装路径下 conf 目录中的 flink-conf.yaml 配置文件中也有状态后端存储相关的配置，为此在 Flink 源码中还特有一个 CheckpointingOptions 类来控制 state 存储的相关配置，该类中有如下配置：</p><ul><li>state.backend: 用于存储和进行状态 checkpoint 的状态后端存储方式，无默认值</li><li>state.checkpoints.num-retained: 要保留的已完成 checkpoint 的最大数量，默认值为 1</li><li>state.backend.async: 状态后端是否使用异步快照方法，默认值为 true</li><li>state.backend.incremental: 状态后端是否创建增量检查点，默认值为 false</li><li>state.backend.local-recovery: 状态后端配置本地恢复，默认情况下，本地恢复被禁用</li><li>taskmanager.state.local.root-dirs: 定义存储本地恢复的基于文件的状态的目录</li><li>state.savepoints.dir: 存储 savepoints 的目录</li><li>state.checkpoints.dir: 存储 checkpoint 的数据文件和元数据</li><li>state.backend.fs.memory-threshold: 状态数据文件的最小大小，默认值是 1024</li></ul><p>虽然配置这么多，但是，Flink 还支持基于每个 Job 单独设置状态后端存储，方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> MemoryStateBackend());  <span class="comment">//设置堆内存存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));   //设置文件存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22tyfs6dj212w0dwaa2.jpg" alt="undefined"></p><p>上面三种方式取一种就好了。但是有三种方式，我们该如何去挑选用哪种去存储状态呢？下面讲讲这三种的特点以及该如何选择。</p><h3 id="如何使用-MemoryStateBackend-及剖析"><a href="#如何使用-MemoryStateBackend-及剖析" class="headerlink" title="如何使用 MemoryStateBackend 及剖析"></a>如何使用 MemoryStateBackend 及剖析</h3><p>如果 Job 没有配置指定状态后端存储的话，就会默认采取 MemoryStateBackend 策略。如果你细心的话，可以从你的 Job 中看到类似日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-04-28 00:16:41.892 [Sink: zhisheng (1/4)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask  - No state backend has been configured, using default (Memory / Job Manager) MemoryStateBackend (data in heap memory / checkpoints to Job Manager) (checkpoints: &apos;null&apos;, savepoints: &apos;null&apos;, asynchronous: TRUE, maxStateSize: 5242880)</span><br></pre></td></tr></table></figure><p>上面日志的意思就是说如果没有配置任何状态存储，使用默认的 MemoryStateBackend 策略，这种状态后端存储把数据以内部对象的形式保存在 Task Managers 的内存（JVM 堆）中，当应用程序触发 checkpoint 时，会将此时的状态进行快照然后存储在 Job Manager 的内存中。因为状态是存储在内存中的，所以这种情况会有点限制，比如：</p><ul><li>不太适合在生产环境中使用，仅用于本地测试的情况较多，主要适用于状态很小的 Job，因为它会将状态最终存储在 Job Manager 中，如果状态较大的话，那么会使得 Job Manager 的内存比较紧张，从而导致 Job Manager 会出现 OOM 等问题，然后造成连锁反应使所有的 Job 都挂掉，所以 Job 的状态与之前的 Checkpoint 的数据所占的内存要小于 JobManager 的内存。</li><li>每个单独的状态大小不能超过最大的 DEFAULT<em>MAX</em>STATE_SIZE(5MB)，可以通过构造 MemoryStateBackend 参数传入不同大小的 maxStateSize。</li><li>Job 的操作符状态和 keyed 状态加起来都不要超过 RPC 系统的默认配置 10 MB，虽然可以修改该配置，但是不建议去修改。</li></ul><p>另外就是 MemoryStateBackend 支持配置是否是异步快照还是同步快照，它有一个字段 asynchronousSnapshots 来表示，可选值有：</p><ul><li>TRUE（true 代表使用异步的快照，这样可以避免因快照而导致数据流处理出现阻塞等问题）</li><li>FALSE（同步）</li><li>UNDEFINED（默认值）</li></ul><p>在构造 MemoryStateBackend 的默认函数时是使用的 UNDEFINED，而不是异步：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//使用的是 UNDEFINED</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>网上有人说默认是异步的，这里给大家解释清楚一下，从上面的那条日志打印的确实也是表示异步，但是前提是你对 State 无任何操作，我跟了下源码，当你没有配置任何的 state 时，它是会在 StateBackendLoader 类中通过 MemoryStateBackendFactory 来创建的 state 的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22u88wmij21k60z0jsl.jpg" alt="undefined"></p><p>继续跟进 MemoryStateBackendFactory 可以发现他这里创建了一个 MemoryStateBackend 实例并通过 configure 方法进行配置，大概流程代码是：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//MemoryStateBackendFactory 类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">createFromConfig</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend().configure(config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//MemoryStateBackend 类中的 config 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">configure</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend(<span class="keyword">this</span>, config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//私有的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">MemoryStateBackend</span><span class="params">(MemoryStateBackend original, Configuration configuration, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.asynchronousSnapshots = original.asynchronousSnapshots.resolveUndefined(</span><br><span class="line">            configuration.getBoolean(CheckpointingOptions.ASYNC_SNAPSHOTS));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据 CheckpointingOptions 类中的 ASYNC_SNAPSHOTS 参数进行设置的</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ConfigOption&lt;Boolean&gt; ASYNC_SNAPSHOTS = ConfigOptions</span><br><span class="line">        .key(<span class="string">"state.backend.async"</span>)</span><br><span class="line">        .defaultValue(<span class="keyword">true</span>) <span class="comment">//默认值就是 true，代表异步</span></span><br><span class="line">        .withDescription(...)</span><br></pre></td></tr></table></figure><p>可以发现最终是通过读取 <code>state.backend.async</code> 参数的默认值（true）来配置是否要异步的进行快照，但是如果你手动配置 MemoryStateBackend 的话，利用无参数的构造方法，那么就不是默认异步，如果想使用异步的话，需要利用下面这个构造函数（需要传入一个 boolean 值，true 代表异步，false 代表同步）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(<span class="keyword">boolean</span> asynchronousSnapshots)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.fromBoolean(asynchronousSnapshots));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果你再细看了这个 MemoryStateBackend 类的话，那么你可能会发现这个构造函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(@Nullable String checkpointPath, @Nullable String savepointPath)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(checkpointPath, savepointPath, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//需要你传入 checkpointPath 和 savepointPath</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个也是用来创建一个 MemoryStateBackend 的，它需要传入的参数是两个路径（checkpointPath、savepointPath），其中 checkpointPath 是写入 checkpoint 元数据的路径，savepointPath 是写入 savepoint 的路径。</p><p>这个来看看 MemoryStateBackend 的继承关系图可以更明确的知道它是继承自 AbstractFileStateBackend，然后 AbstractFileStateBackend 这个抽象类就是为了能够将状态存储中的数据或者元数据进行文件存储的。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22uh2q3hj21120luweq.jpg" alt="undefined"></p><p>所以 FsStateBackend 和 MemoryStateBackend 都会继承该类。</p><h3 id="如何使用-FsStateBackend-及剖析"><a href="#如何使用-FsStateBackend-及剖析" class="headerlink" title="如何使用 FsStateBackend 及剖析"></a>如何使用 FsStateBackend 及剖析</h3><p>这种状态后端存储也是将工作状态存储在 Task Manager 中的内存（JVM 堆）中，但是 checkpoint 的时候，它和 MemoryStateBackend 不一样，它是将状态存储在文件（可以是本地文件，也可以是 HDFS）中，这个文件具体是哪种需要配置，比如：”hdfs://namenode:40010/flink/checkpoints” 或 “file://flink/checkpoints” (通常使用 HDFS 比较多，如果是使用本地文件，可能会造成 Job 恢复的时候找不到之前的 checkkpoint，因为 Job 重启后如果由调度器重新分配在不同的机器的 Task Manager 执行时就会导致这个问题，所以还是建议使用 HDFS 或者其他的分布式文件系统)。</p><p>同样 FsStateBackend 也是支持通过 asynchronousSnapshots 字段来控制是使用异步还是同步来进行 checkpoint 的，异步可以避免在状态 checkpoint 时阻塞数据流的处理，然后还有一点的就是在 FsStateBackend 有个参数 fileStateThreshold，如果状态大小比 MAX<em>FILE</em>STATE_THRESHOLD（1MB） 小的话，那么会将状态数据直接存储在 meta data 文件中，而不是存储在配置的文件中（避免出现很小的状态文件），如果该值为 “-1” 表示尚未配置，在这种情况下会使用默认值（1024，该默认值可以通过 <code>state.backend.fs.memory-threshold</code> 来配置）。</p><p>那么我们该什么时候使用 FsStateBackend 呢？</p><ul><li>如果你要处理大状态，长窗口等有状态的任务，那么 FsStateBackend 就比较适合</li><li>使用分布式文件系统，如 HDFS 等，这样 failover 时 Job 的状态可以恢复</li></ul><p>使用 FsStateBackend 需要注意的地方有什么呢？</p><ul><li>工作状态仍然是存储在 Task Manager 中的内存中，虽然在 Checkpoint 的时候会存在文件中，所以还是得注意这个状态要保证不超过 Task Manager 的内存</li></ul><h3 id="如何使用-RocksDBStateBackend-及剖析"><a href="#如何使用-RocksDBStateBackend-及剖析" class="headerlink" title="如何使用 RocksDBStateBackend 及剖析"></a>如何使用 RocksDBStateBackend 及剖析</h3><p>RocksDBStateBackend 和上面两种都有点不一样，RocksDB 是一种嵌入式的本地数据库，它会在本地文件系统中维护状态，KeyedStateBackend 等会直接写入本地 RocksDB 中，它还需要配置一个文件系统（一般是 HDFS），比如 <code>hdfs://namenode:40010/flink/checkpoints</code>，当触发 checkpoint 的时候，会把整个 RocksDB 数据库复制到配置的文件系统中去，当 failover 时从文件系统中将数据恢复到本地。</p><p>在 Flink 源码中，你也可以看见专门有一个 module 是 flink-statebackend-rocksdb 来放在 flink-state-backends 下面，在后面的版本中可能还会加上 flink-statebackend-heap-spillable 模块用来当作一种新的状态后端存储，感兴趣可以去官网的计划中查看。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22urjlpjj213y0jet9o.jpg" alt="undefined"></p><p>足以证明了官方其实也是推荐使用 RocksDB 来作为状态的后端存储，为什么呢：</p><ul><li>state 直接存放在 RocksDB 中，不需要存在内存中，这样就可以减少 Task Manager 的内存压力，如果是存内存的话大状态的情况下会导致 GC 次数比较多，同时还能在 checkpoint 时将状态持久化到远端的文件系统，那么就比较适合在生产环境中使用</li><li>RocksDB 本身支持 checkpoint 功能</li><li>RocksDBStateBackend 支持增量的 checkpoint，在 RocksDBStateBackend 中有一个字段 enableIncrementalCheckpointing 来确认是否开启增量的 checkpoint，默认是不开启的，在 CheckpointingOptions 类中有个 state.backend.incremental 参数来表示，增量 checkpoint 非常使用于超大状态的场景。</li></ul><p>讲了这么多 RocksDBStateBackend 的好处，那么该如何去使用呢，可以来看看 RocksDBStateBackend 这个类的相关属性以及构造函数。</p><p><strong>属性</strong>：</p><ul><li>checkpointStreamBackend：用于创建 checkpoint 流的状态后端</li><li>localRocksDbDirectories：RocksDB 目录的基本路径，默认是 Task Manager 的临时目录</li><li>enableIncrementalCheckpointing：是否增量 checkpoint</li><li>numberOfTransferingThreads：用于传输(下载和上传)状态的线程数量，默认为 1</li><li>enableTtlCompactionFilter：是否启用压缩过滤器来清除带有 TTL 的状态</li></ul><p><strong>构造函数</strong>：</p><ul><li>RocksDBStateBackend(String checkpointDataUri)：单参数，只传入一个路径</li><li>RocksDBStateBackend(String checkpointDataUri, boolean enableIncrementalCheckpointing)：两个参数，传入 checkpoint 数据目录路径和是否开启增量 checkpoint</li><li>RocksDBStateBackend(StateBackend checkpointStreamBackend)：传入一种 StateBackend</li><li>RocksDBStateBackend(StateBackend checkpointStreamBackend, TernaryBoolean enableIncrementalCheckpointing)：传入一种 StateBackend 和是否开启增量 checkpoint</li><li>RocksDBStateBackend(RocksDBStateBackend original, Configuration config, ClassLoader classLoader)：私有的构造方法，用于重新配置状态后端</li></ul><p>既然知道这么多构造函数了，那么使用就很简单了，根据你的场景考虑使用哪种构造函数创建 RocksDBStateBackend 对象就行了，然后通过 <code>env.setStateBackend()</code> 传入对象实例就行，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p>那么在使用 RocksDBStateBackend 时该注意什么呢：</p><ul><li>当使用 RocksDB 时，状态大小将受限于磁盘可用空间的大小</li><li>状态存储在 RocksDB 中，整个更新和获取状态的操作都是要通过序列化和反序列化才能完成的，跟状态直接存储在内存中，性能可能会略低些</li><li>如果你应用程序的状态很大，那么使用 RocksDB 无非是最佳的选择</li></ul><p>另外在 Flink 源码中有一个专门的 RocksDBOptions 来表示 RocksDB 相关的配置：</p><ul><li>state.backend.rocksdb.localdir：本地目录(在 Task Manager 上)，RocksDB 将其文件放在其中</li><li>state.backend.rocksdb.timer-service.factory：定时器服务实现，默认值是 HEAP</li><li>state.backend.rocksdb.checkpoint.transfer.thread.num：用于在后端传输(下载和上载)文件的线程数，默认是 1</li><li>state.backend.rocksdb.ttl.compaction.filter.enabled：是否启用压缩过滤器来清除带有 TTL 的状态，默认值是 false</li></ul><h3 id="如何选择状态后端存储？"><a href="#如何选择状态后端存储？" class="headerlink" title="如何选择状态后端存储？"></a>如何选择状态后端存储？</h3><p>通过上面三种 State Backends 的介绍，让大家了解了状态存储有哪些种类，然后对每种状态存储是该如何使用的、它们内部的实现、使用场景、需要注意什么都细讲了一遍，三种存储方式各有特点，可以满足不同场景的需求，通常来说，在开发程序之前，我们要先分析自己 Job 的场景和状态大小的预测，然后根据预测来进行选择何种状态存储，如果拿捏不定的话，建议先在测试环境进行测试，只有选择了正确的状态存储后端，这样才能够保证后面自己的 Job 在生产环境能够稳定的运行。</p><h3 id="小结与反思-19"><a href="#小结与反思-19" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节对 Flink 的 State 做了一个很详尽的讲解，不管是从使用方面，还从原理进行深度分析，涉及的有 State 的分类如 Keyed State、Operator State、Raw State、 Managed State、Broadcast State 等。还讲了如何让 State 进行可查询的配置，State 的过期，最后还讲了 State 的三种常见的后端存储方式，并分析了三者适合于哪种场景，同时也都对这几种方式的源码进行解读，目的就是让大家对 State 彻底的了解使用方式和原理实现。</p><p>下面一图来看看 State 在 Flink 中的整体结构：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22v2xtrmj248s16mtc3.jpg" alt="undefined"></p><h2 id="二十二、Flink-Checkpoint-和-Savepoint-区别及其如何配置使用？"><a href="#二十二、Flink-Checkpoint-和-Savepoint-区别及其如何配置使用？" class="headerlink" title="二十二、Flink Checkpoint 和 Savepoint 区别及其如何配置使用？"></a>二十二、Flink Checkpoint 和 Savepoint 区别及其如何配置使用？</h2><p>Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。本节主要讲述在 Flink 中 Checkpoint 和 Savepoint 的使用方式及它们之间的区别。</p><h3 id="Checkpoint-介绍及使用"><a href="#Checkpoint-介绍及使用" class="headerlink" title="Checkpoint 介绍及使用"></a>Checkpoint 介绍及使用</h3><p>为了保障的容错，Flink 需要对状态进行快照。Flink 可以从 Checkpoint 中恢复流的状态和位置，从而使得应用程序发生故障后能够得到与无故障执行相同的语义。</p><p>Flink 的 Checkpoint 有以下先决条件：</p><ul><li>需要具有持久性且支持重放一定时间范围内数据的数据源。例如：Kafka、RabbitMQ 等。这里为什么要求支持重放一定时间范围内的数据呢？因为 Flink 的容错机制决定了，当 Flink 任务失败后会自动从最近一次成功的 Checkpoint 处恢复任务，此时可能需要把任务失败前消费的部分数据再消费一遍，所以必须要求数据源支持重放。假如一个Flink 任务消费 Kafka 并将数据写入到 MySQL 中，任务从 Kafka 读取到数据，还未将数据输出到 MySQL 时任务突然失败了，此时如果 Kafka 不支持重放，就会造成这部分数据永远丢失了。支持重放数据的数据源可以保障任务消费失败后，能够重新消费来保障任务不丢数据。</li><li>需要一个能保存状态的持久化存储介质，例如：HDFS、S3 等。当 Flink 任务失败后，自动从 Checkpoint 处恢复，但是如果 Checkpoint 时保存的状态信息快照全丢了，那就会影响 Flink 任务的正常恢复。就好比我们看书时经常使用书签来记录当前看到的页码，当下次看书时找到书签的位置继续阅读即可，但是如果书签三天两头经常丢，那我们就无法通过书签来恢复阅读。</li></ul><p>Flink 中 Checkpoint 是默认关闭的，对于需要保障 At Least Once 和 Exactly Once 语义的任务，强烈建议开启 Checkpoint，对于丢一小部分数据不敏感的任务，可以不开启 Checkpoint，例如：一些推荐相关的任务丢一小部分数据并不会影响推荐效果。下面来介绍 Checkpoint 具体如何使用。</p><p>首先调用 StreamExecutionEnvironment 的方法 enableCheckpointing(n) 来开启 Checkpoint，参数 n 以毫秒为单位表示 Checkpoint 的时间间隔。Checkpoint 配置相关的 Java 代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 开启 Checkpoint，每 1000毫秒进行一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 语义设置为 EXACTLY_ONCE</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckPoint 的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同一时间，只允许 有 1 个 Checkpoint 在发生</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 两次 Checkpoint 之间的最小时间间隔为 500 毫秒</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当 Flink 任务取消时，保留外部保存的 CheckPoint 信息</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 作业最多允许 Checkpoint 失败 1 次（flink 1.9 开始支持）</span></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 失败后，整个 Flink 任务也会失败（flink 1.9 之前）</span></span><br><span class="line">env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(<span class="keyword">true</span>)</span><br></pre></td></tr></table></figure><p>以上 Checkpoint 相关的参数描述如下所示：</p><ul><li>Checkpoint 语义：EXACTLY<em>ONCE 或 AT</em>LEAST<em>ONCE，EXACTLY</em>ONCE 表示所有要消费的数据被恰好处理一次，即所有数据既不丢数据也不重复消费；AT<em>LEAST</em>ONCE 表示要消费的数据至少处理一次，可能会重复消费。</li><li>Checkpoint 超时时间：如果 Checkpoint 时间超过了设定的超时时间，则 Checkpoint 将会被终止。</li><li>同时进行的 Checkpoint 数量：默认情况下，当一个 Checkpoint 在进行时，JobManager 将不会触发下一个 Checkpoint，但 Flink 允许多个 Checkpoint 同时在发生。</li><li>两次 Checkpoint 之间的最小时间间隔：从上一次 Checkpoint 结束到下一次 Checkpoint 开始，中间的间隔时间。例如，env.enableCheckpointing(60000) 表示 1 分钟触发一次 Checkpoint，同时再设置两次 Checkpoint 之间的最小时间间隔为 30 秒，假如任务运行过程中一次 Checkpoint 就用了50s，那么等 Checkpoint 结束后，理论来讲再过 10s 就要开始下一次 Checkpoint 了，但是由于设置了最小时间间隔为30s，所以需要再过 30s 后，下次 Checkpoint 才开始。注：如果配置了该参数就决定了同时进行的 Checkpoint 数量只能为 1。</li><li>当任务被取消时，外部 Checkpoint 信息是否被清理：Checkpoint 在默认的情况下仅用于恢复运行失败的 Flink 任务，当任务手动取消时 Checkpoint 产生的状态信息并不保留。当然可以通过该配置来保留外部的 Checkpoint 状态信息，这些被保留的状态信息在作业手动取消时不会被清除，这样就可以使用该状态信息来恢复 Flink 任务，对于需要从状态恢复的任务强烈建议配置为外部 Checkpoint 状态信息不清理。可选择的配置项为：</li><li>ExternalizedCheckpointCleanup.RETAIN<em>ON</em>CANCELLATION：当作业手动取消时，保留作业的 Checkpoint 状态信息。注意，这种情况下，需要手动清除该作业保留的 Checkpoint 状态信息，否则这些状态信息将永远保留在外部的持久化存储中。</li><li>ExternalizedCheckpointCleanup.DELETE<em>ON</em>CANCELLATION：当作业取消时，Checkpoint 状态信息会被删除。仅当作业失败时，作业的 Checkpoint 才会被保留用于任务恢复。</li><li>任务失败，当有较新的 Savepoint 时，作业是否回退到 Checkpoint 进行恢复：默认情况下，当 Savepoint 比 Checkpoint 较新时，任务会从 Savepoint 处恢复。</li><li>作业可以容忍 Checkpoint 失败的次数：默认值为 0，表示不能接受 Checkpoint 失败。</li></ul><p>关于 Checkpoint 时，状态后端相关的配置请参阅本课 4.2 节。</p><h3 id="Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用"><a href="#Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用" class="headerlink" title="Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用"></a>Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用</h3><p>Savepoint 与 Checkpoint 类似，同样需要把状态信息存储到外部介质，当作业失败时，可以从外部存储中恢复。Savepoint 与 Checkpoint 的区别很多：</p><table><thead><tr><th style="text-align:center">Checkpoint</th><th style="text-align:center">Savepoint</th></tr></thead><tbody><tr><td style="text-align:center">由 Flink 的 JobManager 定时自动触发并管理</td><td style="text-align:center">由用户手动触发并管理</td></tr><tr><td style="text-align:center">主要用于任务发生故障时，为任务提供给自动恢复机制</td><td style="text-align:center">主要用户升级 Flink 版本、修改任务的逻辑代码、调整算子的并行度，且必须手动恢复</td></tr><tr><td style="text-align:center">当使用 RocksDBStateBackend 时，支持增量方式对状态信息进行快照</td><td style="text-align:center">仅支持全量快照</td></tr><tr><td style="text-align:center">Flink 任务停止后，Checkpoint 的状态快照信息默认被清除</td><td style="text-align:center">一旦触发 Savepoint，状态信息就被持久化到外部存储，除非用户手动删除</td></tr><tr><td style="text-align:center">Checkpoint 设计目标：轻量级且尽可能快地恢复任务</td><td style="text-align:center">Savepoint 的生成和恢复成本会更高一些，Savepoint 更多地关注代码的可移植性和兼容任务的更改操作</td></tr></tbody></table><p>除了上述描述外，Checkpoint 和 Savepoint 在当前的实现上基本相同。</p><p>强烈建议在程序中给算子分配 Operator ID，以便来升级程序。主要通过 <code>uid(String)</code> 方法手动指定算子的 ID ，这些 ID 将用于恢复每个算子的状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.</span><br><span class="line">  <span class="comment">// Stateful source (e.g. Kafka) with ID</span></span><br><span class="line">  .addSource(<span class="keyword">new</span> StatefulSource())</span><br><span class="line">  .uid(<span class="string">"source-id"</span>) <span class="comment">// ID for the source operator</span></span><br><span class="line">  .shuffle()</span><br><span class="line">  <span class="comment">// Stateful mapper with ID</span></span><br><span class="line">  .map(<span class="keyword">new</span> StatefulMapper())</span><br><span class="line">  .uid(<span class="string">"mapper-id"</span>) <span class="comment">// ID for the mapper</span></span><br><span class="line">  <span class="comment">// Stateless printing sink</span></span><br><span class="line">  .print(); <span class="comment">// Auto-generated ID</span></span><br></pre></td></tr></table></figure><p>如果不为算子手动指定 ID，Flink 会为算子自动生成 ID。当 Flink 任务从 Savepoint 中恢复时，是按照 Operator ID 将快照信息与算子进行匹配的，只要这些 ID 不变，Flink 任务就可以从 Savepoint 中恢复。自动生成的 ID 取决于代码的结构，并且对代码更改比较敏感，因此强烈建议给程序中所有有状态的算子手动分配 Operator ID。如下左图所示，一个 Flink 任务包含了 算子 A 和 算子 B，代码中都未指定 Operator ID，所以 Flink 为 Task A 自动生成了 Operator ID 为 aaa，为 Task B 自动生成了 Operator ID 为 bbb，且 Savepoint 成功完成。但是在代码改动后，任务并不能从 Savepoint 中正常恢复，因为 Flink 为算子生成的 Operator ID 取决于代码结构，代码改动后可能会把算子 B 的 Operator ID 改变成 ccc，导致任务从 Savepoint 恢复时，SavePoint 中只有 Operator ID 为 aaa 和 bbb 的状态信息，算子 B 找不到 Operator ID 为 ccc 的状态信息，所以算子 B 不能正常恢复。这里如果在写代码时通过 <code>uid(String)</code> 手动指定了 Operator ID，就不会存在 上述问题了。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga234u9p5rj25eg1oyhaw.jpg" alt="undefined"></p><p>Savepoint 需要用户手动去触发，触发 Savepoint 的方式如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径，用户需要此路径来还原和删除 Savepoint 。</p><p>使用 YARN 触发 Savepoint 的方式如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 和 YARN 应用程序 ID <code>:yarnAppId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径。</p><p>使用 Savepoint 取消 Flink 任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink cancel -s [:targetDirectory] :jobId</span><br></pre></td></tr></table></figure><p>这将自动触发 ID 为 <code>:jobid</code> 的作业进行 Savepoint，并在 Checkpoint 结束后取消该任务。此外，可以指定一个目标文件系统目录来存储 Savepoint 的状态信息，也可以在 flink 的 conf 目录下 flink-conf.yaml 中配置 state.savepoints.dir 参数来指定 Savepoint 的默认目录，触发 Savepoint 时，如果不指定目录则使用该默认目录。无论使用哪种方式配置，都需要保障配置的目录能被所有的 JobManager 和 TaskManager 访问。</p><h3 id="Checkpoint-流程"><a href="#Checkpoint-流程" class="headerlink" title="Checkpoint 流程"></a>Checkpoint 流程</h3><p>Flink 任务 Checkpoint 的详细流程如下所示：</p><p>\1. JobManager 端的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga2355t4kzj23i52i3du7.jpg" alt="undefined"></p><ol start="2"><li>当 task 收到上游所有实例的 barrier 后，向自己的下游继续传递 barrier，然后自身同步进行快照，并将自己的状态异步写入到持久化存储中</li></ol><ul><li>如果是增量 Checkpoint，则只是把最新的一部分更新写入到外部持久化存储中</li><li>为了下游尽快进行 Checkpoint，所以 task 会先发送 barrier 到下游，自身再同步进行快照</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga235ip3bjj23op2j1qr6.jpg" alt="undefined"></p><blockquote><p>注：Task B 必须接收到上游 Task A 所有实例发送的 barrier 时，Task B 才能开始进行快照，这里有一个 barrier 对齐的概念，关于 barrier 对齐的详细介绍请参阅 9.5.1 节 Flink 内部如何保证 Exactly Once 中的 barrier 对齐部分</p></blockquote><ol start="3"><li><p>当 task 将状态信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的CheckPointCoordinator，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除</p></li><li><p>如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga235uyrs8j23sr3937pe.jpg" alt="undefined"></p><p>如果对上述 Checkpoint 过程不理解，在后续 9.5 节 Flink 如何保障 Exactly Once 中会详细介绍 Flink 的 Checkpoint 过程以及为什么这么做。</p><h4 id="基于-RocksDB-的增量-Checkpoint-实现原理"><a href="#基于-RocksDB-的增量-Checkpoint-实现原理" class="headerlink" title="基于 RocksDB 的增量 Checkpoint 实现原理"></a>基于 RocksDB 的增量 Checkpoint 实现原理</h4><p>当使用 RocksDBStateBackend 时，增量 Checkpoint 是如何实现的呢？RocksDB 是一个基于 LSM 实现的 KV 数据库。LSM 全称 Log Structured Merge Trees，LSM 树本质是将大量的磁盘随机写操作转换成磁盘的批量写操作来极大地提升磁盘数据写入效率。一般 LSM Tree 实现上都会有一个基于内存的 MemTable 介质，所有的增删改操作都是写入到 MemTable 中，当 MemTable 足够大以后，将 MemTable 中的数据 flush 到磁盘中生成不可变且内部有序的 ssTable（Sorted String Table）文件，全量数据保存在磁盘的多个 ssTable 文件中。HBase 也是基于 LSM Tree 实现的，HBase 磁盘上的 HFile 就相当于这里的 ssTable 文件，每次生成的 HFile 都是不可变的而且内部有序的文件。基于 ssTable 不可变的特性，才实现了增量 Checkpoint，具体流程如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga2364ybmqj24lp1jdan6.jpg" alt="undefined"></p><p>第一次 Checkpoint 时生成的状态快照信息包含了两个 sstable 文件：sstable1 和 sstable2 及 Checkpoint1 的元数据文件 MANIFEST-chk1，所以第一次 Checkpoint 时需要将 sstable1、sstable2 和 MANIFEST-chk1 上传到外部持久化存储中。第二次 Checkpoint 时生成的快照信息为 sstable1、sstable2、sstable3 及元数据文件 MANIFEST-chk2，由于 sstable 文件的不可变特性，所以状态快照信息的 sstable1、sstable2 这两个文件并没有发生变化，sstable1、sstable2 这两个文件不需要重复上传到外部持久化存储中，因此第二次 Checkpoint 时，只需要将 sstable3 和 MANIFEST-chk2 文件上传到外部持久化存储中即可。这里只将新增的文件上传到外部持久化存储，也就是所谓的增量 Checkpoint。</p><p>基于 LSM Tree 实现的数据库为了提高查询效率，都需要定期对磁盘上多个 sstable 文件进行合并操作，合并时会将删除的、过期的以及旧版本的数据进行清理，从而降低 sstable 文件的总大小。图中可以看到第三次 Checkpoint 时生成的快照信息为sstable3、sstable4、sstable5 及元数据文件 MANIFEST-chk3， 其中新增了 sstable4 文件且 sstable1 和 sstable2 文件合并成 sstable5 文件，因此第三次 Checkpoint 时只需要向外部持久化存储上传 sstable4、sstable5 及元数据文件 MANIFEST-chk3。</p><p>基于 RocksDB 的增量 Checkpoint 从本质上来讲每次 Checkpoint 时只将本次 Checkpoint 新增的快照信息上传到外部的持久化存储中，依靠的是 LSM Tree 中 sstable 文件不可变的特性。对 LSM Tree 感兴趣的同学可以深入研究 RocksDB 或 HBase 相关原理及实现。</p><h3 id="状态如何从-Checkpoint-恢复"><a href="#状态如何从-Checkpoint-恢复" class="headerlink" title="状态如何从 Checkpoint 恢复"></a>状态如何从 Checkpoint 恢复</h3><p>在 Checkpoint 和 Savepoint 的比较过程中，知道了相比 Savepoint 而言，Checkpoint 的成本更低一些，但有些场景 Checkpoint 并不能完全满足我们的需求。所以在使用过程中，如果我们的需求能使用 Checkpoint 来解决优先使用 Checkpoint。当 Flink 任务中的一些依赖组件需要升级重启时，例如 hdfs、Kafka、yarn 升级或者 Flink 任务的 Sink 端对应的 MySQL、Redis 由于某些原因需要重启时，Flink 任务在这段时间也需要重启。但是由于 Flink 任务的代码并没有修改，所以 Flink 任务启动时可以从 Checkpoint 处恢复任务，此时必须配置取消 Flink 任务时保留外部存储的 Checkpoint 状态信息。从 Checkpoint 处恢复任务的命令如下所示，checkpointMetaDataPath 表示 Checkpoint 的目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果 flink on yarn 模式，启动命令如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>问题来了，Flink 自动维护 Checkpoint，所以用户在这里并拿不到任务取消之前最后一次 Checkpoint 的目录。那怎么办呢？如下图所示，在任务取消之前，Flink 任务的 WebUI 中可以看到 Checkpoint 的目录，可以在取消任务之前将此目录保存起来，恢复时就可以从该目录恢复任务。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga236iyh3zj21p00vwn3t.jpg" alt="undefined"></p><p>上述方法最大缺陷就是用户的人力成本太高了，假如需要重启 100 个任务，难道需要用户手动维护 100 个任务的 Checkpoint 目录吗？可以做一个简单后台项目，用于管理和发布 Flink 任务，这里讲述一种通过 rest api 来获取 Checkpoint 目录的方式。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga236rp2l1j21de0j4mz0.jpg" alt="undefined"></p><p>如上图所示是 Flink JobManager 的 overview 页面，只需要将端口号后面的路径和参数按照以下替换即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node107.bigdata.dmp.local.com:35524/jobs/a1c70b36d19b3a9fc2713ba98cfc4a4f/metrics?get=lastCheckpointExternalPath</span><br></pre></td></tr></table></figure><p>调用以上接口，即可返回 a1c70b36d19b3a9fc2713ba98cfc4a4f 对应的 job 最后一次 Checkpoint 的目录，返回格式如下所示。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"id"</span>: <span class="string">"lastCheckpointExternalPath"</span>,</span><br><span class="line">    <span class="attr">"value"</span>: <span class="string">"hdfs:/user/flink/checkpoints/a1c70b36d19b3a9fc2713ba98cfc4a4f/chk-18"</span></span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>通过这种方式可以方便地维护所有 Flink 任务的 Checkpoint 目录，当然也可以通过 Metrics 的 Reporter 将 Checkpoint 目录保存到外部存储介质中，当任务需要从 Checkpoint 处恢复时，则从外部存储中读取到相应的 Checkpoint 目录。</p><p>当设置取消 Flink 任务保留外部的 Checkpoint 状态信息时，可能会带来的负面影响是：长期运行下去，hdfs 上将会保留很多废弃的且不再会使用的 Checkpoint 目录，所以如果开启了此配置，需要制定策略，定期清理那些不再会使用到的 Checkpoint 目录。</p><h3 id="状态如何从-Savepoint-恢复"><a href="#状态如何从-Savepoint-恢复" class="headerlink" title="状态如何从 Savepoint 恢复"></a>状态如何从 Savepoint 恢复</h3><p>如下所示，从 Savepoint 恢复任务的命令与 Checkpoint 恢复命令类似，savepointPath 表示 Savepoint 保存的目录，Savepoint 的各种触发方式都会返回 Savepoint 目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果 flink on yarn 模式，启动命令如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>默认情况下，恢复操作将尝试将 Savepoint 的所有状态映射到要还原的程序。如果删除了算子，则可以通过 <code>--allowNonRestoredState</code>（short：<code>-n</code>）选项跳过那些无法映射到新程序的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath -n xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果从 Savepoint 恢复时，在任务中添加一个需要状态的新算子，会发生什么？向任务添加新算子时，它将在没有任何状态的情况下进行初始化，Savepoint 中包含每个有状态算子的状态，无状态算子根本不是 Savepoint 的一部分，新算子的行为类似于无状态算子。</p><p>如果在任务中对算子进行重新排序，会发生什么？如果给这些算子分配了 ID，它们将像往常一样恢复。如果没有分配 ID ，则有状态算子自动生成的 ID 很可能在重新排序后发生更改，这将导致无法从之前的 Savepoint 中恢复。</p><p>Savepoint 目录里的状态快照信息，目前不支持移动位置，由于技术原因元数据文件中使用绝对路径来保存数据。如果因为某种原因必须要移动 Savepoint 文件，那么有两种方案来实现：</p><ul><li>使用编辑器修改 Savepoint 的元数据文件信息，将旧路径改为新路径</li><li>可以使用 <code>SavepointV2Serializer</code> 类以编程方式读取、操作和重写元数据文件的新路径</li></ul><p>长期使用 Savepoint 同样要注意清理那些废弃 Savepoint 目录的问题。</p><h3 id="小结与反思-20"><a href="#小结与反思-20" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节主要介绍了 Checkpoint、Savepoint、Checkpoint 与 Savepoint 之间的区别以及 Checkpoint 和 Savepoint 具体如何使用并从 Checkpoint 和 Savepoint 中恢复任务。在 Checkpoint 过程中有一个同步做快照的过程，同步在快照期间 Flink 不会处理数据，为什么这里不能处理数据呢？如果做快照的同时处理数据会有什么影响呢？</p><h2 id="二十三、Flink-Table-amp-SQL-概念与通用-API"><a href="#二十三、Flink-Table-amp-SQL-概念与通用-API" class="headerlink" title="二十三、Flink Table &amp; SQL 概念与通用 API"></a>二十三、Flink Table &amp; SQL 概念与通用 API</h2><p>前面的内容都是讲解 DataStream 和 DataSet API 相关的，在 1.2.5 节中讲解 Flink API 时提及到 Flink 的高级 API——Table API&amp;SQL，本节将开始 Table&amp;SQL 之旅。</p><h3 id="新增-Blink-SQL-查询处理器"><a href="#新增-Blink-SQL-查询处理器" class="headerlink" title="新增 Blink SQL 查询处理器"></a>新增 Blink SQL 查询处理器</h3><p>在 Flink 1.9 版本中，合进了阿里巴巴开源的 Blink 版本中的大量代码，其中最重要的贡献就是 Blink SQL 了。在 Blink 捐献给 Apache Flink 之后，社区就致力于为 Table API&amp;SQL 集成 Blink 的查询优化器和 runtime。先来看下 1.8 版本的 Flink Table 项目结构如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25c4l92ij21ig0pudi6.jpg" alt="undefined"></p><p>1.9 版本的 Flink Table 项目结构图如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25drrn61j21iu0wm0w4.jpg" alt="undefined"></p><p>可以发现新增了 flink-sql-parser、flink-table-planner-blink、flink-table-runtime-blink、flink-table-uber-blink 模块，对 Flink Table 模块的重构详细内容可以参考 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions" target="_blank" rel="noopener">FLIP-32</a>。这样对于 Java 和 Scala API 模块、优化器以及 runtime 模块来说，分层更清楚，接口更明确。</p><p>另外 flink-table-planner-blink 模块中实现了新的优化器接口，所以现在有两个插件化的查询处理器来执行 Table API&amp;SQL：1.9 以前的 Flink 处理器和新的基于 Blink 的处理器。基于 Blink 的查询处理器提供了更好的 SQL 覆盖率、支持更广泛的查询优化、改进了代码生成机制、通过调优算子的实现来提升批处理查询的性能。除此之外，基于 Blink 的查询处理器还提供了更强大的流处理能力，包括了社区一些非常期待的新功能（如维表 Join、TopN、去重）和聚合场景缓解数据倾斜的优化，以及内置更多常用的函数，具体可以查看 flink-table-runtime-blink 代码。目前整个模块的结构如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25e8yjstj217e0luaak.jpg" alt="undefined"></p><p>注意：两个查询处理器之间的语义和功能大部分是一致的，但未完全对齐，因为基于 Blink 的查询处理器还在优化中，所以在 1.9 版本中默认查询处理器还是 1.9 之前的版本。如果你想使用 Blink 处理器的话，可以在创建 TableEnvironment 时通过 EnvironmentSettings 配置启用。被选择的处理器必须要在正在执行的 Java 进程的类路径中。对于集群设置，默认两个查询处理器都会自动地加载到类路径中。如果要在 IDE 中运行一个查询，需要在项目中添加 planner 依赖。</p><h3 id="为什么选择-Table-API-amp-SQL？"><a href="#为什么选择-Table-API-amp-SQL？" class="headerlink" title="为什么选择 Table API&amp;SQL？"></a>为什么选择 Table API&amp;SQL？</h3><p>在 1.2 节中介绍了 Flink 的 API 是包含了 Table API&amp;SQL，在 1.3 节中也介绍了在 Flink 1.9 中阿里开源的 Blink 分支中的很强大的 SQL 功能合并进 Flink 主分支，另外通过阿里 Blink 相关的介绍，可以知道阿里在 SQL 功能这块是做了很多的工作。从前面章节的内容可以发现 Flink 的 DataStream/DataSet API 的功能已经很全并且很强大了，常见复杂的数据处理问题也都可以处理，那么社区为啥还在一直推广 Table API&amp;SQL 呢？</p><p>其实通过观察其它的大数据组件，就不会好奇了，比如 Spark、Storm、Beam、Hive 、KSQL（面向 Kafka 的 SQL 引擎）、Elasticsearch、Phoenix（使用 SQL 进行 HBase 数据的查询）等，可以发现 SQL 已经成为各个大数据组件必不可少的数据查询语言，那么 Flink 作为一个大数据实时处理引擎，笔者对其支持 SQL 查询流数据也不足为奇了，但是还是来稍微介绍一下 Table API&amp;SQL。</p><p>Table API&amp;SQL 是一种关系型 API，用户可以像操作数据库一样直接操作流数据，而不再需要通过 DataStream API 来写很多代码完成计算需求，更不用手动去调优你写的代码，另外 SQL 最大的优势在于它是一门学习成本很低的语言，普及率很高，用户基数大，和其他的编程语言相比，它的入门相对简单。</p><p>除了上面的原因，还有一个原因是：可以借助 Table API&amp;SQL 统一流处理和批处理，因为在 DataStream/DataSet API 中，用户开发流作业和批作业需要去了解两种不同的 API，这对于公司有些开发能力不高的数据分析师来说，学习成本有点高，他们其实更擅长写 SQL 来分析。Table API&amp;SQL 做到了批与流上的查询具有同样的语法语义，因此不用改代码就能同时在批和流上执行。</p><p>总结来说，为什么选择 Table API&amp;SQL：</p><ul><li>声明式语言表达业务逻辑</li><li>无需代码编程——易于上手</li><li>查询能够被有效的优化</li><li>查询可以高效的执行</li></ul><h3 id="Flink-Table-项目模块"><a href="#Flink-Table-项目模块" class="headerlink" title="Flink Table 项目模块"></a>Flink Table 项目模块</h3><p>在上文中提及到 Flink Table 在 1.8 和 1.9 的区别，这里还是要再讲解一下这几个依赖，因为只有了解清楚了之后，我们在后面开发的时候才能够清楚挑选哪种依赖。它有如下几个模块：</p><ul><li>flink-table-common：table 中的公共模块，可以用于通过自定义 function，format 等来扩展 Table 生态系统</li><li>flink-table-api-java：支持使用 Java 语言，纯 Table＆SQL API</li><li>flink-table-api-scala：支持使用 Scala 语言，纯 Table＆SQL API</li><li>flink-table-api-java-bridge：支持使用 Java 语言，包含 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li><li>flink-table-api-scala-bridge：支持使用 Scala 语言，带有 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li><li>flink-sql-parser：SQL 语句解析层，主要依赖 calcite</li><li>flink-table-planner：Table 程序的 planner 和 runtime</li><li>flink-table-uber：将上诉模块打成一个 fat jar，在 lib 目录下</li><li>flink-table-planner-blink：Blink 的 Table 程序的 planner（阿里开源的版本）</li><li>flink-table-runtime-blink：Blink 的 Table 程序的 runtime（阿里开源的版本）</li><li>flink-table-uber-blink：将 Blink 版本的 planner 和 runtime 与前面模块（除 flink-table-planner 模块）打成一个 fat jar，在 lib 目录下</li></ul><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25eljzv0j21gg0cmdhq.jpg" alt="undefined"></p><ul><li>flink-sql-client：SQL 客户端</li></ul><h3 id="两种-planner-之间的区别"><a href="#两种-planner-之间的区别" class="headerlink" title="两种 planner 之间的区别"></a>两种 planner 之间的区别</h3><p>上面讲了两种不同的 planner 之间包含的模块有点区别，但是具体有什么区别如下所示：</p><ul><li>Blink planner 将批处理作业视为流的一种特殊情况。因此不支持 Table 和 DataSet 之间的转换，批处理作业会转换成 DataStream 程序，而不会转换成 DataSet 程序，流作业还是转换成 DataStream 程序。</li><li>Blink planner 不支持 BatchTableSource，而是使用有界的（bounded） StreamTableSource 代替它。</li><li>Blink planner 仅支持全新的 Catalog，不支持已经废弃的 ExternalCatalog。</li><li>以前的 planner 中 FilterableTableSource 的实现与现在的 Blink planner 有冲突，在以前的 planner 中是叠加 PlannerExpressions（在未来的版本中会移除），而在 Blink planner 中是 Expressions。</li><li>基于字符串的 KV 键值配置选项仅可以在 Blink planner 中使用。</li><li>PlannerConfig 的实现（CalciteConfig）在两种 planner 中不同。</li><li>Blink planner 会将多个 sink 优化在同一个 DAG 中（只在 TableEnvironment 中支持，StreamTableEnvironment 中不支持），而以前的 planner 是每个 sink 都有一个 DAG 中，相互独立的。</li><li>以前的 planner 不支持 catalog 统计，而 Blink planner 支持。</li></ul><p>在了解到了两种 planner 的区别后，接下来开始 Flink Table API&amp;SQL 之旅。</p><h3 id="添加项目依赖"><a href="#添加项目依赖" class="headerlink" title="添加项目依赖"></a>添加项目依赖</h3><p>因为在 Flink 1.9 版本中有两个 planner，所以得根据你使用的 planner 来选择对应的依赖，假设你选择的是最新的 Blink 版本，那么添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果是以前的 planner，则使用下面这个依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果要自定义 format 格式或者自定义 function，则需要添加 flink-table-common 依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="创建一个-TableEnvironment"><a href="#创建一个-TableEnvironment" class="headerlink" title="创建一个 TableEnvironment"></a>创建一个 TableEnvironment</h3><p>TableEnvironment 是 Table API 和 SQL 的统称，它负责的内容有：</p><ul><li>在内部的 catalog 注册 Table</li><li>注册一个外部的 catalog</li><li>执行 SQL 查询</li><li>注册用户自定义的 function</li><li>将 DataStream 或者 DataSet 转换成 Table</li><li>保持对 ExecutionEnvironment 和 StreamExecutionEnvironment 的引用</li></ul><p>Table 总是会绑定在一个指定的 TableEnvironment，不能在同一个查询中组合不同 TableEnvironment 的 Table，比如 join 或 union 操作。你可以使用下面的几种静态方法创建 TableEnvironment。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 StreamTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, EnvironmentSettings.newInstance().build());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, EnvironmentSettings settings)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, settings, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** <span class="doctag">@deprecated</span> */</span></span><br><span class="line"><span class="meta">@Deprecated</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, EnvironmentSettings.newInstance().build(), tableConfig);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 BatchTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你需要根据你的程序来使用对应的 TableEnvironment，是 BatchTableEnvironment 还是 StreamTableEnvironment。默认两个 planner 都是在 Flink 的安装目录下 lib 文件夹中存在的，所以应该在你的程序中指定使用哪种 planner。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Flink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line">ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure><p>如果在 lib 目录下只存在一个 planner，则可以使用 useAnyPlanner 来创建指定的 EnvironmentSettings。</p><h3 id="Table-API-amp-SQL-应用程序的结构"><a href="#Table-API-amp-SQL-应用程序的结构" class="headerlink" title="Table API&amp;SQL 应用程序的结构"></a>Table API&amp;SQL 应用程序的结构</h3><p>批处理和流处理的 Table API&amp;SQL 作业都有相同的模式，它们的代码结构如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据前面内容创建一个 TableEnvironment，指定是批作业还是流作业</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"></span><br><span class="line"><span class="comment">//用下面的其中一种方式注册一个 Table</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"table1"</span>, ...)          </span><br><span class="line">tableEnv.registerTableSource(<span class="string">"table2"</span>, ...); </span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">"extCat"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册一个 TableSink</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">"outputTable"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据一个 Table API 查询创建一个 Table</span></span><br><span class="line">Table tapiResult = tableEnv.scan(<span class="string">"table1"</span>).select(...);</span><br><span class="line"><span class="comment">//根据一个 SQL 查询创建一个 Table</span></span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM table2 ... "</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 Table API 或者 SQL 的结果发送给 TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//运行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure><h3 id="Catalog-中注册-Table"><a href="#Catalog-中注册-Table" class="headerlink" title="Catalog 中注册 Table"></a>Catalog 中注册 Table</h3><p>Table 有两种类型，输入表和输出表，可以在 Table API&amp;SQL 查询中引用输入表并提供输入数据，输出表可以用于将 Table API&amp;SQL 的查询结果发送到外部系统。输出表可以通过 TableSink 来注册，输入表可以从各种数据源进行注册：</p><ul><li>已经存在的 Table 对象，通过是 Table API 或 SQL 查询的结果</li><li>连接了外部系统的 TableSource，比如文件、数据库、MQ</li><li>从 DataStream 或 DataSet 程序中返回的 DataStream 和 DataSet</li></ul><h4 id="注册-Table"><a href="#注册-Table" class="headerlink" title="注册 Table"></a>注册 Table</h4><p>在 TableEnvironment 中可以像下面这样注册一个 Table：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; <span class="comment">// see "Create a TableEnvironment" section</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//projTable 是一个简单查询的结果</span></span><br><span class="line">Table projTable = tableEnv.scan(<span class="string">"X"</span>).select(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 projTable 表注册为 projectedTable 表</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure><h4 id="注册-TableSource"><a href="#注册-TableSource" class="headerlink" title="注册 TableSource"></a>注册 TableSource</h4><p>TableSource 让你可以访问存储系统（数据库 MySQL、HBase 等）、编码文件（CSV、Parquet、Avro 等）或 MQ（Kafka、RabbitMQ） 中的数据。Flink 为常用组件都提供了 TableSource，另外还提供自定义 TableSource。在 TableEnvironment 中可以像下面这样注册 TableSource：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 TableSource</span></span><br><span class="line">TableSource csvSource = <span class="keyword">new</span> CsvTableSource(<span class="string">"/Users/zhisheng/file"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 csvSource 注册为表</span></span><br><span class="line">tableEnv.registerTableSource(<span class="string">"CsvTable"</span>, csvSource);</span><br></pre></td></tr></table></figure><p>注意：用于 Blink planner 的 TableEnvironment 只能接受 StreamTableSource、LookupableTableSource 和 InputFormatTableSource，用于 Blink planner 批处理的 StreamTableSource 必须是有界的。</p><h4 id="注册-TableSink"><a href="#注册-TableSink" class="headerlink" title="注册 TableSink"></a>注册 TableSink</h4><p>TableSink 可以将 Table API&amp;SQL 查询的结果发送到外部的存储系统去，比如数据库、KV 存储、文件（CSV、Parquet 等）或 MQ 等。Flink 为常用等数据存储系统和文件格式都提供了 TableSink，另外还支持自定义 TableSink。在 TableEnvironment 中可以像下面这样注册 TableSink：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 TableSink</span></span><br><span class="line">TableSink csvSink = <span class="keyword">new</span> CsvTableSink(<span class="string">"/Users/zhisheng/file"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义属性名和类型</span></span><br><span class="line">String[] fieldNames = &#123;<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>&#125;;</span><br><span class="line">TypeInformation[] fieldTypes = &#123;Types.INT, Types.STRING, Types.LONG&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 csvSink 注册为表 CsvSinkTable</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">"CsvSinkTable"</span>, fieldNames, fieldTypes, csvSink);</span><br></pre></td></tr></table></figure><h3 id="注册外部的-Catalog"><a href="#注册外部的-Catalog" class="headerlink" title="注册外部的 Catalog"></a>注册外部的 Catalog</h3><p>外部的 Catalog 可以提供外部的数据库和表的信息，例如它们的名称、schema、统计信息以及如何访问存储在外部数据库、表、文件中的数据。可以通过实现 ExternalCatalog 接口来创建外部的 Catalog，并像下面这样注册外部的 Catalog：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建外部的 catalog</span></span><br><span class="line">ExternalCatalog catalog = <span class="keyword">new</span> InMemoryExternalCatalog();</span><br><span class="line"><span class="comment">//注册 ExternalCatalog</span></span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">"InMemCatalog"</span>, catalog);<span class="comment">//该方法已经标记过期，可以使用 Catalog</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用下面这种</span></span><br><span class="line">Catalog catalog = <span class="keyword">new</span> GenericInMemoryCatalog(<span class="string">"zhisheng"</span>);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">"InMemCatalog"</span>, catalog);</span><br></pre></td></tr></table></figure><p>在注册后，ExternalCatalog 中的表数据信息可以通过 Table API&amp;SQL 查询获取到。Flink 提供了 Catalog 的一种实现类 GenericInMemoryCatalog 用于样例和测试。</p><h3 id="查询-Table"><a href="#查询-Table" class="headerlink" title="查询 Table"></a>查询 Table</h3><h4 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h4><p>先来演示使用 Table API 来完成一个简单聚合查询：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册 Orders 表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//查询注册的 Orders 表</span></span><br><span class="line">Table orders = tableEnv.scan(<span class="string">"Orders"</span>);</span><br><span class="line"><span class="comment">//计算来自中国的顾客的收入</span></span><br><span class="line">Table revenue = orders</span><br><span class="line">  .filter(<span class="string">"cCountry === 'China'"</span>)</span><br><span class="line">  .groupBy(<span class="string">"cID, cName"</span>)</span><br><span class="line">  .select(<span class="string">"cID, cName, revenue.sum AS revSum"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//转换或者提交该结果表</span></span><br><span class="line"><span class="comment">//运行该查询语句</span></span><br></pre></td></tr></table></figure><p>你可以使用 Java 或者 Scala 语言来利用 Table API 开发，而 SQL 却不是这样的。</p><h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><p>上面使用 Table API 的聚合查询样例使用 SQL 来完成就如下面这样：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册 Orders 表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//计算来自中国的顾客的收入</span></span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line"><span class="comment">//转换或者提交该结果表</span></span><br><span class="line"><span class="comment">//运行该查询语句</span></span><br></pre></td></tr></table></figure><p>Flink 的 SQL 是基于实现 SQL 标准的 Apache Calcite，SQL 的查询语句就是全部为字符串，上面这条 SQL 就说明了该如何指定查询并返回结果表，下面演示如何更新。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.sqlUpdate(</span><br><span class="line">    <span class="string">"INSERT INTO RevenueFrance "</span> +</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br></pre></td></tr></table></figure><h4 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API&amp;SQL"></a>Table API&amp;SQL</h4><p>Table API 和 SQL 之间可以相互结合，因为它们最后都是返回的 Table 对象，比如你可以在 SQL 查询返回的对象上定义 Table API 的查询，也可以在 Table API 查询结果返回的对象上定义 SQL 查询。</p><h3 id="提交-Table"><a href="#提交-Table" class="headerlink" title="提交 Table"></a>提交 Table</h3><p>在前面讲解了注册 TableSink，那么将表的结果提交就是将 Table 写入 TableSink，批处理的 Table 只能写入到 BatchTableSink，而流处理的 Table 可以写入进 AppendStreamTableSink、RetractStreamTableSink、UpsertStreamTableSink。使用 Table.insertInto(String tableName) 方法就可以将 Table 写入进已注册的 TableSink，它会根据名字去 catalog 中查找，并对比两者的 schema 是否相同。</p><h3 id="翻译并执行查询"><a href="#翻译并执行查询" class="headerlink" title="翻译并执行查询"></a>翻译并执行查询</h3><p>对于两种不同的 planner，翻译和执行查询的行为是不同的。</p><ul><li>之前的 planner：根据 Table API&amp;SQL 查询的输入是流还是批，然后先优化执行计划，接着对应转换成 DataStream 和 DataSet 程序，当 Table.insertInto() 和 TableEnvironment.sqlUpdate() 方法被调用、Table 转换成 DataStream 或 DataSet 时就会开始将 Table API 和 SQL 进行翻译，一旦翻译翻译完成后，也是和普通作业一样要执行 execute 方法后才开始运行。</li><li>Blink planner：不管 Table API 的输入是批还是流，都会转换成 DataStream 程序，对于 TableEnvironment 和 StreamTableEnvironment 的查询翻译是不一样的，对于 TableEnvironment，是在 TableEnvironment.execute() 调用的时候就会翻译 Table API&amp;SQL，因为 TableEnvironment 会将多个 Sink 优化在同一个 DAG 中，而 StreamTableEnvironment 和之前的 planner 是类似的。</li></ul><h3 id="小结与反思-21"><a href="#小结与反思-21" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节介绍了 Flink 新的 planner，然后详细地和之前的 planner 做了对比，然后对 Table API&amp;SQL 中的概念做了介绍，还通过样例去介绍了它们的通用 API。</p><h1 id="二十四、Flink-Table-API-amp-SQL-功能"><a href="#二十四、Flink-Table-API-amp-SQL-功能" class="headerlink" title="二十四、Flink Table API &amp; SQL 功能"></a>二十四、Flink Table API &amp; SQL 功能</h1><p>在 5.1 节中对 Flink Table API &amp; SQL 的概述和常见 API 都做了介绍，这篇文章先来看下其与 DataStream 和 DataSet API 的集成。</p><h3 id="Flink-Table-和-SQL-与-DataStream-和-DataSet-集成"><a href="#Flink-Table-和-SQL-与-DataStream-和-DataSet-集成" class="headerlink" title="Flink Table 和 SQL 与 DataStream 和 DataSet 集成"></a>Flink Table 和 SQL 与 DataStream 和 DataSet 集成</h3><p>两个 planner 都可以与 DataStream API 集成，只有以前的 planner 才可以集成 DataSet API，所以下面讨论 DataSet API 都是和以前的 planner 有关。</p><p>Table API &amp; SQL 查询与 DataStream 和 DataSet 程序集成是非常简单的，比如可以通过 Table API 或者 SQL 查询外部表数据，进行一些预处理后，然后使用 DataStream 或 DataSet API 继续处理一些复杂的计算，另外也可以将 DataStream 或 DataSet 处理后的数据利用 Table API 或者 SQL 写入到外部表去。总而言之，它们之间互相转换或者集成比较容易。</p><h4 id="Scala-的隐式转换"><a href="#Scala-的隐式转换" class="headerlink" title="Scala 的隐式转换"></a>Scala 的隐式转换</h4><p>Scala Table API 提供了 DataSet、DataStream 和 Table 类的隐式转换，可以通过导入 org.apache.flink.table.api.scala._ 或者 org.apache.flink.api.scala._ 包来启用这些转换。</p><h4 id="将-DataStream-或-DataSet-注册为-Table"><a href="#将-DataStream-或-DataSet-注册为-Table" class="headerlink" title="将 DataStream 或 DataSet 注册为 Table"></a>将 DataStream 或 DataSet 注册为 Table</h4><p>DataStream 或者 DataSet 可以注册为 Table，结果表的 schema 取决于已经注册的 DataStream 和 DataSet 的数据类型。你可以像下面这种方式转换：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable 表</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable"</span>, stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable2 表（表中的字段为 myLong、myString）</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable2"</span>, stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h4 id="将-DataStream-或-DataSet-转换为-Table"><a href="#将-DataStream-或-DataSet-转换为-Table" class="headerlink" title="将 DataStream 或 DataSet 转换为 Table"></a>将 DataStream 或 DataSet 转换为 Table</h4><p>除了可以将 DataStream 或 DataSet 注册为 Table，还可以将它们转换为 Table，转换之后再去使用 Table API 查询就比较方便了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table1 = tableEnv.fromDataStream(stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table2 = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h4 id="将-Table-转换成-DataStream-或-DataSet"><a href="#将-Table-转换成-DataStream-或-DataSet" class="headerlink" title="将 Table 转换成 DataStream 或 DataSet"></a>将 Table 转换成 DataStream 或 DataSet</h4><p>Table 可以转换为 DataStream 或 DataSet，这样就可以在 Table API 或 SQL 查询的结果上运行自定义的 DataStream 或 DataSet 程序。当将一个 Table 转换成 DataStream 或 DataSet 时，需要指定结果 DataStream 或 DataSet 的数据类型，最方便的数据类型是 Row，下面几个数据类型表示不同的功能：</p><ul><li>Row：字段按位置映射，任意数量的字段，支持 null 值，没有类型安全访问。</li><li>POJO：字段按名称映射，POJO 属性必须按照 Table 中的属性来命名，任意数量的字段，支持 null 值，类型安全访问。</li><li>Case Class：字段按位置映射，不支持 null 值，类型安全访问。</li><li>Tuple：按位置映射字段，限制为 22（Scala）或 25（Java）字段，不支持 null 值，类型安全访问。</li><li>原子类型：Table 必须具有单个字段，不支持 null 值，类型安全访问。</li></ul><h5 id="将-Table-转换成-DataStream"><a href="#将-Table-转换成-DataStream" class="headerlink" title="将 Table 转换成 DataStream"></a>将 Table 转换成 DataStream</h5><p>流查询的结果表会动态更新，即每个新的记录到达输入流时结果就会发生变化。所以在将 Table 转换成 DataStream 就需要对表的更新进行编码，有两种将 Table 转换为 DataStream 的模式：</p><ul><li>追加模式（Append Mode）：这种模式只能在动态表仅通过 INSERT 更改修改时才能使用，即仅追加，之前发出的结果不会更新。</li><li>撤回模式（Retract Mode）：任何时刻都可以使用此模式，它使用一个 boolean 标志来编码 INSERT 和 DELETE 的更改。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定类，将表转换为一个 append DataStream</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 append DataStream</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为一个 Retract DataStream Row</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure><h5 id="将-Table-转换成-DataSet"><a href="#将-Table-转换成-DataSet" class="headerlink" title="将 Table 转换成 DataSet"></a>将 Table 转换成 DataSet</h5><p>将 Table 转换成 DataSet 的样例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定一个类将表转换为一个 Row DataSet</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 DataSet</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType);</span><br></pre></td></tr></table></figure><h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>Flink 使用 Calcite 来优化和翻译查询，以前的 planner 不会去优化 join 的顺序，而是按照查询中定义的顺序去执行。通过提供一个 CalciteConfig 对象来调整在不同阶段应用的优化规则集，这个可以通过调用 CalciteConfig.createBuilder() 获得的 builder 来创建，并且可以通过调用tableEnv.getConfig.setCalciteConfig(calciteConfig) 来提供给 TableEnvironment。而在 Blink planner 中扩展了 Calcite 来执行复杂的查询优化，这包括一系列基于规则和成本的优化，比如：</p><ul><li>基于 Calcite 的子查询去相关性</li><li>Project pruning</li><li>Partition pruning</li><li>Filter push-down</li><li>删除子计划中的重复数据以避免重复计算</li><li>重写特殊的子查询，包括两部分：<ul><li>将 IN 和 EXISTS 转换为 left semi-joins</li><li>将 NOT IN 和 NOT EXISTS 转换为 left anti-join</li></ul></li><li>重排序可选的 join<ul><li>通过启用 table.optimizer.join-reorder-enabled</li></ul></li></ul><p>注意：IN/EXISTS/NOT IN/NOT EXISTS 目前只支持子查询重写中的连接条件。</p><h4 id="解释-Table"><a href="#解释-Table" class="headerlink" title="解释 Table"></a>解释 Table</h4><p>Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。你可以通过 TableEnvironment.explain(table) 或者 TableEnvironment.explain() 方法来完成。explain(table) 会返回给定计划的 Table，explain() 会返回多路 Sink 计划的结果（主要用于 Blink planner）。它返回一个描述三个计划的字符串：</p><ul><li>关系查询的抽象语法树，即未优化的逻辑查询计划</li><li>优化的逻辑查询计划</li><li>实际执行计划</li></ul><p>以下代码演示了一个 Table 示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line"></span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, <span class="string">"count, word"</span>);</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, <span class="string">"count, word"</span>);</span><br><span class="line">Table table = table1.where(<span class="string">"LIKE(word, 'F%')"</span>).unionAll(table2);</span><br><span class="line"></span><br><span class="line">System.out.println(tEnv.explain(table));</span><br></pre></td></tr></table></figure><p>通过 explain(table) 方法返回的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">== Abstract Syntax Tree ==</span><br><span class="line">LogicalUnion(all=[true])</span><br><span class="line">  LogicalFilter(condition=[LIKE($1, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">DataStreamUnion(all=[true], union all=[count, word])</span><br><span class="line">  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    DataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Physical Execution Plan ==</span><br><span class="line">Stage 1 : Data Source</span><br><span class="line">    content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 2 : Data Source</span><br><span class="line">    content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">    Stage 3 : Operator</span><br><span class="line">        content : from: (count, word)</span><br><span class="line">        ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">        Stage 4 : Operator</span><br><span class="line">            content : where: (LIKE(word, _UTF-16LE&apos;F%&apos;)), select: (count, word)</span><br><span class="line">            ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">            Stage 5 : Operator</span><br><span class="line">                content : from: (count, word)</span><br><span class="line">                ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>在 Flink 1.9 之前，Flink 的 Table API&amp;SQL 的数据类型与 Flink 中的 TypeInformation 紧密相关。TypeInformation 在 DataStream 和 DataSet API 中使用，另外它还可以描述在分布式中序列化和反序列化基于 JVM 对象所需的所有信息。从 1.9 版本之后，Table API&amp;SQL 会引入一种新类型来作为 API 稳定性和标准的长期解决方案。在以前的 planner 和 Blink planner 的数据类型有点不一致，具体差别可以参考官网。</p><h3 id="时间属性"><a href="#时间属性" class="headerlink" title="时间属性"></a>时间属性</h3><p>在 3.1 节中介绍过 Flink 的多种时间语义，常用的比如 Event time 和 Processing time，那么在 Table API&amp;SQL 中怎么去定义时间语义呢？</p><h4 id="Processing-Time-1"><a href="#Processing-Time-1" class="headerlink" title="Processing Time"></a>Processing Time</h4><p>因为处理时间是额外的数据字段，在原始的事件中是不存在该字段的，那么在将数据流转换成 Table 的时候就需要将这个 Processing time 当作 Table 的一个字段，以供后面需要，比如定义窗口。你可以像下面这样定义：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将附加的逻辑字段声明为 Processing time 属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"Username, Data, UserActionTime.proctime"</span>);</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"UserActionTime"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><p>如果是直接使用 TableSource 的话，那么需要实现 DefinedProctimeAttribute 接口，然后去重写 getProctimeAttribute 方法，返回的字符串表示 Processing time 在 Table 中的字段名。</p><h4 id="Event-time"><a href="#Event-time" class="headerlink" title="Event time"></a>Event time</h4><p>Event time 是在采集上来的事件中就有的，将数据流转换成 Table 的时候需要像下面这样定义：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一种方法：</span></span><br><span class="line"><span class="comment">//提取流数据时间戳并分配水印</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"><span class="comment">//将附加的逻辑字段声明为 Event time 属性，和 Processing time 不同的是这里使用 rowtime</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"Username, Data, UserActionTime.rowtime"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第二种方法：</span></span><br><span class="line"><span class="comment">//从第一个字段提取时间戳，并分配水印</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"UserActionTime.rowtime, Username, Data"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用方式：</span></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"UserActionTime"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><p>使用 TableSource 的话则需要实现 DefinedRowtimeAttributes 接口，重写 getRowtimeAttributeDescriptors 方法，该方法返回一个 RowtimeAttributeDescriptor 列表，其用于描述时间属性的最终名称、时间提取器以及该属性关联的水印策略。</p><h3 id="SQL-Connector"><a href="#SQL-Connector" class="headerlink" title="SQL Connector"></a>SQL Connector</h3><p>在第三部分中介绍了大量的 Flink Connectors 的使用，但是那些都是通过 DataStream API 是去使用，放在 Table API&amp;SQL 中其实不再适合，其实 Flink Table API&amp;SQL 是可以直接连接到外部系统的，然后读取和写入批处理表和流处理表。TableSource 提供从外部系统（数据库、MQ、文件系统等）读取数据，TableSink 将结果存储到数据库中。这里讲解一下该如何去定义 TableSource 和 TableSink 并将它们注册。在官网，它提供了如下这些 Connectors 和 Formats 的下载。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga3besz7gfj21b00uuq4h.jpg" alt="undefined"></p><p>从 Flink 1.6 开始，不仅可以使用编程的方式指定 Connector，还可以使用声明式去定义。下面举个例子（读取 Kafka 中 Avro 格式的数据）来讲解这两种区别。</p><h4 id="使用代码"><a href="#使用代码" class="headerlink" title="使用代码"></a>使用代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  <span class="comment">//声明要连接的外部系统</span></span><br><span class="line">  .connect(</span><br><span class="line">    <span class="keyword">new</span> Kafka()</span><br><span class="line">      .version(<span class="string">"0.10"</span>)</span><br><span class="line">      .topic(<span class="string">"zhisheng_user"</span>)</span><br><span class="line">      .startFromEarliest()</span><br><span class="line">      .property(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>)</span><br><span class="line">      .property(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">  )</span><br><span class="line">  <span class="comment">//定义数据格式</span></span><br><span class="line">  .withFormat(</span><br><span class="line">    <span class="keyword">new</span> Avro()</span><br><span class="line">      .avroSchema(</span><br><span class="line">        <span class="string">"&#123;"</span> +</span><br><span class="line">        <span class="string">"  \"namespace\": \"com.zhisheng\","</span> +</span><br><span class="line">        <span class="string">"  \"type\": \"record\","</span> +</span><br><span class="line">        <span class="string">"  \"name\": \"UserMessage\","</span> +</span><br><span class="line">        <span class="string">"    \"fields\": ["</span> +</span><br><span class="line">        <span class="string">"      &#123;\"name\": \"timestamp\", \"type\": \"string\"&#125;,"</span> +</span><br><span class="line">        <span class="string">"      &#123;\"name\": \"user\", \"type\": \"long\"&#125;,"</span> +</span><br><span class="line">        <span class="string">"      &#123;\"name\": \"message\", \"type\": [\"string\", \"null\"]&#125;"</span> +</span><br><span class="line">        <span class="string">"    ]"</span> +</span><br><span class="line">        <span class="string">"&#125;"</span></span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line">  <span class="comment">//定义 Table schema</span></span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> Schema()</span><br><span class="line">      .field(<span class="string">"rowtime"</span>, Types.SQL_TIMESTAMP)</span><br><span class="line">        .rowtime(<span class="keyword">new</span> Rowtime()</span><br><span class="line">          .timestampsFromField(<span class="string">"timestamp"</span>)</span><br><span class="line">          .watermarksPeriodicBounded(<span class="number">60000</span>)</span><br><span class="line">        )</span><br><span class="line">      .field(<span class="string">"user"</span>, Types.LONG)</span><br><span class="line">      .field(<span class="string">"message"</span>, Types.STRING)</span><br><span class="line">  )</span><br><span class="line">  .inAppendMode()  <span class="comment">//指定流表的 update-mode</span></span><br><span class="line">  .registerTableSource(<span class="string">"zhisheng"</span>);    <span class="comment">//注册表的名字</span></span><br></pre></td></tr></table></figure><h4 id="使用-YAML-文件"><a href="#使用-YAML-文件" class="headerlink" title="使用 YAML 文件"></a>使用 YAML 文件</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tables:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">zhisheng</span>      <span class="comment">#表的名字</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">source</span>           <span class="comment">#定义是 source，还是 sink，或者 both</span></span><br><span class="line"><span class="attr">    update-mode:</span> <span class="string">append</span>    <span class="comment">#指定流表的 update-mode</span></span><br><span class="line">    <span class="comment">#定义要连接的系统</span></span><br><span class="line"><span class="attr">    connector:</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">kafka</span></span><br><span class="line"><span class="attr">      version:</span> <span class="string">"0.10"</span></span><br><span class="line"><span class="attr">      topic:</span> <span class="string">zhisheng_user</span></span><br><span class="line"><span class="attr">      startup-mode:</span> <span class="string">earliest-offset</span></span><br><span class="line"><span class="attr">      properties:</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">zookeeper.connect</span></span><br><span class="line"><span class="attr">          value:</span> <span class="attr">localhost:2181</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">bootstrap.servers</span></span><br><span class="line"><span class="attr">          value:</span> <span class="attr">localhost:9092</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#定义格式</span></span><br><span class="line"><span class="attr">    format:</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">avro</span></span><br><span class="line"><span class="attr">      avro-schema:</span> <span class="string">&gt;</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">          "namespace": "com.zhisheng",</span></span><br><span class="line"><span class="string">          "type": "record",</span></span><br><span class="line"><span class="string">          "name": "UserMessage",</span></span><br><span class="line"><span class="string">            "fields": [</span></span><br><span class="line"><span class="string">              &#123;"name": "ts", "type": "string"&#125;,</span></span><br><span class="line"><span class="string">              &#123;"name": "user", "type": "long"&#125;,</span></span><br><span class="line"><span class="string">              &#123;"name": "message", "type": ["string", "null"]&#125;</span></span><br><span class="line"><span class="string">            ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    #定义 table schema</span></span><br><span class="line"><span class="string"></span><span class="attr">    schema:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">rowtime</span></span><br><span class="line"><span class="attr">        type:</span> <span class="string">TIMESTAMP</span></span><br><span class="line"><span class="attr">        rowtime:</span></span><br><span class="line"><span class="attr">          timestamps:</span></span><br><span class="line"><span class="attr">            type:</span> <span class="string">from-field</span></span><br><span class="line"><span class="attr">            from:</span> <span class="string">ts</span></span><br><span class="line"><span class="attr">          watermarks:</span></span><br><span class="line"><span class="attr">            type:</span> <span class="string">periodic-bounded</span></span><br><span class="line"><span class="attr">            delay:</span> <span class="string">"60000"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">user</span></span><br><span class="line"><span class="attr">        type:</span> <span class="string">BIGINT</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">message</span></span><br><span class="line"><span class="attr">        type:</span> <span class="string">VARCHAR</span></span><br></pre></td></tr></table></figure><h4 id="使用-DDL"><a href="#使用-DDL" class="headerlink" title="使用 DDL"></a>使用 DDL</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE zhisheng (</span><br><span class="line">  `user` BIGINT,</span><br><span class="line">  message VARCHAR,</span><br><span class="line">  ts VARCHAR</span><br><span class="line">) WITH (</span><br><span class="line">  &apos;connector.type&apos; = &apos;kafka&apos;,</span><br><span class="line">  &apos;connector.version&apos; = &apos;0.10&apos;,</span><br><span class="line">  &apos;connector.topic&apos; = &apos;zhisheng_user&apos;,</span><br><span class="line">  &apos;connector.startup-mode&apos; = &apos;earliest-offset&apos;,</span><br><span class="line">  &apos;connector.properties.0.key&apos; = &apos;zookeeper.connect&apos;,</span><br><span class="line">  &apos;connector.properties.0.value&apos; = &apos;localhost:2181&apos;,</span><br><span class="line">  &apos;connector.properties.1.key&apos; = &apos;bootstrap.servers&apos;,</span><br><span class="line">  &apos;connector.properties.1.value&apos; = &apos;localhost:9092&apos;,</span><br><span class="line">  &apos;update-mode&apos; = &apos;append&apos;,</span><br><span class="line">  &apos;format.type&apos; = &apos;avro&apos;,</span><br><span class="line">  &apos;format.avro-schema&apos; = &apos;&#123;</span><br><span class="line">                            &quot;namespace&quot;: &quot;com.zhisheng&quot;,</span><br><span class="line">                            &quot;type&quot;: &quot;record&quot;,</span><br><span class="line">                            &quot;name&quot;: &quot;UserMessage&quot;,</span><br><span class="line">                            &quot;fields&quot;: [</span><br><span class="line">                                &#123;&quot;name&quot;: &quot;ts&quot;, &quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">                                &#123;&quot;name&quot;: &quot;user&quot;, &quot;type&quot;: &quot;long&quot;&#125;,</span><br><span class="line">                                &#123;&quot;name&quot;: &quot;message&quot;, &quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]&#125;</span><br><span class="line">                            ]</span><br><span class="line">                         &#125;&apos;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>上面演示了 Kafka Connector 和 avro 数据格式化在 Table API&amp;SQL 中的使用方式，在官网中还有文件系统和 Elasticsearch Connector、CSV 和 JSON 等的使用说明。</p><h3 id="SQL-Client"><a href="#SQL-Client" class="headerlink" title="SQL Client"></a>SQL Client</h3><p>虽然 Flink Table API&amp;SQL 让使用 SQL 去查询流数据有了可能，但是这些查询语句通常要嵌入在 Java 或者 Scala 程序中，最后在提交到集群运行之前还要通过构建工具打包，这就导致 Table API&amp;SQL 的限制性很大，所以 SQL Client 就起到这么个作用，让用户不再编写任何 Java 或者 Scala 代码，直接编写 SQL 就可以去调试运行，并且可以通过其他命令行实时查看运行的结果，但是该功能目前还比较弱。</p><p>在启动 Flink 后可以通过运行 <code>./bin/sql-client.sh embedded</code> 命令来启动 SQL Client CLI，如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga3bf4ctclj21qc27uthw.jpg" alt="undefined"></p><p>你可以运行下面的命令就可以知道名字和其出现的次数的结果。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">name</span>, <span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt <span class="keyword">FROM</span> (<span class="keyword">VALUES</span> (<span class="string">'Bob'</span>), (<span class="string">'Alice'</span>), (<span class="string">'Greg'</span>), (<span class="string">'Bob'</span>)) <span class="keyword">AS</span> NameTable(<span class="keyword">name</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><p>另外它还支持传入 YAML 文件，你可以在 YAML 文件中如前面内容一样定义的 Kafka Connector 等信息，关于 SQL Client 的更多功能可以查阅官网。</p><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>Hive 是建立在 Hadoop 上的数据仓库基础构架，它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。</p><p>Flink 在 1.9 版本中提供了与 Hive 的双重集成。首先是利用 Hive 的 Metastore 存储 Flink 特定元数据，另一个是 Flink 支持读取和写入 Hive 表。支持的 Hive 2.3.4 和 1.2.1 版本，如果你要使用的话，注意它们的依赖是有点不一样。</p><p>你可以通过 Java、Scala、YAML 连接 Hive，比如使用 Java 代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String name            = <span class="string">"myhive"</span>;</span><br><span class="line">String defaultDatabase = <span class="string">"mydatabase"</span>;</span><br><span class="line">String hiveConfDir     = <span class="string">"/opt/hive-conf"</span>;</span><br><span class="line">String version         = <span class="string">"2.3.4"</span>; <span class="comment">//或者 1.2.1</span></span><br><span class="line"></span><br><span class="line">HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir, version);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">"myhive"</span>, hive);</span><br></pre></td></tr></table></figure><h3 id="小结与反思-22"><a href="#小结与反思-22" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节继续介绍了 Flink Table API&amp;SQL 中的部分 API，然后讲解了 Flink 之前的 planner 和 Blink planner 在某些特性上面的区别，还讲解了 SQL Connector，最后介绍了 SQL Client 和 Hive。</p><h2 id="二十五、Flink-CEP-介绍及其使用场景"><a href="#二十五、Flink-CEP-介绍及其使用场景" class="headerlink" title="二十五、Flink CEP 介绍及其使用场景"></a>二十五、Flink CEP 介绍及其使用场景</h2><h3 id="CEP-是什么？"><a href="#CEP-是什么？" class="headerlink" title="CEP 是什么？"></a>CEP 是什么？</h3><p>CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。它可以用于处理实时数据并在事件流到达时从事件流中提取信息，并根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。</p><p>因为这种事件匹配通常是根据提前制定好的规则去匹配的，而这些规则一般来说不仅多，而且复杂，所以就会引入一些规则引擎来处理这种复杂事件匹配。市面上常用的规则引擎有如下这些。</p><h3 id="规则引擎对比"><a href="#规则引擎对比" class="headerlink" title="规则引擎对比"></a>规则引擎对比</h3><h4 id="Drools"><a href="#Drools" class="headerlink" title="Drools"></a>Drools</h4><p>Drools 是一款使用 Java 编写的开源规则引擎，通常用来解决业务代码与业务规则的分离，它内置的 Drools Fusion 模块也提供 CEP 的功能。</p><p>优势：</p><ul><li>功能较为完善，具有如系统监控、操作平台等功能。</li><li>规则支持动态更新。</li></ul><p>劣势：</p><ul><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Aviator"><a href="#Aviator" class="headerlink" title="Aviator"></a>Aviator</h4><p>Aviator 是一个高性能、轻量级的 Java 语言实现的表达式求值引擎，主要用于各种表达式的动态求值。</p><p>优势：</p><ul><li>支持大部分运算操作符。</li><li>支持函数调用和自定义函数。</li><li>支持正则表达式匹配。</li><li>支持传入变量并且性能优秀。</li></ul><p>劣势：</p><ul><li>没有 if else、do while 等语句，没有赋值语句，没有位运算符。</li></ul><h4 id="EasyRules"><a href="#EasyRules" class="headerlink" title="EasyRules"></a>EasyRules</h4><p>EasyRules 集成了 MVEL 和 SpEL 表达式的一款轻量级规则引擎。</p><p>优势：</p><ul><li>轻量级框架，学习成本低。</li><li>基于 POJO。</li><li>为定义业务引擎提供有用的抽象和简便的应用</li><li>支持从简单的规则组建成复杂规则</li></ul><h4 id="Esper"><a href="#Esper" class="headerlink" title="Esper"></a>Esper</h4><p>Esper 设计目标为 CEP 的轻量级解决方案，可以方便的嵌入服务中，提供 CEP 功能。</p><p>优势：</p><ul><li>轻量级可嵌入开发，常用的 CEP 功能简单好用。</li><li>EPL 语法与 SQL 类似，学习成本较低。</li></ul><p>劣势：</p><ul><li>单机全内存方案，需要整合其他分布式和存储。</li><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h4><p>Flink 是一个流式系统，具有高吞吐低延迟的特点，Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案。</p><p>优势：</p><ul><li>继承了 Flink 高吞吐的特点</li><li>事件支持存储到外部，可以支持较长跨度的时间窗。</li><li>可以支持定时触达（用 followedBy ＋ PartternTimeoutFunction 实现）</li></ul><p>劣势：</p><ul><li>无法动态更新规则（痛点）</li></ul><h3 id="Flink-CEP-介绍"><a href="#Flink-CEP-介绍" class="headerlink" title="Flink CEP 介绍"></a>Flink CEP 介绍</h3><p>前面介绍规则引擎的时候，对 Flink CEP 做了一个简单的介绍，因为搭配了 Flink 实时处理的能力，所以 Flink CEP 能够在流处理的场景去做一些实时的复杂事件匹配，它与传统的数据库查询是不一致的，比如，传统的数据库的数据是静态的，但是查询却是动态的，所以传统的数据库查询做不到实时的反馈查询结果，而 Flink CEP 则是查询规则是静态的，数据是动态实时的，如果它作用于一个无限的数据流上，这就意味着它可以将某种规则的数据匹配一直保持下去（除非作业停止）；另外 Flink CEP 不需要去存储那些与匹配不相关联的数据，遇到这种数据它会立即丢弃。</p><p>虽然 Flink CEP 拥有 Flink 的本身优点和支持复杂场景的规则处理，但是它本身其实也有非常严重的缺点，那就是不能够动态的更新规则。通常引入规则引擎比较友好的一点是可以将一些业务规则抽象出来成为配置，然后更改这些配置后其实是能够自动生效的，但是在 Flink 中却无法做到这点，甚至规则通常还是要写死在代码里面。举个例子，你在一个 Flink CEP 的作业中定义了一条规则：机器的 CPU 使用率连续 30 秒超过 90% 则发出告警，然后将这个作业上线，上线后发现告警很频繁，你可能会觉得可能规则之前定义的不合适，那么接下来你要做的就是将作业取消，然后重新修改代码并进行编译打包成一个 fat jar，接着上传该 jar 并运行。整个流程下来，你有没有想过会消耗多长的时间？五分钟？或者更长？但是你的目的就是要修改一个配置，如果你在作业中将上面的 30 秒和 90% 做成了配置，可能这样所需要的时间会减少，你只需要重启作业，然后通过传入新的参数将作业重新启动，但是重启作业这步是不是不能少，然而对于流作业来说，重启作业带来的代价很大。</p><h3 id="Flink-CEP-如何动态更新规则"><a href="#Flink-CEP-如何动态更新规则" class="headerlink" title="Flink CEP 如何动态更新规则"></a>Flink CEP 如何动态更新规则</h3><p>针对 Flink CEP 不能动态更新规则的问题，笔者看社区是暂时没有提出解决方案，但是在国内的 Flink 技术分享会却看到有几家公司对这块做了优化，让 Flink CEP 支持动态的更新规则，下面分享一下他们几家公司的思路。</p><ul><li>A 公司：用户更新规则后，新规则会被翻译成 Java 代码，并编译打包成可执行 jar，停止作业并使用 Savepoint 将状态保存下来，启动新的作业并读取之前保存的状态，会根据规则文件中的数量和复杂度对作业的数量做一个规划，防止单作业负载过高，架构如下图所示。</li></ul><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtft9btyj20id08taam.jpg" alt="TIM截图20200409001740.png"></p><p>B 公司：规则中心存储规则，规则里面直接存储了 Java 代码，加载这些规则后然后再用 Groovy 做动态编译解析，其架构如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtgbbzxoj21fg0t0t92.jpg" alt="2019-10-28-143822.png"></p><p>C 公司：增加函数，在函数方法中监听规则的变化，如果需要更新则通过 Groovy 加载 Pattern 类进行动态注入，采用 Zookeeper 和 MySQL 管理规则，如果规则发生变化，则从数据库中获取到新的规则，然后更新 Flink CEP 中的 NFA 逻辑，注意状态要根据业务需要选择是否重置，其架构设计如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtgrfc18j21fg0t0t92.jpg" alt="2019-10-28-143822.png"></p><p>第一种方法，笔者不推荐，因为它这样的做法还是要将作业重启，无非就是做了一个自动化的操作，不是人为的手动重启，从 B 公司和 C 公司两种方法可以发现要实现 Flink CEP 动态的更新规则无非要做的就是：</p><ul><li>监听规则的变化</li><li>将规则变成 Java 代码</li><li>通过 Groovy 动态编译解析</li><li>更改 NFA 的内部逻辑</li><li>状态是否保留</li></ul><h3 id="Flink-CEP-使用场景分析"><a href="#Flink-CEP-使用场景分析" class="headerlink" title="Flink CEP 使用场景分析"></a>Flink CEP 使用场景分析</h3><p>上面虽然提到了一个 Flink CEP 的痛点，但是并不能就此把它的优势给抹去，它可以运用的场景其实还有很多，这里笔者拿某些场景来做个分析。</p><h4 id="实时反作弊和风控"><a href="#实时反作弊和风控" class="headerlink" title="实时反作弊和风控"></a>实时反作弊和风控</h4><p>对于电商来说，羊毛党是必不可少的，国内拼多多曾爆出 100 元的无门槛券随便领，当晚被人褥几百亿，对于这种情况肯定是没有做好及时的风控。另外还有就是商家上架商品时通过频繁修改商品的名称和滥用标题来提高搜索关键字的排名、批量注册一批机器账号快速刷单来提高商品的销售量等作弊行为，各种各样的作弊手法也是需要不断的去制定规则去匹配这种行为。</p><h4 id="实时营销"><a href="#实时营销" class="headerlink" title="实时营销"></a>实时营销</h4><p>分析用户在手机 APP 的实时行为，统计用户的活动周期，通过为用户画像来给用户进行推荐。比如用户在登录 APP 后 1 分钟内只浏览了商品没有下单；用户在浏览一个商品后，3 分钟内又去查看其他同类的商品，进行比价行为；用户商品下单后 1 分钟内是否支付了该订单。如果这些数据都可以很好的利用起来，那么就可以给用户推荐浏览过的类似商品，这样可以大大提高购买率。</p><h4 id="实时网络攻击检测"><a href="#实时网络攻击检测" class="headerlink" title="实时网络攻击检测"></a>实时网络攻击检测</h4><p>当下互联网安全形势仍然严峻，网络攻击屡见不鲜且花样众多，这里我们以 DDOS（分布式拒绝服务攻击）产生的流入流量来作为遭受攻击的判断依据。对网络遭受的潜在攻击进行实时检测并给出预警，云服务厂商的多个数据中心会定时向监控中心上报其瞬时流量，如果流量在预设的正常范围内则认为是正常现象，不做任何操作；如果某数据中心在 10 秒内连续 5 次上报的流量超过正常范围的阈值，则触发一条警告的事件；如果某数据中心 30 秒内连续出现 30 次上报的流量超过正常范围的阈值，则触发严重的告警。</p><h3 id="小结与反思-23"><a href="#小结与反思-23" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节介绍了 CEP，并对比了市面上已有的规则引擎，然后介绍了 Flink CEP 的优点和痛点，然后讲解了国内公司对于这个痛点的解决方案，最后讲解了 Flink CEP 的使用场景分析。你们公司有使用 Flink CEP 吗？是什么场景下使用了？</p><h2 id="二十六、Flink-CEP-如何处理复杂事件？"><a href="#二十六、Flink-CEP-如何处理复杂事件？" class="headerlink" title="二十六、Flink CEP 如何处理复杂事件？"></a>二十六、Flink CEP 如何处理复杂事件？</h2><p>6.1 节中介绍 Flink CEP 和其使用场景，本节将详细介绍 Flink CEP 的 API，教会大家如何去使用 Flink CEP。</p><h3 id="准备依赖"><a href="#准备依赖" class="headerlink" title="准备依赖"></a>准备依赖</h3><p>要开发 Flink CEP 应用程序，首先你得在项目的 <code>pom.xml</code> 中添加依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个依赖有两种，一个是 Java 版本的，一个是 Scala 版本，你可以根据项目的开发语言自行选择。</p><h3 id="Flink-CEP-应用入门"><a href="#Flink-CEP-应用入门" class="headerlink" title="Flink CEP 应用入门"></a>Flink CEP 应用入门</h3><p>准备好依赖后，我们开始第一个 Flink CEP 应用程序，这里我们只做一个简单的数据流匹配，当匹配成功后将匹配的两条数据打印出来。首先定义实体类 Event 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后构造读取 Socket 数据流将数据进行转换成 Event，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Event&gt; eventDataStream = env.socketTextStream(<span class="string">"127.0.0.1"</span>, <span class="number">9200</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Event&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (StringUtil.isNotEmpty(s)) &#123;</span><br><span class="line">                String[] split = s.split(<span class="string">","</span>);</span><br><span class="line">                <span class="keyword">if</span> (split.length == <span class="number">2</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Event(Integer.valueOf(split[<span class="number">0</span>]), split[<span class="number">1</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>接着就是定义 CEP 中的匹配规则了，下面的规则表示第一个事件的 id 为 42，紧接着的第二个事件 id 要大于 10，满足这样的连续两个事件才会将这两条数据进行打印出来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; pattern = Pattern.&lt;Event&gt;begin(<span class="string">"start"</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">"start &#123;&#125;"</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() == <span class="number">42</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">).next(<span class="string">"middle"</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">"middle &#123;&#125;"</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() &gt;= <span class="number">10</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        log.info(<span class="string">"p = &#123;&#125;"</span>, p);</span><br><span class="line">        builder.append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getName()).append(<span class="string">"\n"</span>)</span><br><span class="line">                .append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();<span class="comment">//打印结果</span></span><br></pre></td></tr></table></figure><p>然后笔者在终端开启 Socket，输入的两条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">42,zhisheng</span><br><span class="line">20,zhisheng</span><br></pre></td></tr></table></figure><p>作业打印出来的日志如下图：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmti95wqbj222o076q3s.jpg" alt="undefined"></p><p>整个作业 print 出来的结果如下图：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtijoqsqj21z60figma.jpg" alt="undefined"></p><p>好了，一个完整的 Flink CEP 应用程序如上，相信你也能大概理解上面的代码，接着来详细的讲解一下 Flink CEP 中的 Pattern API。</p><h3 id="Pattern-API"><a href="#Pattern-API" class="headerlink" title="Pattern API"></a>Pattern API</h3><p>你可以通过 Pattern API 去定义从流数据中匹配事件的 Pattern，每个复杂 Pattern 都是由多个简单的 Pattern 组成的，拿前面入门的应用来讲，它就是由 <code>start</code> 和 <code>middle</code> 两个简单的 Pattern 组成的，在其每个 Pattern 中都只是简单的处理了流数据。在处理的过程中需要标示该 Pattern 的名称，以便后续可以使用该名称来获取匹配到的数据，如 <code>p.get(&quot;start&quot;).get(0)</code> 它就可以获取到 Pattern 中匹配的第一个事件。接下来我们先来看下简单的 Pattern 。</p><h4 id="单个-Pattern"><a href="#单个-Pattern" class="headerlink" title="单个 Pattern"></a>单个 Pattern</h4><h5 id="数量"><a href="#数量" class="headerlink" title="数量"></a>数量</h5><p>单个 Pattern 后追加的 Pattern 如果都是相同的，那如果要都重新再写一遍，换做任何人都会比较痛苦，所以就提供了 times(n) 来表示期望出现的次数，该 times() 方法还有很多写法，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//期望符合的事件出现 4 次</span></span><br><span class="line"> start.times(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望符合的事件不出现或者出现 4 次</span></span><br><span class="line"> start.times(<span class="number">4</span>).optional();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//期望符合的事件出现 2 次或者 3 次或者 4 次</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次、3 次或 4 次，并尽可能多地重复</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).greedy();</span><br><span class="line"></span><br><span class="line"><span class="comment">//期望出现 2 次、3 次、4 次或者不出现</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).optional();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 0、2、3 或 4 次并尽可能多地重复</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).optional().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件</span></span><br><span class="line"> start.oneOrMore();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件，并尽可能多地重复这些事件</span></span><br><span class="line"> start.oneOrMore().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件或者不出现</span></span><br><span class="line"> start.oneOrMore().optional();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现更多次，并尽可能多地重复或者不出现</span></span><br><span class="line"> start.oneOrMore().optional().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现两个或多个事件</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次或 2 次以上，并尽可能多地重复</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>).greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次或更多的事件，并尽可能多地重复或者不出现</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>).optional().greedy();</span><br></pre></td></tr></table></figure><h5 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h5><p>可以通过 <code>pattern.where()</code>、<code>pattern.or()</code> 或 <code>pattern.until()</code> 方法指定事件属性的条件。条件可以是 <code>IterativeConditions</code> 或<code>SimpleConditions</code>。比如 SimpleCondition 可以像下面这样使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">start.where(<span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"zhisheng"</span>.equals(value.getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h4 id="组合-Pattern"><a href="#组合-Pattern" class="headerlink" title="组合 Pattern"></a>组合 Pattern</h4><p>前面已经对单个 Pattern 做了详细对讲解，接下来讲解如何将多个 Pattern 进行组合来完成一些需求。在完成组合 Pattern 之前需要定义第一个 Pattern，然后在第一个的基础上继续添加新的 Pattern。比如定义了第一个 Pattern 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.&lt;Event&gt;begin(<span class="string">"start"</span>);</span><br></pre></td></tr></table></figure><p>接下来，可以为此指定更多的 Pattern，通过指定的不同的连接条件。比如：</p><ul><li>next()：要求比较严格，该事件一定要紧跟着前一个事件。</li><li>followedBy()：该事件在前一个事件后面就行，两个事件之间可能会有其他的事件。</li><li>followedByAny()：该事件在前一个事件后面的就满足条件，两个事件之间可能会有其他的事件，返回值比上一个多。</li><li>notNext()：不希望前一个事件后面紧跟着该事件出现。</li><li>notFollowedBy()：不希望后面出现该事件。</li></ul><p>具体怎么写呢，可以看下样例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(<span class="string">"middle"</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(<span class="string">"middle"</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDetermin = start.followedByAny(<span class="string">"middle"</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; strictNot = start.notNext(<span class="string">"not"</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxedNot = start.notFollowedBy(<span class="string">"not"</span>).where(...);</span><br></pre></td></tr></table></figure><p>可能概念讲了很多，但是还是不太清楚，这里举个例子说明一下，假设有个 Pattern 是 <code>a b</code>，给定的数据输入顺序是 <code>a c b b</code>，对于上面那种不同的连接条件可能最后返回的值不一样。</p><ol><li>a 和 b 之间使用 next() 连接，那么则返回 {}，即没有匹配到数据</li><li>a 和 b 之间使用 followedBy() 连接，那么则返回 {a, b}</li><li>a 和 b 之间使用 followedByAny() 连接，那么则返回 {a, b}, {a, b}</li></ol><p>相信通过上面的这个例子讲解你就知道了它们的区别，尤其是 followedBy() 和 followedByAny()，笔者一开始也是毕竟懵，后面也是通过代码测试才搞明白它们之间的区别的。除此之外，还可以为 Pattern 定义时间约束。例如，可以通过 <code>pattern.within(Time.seconds(10))</code> 方法定义此 Pattern 应该 10 秒内完成匹配。 该时间不仅支持处理时间还支持事件时间。另外还可以与 consecutive()、allowCombinations() 等组合，更多的请看下图中 Pattern 类的方法。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtisma1xj20we1ku77e.jpg" alt="undefined"></p><h4 id="Group-Pattern"><a href="#Group-Pattern" class="headerlink" title="Group Pattern"></a>Group Pattern</h4><p>业务需求比较复杂的场景，如果要使用 Pattern 来定义的话，可能这个 Pattern 会很长并且还会嵌套，比如由 begin、followedBy、followedByAny、next 组成和嵌套，另外还可以再和 oneOrMore()、times(#ofTimes)、times(#fromTimes, #toTimes)、optional()、consecutive()、allowCombinations() 等结合使用。效果如下面这种：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.begin(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">"start"</span>).where(...).followedBy(<span class="string">"start_middle"</span>).where(...)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">//next 表示连续</span></span><br><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">"next_start"</span>).where(...).followedBy(<span class="string">"next_middle"</span>).where(...)</span><br><span class="line">).times(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//followedBy 代表在后面就行</span></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">"followedby_start"</span>).where(...).followedBy(<span class="string">"followedby_middle"</span>).where(...)</span><br><span class="line">).oneOrMore();</span><br><span class="line"></span><br><span class="line"><span class="comment">//followedByAny</span></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDetermin = start.followedByAny(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">"followedbyany_start"</span>).where(...).followedBy(<span class="string">"followedbyany_middle"</span>).where(...)</span><br><span class="line">).optional();</span><br></pre></td></tr></table></figure><p>关于上面这些 Pattern 操作的更详细的解释可以查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/cep.html#groups-of-patterns" target="_blank" rel="noopener">官网</a>。</p><h4 id="事件匹配跳过策略"><a href="#事件匹配跳过策略" class="headerlink" title="事件匹配跳过策略"></a>事件匹配跳过策略</h4><p>对于给定组合的复杂 Pattern，有的事件可能会匹配到多个 Pattern，如果要控制将事件的匹配数，需要指定跳过策略。在 Flink CEP 中跳过策略有四种类型，如下所示：</p><ul><li>NO_SKIP：不跳过，将发出所有可能的匹配事件。</li><li>SKIP_TO_FIRST：丢弃包含 PatternName 第一个之前匹配事件的每个部分匹配。</li><li>SKIP_TO_LAST：丢弃包含 PatternName 最后一个匹配事件之前的每个部分匹配。</li><li>SKIP_PAST_LAST_EVENT：丢弃包含匹配事件的每个部分匹配。</li><li>SKIP_TO_NEXT：丢弃以同一事件开始的所有部分匹配。</li></ul><p>这几种策略都是根据 AfterMatchSkipStrategy 来实现的，可以看下它们的类结构图，如下所示：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtj2ttocj21r40iudg5.jpg" alt="undefined"></p><p>关于这几种跳过策略的具体区别可以查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/cep.html#after-match-skip-strategy" target="_blank" rel="noopener">官网</a>，至于如何使用跳过策略，其实 AfterMatchSkipStrategy 抽象类中已经提供了 5 种静态方法可以直接使用，方法如下：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtjbbl97j21f6098jrq.jpg" alt="undefined"></p><p>使用方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AfterMatchSkipStrategy skipStrategy = ...; <span class="comment">// 使用 AfterMatchSkipStrategy 调用不同的静态方法</span></span><br><span class="line">Pattern.begin(<span class="string">"start"</span>, skipStrategy);</span><br></pre></td></tr></table></figure><h3 id="检测-Pattern"><a href="#检测-Pattern" class="headerlink" title="检测 Pattern"></a>检测 Pattern</h3><p>编写好了 Pattern 之后，你需要的是将其应用在流数据中去做匹配。这时要做的就是构造一个 PatternStream，它可以通过 <code>CEP.pattern(eventDataStream, pattern)</code> 来获取一个 PatternStream 对象，在 <code>CEP.pattern()</code> 方法中，你可以选择传入两个参数（DataStream 和 Pattern），也可以选择传入三个参数 （DataStream、Pattern 和 EventComparator），因为 CEP 类中它有两个不同参数数量的 pattern 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CEP</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> PatternStream(input, pattern);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern, EventComparator&lt;T&gt; comparator)</span> </span>&#123;</span><br><span class="line">        PatternStream&lt;T&gt; stream = <span class="keyword">new</span> PatternStream(input, pattern);</span><br><span class="line">        <span class="keyword">return</span> stream.withComparator(comparator);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="选择-Pattern"><a href="#选择-Pattern" class="headerlink" title="选择 Pattern"></a>选择 Pattern</h4><p>在获取到 PatternStream 后，你可以通过 select 或 flatSelect 方法从匹配到的事件流中查询。如果使用的是 select 方法，则需要实现传入一个 PatternSelectFunction 的实现作为参数，PatternSelectFunction 具有为每个匹配事件调用的 select 方法，该方法的参数是 <code>Map&gt;</code>，这个 Map 的 key 是 Pattern 的名字，在前面入门案例中设置的 <code>start</code> 和 <code>middle</code> 在这时就起作用了，你可以通过类似 <code>get(&quot;start&quot;)</code> 方法的形式来获取匹配到 <code>start</code> 的所有事件。如果使用的是 flatSelect 方法，则需要实现传入一个 PatternFlatSelectFunction 的实现作为参数，这个和 PatternSelectFunction 不一致地方在于它可以返回多个结果，因为这个接口中的 flatSelect 方法含有一个 Collector，它可以返回多个数据到下游去。两者的样例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        builder.append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"start"</span>).get(<span class="number">0</span>).getName()).append(<span class="string">"\n"</span>)</span><br><span class="line">                .append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getId()).append(<span class="string">","</span>).append(p.get(<span class="string">"middle"</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).flatSelect(<span class="keyword">new</span> PatternFlatSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatSelect</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; map, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, List&lt;Event&gt;&gt; entry : map.entrySet()) &#123;</span><br><span class="line">            collector.collect(entry.getKey() + <span class="string">" "</span> + entry.getValue().get(<span class="number">0</span>).getId() + <span class="string">","</span> + entry.getValue().get(<span class="number">0</span>).getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><p>关于 PatternStream 中的 select 或 flatSelect 方法其实可以传入不同的参数，比如传入 OutputTag 和 PatternTimeoutFunction 去处理延迟的数据，具体查看下图。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtjjaxeaj21v60m6dhu.jpg" alt="undefined"></p><p>如果使用的 Flink CEP 版本是大于等于 1.8 的话，还可以使用 process 方法，在上图中也可以看到在 PatternStream 类中包含了该方法。要使用 process 的话，得传入一个 PatternProcessFunction 的实现作为参数，在该实现中需要重写 processMatch 方法。使用 PatternProcessFunction 比使用 PatternSelectFunction 和 PatternFlatSelectFunction 更好的是，它支持获取应用的的上下文，那么也就意味着它可以访问时间（因为 Context 接口继承自 TimeContext 接口）。另外如果要处理延迟的数据可以与 TimedOutPartialMatchHandler 接口的实现类一起使用。</p><h3 id="CEP-时间属性"><a href="#CEP-时间属性" class="headerlink" title="CEP 时间属性"></a>CEP 时间属性</h3><h4 id="根据事件时间处理延迟数据"><a href="#根据事件时间处理延迟数据" class="headerlink" title="根据事件时间处理延迟数据"></a>根据事件时间处理延迟数据</h4><p>在 CEP 中，元素处理的顺序很重要，当时间策略设置为事件时间时，为了确保能够按照事件时间的顺序来处理元素，先来的事件会暂存在缓冲区域中，然后对缓冲区域中的这些事件按照事件时间进行排序，当水印到达时，比水印时间小的事件会按照顺序依次处理的。这意味着水印之间的元素是按照事件时间顺序处理的。</p><p>注意：当作业设置的时间属性是事件时间是，CEP 中会认为收到的水印时间是正确的，会严格按照水印的时间来处理元素，从而保证能顺序的处理元素。另外对于这种延迟的数据（和 3.5 节中的延迟数据类似），CEP 中也是支持通过 side output 设置 OutputTag 标签来将其收集。使用方式如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PatternStream&lt;Event&gt; patternStream = CEP.pattern(inputDataStream, pattern);</span><br><span class="line"></span><br><span class="line">OutputTag&lt;String&gt; lateDataOutputTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">"late-data"</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;ComplexEvent&gt; result = patternStream</span><br><span class="line">    .sideOutputLateData(lateDataOutputTag)</span><br><span class="line">    .select(</span><br><span class="line">        <span class="keyword">new</span> PatternSelectFunction&lt;Event, ComplexEvent&gt;() &#123;...&#125;</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; lateData = result.getSideOutput(lateDataOutputTag);</span><br></pre></td></tr></table></figure><h4 id="时间上下文"><a href="#时间上下文" class="headerlink" title="时间上下文"></a>时间上下文</h4><p>在 PatternProcessFunction 和 IterativeCondition 中可以通过 TimeContext 访问当前正在处理的事件的时间（Event Time）和此时机器上的时间（Processing Time）。你可以查看到这两个类中都包含了 Context，而这个 Context 继承自 TimeContext，在 TimeContext 接口中定义了获取事件时间和处理时间的方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TimeContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="小结与反思-24"><a href="#小结与反思-24" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节开始通过一个 Flink CEP 案例教大家上手，后面通过讲解 Flink CEP 的 Pattern API，更多详细的还是得去看官网文档，其实也建议大家好好的跟着官网的文档过一遍所有的 API，并跟着敲一些样例来实现，这样在开发需求的时候才能够及时的想到什么场景下该使用哪种 API，接着教了大家如何将 Pattern 与数据流结合起来匹配并获取匹配的数据，最后讲了下 CEP 中的时间概念。</p><p>你公司有使用 Flink CEP 吗？通常使用哪些 API 居多？</p><p>本节涉及代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-libraries/flink-learning-libraries-cep" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-libraries/flink-learning-libraries-cep</a></p><h2 id="二十七、Flink-扩展库——State-Processor-API"><a href="#二十七、Flink-扩展库——State-Processor-API" class="headerlink" title="二十七、Flink 扩展库——State Processor API"></a>二十七、Flink 扩展库——State Processor API</h2><h3 id="State-Processor-API-介绍"><a href="#State-Processor-API-介绍" class="headerlink" title="State Processor API 介绍"></a>State Processor API 介绍</h3><p>能够从外部访问 Flink 作业的状态一直用户迫切需要的功能之一，在 Apache Flink 1.9.0 中新引入了 State Processor API，该 API 让用户可以通过 Flink DataSet 作业来灵活读取、写入和修改 Flink 的 Savepoint 和 Checkpoint。</p><h3 id="在-Flink-1-9-之前是如何处理状态的？"><a href="#在-Flink-1-9-之前是如何处理状态的？" class="headerlink" title="在 Flink 1.9 之前是如何处理状态的？"></a>在 Flink 1.9 之前是如何处理状态的？</h3><p>一般来说，大多数的 Flink 作业都是有状态的，并且随着作业运行的时间越来越久，就会累积越多越多的状态，如果因为故障导致作业崩溃可能会导致作业的状态都丢失，那么对于比较重要的状态来说，损失就会很大。为了保证作业状态的一致性和持久性，Flink 从一开始使用的就是 Checkpoint 和 Savepoint 来保存状态，并且可以从 Savepoint 中恢复状态。在 Flink 的每个新 Release 版本中，Flink 社区添加了越来越多与状态相关的功能以提高 Checkpoint 的速度和恢复速度。</p><p>有的时候，用户可能会有这些需求场景，比如从第三方外部系统访问作业的状态、将作业的状态信息迁移到另一个应用程序等，目前现有支持查询作业状态的功能 Queryable State，但是在 Flink 中目前该功能只支持根据 Key 查找，并且不能保证返回值的一致性。另外该功能不支持添加和修改作业的状态，所以适用的场景还是比较有限。</p><h3 id="使用-State-Processor-API-读写作业状态"><a href="#使用-State-Processor-API-读写作业状态" class="headerlink" title="使用 State Processor API 读写作业状态"></a>使用 State Processor API 读写作业状态</h3><p>在 1.9 版本中的 State Processor API，它完全和之前不一致，该功能使用 InputFormat 和 OutputFormat 扩展了 DataSet API 以读取和写入 Checkpoint 和 Savepoint 数据。由于 DataSet 和 Table API 的互通性，所以也可以使用 Table 或者 SQL API 查询和分析状态的数据。例如，再获取到正在运行的流作业状态的 Checkpoint 后，可以使用 DataSet 批处理程序对其进行分析，以验证该流作业的运行是否正确。另外 State Processor API 还可以修复不一致的状态信息，它提供了很多方法来开发有状态的应用程序，这些方法在以前的版本中因为设计的问题导致作业在启动后不能再修改，否则状态可能会丢失。现在，你可以任意修改状态的数据类型、调整算子的最大并行度、拆分或合并算子的状态、重新分配算子的 uid 等。</p><h3 id="使用-DataSet-读取作业状态"><a href="#使用-DataSet-读取作业状态" class="headerlink" title="使用 DataSet 读取作业状态"></a>使用 DataSet 读取作业状态</h3><p>State Processor API 将作业的状态映射到一个或多个可以单独处理的数据集，为了能够使用该 API，需要先了解这个映射的工作方式，首先来看下有状态的 Flink 作业是什么样子的。Flink 作业是由很多算子组成，通常是一个或多个数据源（Source）、一些实际处理数据的算子（比如 Map／Filter／FlatMap 等）和一个或者多个 Sink。每个算子会在一个或者多个任务中并行运行（取决于并行度），并且可以使用不同类型的状态，算子可能会有零个、一个或多个 Operator State，这些状态会组成一个以算子任务为范围的列表。如果是算子应用在 KeyedStream，它还有零个、一个或者多个 Keyed State，它们的作用域范围是从每个已处理数据中提取 Key，可以将 Keyed State 看作是一个分布式的 Map。</p><p>State Processor API 现在提供了读取、新增和修改 Savepoint 数据的方法，比如从已加载的 Savepoint 中读取数据集，然后将数据集转换为状态并将其保存到 Savepoint。下面分别讲解下这三种方法该如何使用。</p><h4 id="读取现有的-Savepoint"><a href="#读取现有的-Savepoint" class="headerlink" title="读取现有的 Savepoint"></a>读取现有的 Savepoint</h4><p>读取状态首先需要指定一个 Savepoint（或者 Checkpoint） 的路径和状态后端存储的类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment bEnv   = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">ExistingSavepoint savepoint = Savepoint.load(bEnv, <span class="string">"hdfs://path/"</span>, <span class="keyword">new</span> RocksDBStateBackend());</span><br></pre></td></tr></table></figure><p>读取 Operator State 时，只需指定算子的 uid、状态名称和类型信息。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; listState  = savepoint.readListState(<span class="string">"zhisheng-uid"</span>, <span class="string">"list-state"</span>, Types.INT);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Integer&gt; unionState = savepoint.readUnionState(<span class="string">"zhisheng-uid"</span>, <span class="string">"union-state"</span>, Types.INT);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Integer, Integer&gt;&gt; broadcastState = savepoint.readBroadcastState(<span class="string">"zhisheng-uid"</span>, <span class="string">"broadcast-state"</span>, Types.INT, Types.INT);</span><br></pre></td></tr></table></figure><p>如果在状态描述符（StateDescriptor）中使用了自定义类型序列化器 TypeSerializer，也可以指定它：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; listState = savepoint.readListState(</span><br><span class="line">    <span class="string">"zhisheng-uid"</span>, <span class="string">"list-state"</span>, </span><br><span class="line">    Types.INT, <span class="keyword">new</span> MyCustomIntSerializer());</span><br></pre></td></tr></table></figure><p>当读取 Keyed State 时，用户可以指定 KeyedStateReaderFunction 来读取任意列和复杂的状态类型，例如 ListState，MapState 和 AggregatingState。这意味着如果算子包含了有状态的处理函数，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StatefulFunctionWithTime</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Void</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">   ValueState&lt;Integer&gt; state;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> </span>&#123;</span><br><span class="line">      ValueStateDescriptor&lt;Integer&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"state"</span>, Types.INT);</span><br><span class="line">      state = getRuntimeContext().getState(stateDescriptor);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer value, Context ctx, Collector&lt;Void&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      state.update(value + <span class="number">1</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后可以通过定义输出类型和相应的 KeyedStateReaderFunction 进行读取上面的状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KeyedState</span> </span>&#123;</span><br><span class="line">  Integer key;</span><br><span class="line">  Integer value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReaderFunction</span> <span class="keyword">extends</span> <span class="title">KeyedStateReaderFunction</span>&lt;<span class="title">Integer</span>, <span class="title">KeyedState</span>&gt; </span>&#123;</span><br><span class="line">  ValueState&lt;Integer&gt; state;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> </span>&#123;</span><br><span class="line">     ValueStateDescriptor&lt;Integer&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"state"</span>, Types.INT);</span><br><span class="line">     state = getRuntimeContext().getState(stateDescriptor);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readKey</span><span class="params">(Integer key, Context ctx, Collector&lt;KeyedState&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     KeyedState data = <span class="keyword">new</span> KeyedState();</span><br><span class="line">     data.key    = key;</span><br><span class="line">     data.value  = state.value();</span><br><span class="line">     out.collect(data);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DataSet&lt;KeyedState&gt; keyedState = savepoint.readKeyedState(<span class="string">"zhisheng-uid"</span>, <span class="keyword">new</span> ReaderFunction());</span><br></pre></td></tr></table></figure><p>注意：使用 KeyedStateReaderFunction 时，状态描述器（StateDescriptor）必须在 open 方法中注册，否则 RuntimeContext#getState，RuntimeContext#getListState 或 RuntimeContext#getMapState 将导致 RuntimeException。</p><h4 id="写入新的-Savepoint"><a href="#写入新的-Savepoint" class="headerlink" title="写入新的 Savepoint"></a>写入新的 Savepoint</h4><p>写入新的 Savepoint 主要是基于下面三个接口：</p><ul><li>StateBootstrapFunction：用于写入未分区的 Operator State</li><li>BroadcastStateBootstrapFunction：用于写入 Broadcast State</li><li>KeyedStateBootstrapFunction：用于写入 Keyed State</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>  <span class="class"><span class="keyword">class</span> <span class="title">Account</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span> amount;    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AccountBootstrapper</span> <span class="keyword">extends</span> <span class="title">KeyedStateBootstrapFunction</span>&lt;<span class="title">Integer</span>, <span class="title">Account</span>&gt; </span>&#123;</span><br><span class="line">    ValueState&lt;Double&gt; state;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> </span>&#123;</span><br><span class="line">        ValueStateDescriptor&lt;Double&gt; descriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"total"</span>,Types.DOUBLE);</span><br><span class="line">        state = getRuntimeContext().getState(descriptor);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Account value, Context ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        state.update(value.amount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment bEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Account&gt; accountDataSet = bEnv.fromCollection(accounts);</span><br><span class="line"></span><br><span class="line">BootstrapTransformation&lt;Account&gt; transformation = OperatorTransformation</span><br><span class="line">    .bootstrapWith(accountDataSet)</span><br><span class="line">    .keyBy(acc -&gt; acc.id)</span><br><span class="line">    .transform(<span class="keyword">new</span> AccountBootstrapper());</span><br></pre></td></tr></table></figure><p>该 KeyedStateBootstrapFunction 函数支持设置事件时间和处理时间的定时器，定时器不会在该函数中触发，只有在 DataStream 作业中还原后才会激活，如果设置了处理时间的定时器，但是该处理时间已经过期了，那么在恢复作业的时候会立即触发。一旦创建了一个或者多个算子，可以将它们合并为一个 Savepoint。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Savepoint</span><br><span class="line">    .create(backend, <span class="number">128</span>)</span><br><span class="line">    .withOperator(<span class="string">"uid1"</span>, transformation1)</span><br><span class="line">    .withOperator(<span class="string">"uid2"</span>, transformation2)</span><br><span class="line">    .write(savepointPath);</span><br></pre></td></tr></table></figure><h4 id="修改现有的-Savepoint"><a href="#修改现有的-Savepoint" class="headerlink" title="修改现有的 Savepoint"></a>修改现有的 Savepoint</h4><p>除了可以从头开始创建 Savepoint 之外，还可以基于现有的 Savepoint，例如在为现有作业添加新的算子。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Savepoint</span><br><span class="line">    .load(backend, oldPath)</span><br><span class="line">    .withOperator(<span class="string">"uid"</span>, transformation)</span><br><span class="line">    .write(newPath);</span><br></pre></td></tr></table></figure><p>删除或者覆盖现有 Savepoint 中的算子状态，并将其写入。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Savepoint</span><br><span class="line">    .removeOperator(oldOperatorUid)</span><br><span class="line">    .withOperator(oldOperatorUid, transformation)</span><br><span class="line">    .write(path)</span><br></pre></td></tr></table></figure><h3 id="为什么要使用-DataSet-API？"><a href="#为什么要使用-DataSet-API？" class="headerlink" title="为什么要使用 DataSet API？"></a>为什么要使用 DataSet API？</h3><p>社区一直在想将批和流统一，所以在未来 DataSet API 可能会废弃，那么为啥 State Processor API 还要基于 DataSet API 开发呢？这是因为社区在设计这个功能的时候，对 DataStream API 和 Table API 做了评估对比，但没有一个能满足需求的，而又因为 State Processor API 功能对于 Flink API 的进一步发展有至关重要的作用，因此社区决定在 DataSet API 构建 State Processor API 功能，但是尽可能的降低了对 DataSet API 的依赖性，方便后续迁移到其他的 API 中。</p><h3 id="小结与反思-25"><a href="#小结与反思-25" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 1.9 中的 State Processor API 的概念和如何使用，以及该功能的设计背景及需求。有关更多详细信息，请参见 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-43%3A+State+Processor+API" target="_blank" rel="noopener">FLIP-43</a>。对于使用 DataSet API 来完成该功能，你有什么更好的解决方案吗？</p><h2 id="二十八、Flink-扩展库——Machine-Learning"><a href="#二十八、Flink-扩展库——Machine-Learning" class="headerlink" title="二十八、Flink 扩展库——Machine Learning"></a>二十八、Flink 扩展库——Machine Learning</h2><h3 id="Flink-ML-介绍"><a href="#Flink-ML-介绍" class="headerlink" title="Flink-ML 介绍"></a>Flink-ML 介绍</h3><p>ML 是 Machine Learning 的简称，Flink-ML 是 Flink 的机器学习类库。在 Flink 1.9 之前该类库是存在 <code>flink-libraries</code> 模块下的，但是在 Flink 1.9 版本中，为了支持 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-39+Flink+ML+pipeline+and+ML+libs" target="_blank" rel="noopener">FLIP-39</a> ，所以该类库被移除了。</p><p>建立 FLIP-39 的目的主要是增强 Flink-ML 的可伸缩性和易用性。通常使用机器学习的有两类人，一类是机器学习算法库的开发者，他们需要一套标准的 API 来实现算法，每个机器学习算法会在这些 API 的基础上实现；另一类用户是直接利用这些现有的机器学习算法库去训练数据模型，整个训练是要通过很多转换或者算法才能完成的，所以如果能够提供 ML Pipeline，那么对于后一类用户来说绝对是一种福音。虽然在 1.9 中移除了之前的 Flink-ML 模块，但是在 Flink 项目下出现了一个 <code>flink-ml-parent</code> 的模块，该模块有两个子模块 <code>flink-ml-api</code> 和 <code>flink-ml-lib</code>。</p><p><code>flink-ml-api</code> 模块增加了 ML Pipeline 和 MLLib 的接口，它的类结构图如下：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtkk6yflj21cm0si0t4.jpg" alt="undefined"></p><ul><li>Transformer: Transformer 是一种可以将一个表转换成另一个表的算法</li><li>Model: Model 是一种特别的 Transformer，它继承自 Transformer。它通常是由 Estimator 生成，Model 用于推断，输入一个数据表会生成结果表。</li><li>Estimator: Estimator 是一个可以根据一个数据表生成一个模型的算法。</li><li>Pipeline: Pipeline 描述的是机器学习的工作流，它将很多 Transformer 和 Estimator 连接在一起成一个工作流。</li><li>PipelineStage: PipelineStage 是 Pipeline 的基础节点，Transformer 和 Estimator 两个都继承自 PipelineStage 接口。</li><li>Params: Params 是一个参数容器。</li><li>WithParams: WithParams 有一个保存参数的 Params 容器。通常会使用在 PipelineStage 里面，因为几乎所有的算法都需要参数。</li></ul><p>Flink-ML 的 pipeline 流程如下：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtkr17o7j21x00s4abc.jpg" alt="undefined"></p><p><code>flink-ml-lib</code> 模块包括了 DenseMatrix、DenseVector、SparseVector 等类的基本操作。这两个模块是 Flink-ML 的基础模块，相信社区在后面的稳定版本一定会带来更加完善的 Flink-ML 库。</p><h3 id="如何使用-Flink-ML？"><a href="#如何使用-Flink-ML？" class="headerlink" title="如何使用 Flink-ML？"></a>如何使用 Flink-ML？</h3><p>虽然在 Flink 1.9 中已经移除了 Flink-ML 模块，但是在之前的版本还是支持的，如果你们公司使用的是低于 1.9 的版本，那么还是可以使用的，在使用之前引入依赖（假设使用的是 Flink 1.8 版本）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-ml_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>另外如果是要运行的话还是要将 opt 目录下的 flink-ml_2.11-1.8.0.jar 移到 lib 目录下。下面演示下如何训练多元线性回归模型：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//带标签的特征向量</span></span><br><span class="line"><span class="keyword">val</span> trainingData: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = ...</span><br><span class="line"><span class="keyword">val</span> testingData: <span class="type">DataSet</span>[<span class="type">Vector</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataSet: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = ...</span><br><span class="line"><span class="comment">//使用 Splitter 将数据集拆分成训练数据和测试数据</span></span><br><span class="line"><span class="keyword">val</span> trainTestData: <span class="type">DataSet</span>[<span class="type">TrainTestDataSet</span>] = <span class="type">Splitter</span>.trainTestSplit(dataSet)</span><br><span class="line"><span class="keyword">val</span> trainingData: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = trainTestData.training</span><br><span class="line"><span class="keyword">val</span> testingData: <span class="type">DataSet</span>[<span class="type">Vector</span>] = trainTestData.testing.map(lv =&gt; lv.vector)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mlr = <span class="type">MultipleLinearRegression</span>()</span><br><span class="line">  .setStepsize(<span class="number">1.0</span>)</span><br><span class="line">  .setIterations(<span class="number">100</span>)</span><br><span class="line">  .setConvergenceThreshold(<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">mlr.fit(trainingData)</span><br><span class="line"></span><br><span class="line"><span class="comment">//已经形成的模型可以用来预测数据了</span></span><br><span class="line"><span class="keyword">val</span> predictions: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = mlr.predict(testingData)</span><br></pre></td></tr></table></figure><h3 id="Flink-ML-Pipeline-使用"><a href="#Flink-ML-Pipeline-使用" class="headerlink" title="Flink-ML Pipeline 使用"></a>Flink-ML Pipeline 使用</h3><p>之前前面也讲解了 Pipeline 在 Flink-ML 的含义，那么下面演示一下如何通过 Flink-ML 构建一个 Pipeline 作业：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> trainingData: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = ...</span><br><span class="line"><span class="keyword">val</span> testingData: <span class="type">DataSet</span>[<span class="type">Vector</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> scaler = <span class="type">StandardScaler</span>()</span><br><span class="line"><span class="keyword">val</span> polyFeatures = <span class="type">PolynomialFeatures</span>().setDegree(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> mlr = <span class="type">MultipleLinearRegression</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Construct pipeline of standard scaler, polynomial features and multiple linear regression</span></span><br><span class="line"><span class="comment">//构建标准定标器、多项式特征和多元线性回归的流水线</span></span><br><span class="line"><span class="keyword">val</span> pipeline = scaler.chainTransformer(polyFeatures).chainPredictor(mlr)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Train pipeline</span></span><br><span class="line">pipeline.fit(trainingData)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Calculate predictions</span></span><br><span class="line"><span class="keyword">val</span> predictions: <span class="type">DataSet</span>[<span class="type">LabeledVector</span>] = pipeline.predict(testingData)</span><br></pre></td></tr></table></figure><h3 id="小结与反思-26"><a href="#小结与反思-26" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节主要讲了下 Flink-ML 的发展以及为啥在 Flink 1.9 移除该库，并且介绍了其内部的接口和库函数，另外通过两个简短的代码讲解了下如何使用 Flink-ML。如果想了解更多 Flink-ML 的知识可以查看官网。</p><p><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-39+Flink+ML+pipeline+and+ML+libs" target="_blank" rel="noopener">FLIP-39</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/libs/ml/" target="_blank" rel="noopener">Flink-ML</a></p><h2 id="二十九、Flink-扩展库——Gelly"><a href="#二十九、Flink-扩展库——Gelly" class="headerlink" title="二十九、Flink 扩展库——Gelly"></a>二十九、Flink 扩展库——Gelly</h2><h3 id="Gelly-是什么？"><a href="#Gelly-是什么？" class="headerlink" title="Gelly 是什么？"></a>Gelly 是什么？</h3><p>Gelly 是 Flink 的图 API 库，它包含了一组旨在简化 Flink 中图形分析应用程序开发的方法和实用程序。在 Gelly 中，可以使用类似于批处理 API 提供的高级函数来转换和修改图。Gelly 提供了创建、转换和修改图的方法以及图算法库。</p><h3 id="如何使用-Gelly？"><a href="#如何使用-Gelly？" class="headerlink" title="如何使用 Gelly？"></a>如何使用 Gelly？</h3><p>因为 Gelly 是 Flink 项目中库的一部分，它本身不在 Flink 的二进制包中，所以运行 Gelly 项目（Java 应用程序）是需要将 <code>opt/flink-gelly_2.11-1.9.0.jar</code> 移动到 <code>lib</code> 目录中，如果是 Scala 应用程序则需要将 <code>opt/flink-gelly-scala_2.11-1.9.0.jar</code> 移动到 <code>lib</code> 中，接着运行下面的命令就可以运行一个 flink-gelly-examples 项目。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run examples/gelly/flink-gelly-examples_2.11-1.9.0.jar \</span><br><span class="line">    --algorithm GraphMetrics --order directed \</span><br><span class="line">    --input RMatGraph --type integer --scale 20 --simplify directed \</span><br><span class="line">    --output print</span><br></pre></td></tr></table></figure><p>接下来可以在 UI 上看到运行的结果：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtlbb5k5j22j01fijux.jpg" alt="undefined"></p><p>如果是自己创建的 Gelly Java 应用程序，则需要添加如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-gelly_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果是 Gelly Scala 应用程序，添加下面的依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-gelly-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Gelly-API"><a href="#Gelly-API" class="headerlink" title="Gelly API"></a>Gelly API</h3><h4 id="Graph-介绍"><a href="#Graph-介绍" class="headerlink" title="Graph 介绍"></a>Graph 介绍</h4><p>在 Gelly 中，一个图（Graph）由顶点的数据集（DataSet）和边的数据集（DataSet）组成。图中的顶点由 Vertex 类型来表示，一个 Vertex 由唯一的 ID 和一个值来表示。其中 Vertex 的 ID 必须是全局唯一的值，且实现了 Comparable 接口。如果节点不需要由任何值，则该值类型可以声明成 NullValue 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 Vertex&lt;Long，String&gt;</span></span><br><span class="line">Vertex&lt;Long, String&gt; v = <span class="keyword">new</span> Vertex&lt;Long, String&gt;(<span class="number">1L</span>, <span class="string">"foo"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个 Vertex&lt;Long，NullValue&gt;</span></span><br><span class="line">Vertex&lt;Long, NullValue&gt; v = <span class="keyword">new</span> Vertex&lt;Long, NullValue&gt;(<span class="number">1L</span>, NullValue.getInstance());</span><br></pre></td></tr></table></figure><p>Graph 中的边由 Edge 类型来表示，一个 Edge 通常由源顶点的 ID，目标顶点的 ID 以及一个可选的值来表示。其中源顶点和目标顶点的类型必须与 Vertex 的 ID 类型相同。同样的，如果边不需要由任何值，则该值类型可以声明成 NullValue 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Edge&lt;Long, Double&gt; e = <span class="keyword">new</span> Edge&lt;Long, Double&gt;(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">0.5</span>);</span><br><span class="line"><span class="comment">//反转此 edge 的源和目标</span></span><br><span class="line">Edge&lt;Long, Double&gt; reversed = e.reverse();</span><br><span class="line">Double weight = e.getValue(); <span class="comment">// weight = 0.5</span></span><br></pre></td></tr></table></figure><p>在 Gelly 中，一个 Edge 总是从源顶点指向目标顶点。如果图中每条边都能匹配一个从目标顶点到源顶点的 Edge，那么这个图可能是个无向图。同样地，无向图可以用这个方式来表示。</p><h4 id="创建-Graph"><a href="#创建-Graph" class="headerlink" title="创建 Graph"></a>创建 Graph</h4><p>可以通过以下几种方式创建一个 Graph：</p><ul><li>从一个 Edge 数据集合和一个 Vertex 数据集合中创建图。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Vertex&lt;String, Long&gt;&gt; vertices = ...</span><br><span class="line">DataSet&lt;Edge&lt;String, Double&gt;&gt; edges = ...</span><br><span class="line"></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromDataSet(vertices, edges, env);</span><br></pre></td></tr></table></figure><ul><li>从一个表示边的 Tuple2 数据集合中创建图。Gelly 会将每个 Tuple2 转换成一个 Edge，其中第一个元素表示源顶点的 ID，第二个元素表示目标顶点的 ID，图中的顶点和边的 value 值均被设置为 NullValue。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; edges = ...</span><br><span class="line"></span><br><span class="line">Graph&lt;String, NullValue, NullValue&gt; graph = Graph.fromTuple2DataSet(edges, env);</span><br></pre></td></tr></table></figure><ul><li>从一个 Tuple3 数据集和一个可选的 Tuple2 数据集中生成图。在这种情况下，Gelly 会将每个 Tuple3 转换成 Edge，其中第一个元素域是源顶点 ID，第二个域是目标顶点 ID，第三个域是边的值。同样的，每个 Tuple2 会转换成一个顶点 Vertex，其中第一个域是顶点的 ID，第二个域是顶点的 value。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Long&gt;&gt; vertexTuples = env.readCsvFile(<span class="string">"path/to/vertex/input"</span>).types(String.class, Long.class);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple3&lt;String, String, Double&gt;&gt; edgeTuples = env.readCsvFile(<span class="string">"path/to/edge/input"</span>).types(String.class, String.class, Double.class);</span><br><span class="line"></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromTupleDataSet(vertexTuples, edgeTuples, env);</span><br></pre></td></tr></table></figure><ul><li>从一个表示边数据的CSV文件和一个可选的表示节点的CSV文件中生成图。在这种情况下，Gelly会将表示边的CSV文件中的每一行转换成一个Edge，其中第一个域表示源顶点ID，第二个域表示目标顶点ID，第三个域表示边的值。同样的，表示节点的CSV中的每一行都被转换成一个Vertex，其中第一个域表示顶点的ID，第二个域表示顶点的值。为了通过GraphCsvReader生成图，需要指定每个域的类型，可以使用 types、edgeTypes、vertexTypes、keyType 中的方法。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个具有字符串 Vertex id、Long Vertex 和双边缘的图</span></span><br><span class="line">Graph&lt;String, Long, Double&gt; graph = Graph.fromCsvReader(<span class="string">"path/to/vertex/input"</span>, <span class="string">"path/to/edge/input"</span>, env)</span><br><span class="line">                    .types(String.class, Long.class, Double.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个既没有顶点值也没有边值的图</span></span><br><span class="line">Graph&lt;Long, NullValue, NullValue&gt; simpleGraph = Graph.fromCsvReader(<span class="string">"path/to/edge/input"</span>, env).keyType(Long.class);</span><br></pre></td></tr></table></figure><ul><li>从一个边的集合和一个可选的顶点的集合中生成图。如果在图创建的时候顶点的集合没有传入，Gelly 会依据数据的边数据集合自动地生成一个 Vertex 集合。这种情况下，创建的节点是没有值的。或者也可以像下面一样，在创建图的时候提供一个 MapFunction 方法来初始化节点的值。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Vertex&lt;Long, Long&gt;&gt; vertexList = <span class="keyword">new</span> ArrayList...</span><br><span class="line"></span><br><span class="line">List&lt;Edge&lt;Long, String&gt;&gt; edgeList = <span class="keyword">new</span> ArrayList...</span><br><span class="line"></span><br><span class="line">Graph&lt;Long, Long, String&gt; graph = Graph.fromCollection(vertexList, edgeList, env);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将顶点值初始化为顶点ID</span></span><br><span class="line">Graph&lt;Long, Long, String&gt; graph = Graph.fromCollection(edgeList,</span><br><span class="line">                <span class="keyword">new</span> MapFunction&lt;Long, Long&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">map</span><span class="params">(Long value)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> value;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;, env);</span><br></pre></td></tr></table></figure><h4 id="Graph-属性"><a href="#Graph-属性" class="headerlink" title="Graph 属性"></a>Graph 属性</h4><p>Gelly 提供了下列方法来查询图的属性和指标：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Vertex&lt;K, VV&gt;&gt; getVertices()</span><br><span class="line"><span class="comment">//获取边缘数据集</span></span><br><span class="line">DataSet&lt;Edge&lt;K, EV&gt;&gt; getEdges()</span><br><span class="line"><span class="comment">//获取顶点的 id 数据集</span></span><br><span class="line"><span class="function">DataSet&lt;K&gt; <span class="title">getVertexIds</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, K&gt;&gt; <span class="title">getEdgeIds</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">inDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">outDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Tuple2&lt;K, LongValue&gt;&gt; <span class="title">getDegrees</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">numberOfVertices</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">numberOfEdges</span><span class="params">()</span></span></span><br><span class="line"><span class="function">DataSet&lt;Triplet&lt;K, VV, EV&gt;&gt; <span class="title">getTriplets</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><h4 id="Graph-转换"><a href="#Graph-转换" class="headerlink" title="Graph 转换"></a>Graph 转换</h4><ul><li>Map：Gelly 提供了专门的用于转换顶点值和边值的方法。mapVertices 和 mapEdges 会返回一个新图，图中的每个顶点和边的 ID 不会改变，但是顶点和边的值会根据用户自定义的映射方法进行修改。这些映射方法同时也可以修改顶点和边的值的类型。</li><li>Translate：Gelly 还提供了专门用于根据用户定义的函数转换顶点和边的 ID 和值的值及类型的方法（translateGraphIDs/translateVertexValues/translateEdgeValues），是Map 功能的升级版，因为 Map 操作不支持修订顶点和边的 ID。</li><li>Filter：Gelly 支持在图中的顶点上或边上执行一个用户指定的 filter 转换。filterOnEdges 会根据提供的在边上的断言在原图的基础上生成一个新的子图，注意，顶点的数据不会被修改。同样的 filterOnVertices 在原图的顶点上进行 filter 转换，不满足断言条件的源节点或目标节点会在新的子图中移除。该子图方法支持同时对顶点和边应用 filter 函数。</li><li><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtlj8x4dj20wv0mgmxv.jpg" alt="undefined"></li><li>Reverse：Gelly中得reverse()方法用于在原图的基础上，生成一个所有边方向与原图相反的新图。</li><li>Undirected：在前面的内容中，我们提到过，Gelly中的图通常都是有向的，而无向图可以通过对所有边添加反向的边来实现，出于这个目的，Gelly提供了getUndirected()方法，用于获取原图的无向图。</li><li>Union：Gelly的union()操作用于联合当前图和指定的输入图，并生成一个新图，在输出的新图中，相同的节点只保留一份，但是重复的边会保留。如下图所示：</li><li><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtlubuj9j20kx0jimxq.jpg" alt="undefined"></li><li>Difference：Gelly提供了difference()方法用于发现当前图与指定的输入图之间的差异。</li><li>Intersect：Gelly提供了intersect()方法用于发现两个图中共同存在的边，并将相同的边以新图的方式返回。相同的边指的是具有相同的源顶点，相同的目标顶点和相同的边值。返回的新图中，所有的节点没有任何值，如果需要节点值，可以使用joinWithVertices()方法去任何一个输入图中检索。</li></ul><h4 id="Graph-变化"><a href="#Graph-变化" class="headerlink" title="Graph 变化"></a>Graph 变化</h4><p>Gelly 内置下列方法以支持对一个图进行节点和边的增加/移除操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addVertex</span><span class="params">(<span class="keyword">final</span> Vertex&lt;K, VV&gt; vertex)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addVertices</span><span class="params">(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToAdd)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addEdge</span><span class="params">(Vertex&lt;K, VV&gt; source, Vertex&lt;K, VV&gt; target, EV edgeValue)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">addEdges</span><span class="params">(List&lt;Edge&lt;K, EV&gt;&gt; newEdges)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeVertex</span><span class="params">(Vertex&lt;K, VV&gt; vertex)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeVertices</span><span class="params">(List&lt;Vertex&lt;K, VV&gt;&gt; verticesToBeRemoved)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeEdge</span><span class="params">(Edge&lt;K, EV&gt; edge)</span></span></span><br><span class="line"><span class="function">Graph&lt;K, VV, EV&gt; <span class="title">removeEdges</span><span class="params">(List&lt;Edge&lt;K, EV&gt;&gt; edgesToBeRemoved)</span></span></span><br></pre></td></tr></table></figure><h4 id="Neighborhood-Methods"><a href="#Neighborhood-Methods" class="headerlink" title="Neighborhood Methods"></a>Neighborhood Methods</h4><p>邻接方法允许每个顶点针对其所有的邻接顶点或边执行某个集合操作。reduceOnEdges() 可以用于计算顶点所有邻接边的值的集合。reduceOnNeighbors() 可以用于计算邻接顶点的值的集合。这些方法采用联合和交换集合，并在内部利用组合器，显著提高了性能。邻接的范围由 EdgeDirection 来确定，它有三个枚举值，分别是：IN/OUT/ALL，其中 IN 只考虑所有入的邻接边和顶点，OUT 只考虑所有出的邻接边和顶点，而 ALL 考虑所有的邻接边和顶点。举个例子，如下图所示，假设我们想要知道图中出度最小的边权重。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtm2cbjej20hf0au3yl.jpg" alt="undefined"></p><p>下列代码会为每个节点找到出的边集合，然后在集合的基础上执行一个用户定义的方法 SelectMinWeight()。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Graph&lt;Long, Long, Double&gt; graph = ...</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Long, Double&gt;&gt; minWeights = graph.reduceOnEdges(<span class="keyword">new</span> SelectMinWeight(), </span><br><span class="line">EdgeDirection.OUT);</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">SelectMinWeight</span> <span class="keyword">implements</span> <span class="title">ReduceEdgesFunction</span>&lt;<span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Double <span class="title">reduceEdges</span><span class="params">(Double firstEdgeValue, Double secondEdgeValue)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> Math.min(firstEdgeValue, secondEdgeValue);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtmbyzbaj20f70d93yv.jpg" alt="undefined"></p><p>同样的，假设我们需要知道每个顶点的所有邻接边上的权重的值之和，不考虑方向。可以用下面的代码来实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Graph&lt;Long, Long, Double&gt; graph = ...</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Long, Long&gt;&gt; verticesWithSum = graph.reduceOnNeighbors(<span class="keyword">new</span> SumValues(), </span><br><span class="line">EdgeDirection.IN);</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">SumValues</span> <span class="keyword">implements</span> <span class="title">ReduceNeighborsFunction</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">reduceNeighbors</span><span class="params">(Long firstNeighbor, Long secondNeighbor)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> firstNeighbor + secondNeighbor;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果如下图所示：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtmjwr99j20nj0axt9b.jpg" alt="undefined"></p><h4 id="Graph-验证"><a href="#Graph-验证" class="headerlink" title="Graph 验证"></a>Graph 验证</h4><p>Gelly 提供了一个简单的工具用于对输入的图进行校验操作。由于应用程序上下文的不同，根据某些标准，有些图可能有效，也可能无效。例如用户需要校验图中是否包含重复的边。为了校验一个图，可以定义一个定制的 GraphValidator 并实现它的 validate() 方法。InvalidVertexIdsValidator 是 Gelly 预定义的一个校验器，用来校验边上所有的顶点 ID 是否有效，即边上的顶点 ID 在顶点集合中存在。</p><h3 id="小结与反思-27"><a href="#小结与反思-27" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节开始对 Gelly 做了个简单的介绍，然后教大家如何使用 Gelly，接着介绍了 Gelly API，更多关于 Gelly 可以查询<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/gelly/" target="_blank" rel="noopener">官网</a>。 你们公司有什么场景在用该库开发吗？</p><h2 id="三十、Flink-配置详解及如何配置高可用？"><a href="#三十、Flink-配置详解及如何配置高可用？" class="headerlink" title="三十、Flink 配置详解及如何配置高可用？"></a>三十、Flink 配置详解及如何配置高可用？</h2><p>在讲解 7.2 节中如何部署 Flink 作业之前，希望能够再细讲下 Flink 中的配置，虽然在 2.2 节中简单讲解过。</p><h3 id="Flink-配置详解"><a href="#Flink-配置详解" class="headerlink" title="Flink 配置详解"></a>Flink 配置详解</h3><h4 id="flink-conf-yaml"><a href="#flink-conf-yaml" class="headerlink" title="flink-conf.yaml"></a>flink-conf.yaml</h4><p><strong>基础配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># jobManager 的IP地址</span><br><span class="line">jobmanager.rpc.address: localhost</span><br><span class="line"></span><br><span class="line"># JobManager 的端口号</span><br><span class="line">jobmanager.rpc.port: 6123</span><br><span class="line"></span><br><span class="line"># JobManager JVM heap 内存大小</span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line"></span><br><span class="line"># TaskManager JVM heap 内存大小</span><br><span class="line">taskmanager.heap.size: 1024m</span><br><span class="line"></span><br><span class="line"># 每个 TaskManager 提供的任务 slots 数量大小</span><br><span class="line"></span><br><span class="line">taskmanager.numberOfTaskSlots: 1</span><br><span class="line"></span><br><span class="line"># 程序默认并行计算的个数</span><br><span class="line">parallelism.default: 1</span><br><span class="line"></span><br><span class="line"># 文件系统来源</span><br><span class="line"># fs.default-scheme</span><br></pre></td></tr></table></figure><p><strong>高可用性配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 可以选择 &apos;NONE&apos; 或者 &apos;zookeeper&apos;.</span><br><span class="line"># high-availability: zookeeper</span><br><span class="line"></span><br><span class="line"># 文件系统路径，让 Flink 在高可用性设置中持久保存元数据</span><br><span class="line"># high-availability.storageDir: hdfs:///flink/ha/</span><br><span class="line"></span><br><span class="line"># zookeeper 集群中仲裁者的机器 ip 和 port 端口号</span><br><span class="line"># high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line"></span><br><span class="line"># 默认是 open，如果 zookeeper security 启用了该值会更改成 creator</span><br><span class="line"># high-availability.zookeeper.client.acl: open</span><br></pre></td></tr></table></figure><p><strong>容错和检查点配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 用于存储和检查点状态</span><br><span class="line"># state.backend: filesystem</span><br><span class="line"></span><br><span class="line"># 存储检查点的数据文件和元数据的默认目录</span><br><span class="line"># state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints</span><br><span class="line"></span><br><span class="line"># savepoints 的默认目标目录(可选)</span><br><span class="line"># state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints</span><br><span class="line"></span><br><span class="line"># 用于启用/禁用增量 checkpoints 的标志</span><br><span class="line"># state.backend.incremental: false</span><br></pre></td></tr></table></figure><p><strong>Web 前端配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 基于 Web 的运行时监视器侦听的地址.</span><br><span class="line">#jobmanager.web.address: 0.0.0.0</span><br><span class="line"></span><br><span class="line">#  Web 的运行时监视器端口</span><br><span class="line">rest.port: 8081</span><br><span class="line"></span><br><span class="line"># 是否从基于 Web 的 jobmanager 启用作业提交</span><br><span class="line"># jobmanager.web.submit.enable: false</span><br></pre></td></tr></table></figure><p><strong>高级配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># io.tmp.dirs: /tmp</span><br><span class="line"></span><br><span class="line"># 是否应在 TaskManager 启动时预先分配 TaskManager 管理的内存</span><br><span class="line"># taskmanager.memory.preallocate: false</span><br><span class="line"></span><br><span class="line"># 类加载解析顺序，是先检查用户代码 jar（“child-first”）还是应用程序类路径（“parent-first”）。 默认设置指示首先从用户代码 jar 加载类</span><br><span class="line"># classloader.resolve-order: child-first</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 用于网络缓冲区的 JVM 内存的分数。 这决定了 TaskManager 可以同时拥有多少流数据交换通道以及通道缓冲的程度。 如果作业被拒绝或者您收到系统没有足够缓冲区的警告，请增加此值或下面的最小/最大值。 另请注意，“taskmanager.network.memory.min”和“taskmanager.network.memory.max”可能会覆盖此分数</span><br><span class="line"></span><br><span class="line"># taskmanager.network.memory.fraction: 0.1</span><br><span class="line"># taskmanager.network.memory.min: 67108864</span><br><span class="line"># taskmanager.network.memory.max: 1073741824</span><br></pre></td></tr></table></figure><p><strong>Flink 集群安全配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 指示是否从 Kerberos ticket 缓存中读取</span><br><span class="line"># security.kerberos.login.use-ticket-cache: true</span><br><span class="line"></span><br><span class="line"># 包含用户凭据的 Kerberos 密钥表文件的绝对路径</span><br><span class="line"># security.kerberos.login.keytab: /path/to/kerberos/keytab</span><br><span class="line"></span><br><span class="line"># 与 keytab 关联的 Kerberos 主体名称</span><br><span class="line"># security.kerberos.login.principal: flink-user</span><br><span class="line"></span><br><span class="line"># 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）</span><br><span class="line"># security.kerberos.login.contexts: Client,KafkaClient</span><br></pre></td></tr></table></figure><p><strong>ZooKeeper 安全配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 覆盖以下配置以提供自定义 ZK 服务名称</span><br><span class="line"># zookeeper.sasl.service-name: zookeeper</span><br><span class="line"></span><br><span class="line"># 该配置必须匹配 &quot;security.kerberos.login.contexts&quot; 中的列表（含有一个）</span><br><span class="line"># zookeeper.sasl.login-context-name: Client</span><br></pre></td></tr></table></figure><p><strong>HistoryServer</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 你可以通过 bin/historyserver.sh (start|stop) 命令启动和关闭 HistoryServer</span><br><span class="line"></span><br><span class="line"># 将已完成的作业上传到的目录</span><br><span class="line"># jobmanager.archive.fs.dir: hdfs:///completed-jobs/</span><br><span class="line"></span><br><span class="line"># 基于 Web 的 HistoryServer 的地址</span><br><span class="line"># historyserver.web.address: 0.0.0.0</span><br><span class="line"></span><br><span class="line"># 基于 Web 的 HistoryServer 的端口号</span><br><span class="line"># historyserver.web.port: 8082</span><br><span class="line"></span><br><span class="line"># 以逗号分隔的目录列表，用于监视已完成的作业</span><br><span class="line"># historyserver.archive.fs.dir: hdfs:///completed-jobs/</span><br><span class="line"></span><br><span class="line"># 刷新受监控目录的时间间隔（以毫秒为单位）</span><br><span class="line"># historyserver.archive.fs.refresh-interval: 10000</span><br></pre></td></tr></table></figure><h4 id="masters"><a href="#masters" class="headerlink" title="masters"></a>masters</h4><p>以 host:port 构成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br></pre></td></tr></table></figure><h4 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a>slaves</h4><p>里面是每个 worker 节点的 IP/Hostname，每一个 worker 结点之后都会运行一个 TaskManager，一个一行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure><h3 id="Log-配置"><a href="#Log-配置" class="headerlink" title="Log 配置"></a>Log 配置</h3><p>在 Flink 的日志配置文件（<code>logback.xml</code> 或 <code>log4j.properties</code>）中有配置日志存储的地方，<code>logback.xml</code> 配置日志存储的路径是：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"file"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.FileAppender"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;log.file&#125;<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">append</span>&gt;</span>false<span class="tag">&lt;/<span class="name">append</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;60&#125; %X&#123;sourceThread&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>log4j.properties</code> 和 <code>log4j-cli.properties</code> 的配置日志存储的路径是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log4j.appender.file.file=$&#123;log.file&#125;</span><br></pre></td></tr></table></figure><p>从上面两个配置可以看到日志的路径都是由 <code>log.file</code> 变量控制的，如果系统变量没有配置的话，则会使用 <code>bin／flink</code> 脚本里配置的值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-client-$HOSTNAME.log</span><br><span class="line">log_setting=(-Dlog.file=&quot;$log&quot; -Dlog4j.configuration=file:&quot;$FLINK_CONF_DIR&quot;/log4j-cli.properties -Dlogback.configurationFile=file:&quot;$FLINK_CONF_DIR&quot;/logback.xml)</span><br></pre></td></tr></table></figure><p>从上面可以看到 log 里配置的 FLINK<em>LOG</em>DIR 变量是在 bin 目录下的 config.sh 里初始化的。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_FLINK_LOG_DIR=<span class="variable">$FLINK_HOME_DIR_MANGLED</span>/<span class="built_in">log</span></span><br><span class="line">KEY_ENV_LOG_DIR=<span class="string">"env.log.dir"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;FLINK_LOG_DIR&#125;</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">    FLINK_LOG_DIR=$(readFromConfig <span class="variable">$&#123;KEY_ENV_LOG_DIR&#125;</span> <span class="string">"<span class="variable">$&#123;DEFAULT_FLINK_LOG_DIR&#125;</span>"</span> <span class="string">"<span class="variable">$&#123;YAML_CONF&#125;</span>"</span>)</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>从上面可以知道日志默认就是在 Flink 的 log 目录下，你可以通过在 <code>flink-conf.yaml</code> 配置文件中配置 <code>env.log.dir</code> 参数来更改保存日志的目录。另外通过源码可以发现，如果找不到 <code>log.file</code> 环境变量，则会去找 <code>web.log.path</code> 的配置，但是该配置在 Standalone 下是不起作用的，日志依旧是会在 <code>log</code> 目录，在 YARN 下是会起作用的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LogFileLocation <span class="title">find</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> String logEnv = <span class="string">"log.file"</span>;</span><br><span class="line">    String logFilePath = System.getProperty(logEnv);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (logFilePath == <span class="keyword">null</span>) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"Log file environment variable '&#123;&#125;' is not set."</span>, logEnv);</span><br><span class="line">        logFilePath = config.getString(WebOptions.LOG_PATH); <span class="comment">//该值为 web.log.path</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// not configured, cannot serve log files</span></span><br><span class="line">    <span class="keyword">if</span> (logFilePath == <span class="keyword">null</span> || logFilePath.length() &lt; <span class="number">4</span>) &#123;</span><br><span class="line">        LOG.warn(<span class="string">"JobManager log files are unavailable in the web dashboard. "</span> +</span><br><span class="line">            <span class="string">"Log file location not found in environment variable '&#123;&#125;' or configuration key '&#123;&#125;'."</span>,</span><br><span class="line">            logEnv, WebOptions.LOG_PATH);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LogFileLocation(<span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    String outFilePath = logFilePath.substring(<span class="number">0</span>, logFilePath.length() - <span class="number">3</span>).concat(<span class="string">"out"</span>);</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"Determined location of main cluster component log file: &#123;&#125;"</span>, logFilePath);</span><br><span class="line">    LOG.info(<span class="string">"Determined location of main cluster component stdout file: &#123;&#125;"</span>, outFilePath);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> LogFileLocation(resolveFileLocation(logFilePath), resolveFileLocation(outFilePath));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The log file location (may be in /log for standalone but under log directory when using YARN).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ConfigOption&lt;String&gt; LOG_PATH =</span><br><span class="line">    key(<span class="string">"web.log.path"</span>)</span><br><span class="line">        .noDefaultValue()</span><br><span class="line">        .withDeprecatedKeys(<span class="string">"jobmanager.web.log.path"</span>)</span><br><span class="line">        .withDescription(<span class="string">"Path to the log file (may be in /log for standalone but under log directory when using YARN)."</span>);</span><br></pre></td></tr></table></figure><p>另外可能会在本地 IDE 中运行作业出不来日志的情况，这时请检查是否有添加日志的依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.25<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-simple<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.25<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="如何配置-JobManager-高可用？"><a href="#如何配置-JobManager-高可用？" class="headerlink" title="如何配置 JobManager 高可用？"></a>如何配置 JobManager 高可用？</h3><p>JobManager 协调每个 Flink 作业的部署，它负责调度和资源管理。默认情况下，每个 Flink 集群只有一个 JobManager 实例，这样就可能会产生单点故障，如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。如果保证 JobManager 的高可用，则可以避免这个问题。下面分别下如何搭建 Standalone 集群和 YARN 集群高可用的 JobManager。</p><h4 id="搭建-Standalone-集群高可用-JobManager"><a href="#搭建-Standalone-集群高可用-JobManager" class="headerlink" title="搭建 Standalone 集群高可用 JobManager"></a>搭建 Standalone 集群高可用 JobManager</h4><p>Standalone 集群的 JobManager 高可用性的概念是：任何时候只有一个主 JobManager 和多个备 JobManager，以便在主节点失败时有新的 JobManager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别，每个 JobManager 都可以充当主备节点。例如，请考虑以下三个 JobManager 实例的设置。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtn9nfy2j21540nsmzg.jpg" alt="undefined"></p><p><strong>如何配置</strong></p><p>要启用 JobManager 高可用性功能，首先必须在配置文件 flink-conf.yaml 中将高可用性模式设置为 ZooKeeper，配置 ZooKeeper quorum，将所有 JobManager 主机及其 Web UI 端口写入配置文件。每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 ZooKeeper。另外就是高可用存储目录，JobManager 元数据保存在 <code>high-availability.storageDir</code> 指定的文件系统中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS、S3、Ceph、NFS 等，该文件系统中保存了 JobManager 恢复状态需要的所有元数据。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181</span><br><span class="line">high-availability.storageDir: hdfs:///flink/ha/</span><br></pre></td></tr></table></figure><p>Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。 它在我们的 Flink 安装路径下面 /conf/zoo.cfg 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line"># dataDir=/tmp/zookeeper</span><br><span class="line">clientPort=2181</span><br><span class="line"># ZooKeeper quorum peers</span><br><span class="line"># 下面这个配置 ZK 地址</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line"># server.2=host:peer-port:leader-port</span><br></pre></td></tr></table></figure><p>要启动 HA 集群，请配置 masters 文件，该文件包含启动 JobManager 的所有主机以及 Web 用户界面绑定的端口，一行写一个。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">xxx.xxx.xxx.xxx:8081</span><br></pre></td></tr></table></figure><p>默认情况下，JobManager 选一个随机端口作为进程通信端口，可以通过 high-availability.jobmanager.port 更改此设置。此配置接受单个端口（例如 <code>50010</code>），范围（<code>50000-50025</code>）或两者的组合（<code>50010,50011,50020-50025,50050-50075</code>）。</p><p><strong>启动</strong></p><p>配置好了之后的示例如下，假设是配置两个 JobManager 的 Standalone 的集群，在 flink-conf.yaml 中配置高可用模式和 Zookeeper 如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">high-availability.storageDir: hdfs:///flink/recovery</span><br></pre></td></tr></table></figure><p>masters 中配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">localhost:8082</span><br></pre></td></tr></table></figure><p>在 zoo.cfg 中配置 Zookeeper 服务如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure><p>启动 ZooKeeper 集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure><p>启动一个 Flink HA 集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-cluster.sh</span><br><span class="line">Starting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting taskmanager daemon on host localhost.</span><br></pre></td></tr></table></figure><p>停止 ZooKeeper 和集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ bin/stop-cluster.sh</span><br><span class="line">Stopping taskmanager daemon (pid: 7647) on localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7495) on host localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7349) on host localhost.</span><br><span class="line"></span><br><span class="line">$ bin/stop-zookeeper-quorum.sh</span><br><span class="line">Stopping zookeeper daemon (pid: 7101) on host localhost.</span><br></pre></td></tr></table></figure><h4 id="搭建-YARN-集群高可用-JobManager"><a href="#搭建-YARN-集群高可用-JobManager" class="headerlink" title="搭建 YARN 集群高可用 JobManager"></a>搭建 YARN 集群高可用 JobManager</h4><p>当在 YARN 上配置高可用的 JobManager 时，它只会运行一个 JobManager 实例，不会运行多个，该 JobManager 实例失败时，YARN 会将其重新启动。Yarn 的具体行为取决于使用的 YARN 版本。</p><p><strong>如何配置</strong></p><p>在 YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    The maximum number of application master execution attempts.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>当前 YARN 版本的默认值为 2（表示允许单个 JobManager 失败两次）。除了上面可以配置最大重试次数外，还可以在 flink-conf.yaml 配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure><p>这意味着在如果程序启动失败，YARN 会再重试 9 次（9 次重试 + 1 次启动），如果启动 10 次作业还失败，YARN 才会将该任务的状态置为失败。如果因为节点硬件故障或重启，NodeManager 重新同步等操作，需要 YARN 继续尝试启动应用。这些重启尝试不计入 yarn.application-attempts 个数中。</p><p><strong>容器关闭行为</strong></p><ul><li>YARN 2.3.0 &lt; 版本 &lt; 2.4.0. 如果 application master 进程失败，则所有的 container 都会重启。</li><li>YARN 2.4.0 &lt; 版本 &lt; 2.6.0. TaskManager container 在 application master 故障期间，会继续工作。这具有以下优点：作业恢复时间更快，且缩短所有 TaskManager 启动时申请资源的时间。</li><li>YARN 2.6.0 &lt;= version: 将尝试失败有效性间隔设置为 Flink 的 Akka 超时值。尝试失败有效性间隔表示只有在系统在一个间隔期间看到最大应用程序尝试次数后才会终止应用程序，这避免了持久的工作会耗尽它的应用程序尝试。</li></ul><p><strong>启动</strong></p><p>配置好了的示例如下，首先在 flink-conf.yaml 配置 HA 模式和 Zookeeper 集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure><p>在 zoo.cfg 配置 ZooKeeper 服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure><p>启动 Zookeeper 集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure><p>启动 HA 集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/yarn-session.sh -n 2</span><br></pre></td></tr></table></figure><h3 id="小结与反思-28"><a href="#小结与反思-28" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节一开始对 Flink 的所有配置文件做了一个详细的介绍，分析了每种配置的作用和使用场景，然后介绍了 Flink 中的日志配置，最后讲解了下 JobManager 的高可用配置。</p><h2 id="三十一、Flink-Job-如何在-Standalone、YARN、Mesos、K8S-上部署运行？"><a href="#三十一、Flink-Job-如何在-Standalone、YARN、Mesos、K8S-上部署运行？" class="headerlink" title="三十一、Flink Job 如何在 Standalone、YARN、Mesos、K8S 上部署运行？"></a>三十一、Flink Job 如何在 Standalone、YARN、Mesos、K8S 上部署运行？</h2><p>前面内容已经有很多学习案列带大家正式使用了 Flink，其中不仅有讲将 Flink 应用程序在 IDEA 中运行，也有讲将 Flink Job 编译打包上传到 Flink UI 上运行，在这 UI 背后可能是 YARN、Mesos、Kubernetes。那么这节就系统讲下如何部署和运行我们的 Flink Job，大家可以根据自己公司的场景进行选择！</p><p>在讲解完 Flink 中的配置后，接下来接着讲解 Flink 的部署情况。</p><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>第一种方式就是 Standalone 模式，这种模式笔者在前面 2.2 节里面演示的就是这种，我们通过执行命令：<code>./bin/start-cluster.sh</code> 启动一个 Flink Standalone 集群。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ./bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><p>默认的话是启动一个 Job Manager 和一个 Task Manager，我们可以通过 <code>jps</code> 查看进程有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">65425 Jps</span><br><span class="line">51572 TaskManagerRunner</span><br><span class="line">51142 StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure><p>其中上面的 TaskManagerRunner 代表的是 Task Manager 进程，StandaloneSessionClusterEntrypoint 代表的是 Job Manager 进程。上面运行产生的只有一个 Job Manager 和一个 Task Manager，如果是生产环境的话，这样的配置肯定是不够运行多个 Job 的，那么我们该如何在生产环境中配置 standalone 模式的集群呢？我们就需要修改 Flink 安装目录下面的 conf 文件夹里面配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink-conf.yaml</span><br><span class="line">masters</span><br><span class="line">slaves</span><br></pre></td></tr></table></figure><p>将 slaves 中再添加一个 <code>localhost</code>，这样就可以启动两个 Task Manager 了。接着启动脚本 <code>start-cluster.sh</code>，启动日志显示如下：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmto1qmioj21ai0ragox.jpg" alt="undefined"></p><p>可以看见有两个 Task Manager 启动了，再看下 UI 显示的：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmto7hmowj228o1em0ue.jpg" alt="undefined"></p><p>那么如果还想要添加一个 Job Manager 或者 Task Manager 怎么办？总不能再次重启修改配置文件后然后再重启吧！这里你可以这样操作。</p><p><strong>增加一个 Job Manager</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/jobmanager.sh ((start|start-foreground) [host] [webui-port])|stop|stop-all</span><br></pre></td></tr></table></figure><p>但是注意 Standalone 下最多只能运行一个 Job Manager。</p><p><strong>增加一个 Task Manager</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/taskmanager.sh start|start-foreground|stop|stop-all</span><br></pre></td></tr></table></figure><p>比如我执行了 <code>./bin/taskmanager.sh start</code> 命令后：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtoe82ifj21n20nsjuo.jpg" alt="undefined"></p><p>Standalone 模式下可以先对 Flink Job 通过 <code>mvn clean package</code> 编译打包，得到 Jar 包后，可以在 UI 上直接上传 Jar 包，然后点击 Submit 就可以运行了。</p><h3 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h3><p>Flink 不仅仅支持以 standalone 模式运行，还支持在 YARN 上运行，YARN 是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</p><p>相当于 standalone 模式，Flink on YARN 有如下好处：</p><ul><li>资源按需使用，提高集群的资源利用率</li><li>基于 YARN 调度系统，能够自动处理各个角色的 failover（Job Manager 进程异常退出，Yarn ResourceManager 会重新调度 Job Manager 到其他机器；如果 Task Manager 进程异常退出，Job Manager 收到消息后并重新向 Yarn ResourceManager 申请资源，重新启动 Task Manager）</li></ul><p>下图是 Flink on YARN 的架构图：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtolmjvrj227c188myo.jpg" alt="undefined"></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">官网</a> 对 Flink On YARN 讲解很多，包含 Flink 在 YARN 上的安装方式、 Flink YARN Session 和怎么允许单一的 Flink Job、怎么查看在 YARN 上查看日志，已经非常全了，大家可以多多参考官网。</p><h3 id="Mesos"><a href="#Mesos" class="headerlink" title="Mesos"></a>Mesos</h3><p>Apache Mesos 诞生于 UC Berkeley 的一个研究项目，现已成为 Apache Incubator 中的项目。Apache Mesos 把自己定位成一个数据中心操作系统，它能管理上万台的从机。Framework 相当于这个操作系统的应用程序，每当应用程序需要执行，Framework 就会在 Mesos 中选择一台有合适资源（CPU、内存等）的从机来运行。</p><p>Flink 也是支持在 Mesos 上部署运行的，在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/deployment/mesos.html" target="_blank" rel="noopener">官网</a>也有介绍，主要是讲 Flink 运行在 DC/OS（它是具有复杂应用程序管理层的 Mesos 分支，预装了Marathon），如果安装好了 DC/OS 的话，那么你可以使用它的 CLI 工具来安装 Flink，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dcos package install flink</span><br></pre></td></tr></table></figure><p>在 Mesos 上运行 Flink 有两种方式：Flink 会话集群（session cluster）和作业集群（job cluster）。</p><h4 id="会话集群"><a href="#会话集群" class="headerlink" title="会话集群"></a>会话集群</h4><p>Flink 会话集群需要在运行的 Mesos 上部署执行，然后你可以在一个会话集群上运行多个 Flink 作业，在部署会话集群之后，需要将每个作业提交给集群。在 Flink 的安装目录 bin 下，你可以找到 2 个在 Mesos 上启动 Flink 的脚本。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtotrb60j21tg11wjyi.jpg" alt="undefined"></p><ul><li>mesos-appmaster.sh：它将启动 Mesos 应用程序主程序，会注册 Mesos 调度程序，负责启动工作节点</li><li>mesos-taskmanager.sh：Mesos 进程的入口点，你不需要手动执行该脚本，它由 Mesos 工作节点自动启动来启动新的 Task Manager。</li></ul><p>在运行 mesos-appmaster.sh 脚本之前，你需要在 flink-conf.yaml 中定义 mesos.master 配置或者通过启动参数 <code>-Dmesos.master=...</code> 传给进程。当执行 mesos-appmaster.sh 后，它会在执行该脚本的机器上创建一个 Job Manager，另外就是 Task Manager 将作为 Mesos 集群中的任务来运行。</p><h4 id="作业集群"><a href="#作业集群" class="headerlink" title="作业集群"></a>作业集群</h4><p>Flink 作业集群是一个运行单个作业的专用集群，不需要额外的作业提交。在 Flink 的安装目录 bin 下面有 <code>mesos-appmaster-job.sh</code> 脚本，该脚本会启动 Mesos 应用程序主程序，会注册 Mesos 调度程序，检索到 JobGraph 后相应的启动 Task Manager。</p><p>在执行 mesos-appmaster-job.sh 脚本之前，你需要在 flink-conf.yaml 中定义 mesos.master 和 internal.jobgraph-path 或者你可以通过启动参数 <code>-Dmesos.master=... -Dinterval.jobgraph-path=...</code> 传入进程。</p><p>你可以这样获取到 JobGraph：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> JobGraph jobGraph = env.getStreamGraph().getJobGraph();</span><br><span class="line">jobGraph.setAllowQueuedScheduling(<span class="keyword">true</span>);</span><br><span class="line"><span class="keyword">final</span> String jobGraphFilename = <span class="string">"job.graph"</span>;</span><br><span class="line">File jobGraphFile = <span class="keyword">new</span> File(jobGraphFilename);</span><br><span class="line"><span class="keyword">try</span> (FileOutputStream output = <span class="keyword">new</span> FileOutputStream(jobGraphFile);</span><br><span class="line">    ObjectOutputStream obOutput = <span class="keyword">new</span> ObjectOutputStream(output))&#123;</span><br><span class="line">    obOutput.writeObject(jobGraph);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通常你可以像下面这样全部通过启动参数来传入配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/mesos-appmaster.sh \</span><br><span class="line">    -Dmesos.master=master.foobar.org:5050 \</span><br><span class="line">    -Djobmanager.heap.mb=1024 \</span><br><span class="line">    -Djobmanager.rpc.port=6123 \</span><br><span class="line">    -Drest.port=8081 \</span><br><span class="line">    -Dmesos.resourcemanager.tasks.mem=4096 \</span><br><span class="line">    -Dtaskmanager.heap.mb=3500 \</span><br><span class="line">    -Dtaskmanager.numberOfTaskSlots=2 \</span><br><span class="line">    -Dparallelism.default=10</span><br></pre></td></tr></table></figure><p>更多关于 Flink On Mesos 的安装以及配置可以访问 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/deployment/mesos.html" target="_blank" rel="noopener">官网</a> 查看。</p><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p>Kubernetes（k8s）是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。它是一个完备的分布式系统支撑平台，具有完备的集群管理能力，多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和发现机制、內建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制以及多粒度的资源配额管理能力。同时 Kubernetes 提供完善的管理工具，涵盖了包括开发、部署测试、运维监控在内的各个环节。</p><p>因为 Kubernetes 的好处这么多，所以现在好多公司也开始使用 Kubernetes，那么 Flink 也有必要支持部署在 Kubernetes 上，在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/deployment/kubernetes.html" target="_blank" rel="noopener">官方文档</a>里面也介绍了两种部署的方法：</p><ul><li>Flink session cluster</li><li>Flink job cluster</li></ul><p>下面我演示一下如何部署 Flink session cluster 在 K8s 上。首先你需要创建 jobmanager-service、jobmanager-deployment、taskmanager-deployment，在利用 kubectl 命令创建之前你需要分别创建这几个 yaml 文件。</p><ul><li>jobmanager-service.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">    component:</span> <span class="string">jobmanager</span></span><br></pre></td></tr></table></figure><ul><li>jobmanager-deployment.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-jobmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        component:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink:latest</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6123</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6124</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">blob</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ui</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">JOB_MANAGER_RPC_ADDRESS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">flink-jobmanager</span></span><br></pre></td></tr></table></figure><ul><li>taskmanager-deployment.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">flink-taskmanager</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">flink</span></span><br><span class="line"><span class="attr">        component:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">flink:latest</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6121</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6122</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">rpc</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">6125</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">query</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">JOB_MANAGER_RPC_ADDRESS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">flink-jobmanager</span></span><br></pre></td></tr></table></figure><p>创建好这三个文件后，分别执行下面三个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f jobmanager-service.yaml</span><br><span class="line"></span><br><span class="line">kubectl create -f jobmanager-deployment.yaml</span><br><span class="line"></span><br><span class="line">kubectl create -f taskmanager-deployment.yaml</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtp2sjpgj20zd0b5jt7.jpg" alt="undefined"></p><p>然后去 K8s 的 Dashboard 上面查看 Flink 的情况：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtp80dnqj2272178gpu.jpg" alt="undefined"></p><p>我们如果要看 Flink 自带的 UI 的话需要将端口映射一下，使用如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl port-forward service/flink-jobmanager 8081:8081</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtpeamnvj20xf06odgf.jpg" alt="undefined"></p><p>然后访问 <a href="http://localhost:8081/" target="_blank" rel="noopener">http://localhost:8081</a> 就可以看到 Flink 自带的 UI 了：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtppizgyj227a140di3.jpg" alt="undefined"></p><p>如果我们要提交 Job 的话，我们先用命令行来操作一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -d -m localhost:8081 ~/word-count.jar</span><br></pre></td></tr></table></figure><p>执行完命令后的话，就可以去页面看到刚才提交的 Job 了：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtq005q8j22820z8gqi.jpg" alt="undefined"></p><p>另外你也可以通过 Flink UI 上传 Jar 包把 Job run 起来。这里再对上面这几个配置文件进行讲解：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtq5ku6jj20uu0wwjtf.jpg" alt="undefined"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtqczparj20uy0w8abs.jpg" alt="undefined"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtqhdds5j20zq0rgq4f.jpg" alt="undefined"></p><p>启动完了之后的话，如果你想删除就需要使用下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f jobmanager-deployment.yaml</span><br><span class="line"></span><br><span class="line">kubectl delete -f taskmanager-deployment.yaml</span><br><span class="line"></span><br><span class="line">kubectl delete -f jobmanager-service.yaml</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtqn7g0cj20z108ymyd.jpg" alt="undefined"></p><h3 id="小结与反思-29"><a href="#小结与反思-29" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>这部分介绍了下 Flink 的所有配置文件及其配置文件中的参数的作用，然后讲解了 Flink 的多种部署方式，比如 Standalone、YARN、Mesos、Kubernetes。每种方式的差异性还不小，如果你们公司没有使用类似 YARN、Mesoss、K8S 等分布式调度系统，那么只好使用 Standalone 的 Flink 集群了，这种模式比较简单，可以直接使用 Flink 自带的 UI 上传作业 Jar 包，不像和 YARN、K8S 这种一样会有多种方式供你选择。根据平时在社群里面的答疑，目前将 Flink 部署在 YARN 上的是非常多的，Flink on K8S 好多公司也处于在调研阶段。其实具体该选择哪种方式运行 Flink，最主要还是得看自己公司的架构选型和允许分配的资源（人力 &amp; 机器资源 &amp; 作业的数量）。</p><p>附：Confluence 上有个 FLIP 讲 Flink 的部署，感兴趣可以看看。</p><ul><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="noopener">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc</a></li></ul><h2 id="三十二、如何实时监控-Flink-和你的-Job？"><a href="#三十二、如何实时监控-Flink-和你的-Job？" class="headerlink" title="三十二、如何实时监控 Flink 和你的 Job？"></a>三十二、如何实时监控 Flink 和你的 Job？</h2><p>当将 Flink Job Manager、Task Manager 都运行起来了，并且也部署了不少 Flink Job，那么它到底是否还在运行、运行的状态如何、资源 Task Manager 和 Slot 的个数是否足够、Job 内部是否出现异常、计算速度是否跟得上数据生产的速度 等这些问题其实对我们来说是比较关注的，所以就很迫切的需要一个监控系统帮我们把整个 Flink 集群的运行状态给展示出来。通过监控系统我们能够很好的知道 Flink 内部的整个运行状态，然后才能够根据项目生产环境遇到的问题 ‘对症下药’。下面分别来讲下 Job Manager、TaskManager、Flink Job 的监控以及最关心的一些监控指标。</p><h3 id="监控-Job-Manager"><a href="#监控-Job-Manager" class="headerlink" title="监控 Job Manager"></a>监控 Job Manager</h3><p>我们知道 Job Manager 是 Flink 集群的中控节点，类似于 Apache Storm 的 Nimbus 以及 Apache Spark 的 Driver 的角色。它负责作业的调度、作业 Jar 包的管理、Checkpoint 的协调和发起、与 Task Manager 之间的心跳检查等工作。如果 Job Manager 出现问题的话，就会导致作业 UI 信息查看不了，Task Manager 和所有运行的作业都会受到一定的影响，所以这也是为啥在 7.1 节中强调 Job Manager 的高可用问题。</p><p>在 Flink 自带的 UI 上 Job Manager 那个 Tab 展示的其实并没有显示其对应的 Metrics，那么对于 Job Manager 来说常见比较关心的监控指标有哪些呢？</p><h4 id="基础指标"><a href="#基础指标" class="headerlink" title="基础指标"></a>基础指标</h4><p>因为 Flink Job Manager 其实也是一个 Java 的应用程序，那么它自然也会有 Java 应用程序的指标，比如内存、CPU、GC、类加载、线程信息等。</p><ul><li>内存：内存又分堆内存和非堆内存，在 Flink 中还有 Direct 内存，每种内存又有初始值、使用值、最大值等指标，因为在 Job Manager 中的工作其实相当于 Task Manager 来说比较少，也不存储事件数据，所以通常 Job Manager 占用的内存不会很多，在 Flink Job Manager 中自带的内存 Metrics 指标有：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_Status_JVM_Memory_Direct_Count</span><br><span class="line">jobmanager_Status_JVM_Memory_Direct_MemoryUsed</span><br><span class="line">jobmanager_Status_JVM_Memory_Direct_TotalCapacity</span><br><span class="line">jobmanager_Status_JVM_Memory_Heap_Committed</span><br><span class="line">jobmanager_Status_JVM_Memory_Heap_Max</span><br><span class="line">jobmanager_Status_JVM_Memory_Heap_Used</span><br><span class="line">jobmanager_Status_JVM_Memory_Mapped_Count</span><br><span class="line">jobmanager_Status_JVM_Memory_Mapped_MemoryUsed</span><br><span class="line">jobmanager_Status_JVM_Memory_Mapped_TotalCapacity</span><br><span class="line">jobmanager_Status_JVM_Memory_NonHeap_Committed</span><br><span class="line">jobmanager_Status_JVM_Memory_NonHeap_Max</span><br><span class="line">jobmanager_Status_JVM_Memory_NonHeap_Used</span><br></pre></td></tr></table></figure><ul><li>CPU：Job Manager 分配的 CPU 使用情况，如果使用类似 K8S 等资源调度系统，则需要对每个容器进行设置资源，比如 CPU 限制不能超过多少，在 Flink Job Manager 中自带的 CPU 指标有：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_Status_JVM_CPU_Load</span><br><span class="line">jobmanager_Status_JVM_CPU_Time</span><br></pre></td></tr></table></figure><ul><li>GC：GC 信息对于 Java 应用来说是避免不了的，每种 GC 都有时间和次数的指标可以供参考，提供的指标有：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Count</span><br><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_MarkSweep_Time</span><br><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Count</span><br><span class="line">jobmanager_Status_JVM_GarbageCollector_PS_Scavenge_Time</span><br></pre></td></tr></table></figure><h4 id="Checkpoint-指标"><a href="#Checkpoint-指标" class="headerlink" title="Checkpoint 指标"></a>Checkpoint 指标</h4><p>因为 Job Manager 负责了作业的 Checkpoint 的协调和发起功能，所以 Checkpoint 相关的指标就有表示 Checkpoint 执行的时间、Checkpoint 的时间长短、完成的 Checkpoint 的次数、Checkpoint 失败的次数、Checkpoint 正在执行 Checkpoint 的个数等，其对应的指标如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_job_lastCheckpointAlignmentBuffered</span><br><span class="line">jobmanager_job_lastCheckpointDuration</span><br><span class="line">jobmanager_job_lastCheckpointExternalPath</span><br><span class="line">jobmanager_job_lastCheckpointRestoreTimestamp</span><br><span class="line">jobmanager_job_lastCheckpointSize</span><br><span class="line">jobmanager_job_numberOfCompletedCheckpoints</span><br><span class="line">jobmanager_job_numberOfFailedCheckpoints</span><br><span class="line">jobmanager_job_numberOfInProgressCheckpoints</span><br><span class="line">jobmanager_job_totalNumberOfCheckpoints</span><br></pre></td></tr></table></figure><h4 id="重要的指标"><a href="#重要的指标" class="headerlink" title="重要的指标"></a>重要的指标</h4><p>另外还有比较重要的指标就是 Flink UI 上也提供的，类似于 Slot 总共个数、Slot 可使用的个数、Task Manager 的个数（通过查看该值可以知道是否有 Task Manager 发生异常重启）、正在运行的作业数量、作业运行的时间和完成的时间、作业的重启次数，对应的指标如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jobmanager_job_uptime</span><br><span class="line">jobmanager_numRegisteredTaskManagers</span><br><span class="line">jobmanager_numRunningJobs</span><br><span class="line">jobmanager_taskSlotsAvailable</span><br><span class="line">jobmanager_taskSlotsTotal</span><br><span class="line">jobmanager_job_downtime</span><br><span class="line">jobmanager_job_fullRestarts</span><br><span class="line">jobmanager_job_restartingTime</span><br></pre></td></tr></table></figure><h3 id="监控-Task-Manager"><a href="#监控-Task-Manager" class="headerlink" title="监控 Task Manager"></a>监控 Task Manager</h3><p>Task Manager 在 Flink 集群中也是一个个的进程实例，它的数量代表着能够运行作业个数的能力，所有的 Flink 作业最终其实是会在 Task Manager 上运行的，Task Manager 管理着运行在它上面的所有作业的 Task 的整个生命周期，包括了 Task 的启动销毁、内存管理、磁盘 IO、网络传输管理等。</p><p>因为所有的 Task 都是运行运行在 Task Manager 上的，有的 Task 可能会做比较复杂的操作或者会存储很多数据在内存中，那么就会消耗很大的资源，所以通常来说 Task Manager 要比 Job Manager 消耗的资源要多，但是这个资源具体多少其实也不好预估，所以可能会出现由于分配资源的不合理，导致 TaskManager 出现 OOM 等问题。一旦 TaskManager 因为各种问题导致崩溃重启的话，运行在它上面的 Task 也都会失败，Job Manager 与它的通信也会丢失。因为作业出现 failover，所以在重启这段时间它是不会去消费数据的，所以必然就会出现数据消费延迟的问题。对于这种情况那么必然就很需要 TaskManager 的监控信息，这样才能够对整个集群的 TaskManager 做一个提前预警。</p><p>那么在 Flink 中自带的 Task Manager Metrics 有哪些呢？主要也是 CPU、类加载、GC、内存、网络等。其实这些信息在 Flink UI 上也是有，不知道读者有没有细心观察过。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtrftsqsj22la1fwdhv.jpg" alt="undefined"></p><p>在这个 Task Manager 的 Metrics 监控页面通常比较关心的指标有内存相关的，还有就是 GC 的指标，通常一个 Task Manager 出现 OOM 之前会不断的进行 GC，在这个 Metrics 页面它展示了年轻代和老年代的 GC 信息（时间和次数），大家可以细心观察下是否 Task Manager OOM 前老年代和新生代的 GC 次数比较、时间比较长。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtrmwmixj220k0buq2y.jpg" alt="undefined"></p><p>在 Flink Reporter 中提供的 Task Manager Metrics 指标如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">taskmanager_Status_JVM_CPU_Load</span><br><span class="line">taskmanager_Status_JVM_CPU_Time</span><br><span class="line">taskmanager_Status_JVM_ClassLoader_ClassesLoaded</span><br><span class="line">taskmanager_Status_JVM_ClassLoader_ClassesUnloaded</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Count</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Old_Generation_Time</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Count</span><br><span class="line">taskmanager_Status_JVM_GarbageCollector_G1_Young_Generation_Time</span><br><span class="line">taskmanager_Status_JVM_Memory_Direct_Count</span><br><span class="line">taskmanager_Status_JVM_Memory_Direct_MemoryUsed</span><br><span class="line">taskmanager_Status_JVM_Memory_Direct_TotalCapacity</span><br><span class="line">taskmanager_Status_JVM_Memory_Heap_Committed</span><br><span class="line">taskmanager_Status_JVM_Memory_Heap_Max</span><br><span class="line">taskmanager_Status_JVM_Memory_Heap_Used</span><br><span class="line">taskmanager_Status_JVM_Memory_Mapped_Count</span><br><span class="line">taskmanager_Status_JVM_Memory_Mapped_MemoryUsed</span><br><span class="line">taskmanager_Status_JVM_Memory_Mapped_TotalCapacity</span><br><span class="line">taskmanager_Status_JVM_Memory_NonHeap_Committed</span><br><span class="line">taskmanager_Status_JVM_Memory_NonHeap_Max</span><br><span class="line">taskmanager_Status_JVM_Memory_NonHeap_Used</span><br><span class="line">taskmanager_Status_JVM_Threads_Count</span><br><span class="line">taskmanager_Status_Network_AvailableMemorySegments</span><br><span class="line">taskmanager_Status_Network_TotalMemorySegments</span><br><span class="line">taskmanager_Status_Shuffle_Netty_AvailableMemorySegments</span><br><span class="line">taskmanager_Status_Shuffle_Netty_TotalMemorySegments</span><br></pre></td></tr></table></figure><h3 id="监控-Flink-Job"><a href="#监控-Flink-Job" class="headerlink" title="监控 Flink Job"></a>监控 Flink Job</h3><p>对于运行的作业来说，其实我们会更关心其运行状态，如果没有其对应的一些监控信息，那么对于我们来说这个 Job 就是一个黑盒，完全不知道是否在运行，Job 运行状态是什么、Task 运行状态是什么、是否在消费数据、消费数据是咋样（细分到每个 Task）、消费速度能否跟上生产数据的速度、处理数据的过程中是否有遇到什么错误日志、处理数据是否有出现反压问题等等。</p><p>上面列举的这些问题通常来说是比较关心的，那么在 Flink UI 上也是有提供的查看对应的信息的，点开对应的作业就可以查看到作业的执行图，每个 Task 的信息都是会展示出来的，包含了状态、Bytes Received（接收到记录的容量大小）、Records Received（接收到记录的条数）、Bytes Sent（发出去的记录的容量大小）、Records Sent（发出去记录的条数）、异常信息、timeline（作业运行状态的时间线）、Checkpoint 信息。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtru6vruj22k41c8jtk.jpg" alt="undefined"></p><p>这些指标也可以通过 Flink 的 Reporter 进行上报存储到第三方的时序数据库，然后通过类似 Grafana 展示出来。通过这些信息大概就可以清楚的知道一个 Job 的整个运行状态，然后根据这些运行状态去分析作业是否有问题。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmts0qvihj21gk0egdi4.jpg" alt="undefined"></p><p>在流作业中最关键的指标无非是作业的实时性，那么延迟就是衡量作业的是否实时的一个基本参数，但是对于现有的这些信息其实还不知道作业的消费是否有延迟，通常来说可以结合 Kafka 的监控去查看对应消费的 Topic 的 Group 的 Lag 信息，如果 Lag 很大就表明有数据堆积了，另外还有一个办法就是需要自己在作业中自定义 Metrics 做埋点，将算子在处理数据的系统时间与数据自身的 Event Time 做一个差值，求得值就可以知道算子消费的数据是什么时候的了。比如在 1571457964000（2019-10-19 12:06:04）Map 算子消费的数据的事件时间是 1571457604000（2019-10-19 12:00:04），相差了 6 分钟，那么就表明消费延迟了 6 分钟，然后通过 Metrics Reporter 将埋点的 Metrics 信息上传，这样最终就可以获取到作业在每个算子处的消费延迟的时间。</p><p>上面的是针对于作业延迟的判断方法，另外像类似于作业反压的情况，在 Flink 的 UI 也会有展示，具体怎么去分析和处理这种问题在 9.1 节中有详细讲解。</p><p>根据这些监控信息不仅可以做到提前预警，做好资源的扩容（比如增加容器的数量／内存／CPU／并行度／Slot 个数），也还可以找出作业配置的资源是否有浪费。通常来说一个作业的上线可能是会经过资源的预估，然后才会去申请这个作业要配置多少资源，比如算子要使用多少并行度，最后上线后可以通过完整的运行监控信息查看该作业配置的并行度是否有过多或者配置的内存比较大。比如出现下面这些情况的时候可能就是资源出现浪费了：</p><ul><li>作业消费从未发生过延迟，即使在数据流量高峰的时候，也未发生过消费延迟</li><li>作业运行所在的 Task Manager 堆内存使用率异常的低</li><li>作业运行所在的 Task Manager 的 GC 时间和次数非常规律，没有出现异常的现象</li></ul><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmts8eo48j218m0hijsf.jpg" alt="undefined"></p><p>在 Flink Metrics Reporter 上传的指标中大概有下面这些：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">taskmanager_job_task_Shuffle_Netty_Input_Buffers_outPoolUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Input_Buffers_outputQueueLength</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inPoolUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inputExclusiveBuffersUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inputFloatingBuffersUsage</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_Buffers_inputQueueLength</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocal</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInLocalPerSecond</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemote</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBuffersInRemotePerSecond</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInLocal</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInLocalPerSecond</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInRemote</span><br><span class="line">taskmanager_job_task_Shuffle_Netty_Output_numBytesInRemotePerSecond</span><br><span class="line">taskmanager_job_task_buffers_inPoolUsage</span><br><span class="line">taskmanager_job_task_buffers_inputExclusiveBuffersUsage</span><br><span class="line">taskmanager_job_task_buffers_inputFloatingBuffersUsage</span><br><span class="line">taskmanager_job_task_buffers_inputQueueLength</span><br><span class="line">taskmanager_job_task_buffers_outPoolUsage</span><br><span class="line">taskmanager_job_task_buffers_outputQueueLength</span><br><span class="line">taskmanager_job_task_checkpointAlignmentTime</span><br><span class="line">taskmanager_job_task_currentInputWatermark</span><br><span class="line">taskmanager_job_task_numBuffersInLocal</span><br><span class="line">taskmanager_job_task_numBuffersInLocalPerSecond</span><br><span class="line">taskmanager_job_task_numBuffersInRemote</span><br><span class="line">taskmanager_job_task_numBuffersInRemotePerSecond</span><br><span class="line">taskmanager_job_task_numBuffersOut</span><br><span class="line">taskmanager_job_task_numBuffersOutPerSecond</span><br><span class="line">taskmanager_job_task_numBytesIn</span><br><span class="line">taskmanager_job_task_numBytesInLocal</span><br><span class="line">taskmanager_job_task_numBytesInLocalPerSecond</span><br><span class="line">taskmanager_job_task_numBytesInPerSecond</span><br><span class="line">taskmanager_job_task_numBytesInRemote</span><br><span class="line">taskmanager_job_task_numBytesInRemotePerSecond</span><br><span class="line">taskmanager_job_task_numBytesOut</span><br><span class="line">taskmanager_job_task_numBytesOutPerSecond</span><br><span class="line">taskmanager_job_task_numRecordsIn</span><br><span class="line">taskmanager_job_task_numRecordsInPerSecond</span><br><span class="line">taskmanager_job_task_numRecordsOut</span><br><span class="line">taskmanager_job_task_numRecordsOutPerSecond</span><br><span class="line">taskmanager_job_task_operator_currentInputWatermark</span><br><span class="line">taskmanager_job_task_operator_currentOutputWatermark</span><br><span class="line">taskmanager_job_task_operator_numLateRecordsDropped</span><br><span class="line">taskmanager_job_task_operator_numRecordsIn</span><br><span class="line">taskmanager_job_task_operator_numRecordsInPerSecond</span><br><span class="line">taskmanager_job_task_operator_numRecordsOut</span><br><span class="line">taskmanager_job_task_operator_numRecordsOutPerSecond</span><br></pre></td></tr></table></figure><h3 id="最关心的监控指标有哪些"><a href="#最关心的监控指标有哪些" class="headerlink" title="最关心的监控指标有哪些"></a>最关心的监控指标有哪些</h3><p>上面已经提及到 Flink 的 Job Manager、Task Manager 和运行的 Flink Job 的监控以及常用的监控信息，这些指标有的是可以直接在 Flink 的 UI 上观察到的，另外 Flink 提供了 Metrics Reporter 进行上报存储到监控系统中去，然后通过可视化的图表进行展示，在 8.2 节中将教大家如何构建一个完整的监控系统。那么有了这么多监控指标，其实哪些是比较重要的呢，比如说这些指标出现异常的时候可以发出告警及时进行通知，这样可以做到预警作用，另外还可以根据这些信息进行作业资源的评估。下面列举一些笔者觉得比较重要的指标：</p><h4 id="Job-Manager"><a href="#Job-Manager" class="headerlink" title="Job Manager"></a>Job Manager</h4><p>在 Job Manager 中有着该集群中所有的 Task Manager 的个数、Slot 的总个数、Slot 的可用个数、运行的时间、作业的 Checkpoint 情况，笔者觉得这几个指标可以重点关注。</p><ul><li>TaskManager 个数：如果出现 TaskManager 突然减少，可能是因为有 TaskManager 挂掉重启，一旦该 TaskManager 之前运行了很多作业，那么重启带来的影响必然是巨大的。</li><li>Slot 个数：取决于 TaskManager 的个数，决定了能运行作业的最大并行度，如果资源不够，及时扩容。</li><li>作业运行时间：根据作业的运行时间来判断作业是否存活，中途是否掉线过。</li><li>Checkpoint 情况：Checkpoint 是 Job Manager 发起的，并且关乎到作业的状态是否可以完整的保存。</li></ul><h4 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h4><p>因为所有的作业最终都是运行在 TaskManager 上，所以 TaskManager 的监控指标也是异常的监控，并且作业的复杂度也会影响 TaskManager 的资源使用情况，所以 TaskManager 的基础监控指标比如内存、GC 如果出现异常或者超出设置的阈值则需要立马进行告警通知，防止后面导致大批量的作业出现故障重启。</p><ul><li>内存使用率：部分作业的算子会将所有的 State 数据存储在内存中，这样就会导致 TaskManager 的内存使用率会上升，还有就是可以根据该指标看作业的利用率，从而最后来重新划分资源的配置。</li><li>GC 情况：分时间和次数，一旦 TaskManager 的内存率很高的时候，必定伴随着频繁的 GC，如果在 GC 的时候没有得到及时的预警，那么将面临 OOM 风险。</li></ul><h4 id="Flink-Job"><a href="#Flink-Job" class="headerlink" title="Flink Job"></a>Flink Job</h4><p>作业的稳定性和及时性其实就是大家最关心的，常见的指标有：作业的状态、Task 的状态、作业算子的消费速度、作业出现的异常日志。</p><ul><li>作业的状态：在 UI 上是可以看到作业的状态信息，常见的状态变更信息如下图。</li></ul><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtsh4ohfj21m01c2acg.jpg" alt="undefined"></p><p>Task 的状态：其实导致作业的状态发生变化的原因通常是由于 Task 的运行状态出现导致，所以也需要对 Task 的运行状态进行监控，Task 的运行状态如下图。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtso1v09j21c20ykmxz.jpg" alt="undefined"></p><ul><li>作业异常日志：导致 Task 出现状态异常的根因通常是作业中的代码出现各种各样的异常日志，最后可能还会导致作业无限重启，所以作业的异常日志也是需要及时关注。</li><li>作业重启次数：当 Task 状态和作业的状态发生变化的时候，如果作业中配置了重启策略或者开启了 Checkpoint 则会进行作业重启的，重启作业的带来的影响也会很多，并且会伴随着一些不确定的因素，最终导致作业一直重启，这样既不能解决问题，还一直在占用着资源的消耗。</li><li>算子的消费速度：代表了作业的消费能力，还可以知道作业是否发生延迟，可以包含算子接收的数据量和发出去数据量，从而可以知道在算子处是否有发生数据的丢失。</li></ul><h3 id="小结与反思-30"><a href="#小结与反思-30" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中常见的监控对象，比如 Job Manager、Task Manager 和 Flink Job，对于这几个分别介绍了其内部大概有的监控指标，以及在真实生产环境关心的指标，你是否还有其他的监控指标需要补充呢？</p><p>本节涉及的监控指标对应的含义可以参考官网链接：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html#system-metrics" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html#system-metrics</a></p><p>本节涉及的监控指标列表地址：<a href="https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-monitor/flink*monitor*measurements.md" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-monitor/flink*monitor*measurements.md</a></p><h2 id="三十三、如何搭建一套完整的-Flink-监控系统"><a href="#三十三、如何搭建一套完整的-Flink-监控系统" class="headerlink" title="三十三、如何搭建一套完整的 Flink 监控系统?"></a>三十三、如何搭建一套完整的 Flink 监控系统?</h2><p>8.1 节中讲解了 Job Manager、Task Manager 和 Flink Job 的监控，以及需要关注的监控指标有哪些。本节带大家讲解一下如何搭建一套完整的 Flink 监控系统，如果你所在的公司没有专门的监控平台，那么可以根据本节的内容来为公司搭建一套属于自己公司的 Flink 监控系统。</p><h3 id="利用-API-获取监控数据"><a href="#利用-API-获取监控数据" class="headerlink" title="利用 API 获取监控数据"></a>利用 API 获取监控数据</h3><p>熟悉 Flink 的朋友都知道 Flink 的 UI 上面已经详细地展示了很多监控指标的数据，并且这些指标还是比较重要的，所以如果不想搭建额外的监控系统，那么直接利用 Flink 自身的 UI 就可以获取到很多重要的监控信息。这里要讲的是这些监控信息其实也是通过 Flink 自身的 Rest API 来获取数据的，所以其实要搭建一个粗糙的监控平台，也是可以直接利用现有的接口定时去获取数据，然后将这些指标的数据存储在某种时序数据库中，最后用些可视化图表做个展示，这样一个完整的监控系统就做出来了。</p><p>这里通过 Chrome 浏览器的控制台来查看一下有哪些 REST API 是用来提供监控数据的。</p><p>1.在Chrome 浏览器中打开 <code>http://localhost:8081/overview</code> 页面，可以获取到整个 Flink 集群的资源信息：TaskManager 个数（Task Managers）、Slot 总个数（Total Task Slots）、可用 Slot 个数（Available Task Slots）、Job 运行个数（Running Jobs）、Job 运行状态（Finished 0 Canceled 0 Failed 0）等，如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtt7m1dyj22i41hego8.jpg" alt="undefined"></p><p>2.通过 <code>http://localhost:8081/taskmanagers</code> 页面查看 Task Manager 列表，可以知道该集群下所有 Task Manager 的信息（数据端口号（Data Port）、上一次心跳时间（Last Heartbeat）、总共的 Slot 个数（All Slots）、空闲的 Slot 个数（Free Slots）、以及 CPU 和内存的分配使用情况，如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtti86ewj22kg1fogoy.jpg" alt="undefined"></p><p>3.通过 <code>http://localhost:8081/taskmanagers/tm_id</code> 页面查看 Task Manager 的具体情况（这里的 tm_id 是个随机的 UUID 值）。在这个页面上，除了上一条的监控信息可以查看，还可以查看该 Task Manager 的 JVM（堆和非堆）、Direct 内存、网络、GC 次数和时间。内存和 GC 这些信息非常重要，很多时候 Task Manager 频繁重启的原因就是 JVM 内存设置得不合理，导致频繁的 GC，最后使得 OOM 崩溃，不得不重启。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmttpcqoyj22jc1ju427.jpg" alt="undefined"></p><p>另外如果你在 <code>/taskmanagers/tm_id</code> 接口后面加个 <code>/log</code> 就可以查看该 Task Manager 的日志，注意，在 Flink 中的日志和平常自己写的应用中的日志是不一样的。在 Flink 中，日志是以 Task Manager 为概念打印出来的，而不是以单个 Job 打印出来的，如果你的 Job 在多个 Task Manager 上运行，那么日志就会在多个 Task Manager 中打印出来。如果一个 Task Manager 中运行了多个 Job，那么它里面的日志就会很混乱，查看日志时会发现它为什么既有这个 Job 打出来的日志，又有那个 Job 打出来的日志，如果你之前有这个疑问，那么相信你看完这里，就不会有疑问了。</p><p>对于这种设计是否真的好，不同的人有不同的看法，在 Flink 的 Issue 中就有人提出了该问题，Issue 中的描述是希望日志可以是 Job 与 Job 之间的隔离，这样日志更方便采集和查看，对于排查问题也会更快。对此国内有公司也对这一部分做了改进，不知道正在看本书的你是否有什么好的想法可以解决 Flink 的这一痛点。</p><p>4.通过 <code>http://localhost:8081/#/job-manager/config</code> 页面可以看到可 Job Manager 的配置信息，另外通过 <code>http://localhost:8081/jobmanager/log</code> 页面可以查看 Job Manager 的日志详情。</p><p>5.通过 <code>http://localhost:8081/jobs/job_id</code> 页面可以查看 Job 的监控数据，由于指标（包括了 Job 的 Task 数据、Operator 数据、Exception 数据、Checkpoint 数据等）过多，大家可以自己在本地测试查看。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtu06b4wj22iq1jydjm.jpg" alt="undefined"></p><p>上面列举了几个 REST API（不是全部），主要是为了告诉大家，其实这些接口我们都知道，那么我们也可以利用这些接口去获取对应的监控数据，然后绘制出更酷炫的图表，用更直观的页面将这些数据展示出来，这样就能更好地控制。</p><p>除了利用 Flink UI 提供的接口去定时获取到监控数据，其实 Flink 还提供了很多的 reporter 去上报监控数据，比如 JMXReporter、PrometheusReporter、PrometheusPushGatewayReporter、InfluxDBReporter、StatsDReporter 等，这样就可以根据需求去定制获取到 Flink 的监控数据，下面教大家使用几个常用的 reporter。</p><p>相关 Rest API 可以查看官网链接：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html#rest-api-integration" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html#rest-api-integration</a></p><h3 id="Metrics-类型介绍"><a href="#Metrics-类型介绍" class="headerlink" title="Metrics 类型介绍"></a>Metrics 类型介绍</h3><p>可以在继承自 RichFunction 的函数中通过 <code>getRuntimeContext().getMetricGroup()</code> 获取 Metric 信息，常见的 Metrics 的类型有 Counter、Gauge、Histogram、Meter。</p><h4 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h4><p>Counter 用于计数，当前值可以使用 <code>inc()/inc(long n)</code> 递增和 <code>dec()/dec(long n)</code> 递减，在实现 RichFunction 中的函数的 open 方法注册 Counter。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> Counter counter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.counter = getRuntimeContext()</span><br><span class="line">  .getMetricGroup()</span><br><span class="line">  .counter(<span class="string">"zhisheng_counter"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//或者自定义 Counter</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.counter = getRuntimeContext()</span><br><span class="line">  .getMetricGroup()</span><br><span class="line">  .counter(<span class="string">"zhisheng_counter"</span>, <span class="keyword">new</span> CustomCounter());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.counter.inc();</span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Gauge"><a href="#Gauge" class="headerlink" title="Gauge"></a>Gauge</h4><p>Gauge 根据需要提供任何类型的值，要使用 Gauge 的话，需要实现 Gauge 接口，返回值没有规定类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">int</span> valueToExpose = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">getRuntimeContext()</span><br><span class="line">  .getMetricGroup()</span><br><span class="line">  .gauge(<span class="string">"zhisheng_gauge"</span>, <span class="keyword">new</span> Gauge&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> valueToExpose;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">valueToExpose++;</span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Histogram"><a href="#Histogram" class="headerlink" title="Histogram"></a>Histogram</h4><p>Histogram 统计数据的分布情况，比如最小值，最大值，中间值，还有分位数等。使用情况如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> Histogram histogram;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.histogram = getRuntimeContext()</span><br><span class="line">  .getMetricGroup()</span><br><span class="line">  .histogram(<span class="string">"zhisheng_histogram"</span>, <span class="keyword">new</span> MyHistogram());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Long <span class="title">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.histogram.update(value);</span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Meter"><a href="#Meter" class="headerlink" title="Meter"></a>Meter</h4><p>Meter 代表平均吞吐量，使用情况如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> Meter meter;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.meter = getRuntimeContext()</span><br><span class="line">  .getMetricGroup()</span><br><span class="line">  .meter(<span class="string">"myMeter"</span>, <span class="keyword">new</span> MyMeter());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Long <span class="title">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.meter.markEvent();</span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="利用-JMXReporter-获取监控数据"><a href="#利用-JMXReporter-获取监控数据" class="headerlink" title="利用 JMXReporter 获取监控数据"></a>利用 JMXReporter 获取监控数据</h3><p>JMX 对于大家来说应该不太陌生，在 Flink 中默认提供了 JMXReporter 获取到监控数据，不需要额外添加依赖项，但是需要在 flink-conf.yaml 配置文件中加入如下配置即可开启 JMX：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">metrics.reporter.jmx.factory.class:</span> <span class="string">org.apache.flink.metrics.jmx.JMXReporterFactory</span></span><br><span class="line"><span class="string">metrics.reporter.jmx.port:</span> <span class="number">8789</span></span><br></pre></td></tr></table></figure><p>然后利用 JDK 自带的 jconsole 可以查看 MBean 信息。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtughtgaj21cs09swfk.jpg" alt="undefined"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtuk2fq8j20y60ty3zf.jpg" alt="undefined"></p><p>如下图所示，你可以看到左侧是有很多的监控指标，如果点进去是可以查看到每个指标对应的 value 值。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtuptx26j22h61l2td8.jpg" alt="undefined"></p><p>但是你有没有发现这些指标只有 Job Manager 的监控指标，没有 Task Manager 的监控指标，如果你在同一台服务器上面既运行了 Job Manager，又运行了 Task Manager，那么只开启一个端口号那么是只能够监听到一个的数据，如果你要监听多个数据，那么就需要在端口设置里填写一个范围（这里需要特别注意一下），具体配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jmx reporter</span></span><br><span class="line"><span class="string">metrics.reporter.jmx.factory.class:</span> <span class="string">org.apache.flink.metrics.jmx.JMXReporterFactory</span></span><br><span class="line"><span class="string">metrics.reporter.jmx.port:</span> <span class="number">8789</span><span class="bullet">-8799</span></span><br></pre></td></tr></table></figure><p>这样就表示监听了多个端口（从 8789 ～ 8799），那么再通过 jconsole 连接 8790 端口就会出现 Task Manager 的监控指标数据了。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtuww99zj22h61l2ae1.jpg" alt="undefined"></p><p>查看日志也可以看到开启 JMX 成功的日志，如下所示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2019-10-07 10:52:51,839 INFO  org.apache.flink.metrics.jmx.JMXReporter                      - Started JMX server on port 8789.</span><br><span class="line">2019-10-07 10:52:51,839 INFO  org.apache.flink.metrics.jmx.JMXReporter                      - Configured JMXReporter with &#123;port:8789-8799&#125;</span><br><span class="line">2019-10-07 10:52:51,840 INFO  org.apache.flink.runtime.metrics.ReporterSetup                - Configuring jmx with &#123;factory.class=org.apache.flink.metrics.jmx.JMXReporterFactory, port=8789-8799&#125;.</span><br><span class="line">2019-10-07 10:52:51,841 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl           - Reporting metrics for reporter jmx of type org.apache.flink.metrics.jmx.JMXReporter.</span><br></pre></td></tr></table></figure><h3 id="利用-PrometheusReporter-获取监控数据"><a href="#利用-PrometheusReporter-获取监控数据" class="headerlink" title="利用 PrometheusReporter 获取监控数据"></a>利用 PrometheusReporter 获取监控数据</h3><p>要使用该 reporter 的话，需要将 opt 目录下的 <code>flink-metrics-prometheus-1.9.0.jar</code> 依赖放到 lib 目录下，可以配置的参数有：</p><ul><li>port：该参数为可选项，Prometheus 监听的端口，默认是 9249，和上面使用 JMXReporter 一样，如果是在一台服务器上既运行了 Job Manager，又运行了 TaskManager，则使用端口范围，比如 <code>9249-9259</code>。</li><li>filterLabelValueCharacters：该参数为可选项，表示指定是否过滤标签值字符，如果开启，则删除所有不匹配 <code>[a-zA-Z0-9:_]</code> 的字符，否则不会删除任何字符。</li></ul><p>除了上面两个可选参数，另外一个参数是必须要在 <code>flink-conf.yaml</code> 中配置的，那就是 metrics reporter class。比如像下面这样配置：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">metrics.reporter.prom.class:</span> <span class="string">org.apache.flink.metrics.prometheus.PrometheusReporter</span></span><br></pre></td></tr></table></figure><p>Flink 中的 metrics 类型和 Prometheus 中 metrics 类型对比如下：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtv3v4ppj227i0g6757.jpg" alt="undefined"></p><h3 id="利用-PrometheusPushGatewayReporter-获取监控数据"><a href="#利用-PrometheusPushGatewayReporter-获取监控数据" class="headerlink" title="利用 PrometheusPushGatewayReporter 获取监控数据"></a>利用 PrometheusPushGatewayReporter 获取监控数据</h3><p>PushGateway 是 Prometheus 生态中一个重要工具，使用它的原因主要是：</p><ul><li>Prometheus 采用 pull 模式，可能由于 Prometheus 和其他 target 对象不在一个子网或者防火墙原因，导致 Prometheus 无法直接拉取各个 target 数据。</li><li>在监控业务数据的时候，需要将不同数据汇总, 由 Prometheus 统一收集。</li></ul><p>那么使用 PrometheusPushGatewayReporter 的话，该 reporter 会定时将 metrics 数据推送到 PushGateway，然后再由 Prometheus 去拉取这些 metrics 数据。如果使用 PrometheusPushGatewayReporter 收集数据的话，也是需要将 opt 目录下的 <code>flink-metrics-prometheus-1.9.0.jar</code> 依赖放到 lib 目录下的，可配置的参数有：</p><ul><li>deleteOnShutdown：默认值是 true，表示是否在关闭时从 PushGateway 删除指标。</li><li>filterLabelValueCharacters：默认值是 true，表示是否过滤标签值字符，如果开启，则不符合 <code>[a-zA-Z0-9:_]</code> 的字符都将被删除。</li><li>host：无默认值，配置 PushGateway 服务所在的机器 IP。</li><li>jobName：无默认值，要上报 Metrics 的 Job 名称。</li><li>port：默认值是 -1，这里配置 PushGateway 服务的端口。</li><li>randomJobNameSuffix：默认值是 true，指定是否将随机后缀名附加到作业名。</li></ul><p>在 flink-conf.yaml 中配置的样例如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">metrics.reporter.promgateway.class:</span> <span class="string">org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter</span></span><br><span class="line"><span class="string">metrics.reporter.promgateway.host:</span> <span class="string">localhost</span></span><br><span class="line"><span class="string">metrics.reporter.promgateway.port:</span> <span class="number">9091</span></span><br><span class="line"><span class="string">metrics.reporter.promgateway.jobName:</span> <span class="string">zhisheng</span></span><br><span class="line"><span class="string">metrics.reporter.promgateway.randomJobNameSuffix:</span> <span class="literal">true</span></span><br><span class="line"><span class="string">metrics.reporter.promgateway.deleteOnShutdown:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="利用-InfluxDBReporter-获取监控数据"><a href="#利用-InfluxDBReporter-获取监控数据" class="headerlink" title="利用 InfluxDBReporter 获取监控数据"></a>利用 InfluxDBReporter 获取监控数据</h3><p>Flink 里面提供了 InfluxDBReporter 支持将 Flink 的 metrics 数据直接存储到 InfluxDB 中，在源码中该模块是通过 MetricMapper 类将 MeasurementInfo（这个类是 metric 的数据结构，里面含有两个字段 name 和 tags） 和 Gauge、Counter、Histogram、Meter 组装成 InfluxDB 中的 Point 数据，Point 结构如下（主要就是构造 metric name、fields、tags 和 timestamp）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> String measurement;</span><br><span class="line"><span class="keyword">private</span> Map&lt;String, String&gt; tags;</span><br><span class="line"><span class="keyword">private</span> Long time;</span><br><span class="line"><span class="keyword">private</span> TimeUnit precision;</span><br><span class="line"><span class="keyword">private</span> Map&lt;String, Object&gt; fields;</span><br></pre></td></tr></table></figure><p>然后在 InfluxdbReporter 类中将 metric 数据导入 InfluxDB，该类继承自 AbstractReporter 抽象类，实现了 Scheduled 接口，有下面 3 个属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> String database;</span><br><span class="line"><span class="keyword">private</span> String retentionPolicy;</span><br><span class="line"><span class="keyword">private</span> InfluxDB influxDB;</span><br></pre></td></tr></table></figure><p>在 open 方法中获取配置文件中的 InfluxDB 设置，然后初始化 InfluxDB 相关的配置，构造 InfluxDB 客户端：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(MetricConfig config)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//获取到 host 和 port</span></span><br><span class="line">    String host = getString(config, HOST);</span><br><span class="line">    <span class="keyword">int</span> port = getInteger(config, PORT);</span><br><span class="line">    <span class="comment">//判断 host 和 port 是否合法</span></span><br><span class="line">    <span class="keyword">if</span> (!isValidHost(host) || !isValidPort(port)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Invalid host/port configuration. Host: "</span> + host + <span class="string">" Port: "</span> + port);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//获取到 InfluxDB database</span></span><br><span class="line">    String database = getString(config, DB);</span><br><span class="line">    <span class="keyword">if</span> (database == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"'"</span> + DB.key() + <span class="string">"' configuration option is not set"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    String url = String.format(<span class="string">"http://%s:%d"</span>, host, port);</span><br><span class="line">    <span class="comment">//获取到 InfluxDB username 和 password</span></span><br><span class="line">    String username = getString(config, USERNAME);</span><br><span class="line">    String password = getString(config, PASSWORD);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.database = database;</span><br><span class="line">    <span class="comment">//InfluxDB 保留政策</span></span><br><span class="line">    <span class="keyword">this</span>.retentionPolicy = getString(config, RETENTION_POLICY);</span><br><span class="line">    <span class="keyword">if</span> (username != <span class="keyword">null</span> &amp;&amp; password != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">//如果有用户名和密码，根据 url 和 用户名密码来创建连接</span></span><br><span class="line">        influxDB = InfluxDBFactory.connect(url, username, password);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//否则就根据 url 连接</span></span><br><span class="line">        influxDB = InfluxDBFactory.connect(url);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.info(<span class="string">"Configured InfluxDBReporter with &#123;host:&#123;&#125;, port:&#123;&#125;, db:&#123;&#125;, and retentionPolicy:&#123;&#125;&#125;"</span>, host, port, database, retentionPolicy);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在 report 方法中调用一个内部 buildReport 方法来构造 BatchPoints，将一批 Point 放在该对象中，BatchPoints 对象的属性如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> String database;</span><br><span class="line"><span class="keyword">private</span> String retentionPolicy;</span><br><span class="line"><span class="keyword">private</span> Map&lt;String, String&gt; tags;</span><br><span class="line"><span class="keyword">private</span> List&lt;Point&gt; points;</span><br><span class="line"><span class="keyword">private</span> ConsistencyLevel consistency;</span><br><span class="line"><span class="keyword">private</span> TimeUnit precision;</span><br></pre></td></tr></table></figure><p>通过 buildReport 方法返回的 BatchPoints 如果不为空，则会通过 write 方法将 BatchPoints 写入 InfluxDB：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (report != <span class="keyword">null</span>) &#123;</span><br><span class="line">    influxDB.write(report);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在使用 InfluxDBReporter 时需要注意：</p><p>1.必须复制 Flink 安装目录下的 <code>/opt/flink-metrics-influxdb-1.9.0.jar</code> 到 flink 的 lib 目录下，否则运行起来会报错如下：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtvdrodkj22021ngtm1.jpg" alt="undefined"></p><p>2.如下所示，在 flink-conf.yaml 中添加 InfluxDB 相关的配置。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">metrics.reporter.influxdb.class：org.apache.flink.metrics.influxdb.InfluxdbReporter</span></span><br><span class="line"><span class="string">metrics.reporter.influxdb.host：localhost</span>  <span class="comment"># InfluxDB服务器主机</span></span><br><span class="line"><span class="string">metrics.reporter.influxdb.port:</span> <span class="number">8086</span>   <span class="comment"># 可选）InfluxDB 服务器端口，默认为 8086</span></span><br><span class="line"><span class="string">metrics.reporter.influxdb.db：zhisheng</span> <span class="comment"># 用于存储指标的 InfluxDB 数据库  </span></span><br><span class="line"><span class="string">metrics.reporter.influxdb.username：zhisheng</span> <span class="comment"># （可选）用于身份验证的 InfluxDB 用户名</span></span><br><span class="line"><span class="string">metrics.reporter.influxdb.password：123456</span> <span class="comment"># （可选）InfluxDB 用户名用于身份验证的密码</span></span><br><span class="line"><span class="string">metrics.reporter.influxdb.retentionPolicy:</span> <span class="string">one_hour</span> <span class="comment">#（可选）InfluxDB 数据保留策略，默认为服务器上数据库定义的保留策略</span></span><br></pre></td></tr></table></figure><p>如果填错了密码会报鉴权失败的错误：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtvk415oj21tu0h6q6i.jpg" alt="undefined"></p><h3 id="安装-InfluxDB-和-Grafana"><a href="#安装-InfluxDB-和-Grafana" class="headerlink" title="安装 InfluxDB 和 Grafana"></a>安装 InfluxDB 和 Grafana</h3><h4 id="安装-InfluxDB"><a href="#安装-InfluxDB" class="headerlink" title="安装 InfluxDB"></a>安装 InfluxDB</h4><p>InfluxDB 是一款时序数据库，使用它作为监控数据存储的公司也有很多，可以根据 InfluxDB 官网：<a href="https://docs.influxdata.com/influxdb/v1.7/introduction/installation/" target="_blank" rel="noopener">https://docs.influxdata.com/influxdb/v1.7/introduction/installation/</a> 的安装步骤来操作。</p><p>1、配置 InfluxDB 下载源。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo</span><br><span class="line">[influxdb]</span><br><span class="line">name = InfluxDB Repository - RHEL \$releasever</span><br><span class="line">baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable</span><br><span class="line">enabled = 1</span><br><span class="line">gpgcheck = 1</span><br><span class="line">gpgkey = https://repos.influxdata.com/influxdb.key</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>2、根据 yum 安装命令操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install influxdb</span><br></pre></td></tr></table></figure><p>3、启停 InfluxDB。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//启动 influxdb 命令</span><br><span class="line">systemctl start influxdb</span><br><span class="line">//重启 influxdb 命令</span><br><span class="line">systemctl restart influxd</span><br><span class="line">//停止 influxdb 命令</span><br><span class="line">systemctl stop influxd</span><br><span class="line">//设置开机自启动</span><br><span class="line">systemctl enable influxdb</span><br></pre></td></tr></table></figure><p>4、InfluxDB 相关的命令操作。</p><p>启动好 InfluxDB 后执行 <code>influx</code> 命令，然后使用下面命令来创建用户：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE USER zhisheng WITH PASSWORD &apos;123456&apos; WITH ALL PRIVILEGES</span><br></pre></td></tr></table></figure><p>然后执行 <code>show users;</code> 命令查看创建的用户。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtvrdt1pj21yu0l8ac4.jpg" alt="undefined"></p><p>对 InfluxDB 开启身份验证，编辑 InfluxDB 配置文件 <code>/etc/influxdb/influxdb.conf</code> ，将 <code>auth-enabled</code> 设置为 <code>true</code>。然后重启 InfluxDB，再次使用 influx 命令进入的话，这时候查看用户或者数据的话，就会报异常（需要使用用户名和密码认证登录）。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtvyko2aj216u0ledia.jpg" alt="undefined"></p><p>这时需要使用下面命令的命令才能够登录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">influx -username  zhisheng -password 123456</span><br></pre></td></tr></table></figure><p>重新登录就能查询到用户和数据了。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtw40a2ej21j40jqab0.jpg" alt="undefined"></p><p>然后创建一个叫 zhisheng 的数据库，后面会将 Flink 中的监控数据存储到该数据库下。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtwcmf3pj20ii0p2q3w.jpg" alt="undefined"></p><h4 id="安装-Grafana"><a href="#安装-Grafana" class="headerlink" title="安装 Grafana"></a>安装 Grafana</h4><p>Grafana 是一款优秀的图表可视化组件，它拥有超多酷炫的图表，并支持自定义配置，用它来做监控的 Dashboard 简直特别完美。</p><p>1、下载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://dl.grafana.com/oss/release/grafana-6.3.6-1.x86_64.rpm</span><br></pre></td></tr></table></figure><p>2、安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum localinstall grafana-6.3.6-1.x86_64.rpm</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtwj6j1aj21241iodlc.jpg" alt="undefined"></p><p>3、启停 Grafana</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//启动 Grafana</span><br><span class="line">systemctl start grafana-server</span><br><span class="line">//停止 Grafana</span><br><span class="line">systemctl stop grafana-server</span><br><span class="line">//重启 Grafana</span><br><span class="line">systemctl restart grafana-server</span><br><span class="line">//设置开机自启动</span><br><span class="line">systemctl enable grafana-server</span><br></pre></td></tr></table></figure><p>然后访问 <code>http://54tianzhisheng.cn:3000</code> 就可以登录了。第一次登录的默认账号密码是 <code>admin／admin</code>，会提示修改密码。</p><h3 id="配置-Grafana-展示监控数据"><a href="#配置-Grafana-展示监控数据" class="headerlink" title="配置 Grafana 展示监控数据"></a>配置 Grafana 展示监控数据</h3><p>登录 Grafana 后，需要配置数据源，Grafana 支持的数据源有很多，比如 InfluxDB、Prometheus 等，选择不同的数据源都可以绘制出很酷炫的图表，如果你公司有使用 Prometheus 做监控系统的，那么可以选择 Prometheus 作为数据源，这里演示就选择 InfluxDB，然后填写 InfluxDB 的地址和用户名密码。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtwqp47zj21xg21itic.jpg" alt="undefined"></p><p>配置好数据源之后，接下来就是要根据数据源来添加数据图表，因为构造数据图表首先得知道有哪些指标，所以这里先看下分别有哪些指标，这里分 Job Manager、TaskManager 和 Job 三大类。具体有哪些指标其实是可以根据 InfluxDB 里面的 measurements 来查看的，我在 GitHub 放了一份完整的 <a href="https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-monitor/flink_monitor_measurements.md" target="_blank" rel="noopener">measurements 列表</a> 以供大家查阅，在 8.1.4 和 8.2.1 节中也都讲解了比较关心的指标，这里展示下如何在 Grafana 中根据这些指标来配置可视化图表。</p><p>1、添加图表</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtwwkhf0j22jw158adi.jpg" alt="undefined"></p><p>2、配置图表从哪个数据源获取数据、选择哪种指标、选择分组、选择单位、添加多个指标、图表命名</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtx2wen8j22kg1hkgpu.jpg" alt="undefined"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtx9fvnlj22fm1gyjun.jpg" alt="undefined"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtxfaku7j22c819oq5l.jpg" alt="undefined"></p><p>3、配置告警</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtxm3sk7j22im1hetby.jpg" alt="undefined"></p><p>这样一个完整的监控图表就配置出来了，有些指标可能直接用数字展示就比较友好，另外还有就是要注意单位，大家可以好好琢磨研究一下 Grafana 的自定义可视化图表的配置，配置好了比较重要的监控指标之后，效果如下图所示：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtxwm16hj22kw1giad6.jpg" alt="undefined"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmty074qtj22kw13wn17.jpg" alt="undefined"></p><p>好了，一个 Flink 的监控系统已经完全搭建好了，从数据采集、数据存储、数据展示、告警整个链路都支持，可以适应大部分公司的场景了，如果还需要做更多的定制化，比如添加更多的监控指标，那么你可以在你的 Job 里面自定义 metrics 做埋点，然后还是通过 reporter 进行数据上报，最后依旧用 Grafana 配置图表展示。</p><h3 id="小结与反思-31"><a href="#小结与反思-31" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了如何利用 API 去获取监控数据，对 Metrics 的类型进行介绍，然后还介绍了怎么利用 Reporter 去将 Metrics 数据进行上报，并通过 InfluxDB + Grafana 搭建了一套 Flink 的监控系统。另外你还可以根据公司的需要使用其他的存储方案来存储监控数据，Grafana 也支持不同的数据源，你们公司的监控系统架构是怎么样的，是否可以直接接入这套监控系统？</p><h2 id="三十四、如何处理-Flink-Job-BackPressure-（反压）问题？"><a href="#三十四、如何处理-Flink-Job-BackPressure-（反压）问题？" class="headerlink" title="三十四、如何处理 Flink Job BackPressure （反压）问题？"></a>三十四、如何处理 Flink Job BackPressure （反压）问题？</h2><p>反压（BackPressure）机制被广泛应用到实时流处理系统中，流处理系统需要能优雅地处理反压问题。反压通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。反压机制是指系统能够自己检测到被阻塞的 Operator，然后自适应地降低源头或上游数据的发送速率，从而维持整个系统的稳定。</p><p>Flink 任务一般运行在多个节点上，数据从上游算子发送到下游算子需要网络传输，若系统在反压时想要降低数据源头或上游算子数据的发送速率，那么肯定也需要网络传输。所以下面先来了解一下 Flink 的网络流控（Flink 对网络数据流量的控制）机制。</p><h3 id="Flink-流处理为什么需要网络流控"><a href="#Flink-流处理为什么需要网络流控" class="headerlink" title="Flink 流处理为什么需要网络流控"></a>Flink 流处理为什么需要网络流控</h3><p>下图是一个简单的 Flink 流任务执行图：任务首先从 Kafka 中读取数据、通过 map 算子对数据进行转换、keyBy 按照指定 key 对数据进行分区（key 相同的数据经过 keyBy 后分到同一个 subtask 实例中），keyBy 后对数据进行 map 转换，然后使用 Sink 将数据输出到外部存储。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtywiigyj25892pe4qp.jpg" alt="undefined"></p><p>众所周知，在大数据处理中，无论是批处理还是流处理，单点处理的性能总是有限的，我们的单个 Job 一般会运行在多个节点上，通过多个节点共同配合来提升整个系统的处理性能。图中，任务被切分成 4 个可独立执行的 subtask 分别是 A0、A1、B0、B1，在数据处理过程中就会存在 shuffle。例如，subtask A0 处理完的数据经过 keyBy 后被发送到 subtask B0、B1 所在节点去处理。那么问题来了，subtask A0 应该以多快的速度向 subtask B0、B1 发送数据呢？把上述问题抽象化，如下图所示，将 subtask A0 当作 Producer，subtask B0 当做 Consumer，上游 Producer 向下游 Consumer 发送数据，在发送端和接收端有相应的 Send Buffer 和 Receive Buffer，但是上游 Producer 生产数据的速率比下游 Consumer 消费数据的速率大，Producer 生产数据的速率为 2MB/s， Consumer 消费数据速率为 1MB/s，Receive Buffer 容量只有 5MB，所以过了 5 秒后，接收端的 Receive Buffer 满了。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtz83at9j24ap18r467.jpg" alt="undefined"></p><p>下游消费速率慢，且接收区的 Receive Buffer 有限，如果上游一直有源源不断的数据，那么将会面临着以下两种情况：</p><ol><li>下游消费者的缓冲区放不下数据，导致下游消费者会丢弃新到达的数据。</li><li>为了不丢弃数据，所以下游消费者的 Receive Buffer 持续扩张，最后耗尽消费者的内存，导致 OOM 程序挂掉。</li></ol><p>常识告诉我们，这两种情况在生产环境下都是不能接受的，第一种会丢数据、第二种会把应用程序挂掉。所以，该问题的解决方案不应该是下游 Receive Buffer 一直累积数据，而是上游 Producer 发现下游 Consumer 消费比较慢的时候，应该在 Producer 端做出限流的策略，防止在下游 Consumer 端无限制地堆积数据。那上游 Producer 端该如何做限流呢？可以采用下图所示静态限流的策略：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtzhxbo3j24ap1i7akj.jpg" alt="undefined"></p><p>静态限速的思想就是，提前已知下游 Consumer 端的消费速率，然后在上游 Producer 端使用类似令牌桶的思想，限制 Producer 端生产数据的速率，从而控制上游 Producer 端向下游 Consumer 端发送数据的速率。但是静态限速会存在问题：</p><ol><li>通常无法事先预估下游 Consumer 端能承受的最大速率。</li><li>就算通过某种方式预估出下游 Consumer 端能承受的最大速率，在运行过程中也可能会因为网络抖动、CPU 共享竞争、内存紧张、IO阻塞等原因造成下游 Consumer 的吞吐量降低，但是上游 Producer 的吞吐量正常，然后又会出现之前所说的下游接收区的 Receive Buffer 有限，上游一直有源源不断的数据发送到下游的问题，还是会造成下游要么丢数据，要么为了不丢数据 buffer 不断扩充导致下游 OOM 的问题。</li></ol><p>综上所述，我们发现了，上游 Producer 端必须有一个限流的策略，且静态限流是不可靠的，于是就需要一个动态限流的策略。可以采用下图所示的动态反馈策略：</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmtzqmmcij24n118rdoq.jpg" alt="undefined"></p><p>下游 Consumer 端会频繁地向上游 Producer 端进行动态反馈，告诉 Producer 下游 Consumer 的负载能力，从而使 Producer 端可以动态调整向下游 Consumer 发送数据的速率，以实现 Producer 端的动态限流。当 Consumer 端处理较慢时，Consumer 将负载反馈到 Producer 端，Producer 端会根据反馈适当降低 Producer 自身从上游或者 Source 端读数据的速率来降低向下游 Consumer 发送数据的速率。当 Consumer 处理负载能力提升后，又及时向 Producer 端反馈，Producer 会通过提升自身从上游或 Source 端读数据的速率来提升向下游发送数据的速率，通过动态反馈的策略来动态调整系统整体的吞吐量。</p><p>读到这里，应该知道 Flink 为什么需要网络流控机制了，并且知道 Flink 的网络流控机制必须是一个动态反馈的策略。但是还有以下几个问题：</p><ol><li>Flink 中数据具体是怎么从上游 Producer 端发送到下游 Consumer 端的？</li><li>Flink 的动态限流具体是怎么实现的？下游的负载能力和压力是如何传递给上游的？</li></ol><p>带着这两个问题，学习下面的 Flink 网络流控与反压机制。</p><h3 id="Flink-1-5-之前网络流控机制介绍"><a href="#Flink-1-5-之前网络流控机制介绍" class="headerlink" title="Flink 1.5 之前网络流控机制介绍"></a>Flink 1.5 之前网络流控机制介绍</h3><p>在 Flink 1.5 之前，Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！Flink 利用自身作为纯数据流引擎的优势来优雅地响应反压问题。下面我们会深入分析 Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速的。</p><p>如下图所示，Job 分为 Task A、B、C，Task A 是 Source Task、Task B 处理转换数据、Task C 是 Sink Task。</p><ul><li><p>Task A 从外部 Source 端读取到数据后将数据序列化放到 Send Buffer 中，再由 Task A 的 Send Buffer 发送到 Task B 的 Receive Buffer；</p></li><li><p>Task B 的算子从 Task B 的 Receive Buffer 中将数据反序列后进行处理，将处理后数据序列化放到 Task B 的 Send Buffer 中，再由 Task B 的 Send Buffer 发送到 Task C 的 Receive Buffer；</p></li><li><p>Task C 再从 Task C 的 Receive Buffer 中将数据反序列后输出到外部 Sink 端，这就是所有数据的传输和处理流程。</p></li><li><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu01wapij24uk0xkahl.jpg" alt="undefined"></p></li><li><p>Flink 中，动态反馈策略原理比较简单，假如 Task C 由于各种原因吞吐量急剧降低，那么肯定会造成 Task C 的 Receive Buffer 中堆积大量数据，此时 Task B 还在给 Task C 发送数据，但是毕竟内存是有限的，持续一段时间后 Task C 的 Receive Buffer 满了，此时 Task B 发现 Task C 的 Receive Buffer 满了后，就不会再往 Task C 发送数据了，Task B 处理完的数据就开始往 Task B 的 Send Buffer 积压，一段时间后 Task B 的 Send Buffer 也满了，Task B 的处理就会被阻塞，这时 Task A 还在往 Task B 的 Receive Buffer 发送数据。</p><p>同样的道理，Task B 的 Receive Buffer 很快满了，导致 Task A 不再往 Task B 发送数据，Task A 的 Send Buffer 也会被用完，Task A 是 Source Task 没有上游，所以 Task A 直接降低从外部 Source 端读取数据的速率甚至完全停止读取数据。</p><p>通过以上原理，Flink 将下游的压力传递给上游。</p><p>如果下游 Task C 的负载能力恢复后，如何将负载提升的信息反馈给上游呢？</p><p>实际上 Task B 会一直向 Task C 发送探测信号，检测 Task C 的 Receive Buffer 是否有足够的空间，当 Task C 的负载能力恢复后，Task C 会优先消费 Task C Receive Buffer 中的数据，Task C Receive Buffer 中有足够的空间时，Task B 会从 Send Buffer 继续发送数据到 Task C 的 Receive Buffer，Task B 的 Send Buffer 有足够空间后，Task B 又开始正常处理数据，很快 Task B 的 Receive Buffer 中也会有足够空间，同理，Task A 会从 Send Buffer 继续发送数据到 Task B 的 Receive Buffer，Task A 的 Receive Buffer 有足够空间后，Task A 就可以从外部的 Source 端开始正常读取数据了。</p><p>通过以上原理，Flink 将下游负载过低的消息传递给上游。所以说 Flink 利用自身纯数据流引擎的优势优雅地响应反压问题，并没有任何复杂的机制来解决反压。上述流程，就是 Flink 动态限流（反压机制）的简单描述，可以看到 Flink 的反压是从下游往上游传播的，一直往上传播到 Source Task 后，Source Task 最终会降低或提升从外部 Source 端读取数据的速率。</p><p>如下图所示，对于一个 Flink 任务，动态反馈要考虑如下两种情况：</p><p>\1. 跨 Task，动态反馈具体如何从下游 Task 的 Receive Buffer 反馈给上游 Task 的 Send Buffer。</p><ul><li>当下游 Task C 的 Receive Buffer 满了，如何告诉上游 Task B 应该降低数据发送速率；</li><li>当下游 Task C 的 Receive Buffer 空了，如何告诉上游 Task B 应该提升数据发送速率。</li></ul><blockquote><p>注：这里又分了两种情况，Task B 和 Task C 可能在同一个 TaskManager 上运行，也有可能不在同一个 TaskManager 上运行。</p><ol><li>Task B 和 Task C 在同一个 TaskManager 运行指的是：一个 TaskManager 包含了多个 Slot，Task B 和 Task C 都运行在这个 TaskManager 上。此时 Task B 给 Task C 发送数据实际上是同一个 JVM 内的数据发送，所以<strong>不存在网络通信</strong>。</li><li>Task B 和 Task C 不在同一个 TaskManager 运行指的是：Task B 和 Task C 运行在不同的 TaskManager 中。此时 Task B 给 Task C 发送数据是跨节点的，所以<strong>会存在网络通信</strong>。</li></ol></blockquote><p>\2. Task 内，动态反馈如何从内部的 Send Buffer 反馈给内部的 Receive Buffer。</p><ul><li>当 Task B 的 Send Buffer 满了，如何告诉 Task B 内部的 Receive Buffer，自身的 Send Buffer 已经满了？要让 Task B 的 Receive Buffer 感受到压力，才能把下游的压力传递到 Task A。</li><li>当 Task B 的 Send Buffer 空了，如何告诉 Task B 内部的 Receive Buffer 下游 Send Buffer 空了，并把下游负载很低的消息传递给 Task A。</li></ul><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu0fonm6j24uk0xkdtz.jpg" alt="undefined"></p><p>到目前为止，动态反馈的具体细节抽象成了三个问题：</p><ul><li>跨 Task 且 Task 不在同一个 TaskManager 内，动态反馈具体如何从下游 Task 的 Receive Buffer 反馈给上游 Task 的 Send Buffer；</li><li>跨 Task 且 Task 在同一个 TaskManager 内，动态反馈具体如何从下游 Task 的 Receive Buffer 反馈给上游 Task 的 Send Buffer；</li><li>Task 内，动态反馈具体如何从 Task 内部的 Send Buffer 反馈给内部的 Receive Buffer。</li></ul><h4 id="TaskManager-之间网络传输相关组件"><a href="#TaskManager-之间网络传输相关组件" class="headerlink" title="TaskManager 之间网络传输相关组件"></a>TaskManager 之间网络传输相关组件</h4><p>如下图所示，是 TaskManager 之间数据传输流向，可以看到：</p><ul><li>Source Task 给 Task B 发送数据，Source Task 做为 Producer，Task B 做为 Consumer，Producer 端产生的数据最后通过网络发送给 Consumer 端。</li><li>Producer 端 Operator 实例对一条条的数据进行处理，处理完的数据首先缓存到 ResultPartition 内的 ResultSubPartition 中。</li><li>ResultSubPartition 中一个 Buffer 写满或者超时后，就会触发将 ResultSubPartition 中的数据拷贝到 Producer 端 Netty 的 Buffer 中，之后又把数据拷贝到 Socket 的 Send Buffer 中，这里有一个从用户态拷贝到内核态的过程，最后通过 Socket 发送网络请求，把 Send Buffer 中的数据发送到 Consumer 端的 Receive Buffer。</li><li>数据到达 Consumer 端后，再依次从 Socket 的 Receive Buffer 拷贝到 Netty 的 Buffer，再拷贝到 Consumer Operator InputGate 内的 InputChannel 中，最后 Consumer Operator 就可以读到数据进行处理了。</li></ul><p>这就是两个 TaskManager 之间的数据传输过程，我们可以看到发送方和接收方各有三层的 Buffer。当 Task B 往下游发送数据时，整个流程与 Source Task 给 Task B 发送数据的流程类似。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu0r9of4j25n424ek67.jpg" alt="undefined"></p><p>根据上述流程，下表中对 Flink 通信相关的一些术语进行介绍：</p><p>| 概念/术语          | 解释                                                         |<br>| :—————– | :———————————————————– |<br>| ResultPartition    | 生产者生产的数据首先写入到 ResultPartition 中，一个 Operator 实例对应一个ResultPartition。 |<br>| ResultSubpartition | 一个 ResultPartition 是由多个 ResultSubpartition 组成。当 Producer Operator 实例生产的数据要发送给下游 Consumer Operator n 个实例时，那么该 Producer Operator 实例对应的 ResultPartition 中就包含 n 个 ResultSubpartition。 |<br>| InputGate          | 消费者消费的数据来自于 InputGate 中，一个 Operator 实例对应一个InputGate。网络中传输的数据会写入到 Task 的 InputGate。 |<br>| InputChannel       | 一个 InputGate 是由多个 InputChannel 组成。当 Consumer Operator 实例读取的数据来自于上游 Producer Operator n 个实例时，那么该 Consumer Operator 实例对应的 InputGate 中就包含 n 个 InputChannel。 |<br>| RecordReader       | 用于将记录从Buffer中读出。                                   |<br>| RecordWriter       | 用于将记录写入Buffer。                                       |<br>| LocalBufferPool    | 为 ResultPartition 或 InputGate 分配内存，每一个 ResultPartition 或 InputGate分别对应一个 LocalBufferPool。 |<br>| NetworkBufferPool  | 为 LocalBufferPool 分配内存，NetworkBufferPool 是 Task 之间共享的，每个 TaskManager 只会实例化一个。 |</p><p>InputGate 和 ResultPartition 的内存是如何申请的呢？如下图所示，了解一下 Flink 网络传输相关的内存管理。</p><ul><li>在 TaskManager 初始化时，Flink 会在 NetworkBufferPool 中生成一定数量的内存块 MemorySegment，内存块的总数量就代表了网络传输中所有可用的内存。</li><li>NetworkBufferPool 是 Task 之间共享的，每个 TaskManager 只会实例化一个。</li><li>Task 线程启动时，会为 Task 的 InputChannel 和 ResultSubPartition 分别创建一个 LocalBufferPool。InputGate 或 ResultPartition 需要写入数据时，会向相对应的 LocalBufferPool 申请内存（图中①），当 LocalBufferPool 没有足够的内存且还没到达 LocalBufferPool 设置的上限时，就会向 NetworkBufferPool 申请内存（图中②），并将内存分配给相应的 InputChannel 或 ResultSubPartition（图③④）。</li><li>虽然可以申请，但是必须明白内存申请肯定是有限制的，不可能无限制的申请，我们在启动任务时可以指定该任务最多可能申请多大的内存空间用于 NetworkBufferPool。</li><li>当 InputChannel 的内存块被 Operator 读取消费掉或 ResultSubPartition 的内存块已经被写入到了 Netty 中，那么 InputChannel 和 ResultSubPartition 中的内存块就可以还给 LocalBufferPool 了（图中⑤），如果 LocalBufferPool 中有较多空闲的内存块，就会还给 NetworkBufferPool （图中⑥）。</li></ul><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu14j8zbj24dz2977fd.jpg" alt="undefined"></p><p>了解了 Flink 网络传输相关的内存管理，我们来分析 3 种动态反馈的具体细节。</p><h4 id="跨-Task-且-Task-不在同一个-TaskManager-内时，反压如何向上游传播"><a href="#跨-Task-且-Task-不在同一个-TaskManager-内时，反压如何向上游传播" class="headerlink" title="跨 Task 且 Task 不在同一个 TaskManager 内时，反压如何向上游传播"></a>跨 Task 且 Task 不在同一个 TaskManager 内时，反压如何向上游传播</h4><p>如下图所示，Producer 端生产数据速率为 2MB/s，Consumer 消费数据速率为 1MB/s。持续下去，下游消费较慢，Buffer 容量又是有限的，那 Flink 反压是怎么做的？</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu1fqtm9j25pf28mwvx.jpg" alt="undefined"></p><p>数据从 Task A 的 ResultSubPartition 按照上面的流程最后传输到 Task B 的 InputChannel 供 Task B 读取并计算。持续一段时间后，由于 Task B 消费比较慢，导致 InputChannel 被占满了，所以 InputChannel 向 LocalBufferPool 申请新的 Buffer 空间，LocalBufferPool 分配给 InputChannel 一些 Buffer。</p><p>再持续一段时间后，InputChannel 重复向 LocalBufferPool 申请 Buffer 空间，导致 LocalBufferPool 内的 Buffer 空间被用完了，所以 LocalBufferPool 向 NetWorkBufferPool 申请 Buffer 空间，NetWorkBufferPool 给 LocalBufferPool 分配 Buffer。再持续下去，NetWorkBufferPool 也用完了，或者说 NetWorkBufferPool 不能把自己的 Buffer 全分配给 Task B 对应的 LocalBufferPool，因为 TaskManager 上一般会运行了多个 Task，每个 Task 只能使用 NetWorkBufferPool 中的一部分。</p><p>此时可以认为 Task B 把自己可以使用的 LocalBufferPool 和 NetWorkBufferPool 都用完了。此时 Netty 还想把数据写入到 InputChannel，但是发现 InputChannel 满了，所以 Socket 层会把 Netty 的 autoRead disable，Netty 不会再从 Socket 中去读消息。由于 Netty 不从 Socket 的 Receive Buffer 读数据了，所以很快 Socket 的 Receive Buffer 就会变满，TCP 的 Socket 通信有动态反馈的流控机制，会把下游容量为 0 的消息反馈给上游发送端，所以上游的 Socket 就不会往下游再发送数据。</p><p>可以看到下图中 Consumer 端多个通道显示 ❌，表示该通道所能提供的内存已经被申请完，数据已经不能往下游写了，发生了阻塞。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu286qy3j25pf2jfdzt.jpg" alt="undefined"></p></li></ul><p>此时 Task A 持续生产数据，发送端 Socket 的 Send Buffer 很快被打满，所以 Task A 端的 Netty 也会停止往 Socket 写数据。数据会在 Netty 的 Buffer 中缓存数据，Netty 的 Buffer 是无界的，可以设置 Netty 的高水位，即：设置一个 Netty 中 Buffer 的上限。</p><p>所以每次 ResultSubPartition 向 Netty 中写数据时，都会检测 Netty 是否已经到达高水位，如果达到高水位就不会再往 Netty 中写数据，防止 Netty 的 Buffer 无限制的增长。接下来，数据会在 Task A 的 ResultSubPartition 中累积，ResultSubPartition 数据写满后，会向 LocalBufferPool 申请新的 Buffer 空间，LocalBufferPool 分配给 ResultSubPartition 一些 Buffer。</p><p>持续下去 LocalBufferPool 也会用完，LocalBufferPool 再向 NetWorkBufferPool 申请 Buffer。NetWorkBufferPool 也会被用完，或者说 NetWorkBufferPool 不能把自己的 Buffer 全分配给 Task A 对应的 LocalBufferPool，因为 TaskManager 上一般会运行了多个 Task，每个 Task 只能使用 NetWork BufferPool 中的一部分。此时，Task A 已经申请不到任何的 Buffer 了，Task A 的 Record Writer 输出就被 wait，Task A 不再生产数据。如下图所示，Producer 和 Consumer 端所有的通道都被阻塞。</p><p>当下游 Task B 持续消费，Task B 的 InputChannel 中部分的 Buffer 可以被回收，所有被阻塞的数据通道会被一个个打开，之后 Task A 又可以开始正常的生产数据了。通过上述的整个流程，来动态反馈，保障各个 Buffer 都不会因为数据太多导致内存溢出。</p><h4 id="跨-Task-且-Task-在同一个-TaskManager-内，反压如何向上游传播"><a href="#跨-Task-且-Task-在同一个-TaskManager-内，反压如何向上游传播" class="headerlink" title="跨 Task 且 Task 在同一个 TaskManager 内，反压如何向上游传播"></a>跨 Task 且 Task 在同一个 TaskManager 内，反压如何向上游传播</h4><p>一般情况下，一个 TaskManager 内会运行多个 slot，每个 slot 内运行一个 SubTask。所以，Task 之间的数据传输可能存在上游的 Task A 和下游的 Task B 运行在同一个 TaskManager 的情况，整个数据传输流程与上述类似，只不过由于 Task A 和 B 运行在同一个 JVM，所以不需要网络传输的环节，Task A 会将 Buffer 直接交给 Task B，一旦 Task B 消费了该 Buffer，则该 Buffer 就会被 Task A ResultSubPartition 对应的 LocalBufferPool 回收。</p><p>如果 Task B 消费的速度一直比 Task A 生产的速度慢，持续下去就会导致 Task A 申请不到 LocalBufferPool，最终造成 Task A 生产数据被阻塞。当下游 Task B 消费速度恢复后，Task A 就可以回收 ResultSubPartition 对应的已经被 Task B 消费的 Buffer，Task A 又可以正常的开始生产数据了，通过上述流程，来实现跨 Task 且 Task 在同一个 TaskManager 内的动态反馈。</p><h4 id="Task-内部，反压如何向上游传播"><a href="#Task-内部，反压如何向上游传播" class="headerlink" title="Task 内部，反压如何向上游传播"></a>Task 内部，反压如何向上游传播</h4><p>假如 Task A 的下游所有 Buffer 都占满了，那么 Task A 的 Record Writer 会被 block，Task A 的 Record Reader、Operator、Record Writer 都属于同一个线程，Task A 的 Record Reader 也会被 block。</p><p>这里分为两种情况，假如 Task A 是 Source Task，那么 Task A 就不会从外部的 Source 端读取数据，假如 Task A 还有上游的 Task，那么 Task A 就不会从自身的 InputChannel 中读取数据，然后又通过第一种动态反馈策略，将 Task A 的压力反馈给 Task A 的上游 Task。</p><p>当 Task A 的下游消费恢复后，ResultSubPartition 就可以申请到 Buffer，Task A 的 Record Writer 就不会被 block，Task A 就可以恢复正常的消费。通过上述流程，来实现 Task 内部的动态反馈。</p><p>通过以上三种情况的分析，得出的结论：<strong>Flink 1.5 之前并没有特殊的机制来处理反压，因为 Flink 中的数据传输相当于已经提供了应对反压的机制。</strong></p><h3 id="基于-Credit-的反压机制"><a href="#基于-Credit-的反压机制" class="headerlink" title="基于 Credit 的反压机制"></a>基于 Credit 的反压机制</h3><h4 id="1-5-之前反压机制存在的问题"><a href="#1-5-之前反压机制存在的问题" class="headerlink" title="1.5 之前反压机制存在的问题"></a>1.5 之前反压机制存在的问题</h4><p>看似完美的反压机制，其实是有问题的。</p><p>如下图所示，我们的任务有 4 个 SubTask，SubTask A 是 SubTask B的上游，即 SubTask A 给 SubTask B 发送数据。Job 运行在两个 TaskManager中，TaskManager 1 运行着 SubTask A1 和 SubTask A2，TaskManager 2 运行着 SubTask B1 和 SubTask B2。假如 SubTask B2 遇到瓶颈、处理速率有所下降，上游源源不断地生产数据，最后导致 SubTask A2 与 SubTask B2 产生反压。虽然此时 SubTask B1 没有压力，但是发现在 SubTask A1 和 A2 中都积压了很多 SubTask B1 的数据。本来只是 SubTask B2 遇到瓶颈了，但是也影响到 SubTask B1 的正常处理，为什么呢？</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu2u967sj24vj1w34ez.jpg" alt="undefined"></p><p>这里需要明确一点：不同 Job 之间的每个（远程）网络连接将在 Flink 的网络堆栈中获得自己的TCP通道。但是，如果同一 Task 的不同 SubTask 被安排到同一个 TaskManager，则它们与其他 TaskManager 的网络连接将被多路复用并共享一个TCP信道以减少资源使用。图中的 SubTask A1 和 A2 是 Task A 不同并行度的实例，且安排到同一个 TaskManager 内部，所以 SubTask A1 和 A2 与其他 TaskManager 进行网络数据传输时共享同一个 TCP 信道。同理，SubTask B1 和 B2 与其他 TaskManager 进行网络数据传输时也共享同一个 TCP 信道。所以，图中所示的 A1 → B1、A1 → B2、A2 → B1、A2 → B2 这四个网络连接将会多路复用共享一个 TCP 信道。</p><p>从上面跨 TaskManager 的反压流程，我们知道现在 SubTask B1 没有压力，根据跨 TaskManager 之间的动态反馈（反压）原理，当 SubTask A2 与 SubTask B2 产生反压时，会把 TaskManager1 端任务对应 Socket 的 Send Buffer 和 TaskManager2 端该任务对应 Socket 的 Receive Buffer 占满，也就是说多路复用的 TCP 通道被完全阻塞了或者整个 TCP 通道的传输速率大大降低了，导致 SubTask A1 和 SubTask A2 发送给 SubTask B1 的数据被阻塞了，使得本来没有压力的 SubTask B1 现在也接收不到数据了。所以，Flink 1.5 之前的反压机制会存在当一个 SubTask 出现反压时，可能导致其他正常的 SubTask 也接收不到数据。</p><h4 id="基于-Credit-的反压机制原理"><a href="#基于-Credit-的反压机制原理" class="headerlink" title="基于 Credit 的反压机制原理"></a>基于 Credit 的反压机制原理</h4><p>为了解决上述所描述的问题，Flink 1.5 之后的版本，引入了基于 Credit 的反压机制。如下图所示，反压机制直接作用于 Flink 的应用层，即在 ResultSubPartition 和 InputChannel 这一层引入了反压机制。基于 Credit 的流量控制可确保发送端已经发送的任何数据，接收端都具有足够的 Buffer 来接收。上游 SubTask 给下游 SubTask 发送数据时，会把 Buffer 中要发送的数据和上游 ResultSubPartition 堆积的数据量 Backlog size 发给下游，下游接收到上游发来的 Backlog size 后，会向上游反馈现在的 Credit 值，Credit 值表示目前下游可以接收上游的 Buffer 量，1 个Buffer 等价于 1 个 Credit。上游接收到下游反馈的 Credit 值后，上游下次最多只会发送 Credit 个数据到下游，保障不会有数据积压在 Socket 这一层。</p><p>Flink 1.5 之前一个 Operator 实例对应一个InputGate，每个 InputGate 的多个 InputChannel 共用一个 LocalBufferPool。Flink 1.5 之后每个 Operator 实例的每个远程输入通道(Remote InputChannel)现在都有自己的一组独占缓冲区(Exclusive Buffer)，而不是只有一个共享的 LocalBufferPool。与之前不同，LocalBufferPool 的缓冲区称为流动缓冲区(Floating buffers)，每个 Operator 对应一个 Floating buffers，Floating buffers 内的 buffer 会在 InputChannel 间流动并且可用于每个 InputChannel。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu37bif0j251w1fwqgi.jpg" alt="undefined"></p><p>如上图所示，上游 SubTask A2 发送完数据后，还有 4 个 Buffer 被积压，会把要发送的 Buffer 数据和 Backlog size = 4 一块发送给下游 SubTask B2，下游接受到数据后，知道上游积压了 4 个Buffer 的数据，于是向 Buffer Pool 申请 Buffer，申请完成后由于容量有限，下游 InputChannel 目前仅有 2 个 Buffer 空间，所以，SubTask B2 会向上游 SubTask A2 反馈 Channel Credit = 2，上游就知道了下游目前最多只能承载 2 个 Buffer 的数据。</p><p>所以下一次上游给上游发送数据时，最多只给下游发送 2 个 Buffer 的数据。当下游 SubTask 反压比较严重时，可能就会向上游反馈 Channel Credit = 0，此时上游就知道下游目前对应的 InputChannel 没有可用空间了，所以就不向下游发送数据了。</p><p>此时，上游还会定期向下游发送探测信号，检测下游返回的 Credit 是否大于 0，当下游返回的 Credit 大于 0 表示下游有可用的 Buffer 空间，上游就可以开始向下游发送数据了。</p><p>通过这种基于 Credit 的反馈策略，就可以保证每次上游发送的数据都是下游 InputChannel 可以承受的数据量，所以在公用的 TCP 这一层就不会产生数据堆积而影响其他 SubTask 通信。基于 Credit 的反压机制还带来了一个优势：由于我们在发送方和接收方之间缓存较少的数据，可能会更早地将反压反馈给上游，缓冲更多数据只是把数据缓冲在内存中，并没有提高处理性能。</p><h3 id="Flink-如何定位产生反压的位置？"><a href="#Flink-如何定位产生反压的位置？" class="headerlink" title="Flink 如何定位产生反压的位置？"></a>Flink 如何定位产生反压的位置？</h3><h4 id="反压监控原理介绍"><a href="#反压监控原理介绍" class="headerlink" title="反压监控原理介绍"></a>反压监控原理介绍</h4><p>Flink 的反压太过于天然了，导致无法简单地通过监控 BufferPool 的使用情况来判断反压状态。Flink 通过对运行中的任务进行采样来确定其反压，如果一个 Task 因为反压导致处理速度降低了，那么它肯定会卡在向 LocalBufferPool 申请内存块上。那么该 Task 的 stack trace 应该是这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Object.wait(Native Method)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:163)</span><br><span class="line">o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) &lt;--- BLOCKING request</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><p>Flink 的反压监控就是依赖上述原理，通过不断对每个 Task 的 stack trace 采样来进行反压监控。由于反压监控对正常的任务运行有一定影响，因此只有当 Web 页面切换到 Job 的 BackPressure 页面时，JobManager 才会对该 Job 触发反压监控。</p><p>默认情况下，JobManager 会触发 100 次 stack trace 采样，每次间隔 50ms 来确定反压。Web 界面看到的比率表示在内部方法调用中有多少 stack trace 被卡在 LocalBufferPool.requestBufferBlocking()，例如：0.01 表示在 100 个采样中只有 1 个被卡在 LocalBufferPool.requestBufferBlocking()。采样得到的比例与反压状态的对应关系如下：</p><ul><li>OK：0 &lt;= 比例 &lt;= 0.10</li><li>LOW：0.10 &lt; 比例 &lt;= 0.5</li><li>HIGH：0.5 &lt; 比例 &lt;= 1</li></ul><p>Task 的状态为 OK 表示没有反压，HIGH 表示这个 Task 被反压。</p><h4 id="利用-Flink-Web-UI-定位产生反压的位置"><a href="#利用-Flink-Web-UI-定位产生反压的位置" class="headerlink" title="利用 Flink Web UI 定位产生反压的位置"></a>利用 Flink Web UI 定位产生反压的位置</h4><p>如下图所示，表示 Flink Web UI 中 BackPressure 选项卡，可以查看任务中 subtask 的反压状态。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu3kjwjjj21g00tm78k.jpg" alt="undefined"></p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu3pgp98j21b90tm75i.jpg" alt="undefined"></p><p>如果看到一个 Task 发生反压警告（例如：High），意味着它生产数据的速率比下游 Task 消费数据的速率要快。在工作流中数据记录是从上游向下游流动的（例如：从 Source 到 Sink）。反压沿着相反的方向传播，沿着数据流向上游传播。以一个简单的 Source -&gt; Sink Job 为例。如果看到 Source 发生了警告，意味着 Sink 消费数据的速率比 Source 生产数据的速率要慢，Sink 的吞吐量降低了，Sink 正在向上游的 Source 算子产生反压。应该找 Sink 出了什么问题导致反压，而不是找 Source 出了什么问题。</p><p>假如一个 Job 由 Task A、B、C 组成，数据流向是 A → B → C，当看到 Task A、B 的反压状态为 HIGH，Task C 的反压状态为 OK 时，实际上是 C 的吞吐量较低导致的，为什么呢？从实现原理来讲，当 Task C 吞吐量较低时，Task C 会产生反压且 InputChannel 所有可以申请的 Buffer 已经占满了，Task C 会给上游 Task B 返回 Credit = 0，导致 Task B 的数据发送不到 Task C，Task C 此时不需要申请 Buffer 空间，所以 Task C 的 stack trace 不会卡在 LocalBufferPool.requestBufferBlocking()，Task C 此时在处理那些 InputChannel 中待处理的数据。再来分析 Task B，Task B 此时正在处理数据，需要将处理完的数据输出到 ResultSubPartition，但此时 ResultSubPartition 在 LocalBufferPool 申请不到空闲的 Buffer 空间，所以 Task B 会卡在 LocalBufferPool.requestBufferBlocking() 这一步等待申请 Buffer 空间。同理可得，当 Task C 反压比较严重时，Task B 上游的 Task A 也会卡在 LocalBufferPool.requestBufferBlocking()。得出结论：当 Flink 的某个 Task 出现故障导致吞吐量严重下降时，在 Flink 的反压页面，我们会看到该 Task 的反压状态为 OK，而该 Task 上游所有 Task 的反压状态为 HIGH。所以，我们根据 Flink 的 BackPressure 页面去定位哪个 Task 出故障时，首先要找到反压状态为 HIGH 的最后一个 Task，该 Task 紧跟的下一个 Task 就是我们要找的有故障的 Task。</p><p>如下图所示是一个 Job 的执行计划图，任务被切分为三个 Operator 分别是 Source、FlatMap、Sink。当看到 Source 和 FlatMap 的 BackPressure 页面都显示 HIGH，Sink 的 BackPressure 页面显示 OK 时，意思是任务产生了反压，且反压的根源是 Sink，也就是说 Sink 算子目前遇到了性能瓶颈吞吐量较低，下一步就应该定位什么原因导致 Sink 算子吞吐量降低。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu3wdo6wj21k705qwfm.jpg" alt="undefined"></p><p>当集群中网络 IO 遇到瓶颈时也可能会导致 Job 产生反压，假设有两个 Task A 和 B，Task A 是 Task B 的上游，若 Task B 的吞吐量很高，但是由于网络瓶颈，造成 Task A 的数据不能快速的发送给 Task B，所以导致上游 Task A 被反压了。此时在反压监控页面也会看到 Task A 的反压状态为 HIGH、Task B 的反压状态为 OK，但实际上并不是 Task B 遇到了瓶颈。像这种网络遇到瓶颈的情况应该比较少见，但大家要清楚可能会出现，如果发现 Task B 没有任何瓶颈时，要注意查看是不是网络瓶颈导致。</p><p>如下图所示是一个多 Sink 任务流的执行计划，任务被切分为四个 Operator 分别是 Source、FlatMap、HBase Sink 和 Redis Sink。当看到 Source 和 FlatMap 的 BackPressure 页面都显示 HIGH，HBase Sink 和 Redis Sink 的 BackPressure 页面显示 OK 时，意思是任务产生了反压，且反压的根源是 Sink，但此时无法判断到底是 HBase Sink 还是 Redis Sink 出现了故障。这种情况，该如何来定位反压的来源呢？来学习我们下一部分利用 Flink Metrics 定位产生反压的位置。</p><p><img src="https://ws1.sinaimg.cn/large/bec9bff2gy1gdmu43l5cvj21ib0eodhd.jpg" alt="undefined"></p><h4 id="利用-Flink-Metrics-定位产生反压的位置"><a href="#利用-Flink-Metrics-定位产生反压的位置" class="headerlink" title="利用 Flink Metrics 定位产生反压的位置"></a>利用 Flink Metrics 定位产生反压的位置</h4><p>当某个 Task 吞吐量下降时，基于 Credit 的反压机制，上游不会给该 Task 发送数据，所以该 Task 不会频繁卡在向 Buffer Pool 去申请 Buffer。反压监控实现原理就是监控 Task 是否卡在申请 buffer 这一步，所以遇到瓶颈的 Task 对应的反压页面必然会显示 OK，即表示没有受到反压。如果该 Task 吞吐量下降，造成该 Task 上游的 Task 出现反压时，必然会存在：该 Task 对应的 InputChannel 变满，已经申请不到可用的 Buffer 空间。</p><p>如果该 Task 的 InputChannel 还能申请到可用 Buffer，那么上游就可以给该 Task 发送数据，上游 Task 也就不会被反压了，所以说遇到瓶颈且导致上游 Task 受到反压的 Task 对应的 InputChannel 必然是满的（这里不考虑网络遇到瓶颈的情况）。</p><p>从这个思路出发，可以对该 Task 的 InputChannel 的使用情况进行监控，如果 InputChannel 使用率 100%，那么该 Task 就是我们要找的反压源。Flink Metrics 提供了 inputExclusiveBuffersUsage、 inputFloatingBuffersUsage、inPoolUsage 等参数可以帮助我们来监控 InputChannel 的 Buffer 使用情况。</p><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">解释</th></tr></thead><tbody><tr><td style="text-align:left">inputFloatingBuffersUsage</td><td style="text-align:left">每个 Operator 实例对应一个 FloatingBuffers，inputFloatingBuffersUsage 表示 Operator 对应的 FloatingBuffers 使用率</td></tr><tr><td style="text-align:left">inputExclusiveBuffersUsage</td><td style="text-align:left">每个 Operator 实例的每个远程输入通道(Remote InputChannel)都有自己的一组独占缓冲区(ExclusiveBuffer)，inputExclusiveBuffersUsage 表示 ExclusiveBuffer 的使用率</td></tr><tr><td style="text-align:left">inPoolUsage</td><td style="text-align:left">Flink 1.5 - 1.8 中的 inPoolUsage 表示 inputFloatingBuffersUsage。Flink 1.9 及以上版本 inPoolUsage 表示 inputFloatingBuffersUsage 和 inputExclusiveBuffersUsage 的总和</td></tr></tbody></table><p>Flink 输入 BufferPool 相关的 Metrics 还有 inputQueueLength 指标，类似于 inPoolUsage，但是 inputQueueLength 表示的是 Buffer 使用的个数，而 inPoolUsage 表示的使用率。有时候我们看到 buffer 使用的个数并不知道其是否压力大，因为我们没有拿到 buffer 的总数量，所以使用率会更直观，强烈建议使用 inPoolUsage。</p><p>上图案例中，若无法判断是 Redis Sink 还是 HBase Sink 吞吐量降低导致 Job 反压，只需要在 Flink Web UI 的 Metrics 页面分别查看两个 Task 的 inputExclusiveBuffersUsage、 inputFloatingBuffersUsage、inPoolUsage 参数，我们就可以定位到反压源。</p><p>定位反压源的流程首先通过 Flink Web UI 来定位，如果定位不到再通过 Metrics 来辅助我们精确定位。但上述几个 Metrics 参数仅适用于网络传输的情况，当任务执行过程中不存在数据网络传输时，就不存在 InputChannel 变满的情况，此时也无法通过 Metrics 来定位反压源，可以凭借开发者的经验或者改动代码、删掉可能是瓶颈的算子然后发布看处理性能是否提升来定位反压源。</p><h3 id="定位到反压来源后，该如何处理？"><a href="#定位到反压来源后，该如何处理？" class="headerlink" title="定位到反压来源后，该如何处理？"></a>定位到反压来源后，该如何处理？</h3><p>假设确定了反压源（瓶颈）的位置，下一步就是分析为什么会发生这种情况。下面，列出了从最基本到比较复杂的一些反压潜在原因。建议先检查基本原因，然后再深入研究更复杂的原因，最后找出导致瓶颈的原因。</p><p>还请记住，反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。另外，请记住，断断续续的反压会影响我们分析和解决问题。话虽如此，以下几点需要检查。</p><h4 id="系统资源"><a href="#系统资源" class="headerlink" title="系统资源"></a>系统资源</h4><p>首先，应该检查涉及服务器基本资源的使用情况，如 CPU、网络或磁盘 I/O。如果某些资源被充分利用或大量使用，则可以执行以下操作之一：</p><ol><li>尝试优化代码。代码分析器在这种情况下很有用。</li><li>针对特定的资源调优 Flink。</li><li>通过增加并行度或增加群集中的服务器数量来横向扩展。</li><li>减少瓶颈算子上游的并行度，从而减少瓶颈算子接受的数据量。这个方案虽然可以使得瓶颈算子压力减少，但是不建议，可能会造成整个 Job 的数据延迟增大。</li></ol><h4 id="垃圾收集（GC）"><a href="#垃圾收集（GC）" class="headerlink" title="垃圾收集（GC）"></a>垃圾收集（GC）</h4><p>通常，长时间GC暂停会导致性能问题。您可以通过打印调试GC日志（通过 <code>-XX:+PrintGCDetails</code>）或使用某些内存或 GC 分析器来验证是否处于这种情况。由于处理 GC 问题高度依赖于应用程序，独立于 Flink，因此不会在此详细介绍。</p><h4 id="CPU-线程瓶颈"><a href="#CPU-线程瓶颈" class="headerlink" title="CPU/线程瓶颈"></a>CPU/线程瓶颈</h4><p>有时，一个或几个线程导致 CPU 瓶颈，而整个机器的 CPU 使用率仍然相对较低，则可能无法看到 CPU 瓶颈。例如，48 核的服务器上，单个 CPU 瓶颈的线程仅占用 2％ 的 CPU 使用率，就算单个线程发生了 CPU 瓶颈，我们也看不出来。可以考虑使用代码分析器，它们可以显示每个线程的 CPU 使用情况来识别热线程。</p><h4 id="线程竞争"><a href="#线程竞争" class="headerlink" title="线程竞争"></a>线程竞争</h4><p>与上面的 CPU/线程瓶颈问题类似，subtask 可能会因为共享资源上高负载线程的竞争而成为瓶颈。同样，CPU 分析器是你最好的朋友！考虑在用户代码中查找同步开销、锁竞争，尽管避免在用户代码中添加同步。</p><h4 id="负载不平衡"><a href="#负载不平衡" class="headerlink" title="负载不平衡"></a>负载不平衡</h4><p>如果瓶颈是由数据倾斜引起的，可以尝试通过将数据分区的 key 进行加盐或通过实现本地预聚合来减轻数据倾斜的影响。关于数据倾斜的详细解决方案，会在第 9.6 节详细讨论。</p><h4 id="外部依赖"><a href="#外部依赖" class="headerlink" title="外部依赖"></a>外部依赖</h4><p>如果发现我们的 Source 端数据读取性能比较低或者 Sink 端写入性能较差，需要检查第三方组件是否遇到瓶颈，例如，Kafka 集群是否需要扩容，Kafka 连接器是否并行度较低，HBase 的 rowkey 是否遇到热点问题。关于第三方组件的性能问题，需要结合具体的组件来分析，这里不进行详细介绍。</p><p>以上情况并非很详细。通常，为了解决瓶颈和减小反压，首先要分析反压发生的位置，然后找出原因并解决。</p><h3 id="小结与反思-32"><a href="#小结与反思-32" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节详细介绍 Flink 的反压机制，首先讲述了 Flink 为什么需要网络流控机制，再介绍了 Flink 1.5 之前的网络流控机制以及存在的问题，从而引出了基于 Credit 的反压机制，最后讲述 Flink 如何定位产生反压的位置以及定位到反压源后该如何处理。某个 Flink Job 中从 Source 到 Sink 的所有算子都不产生 shuffle，当任务的吞吐量降低时，无论是通过 Flink WebUI 还是 Metrics 都无法来辅助定位任务的瓶颈，对于这种情况，如何来定位任务的瓶颈，并解决呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;大数据实时计算引擎-Flink-实战与性能优化&quot;&gt;&lt;a href=&quot;#大数据实时计算引擎-Flink-实战与性能优化&quot; class=&quot;headerlink&quot; title=&quot;大数据实时计算引擎 Flink 实战与性能优化&quot;&gt;&lt;/a&gt;大数据实时计算引擎 Flink 实战与性能优化&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g9a0z5j7i6j20bp04tdgk.jpg&quot; alt=&quot;undefined&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Apache/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>clearLastUpdated</title>
    <link href="http://yoursite.com/2020/03/17/clearLastUpdated/"/>
    <id>http://yoursite.com/2020/03/17/clearLastUpdated/</id>
    <published>2020-03-17T02:49:51.000Z</published>
    <updated>2020-04-10T17:05:42.554Z</updated>
    
    <content type="html"><![CDATA[<p>mvn库 windows清理脚本<br>需要把mvn的位置改成自己的<br><a id="more"></a><br>cls<br>@ECHO OFF<br>SET CLEAR_PATH=C:<br>SET CLEAR_DIR=C:\Users\Administrator.m2\repository<br>color 0a<br>TITLE ClearLastUpdated For Windows<br>GOTO MENU<br>:MENU<br>CLS<br>ECHO.<br>ECHO. <em> </em> <em> </em>  ClearLastUpdated For Windows  <em> </em> <em> </em><br>ECHO. <em> </em><br>ECHO. <em> 1 清理</em>.lastUpdated <em><br>ECHO. </em> <em><br>ECHO. </em> 2 查看<em>.lastUpdated </em><br>ECHO. <em> </em><br>ECHO. <em> 3 退 出 </em><br>ECHO. <em> </em><br>ECHO. <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em><br>ECHO.<br>ECHO.请输入选择项目的序号：<br>set /p ID=<br>IF “%id%”==”1” GOTO cmd1<br>IF “%id%”==”2” GOTO cmd2<br>IF “%id%”==”3” EXIT<br>PAUSE<br>:cmd1<br>ECHO. 开始清理<br>%CLEAR_PATH%<br>cd %CLEAR_DIR%<br>for /r %%i in (<em>.lastUpdated) do del %%i<br>ECHO.OK<br>PAUSE<br>GOTO MENU<br>:cmd2<br>ECHO. 查看</em>.lastUpdated文件<br>%CLEAR_PATH%<br>cd %CLEAR_DIR%<br>for /r %%i in (*.lastUpdated) do echo %%i<br>ECHO.OK<br>PAUSE<br>GOTO MENU </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;mvn库 windows清理脚本&lt;br&gt;需要把mvn的位置改成自己的&lt;br&gt;
    
    </summary>
    
      <category term="Script" scheme="http://yoursite.com/categories/Script/"/>
    
    
      <category term="Script" scheme="http://yoursite.com/tags/Script/"/>
    
  </entry>
  
  <entry>
    <title>SQL 从入门到精通</title>
    <link href="http://yoursite.com/2019/11/28/GitChat%20SQL%E8%AF%BE%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/11/28/GitChat SQL课程/</id>
    <published>2019-11-28T01:02:26.000Z</published>
    <updated>2019-11-28T01:02:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>SQL是一项重要的技能，可能是普通程序员在工作中最常用的。面试的时候造飞机大炮，最后可能还是得实实在在得写SQL。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cm9vd6brj20sg09hq9w.jpg" alt="SQL-1024x341.png"></p><a id="more"></a> <h1 id="Structured-Query-Language"><a href="#Structured-Query-Language" class="headerlink" title="Structured Query Language"></a>Structured Query Language</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>1970 年 IBM 的 E.F. Codd 博士发表了论文《A Relational Model of Data for Large Shared Data Banks》并创建了关系模型，通过一个简单的数据结构（关系，也就是二维表）来实现数据的存储。 </p><p>1979 年 Relational Software, Inc.（后来改名为 Oracle）发布了第一个商用的关系数据库产品。随后出现了大量的关系数据库管理系统，包括 MySQL、SQL Server、PostgreSQL 以及大数据分析平台 Apache Hive、Spark SQL、Presto 等。至今，关系数据库仍然是数据库领域的主流。 </p><p>以下是著名的数据库系统排名网站 <a href="https://db-engines.com/en/ranking" target="_blank" rel="noopener">DB-Engines</a> 上各种数据库的排名情况，关系数据库占据了绝对的优势。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmdeqm4cj20rl0abq43.jpg" alt="undefined"></p><p>SQL（Structured Query Language，结构化查询语言）是访问和操作关系数据库的标准语言。只要是关系数据库，都可以使用 SQL 进行访问和控制。SQL 同样由 IBM 在 1970 年代开发，1986 年成为 ANSI 标准，并且在 1987 年成为 ISO 标准。SQL 标准随后经历了多次修订，最新的版本为 SQL:2019，增加了多维数组（MDA）的支持。下图是 SQL 标准的发展历程和主要的新增功能。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmdrn7cdj21ln0ml46x.jpg" alt="undefined"></p><p>对于 SQL 标准，人们最熟悉的就是 SQL92 或者 SQL99。但实际上经过多次修改，SQL 早已不是 40 年前的 SQL；如今它已经相当完备，功能强大，并且能够同时支持关系模型和非关系（XML、JSON）模型。具体来说，最新的 SQL 标准包含 10 个部分：</p><ul><li>ISO/IEC 9075-1 信息技术 – 数据库语言 – SQL – 第1部分：框架（SQL/框架）</li><li>ISO/IEC 9075-2 信息技术 – 数据库语言 – SQL – 第2部分：基本原则（SQL/基本原则）</li><li>ISO/IEC 9075-3 信息技术 – 数据库语言 – SQL – 第3部分：调用级接口（SQL/CLI）</li><li>ISO/IEC 9075-4 信息技术 – 数据库语言 – SQL – 第4部分：持久存储模块（SQL/PSM）</li><li>ISO/IEC 9075-9 信息技术 – 数据库语言 – SQL – 第9部分：外部数据管理（SQL/MED）</li><li>ISO/IEC 9075-10 信息技术 – 数据库语言 – SQL – 第10部分：对象语言绑定（SQL/OLB）</li><li>ISO/IEC 9075-11 信息技术 – 数据库语言 – SQL – 第11部分：信息与定义概要（SQL/Schemata）</li><li>ISO/IEC 9075-13 信息技术 – 数据库语言 – SQL – 第13部分：使用 Java 编程语言的 SQL 程序与类型（SQL/JRT）</li><li>ISO/IEC 9075-14 信息技术 – 数据库语言 – SQL – 第14部分：XML 相关规范（SQL/XML）</li><li>ISO/IEC 9075-15 信息技术 – 数据库语言 – SQL – 第15部分：多维数组（SQL/MDA）</li></ul><p>为了便于学习，通常将主要的 SQL 语句分为以下几个类别：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cme6q6syj21ir0k5gs0.jpg" alt="undefined"></p><ul><li><strong>DQL</strong>（data query language），<strong>数据查询语言</strong>；也就是 SELECT 语句，用于查询数据库中的数据和信息。</li><li><strong>DML</strong>（data manipulation language），<strong>数据操作语言</strong>；用于对表中的数据进行增加（INSERT）、修改（UPDATE）、删除（DELETE）以及合并（MERGE）操作。</li><li><strong>DDL</strong>（data definition language），<strong>数据定义语言</strong>；主要用于定义数据库中的对象（例如表或索引），包括创建对象（CREATE）、修改对象（ALTER）和删除对象（DROP）等。</li><li><strong>TCL</strong>（transaction control language），<strong>事务控制语言</strong>；用于管理数据库的事务，主要包括启动一个事务（BEGIN TRANSACTION）、提交事务（COMMIT）、回退事务（ROLLBACK）和事务保存点（SAVEPOINT）。</li><li><strong>DCL</strong>（data control language），<strong>数据控制语言</strong>；用于控制数据的访问权限，主要有授权（GRANT）和撤销（REVOKE）。</li></ul><p>SQL 是一种标准，不同厂商基于 SQL 标准实现了自己的数据库产品，例如 Oracle、MySQL 等。这些数据库都在一定程度上兼容 SQL 标准，具有一定的可移植性。但另一方面，它们都存在许多专有的扩展，没有任何一种产品完全遵循标准。 </p><h4 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h4><p>随着互联网的发展和大数据的兴起，出现了各种各样的非关系（NoSQL）数据库。NoSQL 代表 <strong>Not only SQL</strong>，表明它是针对传统关系数据库的补充和升级，而不是为了替代关系数据库。</p><p>NoSQL 数据库主要用于解决关系数据库在某些特定场景下的局限性，比如海量存储和水平扩展；但同时也会为此牺牲某些关系数据库的特性，例如对事务强一致性的支持和标准 SQL 接口。因此，这类数据库主要用于对一致性要求不是非常严格的互联网业务。常见的 NoSQL 数据库可以分为以下几类：</p><ul><li>文档数据库，例如 <a href="https://www.mongodb.com/" target="_blank" rel="noopener">MongoDB</a>（MongoDB 4.0 增加了多文档事务的特性）；</li><li>键值存储，例如 <a href="https://redis.io/" target="_blank" rel="noopener">Redis</a>；</li><li>全文搜索引擎，例如 <a href="https://www.elastic.co/products/elasticsearch" target="_blank" rel="noopener">Elasticsearch</a>；</li><li>宽列存储数据库，例如 <a href="http://cassandra.apache.org/" target="_blank" rel="noopener">Cassandra</a>；</li><li>图形数据库，例如 <a href="https://neo4j.com/" target="_blank" rel="noopener">Neo4J</a>。</li></ul><p>另一方面，关系数据库也在积极拥抱变化，添加了许多非关系模型（XML 和 JSON）支持。以最流行的开源关系数据库 MySQL 为例，最新的 MySQL 8.0 版本增加了 JSON 文档存储的支持，并且推出了一个新的概念：NoSQL + SQL = MySQL。以下是 MySQL 官方的宣传图。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmfau87wj20c80el74n.jpg" alt="undefined"></p><p>Oracle、SQL Server 以及 PostgreSQL 同样也进行了类似的扩展，可以支持原生的 XML 和 JSON 数据，并且提供了许多标准的 SQL 接口。</p><h4 id="NewSQL"><a href="#NewSQL" class="headerlink" title="NewSQL"></a>NewSQL</h4><p>中国有句古话：<strong>天下大势，合久必分，分久必合</strong>。数据库领域的发展也印证了这一规律，为了同时获得关系数据库对于事务的支持和标准的 SQL 接口，以及非关系数据库的高度扩展性和高性能。如今市场上已经出现了一类新型关系型数据库系统：NewSQL 数据库。</p><p>比较有代表性的 NewSQL 数据库包括 Google Spanner、VoltDB、PostgreSQL-XL 以及国产的 TiDB。这类新型数据库是数据库领域最新的发展方向，有志于在数据库行业发展的同学可以加以关注。</p><h4 id="为什么要学习-SQL？"><a href="#为什么要学习-SQL？" class="headerlink" title="为什么要学习 SQL？"></a>为什么要学习 SQL？</h4><p>让我们回到专栏的主题，为什么要学习 SQL 呢？简单来说，因为有用。下图是 Stack Overflow 在 2019 年关于最流行编程技术的调查结果。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmfl8agzj20nc0ict9f.jpg" alt="undefined"></p><p>作为数据处理领域的专用语言，SQL 排在了第三位，超过 50% 的开发者都需要使用到 SQL。那么，具体什么职位需要使用 SQL，用 SQL 来做什么？</p><ul><li><strong>数据分析师</strong>：显然这是一群依靠分析数据为生的人，必不可少需要与数据库打交道，SQL 是他们必备技能之一。</li><li><strong>数据科学家</strong>：与数据分析师一样，数据科学家的日常工作也离不开数据的处理，不可避免需要使用 SQL。</li><li><strong>数据库开发工程师</strong>：这个职位基本就是写 SQL 代码，实现业务逻辑。</li><li><strong>数据库管理员</strong>：也就是 DBA，主要职责是管理和维护数据库，除了会写 SQL，还需要负责审核开发人员编写的 SQL 代码。</li><li><strong>后端工程师</strong>：后端开发必然需要涉及数据的处理，需要通过 SQL 与数据库进行交互。</li><li><strong>全栈工程师</strong>：既然是全栈，自然包括后端数据的处理。</li><li><strong>移动开发工程师</strong>：作为一名移动开发工程师，一定对 SQLite 数据库不会陌生，它是在移动设备中普遍存在的嵌入式数据库。</li><li><strong>产品经理</strong>：产品经理需要了解产品的情况，而数据是最好的说明方式，了解 SQL 非常有利于对产品的把握。</li></ul><p>SQL 不但应用广泛，而且简单易学。因为它在设计之初就考虑了非技术人员的使用需求，SQL 语句全都是由简单的英语单词组成，使用者只需要声明自己想要的结果，而将具体的实现过程交给数据库管理系统。</p><p>学习编程，你可能会犹豫选择 C++ 还是 Java；入门数据科学，你可能会纠结于选择 Python 还是 R；但无论如何，SQL 都是 IT 从业人员不可或缺的一项技能！</p><p>本专栏主要讨论 SQL 编程技术和思想，分为四个部分：基础篇、进阶篇、开发篇以及扩展篇。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmg49ezgj20qx0k575o.jpg" alt="undefined"></p><p><strong>第一部分：基础篇</strong>。首先介绍数据库领域的最新发展，回顾数据库和 SQL 的核心概念；然后讨论如何使用 SELECT 语句查询数据，过滤数据、对结果进行排序、实现排行榜与分页效果；同时还会介绍常见的 SQL 函数、CASE 表达式以及数据的分组汇总；最后是一个分析世界银行全球 GDP 数据的实战案例。</p><p><strong>第二部分：进阶篇</strong>。主要包括 SQL 数据分析的一些高级功能：空值的问题、多表连接查询、子查询、集合运算、通用表表达式与递归查询、高级分组与多维度交叉分析、窗口函数与高级报表以及基于行模式识别的数据流分析等。</p><p><strong>第三部分：开发篇</strong>。讲述数据库设计与开发过程中涉及到的一些实用知识。包括如何设计规范化的数据库、如何管理数据库对象、如何对数据进行增删改、数据库事务的概念、索引的原理；同时还会介绍视图的概念、如何使用存储过程实现业务逻辑以及如何利用触发器实现用户操作的审计。</p><p><strong>第四部分：扩展篇</strong>。我们将分析 SQL 语句的执行计划与查询语句的优化、使用 SQL 处理 JSON 数据、在 Python 和 Java 中执行 SQL 语句，并介绍动态语句和 SQL 注入攻击的预防。在专栏的最后，我们将探讨一下 SQL 编程中的道与术。</p><h2 id="SQL的世界里一切都是关系"><a href="#SQL的世界里一切都是关系" class="headerlink" title="SQL的世界里一切都是关系"></a>SQL的世界里一切都是关系</h2><p>本篇我们将会介绍 SQL 的基本特性以及最重要的一个编程思想：<strong>一切都是关系</strong>。让我们先来回顾一下关系数据库的几个基本概念。 </p><p>关系数据库<br>关系数据库（Relational database）是指基于关系模型的数据库。关系模型由关系数据结构、关系操作集合、关系完整性约束三部分组成。<br>数据结构<br>在关系模型中，用于存储数据的逻辑结构称为关系（Relation）；对于使用者而言，关系就是二维表（Table）。<br>以下是一个员工信息表，它和 Excel 表格非常类似，由行（Row）和列（Column）组成。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmm39ukfj20gi061q37.jpg" alt="undefined"></p><p>在不同的场景下，大家可能会听到关于同一个概念的不同说法。在此，我们列出了关系数据库中的一些常见概念：</p><ul><li><strong>关系</strong>，也称为<strong>表</strong>，用于表示现实世界中的实体（Entity）或者实体之间的联系（Relationship）。举例来说，一个公司的员工、部门和职位都是实体，分别对应员工信息表、部门信息表和职位信息表；销售的产品和订单都是实体，同时它们之间存在联系，对应订单明细表。</li><li><strong>行</strong>，也称为<strong>记录</strong>（Record），代表了关系中的单个实体。上图中工号为 4 的数据行存储了“诸葛亮”的相关信息。关系（表）可以看作是由行组成的集合。</li><li><strong>列</strong>，也称为<strong>字段</strong>（Field），表示实体的某个属性。上图中的第二列包含了员工的姓名。表中的每个列都有一个对应的数据类型，常见的数据类型包括字符类型、数字类型、日期时间类型等。</li></ul><p>有了关系结构之后，就需要定义基于关系的数据操作。</p><p>操作集合</p><p>常见的数据操作包括<strong>增加</strong>（Create）、<strong>查询</strong>（Retrieve）、<strong>更新</strong>（Update）以及<strong>删除</strong>（Delete），或者统称为<strong>增删改查</strong>（CRUD）。</p><p>其中，使用最多、也最复杂的操作就是查询，具体来说包括<strong>选择</strong>（Selection）、<strong>投影</strong>（Projection）、<strong>并集</strong>（Union）、<strong>交集</strong>（Intersection）、<strong>差集</strong>（exception）以及<strong>笛卡儿积</strong>（Cartesian product）等。我们将会介绍如何使用 SQL 语句完成以上各种数据操作。</p><p>为了维护数据的完整性或者满足业务需求，关系模型还定义了完整性约束。</p><p>关系性约束</p><p>关系模型中定义了三种完整性约束：<strong>实体完整性</strong>、<strong>参照完整性</strong>以及<strong>用户定义完整性</strong>。</p><ul><li><strong>实体完整性</strong>是指表的主键字段不能为空。现实中的每个实体都具有唯一性，比如每个人都有唯一的身份证号；在关系数据库中，这种唯一标识每一行数据的字段称为主键（Primary Key），主键字段不能为空。每个表可以有且只能有一个主键。</li><li><strong>参照完整性</strong>是指外键参照的完整性。外键（Foreign Key）代表了两个表之间的关联关系，比如员工属于某个部门；因此员工表中存在部门编号字段，引用了部门表中的部门编号字段。对于外键引用，被引用的数据必须存在，员工不可能属于一个不存在的部门；删除某个部门之前，也需要对部门中的员工进行相应的处理。</li><li><p><strong>用户定义完整性</strong>是指基于业务需要自定义的约束。非空约束（NOT NULL）确保了相应的字段不会出现空值，例如员工一定要有姓名；唯一约束（UNIQUE）用于确保字段中的值不会重复，每个员工的电子邮箱必须唯一；检查约束（CHECK）可以定义更多的业务规则。例如，薪水必须大于 0 ，字符必须大写等；默认值（DEFAULT）用于向字段中插入默认的数据。</p><p>本专栏涉及的 4 种数据库对于这些完整性约束的支持情况如下： </p></li></ul><table><thead><tr><th style="text-align:left">数据库</th><th style="text-align:left">非空约束</th><th style="text-align:left">唯一约束</th><th style="text-align:left">主键约束</th><th style="text-align:left">外键约束</th><th style="text-align:left">检查约束</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Oracle</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td></tr><tr><td style="text-align:left"><strong>MySQL</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持*</td><td style="text-align:left">支持*</td><td style="text-align:left">支持</td></tr><tr><td style="text-align:left"><strong>SQL Server</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td></tr><tr><td style="text-align:left"><strong>PostgreSQL</strong></td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td><td style="text-align:left">支持</td></tr></tbody></table><p>MySQL 中只有 InnoDB 存储引擎支持外键约束；MySQL 8.0.16 增加了对检查约束的支持。</p><blockquote><p>存储引擎（Storage Engine）是 MySQL 中用于管理、访问和修改物理数据的组件，不同的存储引擎提供了不同的功能和特性。从 MySQL 5.5 开始默认使用 InnoDB 存储引擎，支持事务处理（ACID）、行级锁定、故障恢复、多版本并发控制（MVCC）以及外键约束等。</p></blockquote><p>关系数据库使用 SQL 作为访问和操作数据的标准语言。现在，让我们来直观感受一下 SQL 语句的特点。</p><p>SQL：一种面向集合的编程语言</p><blockquote><p>本节会出现几个示例，我们还没有正式开始学习 SQL 语句，可以暂时不必理会细节。</p></blockquote><p>语法特性</p><p>SQL 是一种声明性的编程语言，语法接近于自然语言（英语）。通过几个简单的英文单词，例如 SELECT、INSERT、UPDATE、CREATE、DROP 等，完成大部分的数据库操作。以下是一个简单的查询示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">  <span class="keyword">FROM</span> employee</span><br><span class="line"> <span class="keyword">WHERE</span> salary &gt; <span class="number">10000</span></span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> emp_id;</span><br></pre></td></tr></table></figure><p>即使没有学过 SQL 语句，但只要知道几个单词的意思，就能明白该语句的作用。它查询员工表（employee）中月薪（salary）大于 10000 的员工，返回工号、姓名以及月薪，并且按照工号进行排序。可以看出，SQL 语句非常简单直观。</p><p>以上查询中的 SELECT、FROM 等称为关键字（也称为子句），一般大写；表名、列名等内容一般小写；分号（;）表示语句的结束。SQL 语句不区分大小写，但是遵循一定的规则可以让代码更容易阅读。</p><blockquote><p>SQL 是一种声明式的语言，声明式语言的主要思想是告诉计算机想要什么结果（what），但不指定具体怎么做。这类语言还包括 HTML、正则表达式以及函数式编程等。</p></blockquote><p>面向集合</p><p>对于 SQL 语句而言，它所操作的对象是一个集合（表），操作的结果也是一个集合（表）。例如以下查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>其中 employee 是一个表，它是该语句查询的对象；同时，查询的结果也是一个表。所以，我们可以继续扩展该查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">       <span class="keyword">SELECT</span> emp_id, emp_name, salary</span><br><span class="line">         <span class="keyword">FROM</span> employee</span><br><span class="line">       ) dt;</span><br></pre></td></tr></table></figure><p>我们将括号中的查询结果（取名为 dt）作为输入值，传递给了外面的查询；最终整个语句的结果仍然是一个表。在第 17 篇中，我们将会介绍这种嵌套在其他语句中的查询就是子查询（Subquery）。</p><p>SQL 中的查询可以完成各种数据操作，例如过滤转换、分组汇总、排序显示等；但是它们本质上都是针对表的操作，结果也是表。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cmy2e08yj217a093wf1.jpg" alt="undefined"></p><p>不仅仅是查询语句，SQL 中的插入、更新和删除都以集合为操作对象。我们再看一个插入数据的示例： </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t(<span class="keyword">id</span> <span class="built_in">INTEGER</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 适用于 MySQL、SQL Server 以及 PostgreSQL</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1</span>), (<span class="number">2</span>), (<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>我们首先使用 CREATE TABLE 语句创建了一个表，然后使用 INSERT INTO 语句插入数据。在执行插入操作之前，会在内存中创建一个包含 3 条数据的临时集合（表），然后将该集合插入目标表中。由于我们通常一次插入一条数据，以为是按照数据行进行插入；实际上，一条数据也是一个集合，只不过它只有一个元素而已。</p><p>Oracle 不支持以上插入<strong>多行数据</strong>的语法，可以使用下面的插入语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 适用于 Oracle</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">1</span> <span class="keyword">FROM</span> DUAL</span><br><span class="line"> <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">2</span> <span class="keyword">FROM</span> DUAL</span><br><span class="line"> <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">3</span> <span class="keyword">FROM</span> DUAL;</span><br></pre></td></tr></table></figure><p>UNION ALL 是 SQL 中的并集运算，用于将两个集合组成一个更大的集合。此外，SQL 还支持交集运算（INTERSECT）、差集运算（EXCEPT）以及笛卡儿积（Cartesian product）。我们会在第 18 篇中介绍这些内容，它们也都是以集合为对象的操作。</p><p>我们已经介绍了 SQL 语言的声明性和面向集合的编程思想。在正式学习编写 SQL 语句之前，还需要进行一些准备工作，主要就是安装示例数据库。</p><p>示例数据库</p><p>在本专栏的学习过程中，我们主要使用一个虚构的公司数据模型。该示例数据库包含 3 个表：员工表（employee）、部门表（department）和职位表（job）。以下是它们的结构图，也称为<strong>实体-关系图</strong>（Entity-Relational Diagram）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cnhvzfk5j20e908y0sw.jpg" alt="undefined"></p><ul><li><strong>部门表</strong>（department），包含部门编号（dept<em>id）和部门名称（dept</em>name）字段，主键为部门编号。该表共计 6 条数据。</li><li><strong>职位表</strong>（job），包含职位编号（job<em>id）和职位名称（job</em>title）字段，主键为职位编号。该表共计 10 条数据。</li><li><strong>员工表</strong>（employee），包含员工编号（emp<em>id）和员工姓名（emp</em>name）等字段，主键为员工编号，部门编号（dept<em>id）字段是引用部门表的外键，职位编号（job</em>id）字段是引用职位表的外键，经理编号（manager）字段是引用员工表自身的外键。该表共计 25 条数据。</li></ul><p>我们在 GitHub 上为大家提供了示例表和初始数据的创建脚本和安装说明，支持 Oracle、MySQL、SQL Server 以及 PostgreSQL。点击<a href="https://github.com/dongxuyang1985/thinking_in_sql" target="_blank" rel="noopener">链接</a>进行下载。</p><p>运行这些脚本之前，需要先安装数据库软件。网络上有很多这类安装教程可以参考；如果无法安装数据库，也可以使用这个免费的在线 SQL 开发环境：<a href="http://sqlfiddle.com/" target="_blank" rel="noopener">http://sqlfiddle.com</a>，它提供了各种常见的关系数据库服务。下图是使用 MySQL 运行示例脚本的结果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cnigeozjj20zk0gbdhp.jpg" alt="undefined"></p><p>选择数据库之后，将创建表和插入数据的脚本复制到左边窗口，点击“Build Schema”进行初始化；点击“Browser”可以查看表结构；在右侧窗口输入 SQL 语句，点击“Run SQL”运行并查看结果。该工具提供的数据库不是最新版本，但是可以运行大部分的示例。</p><p>本专栏中所有的示例都在以下数据库版本中进行了验证：</p><ul><li>Oracle database 18c</li><li>MySQL 8.0</li><li>SQL Server 2017</li><li>PostgreSQL 12</li></ul><p>我们使用 <a href="https://dbeaver.io/" target="_blank" rel="noopener">DBeaver</a> 开发工具编写所有的 SQL 语句，该工具的安装和使用可以参考我的<a href="https://tonydong.blog.csdn.net/article/details/89683422" target="_blank" rel="noopener">博客文章</a>。当然，你也可以使用自己喜欢的开发工具。</p><p>小结</p><p>关系模型中定义了一个简单的数据结构，即关系（表），用于存储数据。SQL 是关系数据库的通用标准语言，它使用接近于自然语言（英语）的语法，通过声明的方式执行数据定义、数据操作、访问控制等。对于 SQL 而言，一切都是关系（表）。</p><p>参考文献</p><ul><li>[美] Abraham Silberschatz，Henry F.Korth，S.Sudarshan 著，杨冬青，李红燕，唐世渭 译 ，《数据库系统概念（原书第6版）》，机械工业出版社，2012</li></ul><h2 id="SELECT-初步探索"><a href="#SELECT-初步探索" class="headerlink" title="SELECT 初步探索"></a>SELECT 初步探索</h2><p>在 employee 表中，存储了关于员工的信息。假设现在打算群发邮件，需要找出所有员工的姓名、性别和电子邮箱。在 SQL 中可以通过一个简单的查询语句来实现：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_name, sex, email</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>其中 SELECT 表示查询，随后列出需要返回的字段，多个字段使用逗号分隔；FROM 表示要从哪个表中进行查询；分号表示 SQL 语句的结束。该语句执行的结果如下（显示部分数据）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9coidg6c2j20ae06mweo.jpg" alt="undefined"></p><p>这种查询表中指定字段的操作在关系运算中被称为<strong>投影</strong>（Projection），使用 SELECT 子句进行表示。投影是针对表进行的垂直选择，保留需要的字段用于生成新的表。以下是投影操作的示意图： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9coj8sb0sj20yd0ikmy4.jpg" alt="undefined"></p><p>投影操作中包含一个特殊的操作，就是查询表中所有的字段。</p><h3 id="查询全部字段"><a href="#查询全部字段" class="headerlink" title="查询全部字段"></a>查询全部字段</h3><p>查看表中的全部字段可以使用一个简单的写法，就是使用星号（*）表示全部字段。例如，以下语句查询员工表中的所有数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * </span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>数据库在解析该语句时，会使用表中的字段名进行扩展：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id, emp_name, sex, dept_id, manager,</span><br><span class="line">       hire_date, job_id, salary, bonus, email</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>该语句执行的结果如下（显示部分数据）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cojmu3vkj20th06m74w.jpg" alt="undefined"></p><blockquote><p><strong>注意</strong>：星号可以便于快速编写查询语句，但是在实际项目中不要使用这种写法。因为应用程序可能并不需要所有的字段，避免返回过多的无用数据；另外，当表结构发生变化时，星号返回的信息也会发生改变。 </p></blockquote><p>除了查询表的字段之外，SELECT 语句还支持扩展的投影操作，包括基于字段的算术运算、函数和表达式等。</p><p>扩展操作</p><p>以下示例返回了员工的姓名、一年的工资（12 个月的月薪）以及电子邮箱的大写形式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_name,</span><br><span class="line">       salary * <span class="number">12</span>,</span><br><span class="line">       <span class="keyword">UPPER</span>(email)</span><br><span class="line">  <span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>其中 UPPER 是 SQL 中将字符串转换为大写的函数，函数将在第 8 篇中进行介绍。该语句的结果如下（显示部分数据）：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9cokqdusej20cl06mt91.jpg" alt="undefined"></p><p>在上面的结果中，返回字段的名称不是很好理解；能不能给它指定一个更明确的标题呢？这就需要使用到 SQL 中的别名（Alias）功能了。</p><h3 id="使用别名"><a href="#使用别名" class="headerlink" title="使用别名"></a>使用别名</h3><p>为了提高查询结果的可读性，可以使用别名为表或者字段指定一个临时的名称。SQL 中使用关键字 AS 指定别名。我们为上面的示例指定一些更好理解的标题：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.emp_name <span class="keyword">AS</span> <span class="string">"姓名"</span>,</span><br><span class="line">       salary * <span class="number">12</span> <span class="keyword">AS</span> <span class="string">"工资"</span>,</span><br><span class="line">       <span class="keyword">UPPER</span>(email) <span class="string">"电子邮箱"</span></span><br><span class="line">  <span class="keyword">FROM</span> employee <span class="keyword">AS</span> e; <span class="comment">-- Oracle 需要去掉此处的 AS</span></span><br></pre></td></tr></table></figure><blockquote><p>别名中的关键字 AS 可以省略。对于 Oracle 而言，表别名不支持 AS 关键字，省略掉即可。</p></blockquote><p>首先，我们为 employee 表指定了一个表别名 e；然后为查询的结果字段指定了 3 个更明确的列别名（使用双引号）。在查询中为表指定别名之后，引用表中的字段时可以加上别名限定，例如 e.emp_name，表示要查看哪个表中的字段。以下是使用别名之后的效果：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9col02qedj20ah06m74l.jpg" alt="undefined"></p><p>在 SQL 语句中使用别名不会修改数据库中存储的表名或者列名，别名只在当前语句中生效。</p><p>在上面的示例中，我们还用到了 SQL 中的另一个功能：注释。</p><h3 id="SQL-注释"><a href="#SQL-注释" class="headerlink" title="SQL 注释"></a>SQL 注释</h3><p>在 SQL 中可以像其他编程语言一样使用注释；注释可以方便我们理解代码的作用，但不会被执行。</p><p>SQL中的注释分为单行注释和多行注释。单行注释以两个连字符（–）开始，直到这一行结束；上一节中的示例就使用了单行注释。SQL 使用 C 语言风格的多行注释（/<em> … </em>/），例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.emp_name <span class="keyword">AS</span> <span class="string">"姓名"</span>,</span><br><span class="line">       salary * <span class="number">12</span> <span class="keyword">AS</span> <span class="string">"工资"</span>,</span><br><span class="line">       <span class="keyword">UPPER</span>(email) <span class="string">"电子邮箱"</span></span><br><span class="line"><span class="comment">/* 备注：SQL 别名使用示例</span></span><br><span class="line"><span class="comment">   作者：TonyDong</span></span><br><span class="line"><span class="comment">   日期：2019-11-01</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">  <span class="keyword">FROM</span> employee <span class="keyword">AS</span> e;</span><br></pre></td></tr></table></figure><blockquote><p>MySQL中的 # 也可以用于表示单行注释。</p></blockquote><p>在 SQL 中，SELECT … FROM … 是最基本的查询形式；但是，有时候我们会看到一种更简单的查询：只有 SELECT 子句，没有 FROM 子句的查询。</p><h3 id="无表查询"><a href="#无表查询" class="headerlink" title="无表查询"></a>无表查询</h3><p>以下查询没有 FROM 子句，用于计算一个表达式的值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- MySQL、SQL Server 以及 PostgreSQL 实现</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">1</span>+<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">1+1|</span><br><span class="line"><span class="comment">---|</span></span><br><span class="line">  2|</span><br></pre></td></tr></table></figure><p>这种形式的查询语句通常用于快速查找信息，或者当作计算器使用。但是需要注意的是这种语法并不属于 SQL 标准，而是数据库产品自己的扩展。MySQL、SQL Server 以及 PostgreSQL 都支持无表查询；对于 Oracle 而言，可以使用以下等价的形式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Oracle 实现</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">1</span>+<span class="number">1</span></span><br><span class="line">  <span class="keyword">FROM</span> dual;</span><br><span class="line"></span><br><span class="line">1+1|</span><br><span class="line"><span class="comment">---|</span></span><br><span class="line">  2|</span><br></pre></td></tr></table></figure><p>dual 是 Oracle 中的一个特殊的表；它只有一个字段且只包含一行数据，就是为了方便快速查询信息。另外，MySQL 也提供了 dual 表。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本篇我们学习了如何使用 SELECT 和 FROM 查询表中的数据，通过投影操作获取指定的字段信息。SQL 不仅仅能够查询表中的数据，还可以返回算术运算、函数和表达式的结果。在许多数据库中，不包含 FROM 子句的无表查询可以用于快速获取信息。另外，别名和注释都可以让我们编写的 SQL 语句更易阅读和理解。</p><p><strong>练习题</strong>：查询部门表（department）和职位表（job）中的数据，熟悉它们的字段结构和内容。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="https://www.sqlstyle.guide/zh/" target="_blank" rel="noopener">SQL 编程风格指南</a>。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL是一项重要的技能，可能是普通程序员在工作中最常用的。面试的时候造飞机大炮，最后可能还是得实实在在得写SQL。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g9cm9vd6brj20sg09hq9w.jpg&quot; alt=&quot;SQL-1024x341.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="GitChat" scheme="http://yoursite.com/categories/GitChat/"/>
    
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Pulsar与Kafka对比</title>
    <link href="http://yoursite.com/2019/11/25/Pulsar%E4%B8%8EKafka%E5%AF%B9%E6%AF%94/"/>
    <id>http://yoursite.com/2019/11/25/Pulsar与Kafka对比/</id>
    <published>2019-11-25T01:42:49.000Z</published>
    <updated>2020-04-10T17:11:03.009Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pulsar与Kafka对比"><a href="#Pulsar与Kafka对比" class="headerlink" title="Pulsar与Kafka对比"></a>Pulsar与Kafka对比</h1><blockquote><p>Pulsar 是一个比较新的MQ，和MQ的王者Kafka进行对比。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9a1gg550rj20y107tq3l.jpg" alt="TIM截图20191125094052.png"></p><a id="more"></a> <h2 id="Pulsar架构"><a href="#Pulsar架构" class="headerlink" title="Pulsar架构"></a>Pulsar架构</h2><p>Apache Pulsar 是一个企业级的分布式消息系统，最初由 Yahoo 开发，在 2016 年开源，并于2018年9月毕业成为 Apache 基金会的顶级项目。Pulsar 已经在 Yahoo 的生产环境使用了三年多，主要服务于Mail、Finance、Sports、 Flickr、 the Gemini Ads platform、 Sherpa (Yahoo 的 KV 存储)。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph5xgvnyj20m80epjx9.jpg" alt="undefined"></p><h2 id="kafka队列优先级模型"><a href="#kafka队列优先级模型" class="headerlink" title="kafka队列优先级模型"></a>kafka队列优先级模型</h2><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph3n5p4wj20m80j4gsu.jpg" alt="undefined"></p><p>在该方案中，个推将优先级统一设定为高、中、低三个级别。具体操作方案如下：</p><ol><li>对某个优先级根据 task (单次推送任务)维度，存入不同的 Topic，一个 task 只写入一个 Topic，一个 Topic 可存多个 task；</li><li>消费模块根据优先级配额(如 6:3:1)，获取不同优先级的消息数，同一优先级轮询获取消息；这样既保证了高优先级用户可以更快地发送消息，又避免了低优先级用户出现没有下发的情况。</li></ol><h2 id="Kafka方案遇到的问题"><a href="#Kafka方案遇到的问题" class="headerlink" title="Kafka方案遇到的问题"></a>Kafka方案遇到的问题</h2><p>随着个推业务的不断发展，接入的 APP 数量逐渐增多，第一版的优先级方案也逐渐暴露出一些问题：</p><ol><li>当相同优先级的 APP 在同一时刻推送任务越来越多时，后面进入的 task 消息会因为前面 task 消息还存在队列情况而出现延迟。如下图所示, 当 task1 消息量过大时，在task1 消费结束前，taskN 将一直处于等待状态。</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph4vwowhj20gz01pa9t.jpg" alt="undefined"></p><ol><li>Kafka 在 Topic 数量由 64 增长到 256 时，吞吐量下降严重，Kafka 的每个 Topic、每个分区都会对应一个物理文件。当 Topic 数量增加时，消息分散的落盘策略会导致磁盘 IO 竞争激烈，因此我们不能仅通过增加 Topic 数量来缓解第一点中的问题。</li></ol><p>基于上述问题，需要可以创建大量的 Topic, 同时吞吐性能不能比 Kafka 逊色。经过一段时间的调研，Apache Pulsar 引起了我们的关注。</p><h2 id="Topic数量"><a href="#Topic数量" class="headerlink" title="Topic数量"></a>Topic数量</h2><p>Pulsar 可以支持百万级别 Topic 数量的扩展，同时还能一直保持良好的性能。Topic 的伸缩性取决于它的内部组织和存储方式。Pulsar 的数据保存在 bookie (BookKeeper 服务器)上，处于写状态的不同 Topic 的消息，在内存中排序，最终聚合保存到大文件中，在 Bookie 中需要更少的文件句柄。另一方面 Bookie 的 IO 更少依赖于文件系统的 Pagecache，Pulsar 也因此能够支持大量的主题。</p><h2 id="消费模型"><a href="#消费模型" class="headerlink" title="消费模型"></a>消费模型</h2><p>Pulsar 支持三种消费模型：Exclusive、Shared 和Failover。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph7te7d2j20m80epjx9.jpg" alt="undefined"></p><p><strong>Exclusive (独享)</strong>：一个 Topic 只能被一个消费者消费。Pulsar 默认使用这种模式。</p><p><strong>Shared(共享)</strong>：共享模式，多个消费者可以连接到同一个 Topic，消息依次分发给消费者。当一个消费者宕机或者主动断开连接时，那么分发给这个消费者的未确认(ack)的消息会得到重新调度，分发给其他消费者。</p><p><strong>Failover (灾备)</strong>：一个订阅同时只有一个消费者，可以有多个备份消费者。一旦主消费者故障，则备份消费者接管。不会出现同时有两个活跃的消费者。</p><p>Exclusive和Failover订阅，仅允许一个消费者来使用和消费每个订阅的Topic。这两种模式都按 Topic 分区顺序使用消息。它们最适用于需要严格消息顺序的流(Stream)用例。</p><p>Shared 允许每个主题分区有多个消费者。同一个订阅中的每个消费者仅接收Topic分区的一部分消息。Shared最适用于不需要保证消息顺序队列(Queue)的使用模式，并且可以按照需要任意扩展消费者的数量。</p><h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><p>Pulsar 引入了 Apache BookKeeper 作为存储层，BookKeeper 是一个专门为实时系统优化过的分布式存储系统，具有可扩展、高可用、低延迟等特性。具体介绍，请参考 <a href="https://github.com/apache/bookkeeper" target="_blank" rel="noopener">BookKeeper官网</a>。</p><h3 id="Segment"><a href="#Segment" class="headerlink" title="Segment"></a>Segment</h3><p>BookKeeper以 Segment (在 BookKeeper 内部被称作 ledger) 作为存储的基本单元。从 Segment 到消息粒度，都会均匀分散到 BookKeeper 的集群中。这种机制保证了数据和服务均匀分散在 BookKeeper 集群中。</p><p>Pulsar 和 Kafka 都是基于 partition 的逻辑概念来做 Topic 存储的。最根本的不同是，Kafka 的物理存储是以 partition 为单位的，每个 partition 必须作为一个整体(一个目录)存储在某个 broker 上。 而 Pulsar 的 partition 是以 segment 作为物理存储的单位，每个 partition 会再被打散并均匀分散到多个 bookie 节点中。</p><p>这样的直接影响是，Kafka 的 partition 的大小，受制于单台 broker 的存储；而 Pulsar 的 partition 则可以利用整个集群的存储容量。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph8z7mslj20zk0f5jww.jpg" alt="undefined"></p><h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>当 partition 的容量达到上限后，需要扩容的时候，如果现有的单台机器不能满足，Kafka 可能需要添加新的存储节点，并将 partition 的数据在节点之间搬移达到 rebalance 的状态。</p><p>而 Pulsar 只需添加新的 Bookie 存储节点即可。新加入的节点由于剩余空间大，会被优先使用，接收更多的新数据；整个扩容过程不涉及任何已有数据的拷贝和搬移。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8ph9lc46zj20m80ifgq8.jpg" alt="undefined"></p><h2 id="Broker故障"><a href="#Broker故障" class="headerlink" title="Broker故障"></a>Broker故障</h2><p>Pulsar 在单个节点失败时也会体现同样的优势。如果 Pulsar 的某个服务节点 broker 失效，由于 broker 是无状态的，其他的 broker 可以很快接管 Topic，不会涉及 Topic 数据的拷贝；如果存储节点 Bookie 失效，在集群后台中，其他的 Bookie 会从多个 Bookie 节点中并发读取数据，并对失效节点的数据自动进行恢复，对前端服务不会造成影响。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pha2rmerj20m80lgtei.jpg" alt="undefined"></p><h2 id="Bookie故障"><a href="#Bookie故障" class="headerlink" title="Bookie故障"></a>Bookie故障</h2><p>Apache BookKeeper 中的副本修复是 Segment (甚至是 Entry)级别的多对多快速修复。这种方式只会复制必须的数据，这比重新复制整个主题分区要精细。如下图所示，当错误发生时， Apache BookKeeper 可以从 bookie 3 和 bookie 4 中读取 Segment 4 中的消息，并在 bookie 1 处修复 Segment 4。所有的副本修复都在后台进行，对 Broker 和应用透明。</p><p>当某个 Bookie 节点出错时，BookKeeper会自动添加可用的新 Bookie 来替换失败的 Bookie，出错的 Bookie 中的数据在后台恢复，所有 Broker 的写入不会被打断，而且不会牺牲主题分区的可用性。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8phagh11tj20m00m8agw.jpg" alt="undefined"></p><h2 id="基于-Pulsar-的优先级队列方案"><a href="#基于-Pulsar-的优先级队列方案" class="headerlink" title="基于 Pulsar 的优先级队列方案"></a>基于 Pulsar 的优先级队列方案</h2><p>在设计思路上，Pulsar 方案和 Kafka 方案并没有多大区别。但在新方案中，个推技术团队借助 Pulsar 的特性，解决了 Kafka 方案中存在的问题。</p><ol><li>根据 task 动态生成 Topic，保证了后进入的 task 不会因为其他 task 消息堆积而造成等待情况。</li><li>中高优先级 task 都独享一个 Topic，低优先级 task 共享 n 个 Topic。</li><li>相同优先级内，各个 task 轮询读取消息，配额满后流转至下一个优先级。</li><li>相同优先级内, 各个 task 可动态调整 quota， 在相同机会内，可读取更多消息。</li><li>利用 Shared 模式, 可以动态添加删除 consumer，且不会触发 Rebalance 情况。</li><li>利用 BookKeeper 特性，可以更灵活的添加存储资源。</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8phbi9bo2j20m80bkjv1.jpg" alt="undefined"></p><h2 id="Pulsar实践范例"><a href="#Pulsar实践范例" class="headerlink" title="Pulsar实践范例"></a>Pulsar实践范例</h2><ol><li>不同 subscription 之间相对独立，如果想要重复消费某个 Topic 的消息，需要使用不同的 subscriptionName 订阅；但是一直增加新的 subscriptionName，backlog 会不断累积。</li><li>如果 Topic 无人订阅，发给它的消息默认会被删除。因此如果 producer 先发送，consumer 后接收，一定要确保 producer 发送之前，Topic 有 subscription 存在(哪怕 subscribe 之后 close 掉)，否则这段时间发送的消息会导致无人处理。</li><li>如果既没有人发送消息，又没有人订阅消息，一段时间后 Topic 会自动删除。</li><li>Pulsar 的 TTL 等设置，是针对整个 namespace 起效的，无法针对单个 Topic。</li><li>Pulsar 的键都建立在 zookeeper 的根目录上，在初始化时建议增加总节点名。</li><li>目前 Pulsar 的 java api 设计，消息默认需要显式确认，这一点跟 Kafka 不一样。</li><li>Pulsar dashboard 上的 storage size 和 prometheus 上的 storage size (包含副本大小)概念不一样。</li><li>把<code>dbStorage_rocksDB_blockCacheSize</code> 设置的足够大；当消息体量大，出现backlog 大量堆积时, 使用默认大小(256M)会出现读耗时过大情况，导致消费变慢。</li><li>使用多 partition，提高吞吐。</li><li>在系统出现异常时，主动抓取 stats 和 stats-internal，里面有很多有用数据。</li><li>如果业务中会出现单 Topic 体量过大的情况，建议把 <code>backlogQuotaDefaultLimitGB</code> 设置的足够大(默认10G), 避免因为默认使用<code>producer_request_hold</code> 模式出现 block producer 的情况；当然可以根据实际业务选择合适的 <code>backlogQuotaDefaultRetentionPolicy</code>。</li><li>根据实际业务场景主动选择 backlog quota。</li><li>prometheus 内如果发现读耗时为空情况，可能是因为直接读取了缓存数据；Pulsar 在读取消息时会先读取 write cache, 然后读取 read cache；如果都没有命中, 则会在 RocksDB 中读取条目位子后，再从日志文件中读取该条目。</li><li>写入消息时, Pulsar 会同步写入 journal 和 write cache；write cache 再异步写入日志文件和 RocksDB； 所以有资源的话，建议 journal 盘使用SSD。</li></ol><h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>Bossie Awards中对 Pulsar 点评如下：“<strong>Pulsar 旨在取代 Apache Kafka 多年的主宰地位。</strong>Pulsar在很多情况下提供了比 Kafka 更快的吞吐量和更低的延迟，并为开发人员提供了一组兼容的 API，让他们可以很轻松地从 Kafka 切换到 Pulsar。Pulsar 的最大优点在于它提供了比 Apache Kafka 更简单明了、更健壮的一系列操作功能，特别在解决可观察性、地域复制和多租户方面的问题。在运行大型 Kafka 集群方面感觉有困难的企业可以考虑转向使用 Pulsar。”</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Pulsar与Kafka对比&quot;&gt;&lt;a href=&quot;#Pulsar与Kafka对比&quot; class=&quot;headerlink&quot; title=&quot;Pulsar与Kafka对比&quot;&gt;&lt;/a&gt;Pulsar与Kafka对比&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Pulsar 是一个比较新的MQ，和MQ的王者Kafka进行对比。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g9a1gg550rj20y107tq3l.jpg&quot; alt=&quot;TIM截图20191125094052.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Kafka" scheme="http://yoursite.com/categories/Apache/Kafka/"/>
    
    
      <category term="Pulsar与Kafka对比" scheme="http://yoursite.com/tags/Pulsar%E4%B8%8EKafka%E5%AF%B9%E6%AF%94/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kylin</title>
    <link href="http://yoursite.com/2019/11/22/Kylin/"/>
    <id>http://yoursite.com/2019/11/22/Kylin/</id>
    <published>2019-11-22T10:21:34.000Z</published>
    <updated>2019-11-22T10:21:34.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Kylin是在Hadoop上的SQL层，最近对Phoenix调研完成之后，对Kylin产生了兴趣。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v1qog93xj209d08laa6.jpg" alt="undefined"></p><a id="more"></a> <h1 id="Kylin"><a href="#Kylin" class="headerlink" title="Kylin"></a>Kylin</h1><h2 id="第一章-概述"><a href="#第一章-概述" class="headerlink" title="第一章 概述"></a>第一章 概述</h2><h3 id="1-1-Kylin定义"><a href="#1-1-Kylin定义" class="headerlink" title="1.1 Kylin定义"></a>1.1 Kylin定义</h3><p>Apache Kylin是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。</p><h3 id="1-2-Kylin特点"><a href="#1-2-Kylin特点" class="headerlink" title="1.2 Kylin特点"></a>1.2 Kylin特点</h3><p>Kylin的主要特点包括支持SQL接口、支持超大规模数据集、亚秒级响应、可伸缩性、高吞吐率、BI工具集成等。</p><p>1）标准SQL接口：Kylin是以标准的SQL作为对外服务的接口。</p><p>2）支持超大数据集：Kylin对于大数据的支撑能力可能是目前所有技术中最为领先的。早在2015年eBay的生产环境中就能支持百亿记录的秒级查询，之后在移动的应用场景中又有了千亿记录秒级查询的案例。</p><p>3）亚秒级响应：Kylin拥有优异的查询相应速度，这点得益于预计算，很多复杂的计算，比如连接、聚合，在离线的预计算过程中就已经完成，这大大降低了查询时刻所需的计算量，提高了响应速度。</p><p>4）可伸缩性和高吞吐率：单节点Kylin可实现每秒70个查询，还可以搭建Kylin的集群。</p><p>5）BI工具集成</p><p>Kylin可以与现有的BI工具集成，具体包括如下内容。</p><p>ODBC：与Tableau、Excel、PowerBI等工具集成</p><p>JDBC：与Saiku、BIRT等Java工具集成</p><p>Restate：与JavaScript、Web网页集成</p><p>Kylin开发团队还贡献了<strong>Stippling</strong>的插件，也可以使用Stippling来访问Kylin服务。</p><h3 id="1-3-Kylin架构"><a href="#1-3-Kylin架构" class="headerlink" title="1.3 Kylin架构"></a>1.3 Kylin架构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v1uuzn2pj215o0k5ta3.jpg" alt="undefined"></p><p>1）REST Server</p><p>REST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。</p><p>2）查询引擎（Query Engine）</p><p>当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 </p><p>3）Routing</p><p>负责将解析的SQL生成的执行计划转换成cube缓存的查询，cube是通过预计算缓存在hbase中，这部分查询可以在秒级设置毫秒级完成，而且还有一些操作使用过的查询原始数据（存储在Hadoop的hdfs中通过hive查询）。这部分查询延迟较高。</p><p>4）元数据管理工具（Metadata）</p><p>Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin的元数据存储在hbase中。 </p><p>5）任务引擎（Cube Build Engine）</p><p>这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及Map Reduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决其间出现的故障。</p><h3 id="1-4-Kylin工作原理"><a href="#1-4-Kylin工作原理" class="headerlink" title="1.4 Kylin工作原理"></a>1.4 Kylin工作原理</h3><p>Apache Kylin的工作原理本质上是MOLAP（Multidimension On-Line Analysis Processing）Cube，也就是多维立方体分析。是数据分析中非常经典的理论，下面对其做简要介绍。</p><h4 id="1-4-1-纬度和度量"><a href="#1-4-1-纬度和度量" class="headerlink" title="1.4.1 纬度和度量"></a>1.4.1 纬度和度量</h4><p>维度：即观察数据的角度。比如员工数据，可以从性别角度来分析，也可以更加细化，从入职时间或者地区的维度来观察。维度是一组离散的值，比如说性别中的男和女，或者时间维度上的每一个独立的日期。因此在统计时可以将维度值相同的记录聚合在一起，然后应用聚合函数做累加、平均、最大和最小值等聚合计算。</p><p>度量：即被聚合（观察）的统计值，也就是聚合运算的结果。比如说员工数据中不同性别员工的人数，又或者说在同一年入职的员工有多少。</p><p>基数：某个维度的种类数。比如说性别维度，基数为2（男和女）。按照某个维度进行聚合，结果数据的大小主要取决于该维度的基数。</p><h4 id="1-4-2-Cube和Cuboid"><a href="#1-4-2-Cube和Cuboid" class="headerlink" title="1.4.2 Cube和Cuboid"></a>1.4.2 Cube和Cuboid</h4><p>有了维度跟度量，一个数据或数据模型上的所有字段就可以分类了，他们要么是纬度要么是度量（可以被聚合）。于是就有了根据维度和度量做预计算的理论。</p><p>给定一个数据模型，我们可以对其上的所有维度进行聚合，对于N个维度来说，组合的所有可能性共有2^n种。对于每一种维度的组合，将度量值做聚合计算，然后将结果保存为一个物化视图，称为Cuboid。所有维度组合的Cuboid作为一个整体，称为Cube。</p><p>下面举一个简单的例子说明，假设有一个电商的销售数据集，其中维度包括时间[time]、商品[item]、地区[location]和供应商[supplier]，度量为销售额。那么所有维度的组合就有2^4 = 16种，如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v2nn4skoj20hq0db75f.jpg" alt="undefined"></p><p>一维度（1D）的组合有：[time]、[item]、[location]和[supplier]4种；</p><p>二维度（2D）的组合有：[time, item]、[time, location]、[time, supplier]、[item, location]、[item, supplier]、[location, supplier]3种；</p><p>三维度（3D）的组合也有4种；</p><p>最后还有零维度（0D）和四维度（4D）各有一种，总共16种。</p><p>注意：每一种维度组合就是一个Cuboid，16个Cuboid整体就是一个Cube。</p><h4 id="1-4-3-核心算法"><a href="#1-4-3-核心算法" class="headerlink" title="1.4.3 核心算法"></a>1.4.3 核心算法</h4><p>Kylin的工作原理就是对数据模型做Cube预计算，并利用计算的结果加速查询：</p><p>1）指定数据模型，定义维度和度量；</p><p>2）预计算Cube，计算所有Cuboid并保存为物化视图；</p><p>预计算过程是Kylin从Hive中读取原始数据，按照我们选定的维度进行计算，并将结果集保存到Hbase中，默认的计算引擎为MapReduce，可以选择Spark作为计算引擎。一次build的结果，我们称为一个Segment。构建过程中会涉及多个Cuboid的创建，具体创建过程由kylin.cube.algorithm参数决定，参数值可选 auto，layer 和 inmem， 默认值为 auto，即 Kylin 会通过采集数据动态地选择一个算法 (layer or inmem)，如果用户很了解 Kylin 和自身的数据、集群，可以直接设置喜欢的算法。</p><p>3）执行查询，读取Cuboid，运行，产生查询结果。</p><h5 id="1-4-3-1-逐层构建算法"><a href="#1-4-3-1-逐层构建算法" class="headerlink" title="1.4.3.1 逐层构建算法"></a>1.4.3.1 逐层构建算法</h5><p>我们知道，一个N维的Cube，是由1个N维子立方体、N个(N-1)维子立方体、N*(N-1)/2个(N-2)维子立方体、……、N个1维子立方体和1个0维子立方体构成，总共有2^N个子立方体组成，在逐层算法中，按维度数逐层减少来计算，每个层级的计算（除了第一层，它是从原始数据聚合而来），是基于它上一层级的结果来计算的。比如，[Group by A, B]的结果，可以基于[Group by A, B, C]的结果，通过去掉C后聚合得来的；这样可以减少重复计算；当 0维度Cuboid计算出来的时候，整个Cube的计算也就完成了。每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N+1次MapReduce Job。</p><p><strong>算法优点</strong>：</p><p>每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N+1次MapReduce Job。2）受益于Hadoop的日趋成熟，此算法对集群要求低，运行稳定；在内部维护Kylin的过程中，很少遇到在这几步出错的情况；即便是在Hadoop集群比较繁忙的时候，任务也能完成。</p><p>算法缺点</p><p>2）受益于Hadoo    p的日趋成熟，此算法对集群要求低，运行稳定；在内部维护Kylin的过程中，很少遇到在这几步出错的情况；即便是在Hadoop集群比较繁忙的时候，任务也能完成。</p><p><strong>算法缺点：</strong></p><p>1）当Cube有比较多维度的时候，所需要的MapReduce任务也相应增加；由于Hadoop的任务调度需要耗费额外资源，特别是集群较庞大的时候，反复递交任务造成的额外开销会相当可观；</p><p>2）此算法会对Hadoop MapReduce输出较多数据; 虽然已经使用了Combiner来减少从Mapper端到Reducer端的数据传输，所有数据依然需要通过Hadoop MapReduce来排序和组合才能被聚合，无形之中增加了集群的压力;</p><p>3）对HDFS的读写操作较多：由于每一层计算的输出会用做下一层计算的输入，这些Key-Value需要写到HDFS上；当所有计算都完成后，Kylin还需要额外的一轮任务将这些文件转成HBase的HFile格式，以导入到HBase中去；</p><p>总体而言，该算法的效率较低，尤其是当Cube维度数较大的时候。</p><h4 id="1-4-3-2-快速构建算法（inmem）"><a href="#1-4-3-2-快速构建算法（inmem）" class="headerlink" title="1.4.3.2 快速构建算法（inmem）"></a>1.4.3.2 快速构建算法（inmem）</h4><p>也被称作“逐段”(By Segment) 或“逐块”(By Split) 算法，从1.5.x开始引入该算法，利用Mapper端计算先完成大部分聚合，再将聚合后的结果交给Reducer，从而降低对网络瓶颈的压力。该算法的主要思想是，对Mapper所分配的数据块，将它计算成一个完整的小Cube 段（包含所有Cuboid）；每个Mapper将计算完的Cube段输出给Reducer做合并，生成大Cube，也就是最终结果；如图所示解释了此流程。</p><p>与旧算法相比，快速算法主要有两点不同：</p><p>1） Mapper会利用内存做预聚合，算出所有组合；Mapper输出的每个Key都是不同的，这样会减少输出到Hadoop MapReduce的数据量；</p><p>2）一轮MapReduce便会完成所有层次的计算，减少Hadoop任务的调配。</p><h2 id="第二章：Kylin安装指南"><a href="#第二章：Kylin安装指南" class="headerlink" title="第二章：Kylin安装指南"></a>第二章：Kylin安装指南</h2><h3 id="2-1安装地址"><a href="#2-1安装地址" class="headerlink" title="2.1安装地址"></a>2.1安装地址</h3><p>1.官网地址：</p><p><a href="http://kylin.apache.org/cn/" target="_blank" rel="noopener">http://kylin.apache.org/cn/</a></p><p>2.官方文档</p><p><a href="http://kylin.apache.org/cn/docs/" target="_blank" rel="noopener">http://kylin.apache.org/cn/docs/</a></p><p>3.下载地址</p><p><a href="http://kylin.apache.org/cn/download/" target="_blank" rel="noopener">http://kylin.apache.org/cn/download/</a></p><h3 id="2-2-软件要求"><a href="#2-2-软件要求" class="headerlink" title="2.2 软件要求"></a>2.2 软件要求</h3><ul><li><p>Hadoop: 2.7+, 3.1+ (since v2.5)</p></li><li><p>Hive: 0.13 - 1.2.1+</p></li><li><p>HBase: 1.1+, 2.0 (since v2.5)</p></li><li><p>Spark (可选) 2.3.0+</p></li><li><p>Kafka (可选) 1.0.0+ (since v2.5)</p></li><li><p>JDK: 1.8+ (since v2.5)</p></li><li><p>OS: Linux only, CentOS 6.5+ or Ubuntu 16.0.4+</p></li></ul><p>官网提示：在 Hortonworks HDP 2.2-2.6 and 3.0, Cloudera CDH 5.7-5.11 and 6.0, AWS EMR 5.7-5.10, Azure HDInsight 3.5-3.6 上测试通过。</p><h3 id="2-3-硬件要求："><a href="#2-3-硬件要求：" class="headerlink" title="2.3 硬件要求："></a>2.3 硬件要求：</h3><p>运行 Kylin 的服务器的最低配置为 4 core CPU，16 GB 内存和 100 GB 磁盘。 对于高负载的场景，建议使用 24 core CPU，64 GB 内存或更高的配置。</p><h3 id="2-4-Hadoop环境"><a href="#2-4-Hadoop环境" class="headerlink" title="2.4 Hadoop环境"></a>2.4 Hadoop环境</h3><p>Kylin依赖于Hadoop集群处理大量的数据集。需要准备一个配置好HDFS，YARN，MapReduce,，Hive，HBase，Zookeeper和其他服务的Hadoop 集群供 Kylin 运行。<br>   Kylin可以在 Hadoop 集群的任意节点上启动。方便起见，可以在master节点上运行Kylin。但为了更好的稳定性，官网建议将Kylin部署在一个干净的Hadoop client节点上，该节点上 Hive，HBase，HDFS 等命令行已安装好且client 配置（如 <code>core-site.xml</code>，<code>hive-site.xml</code>，<code>hbase-site.xml</code>及其他）也已经合理的配置且其可以自动和其它节点同步。</p><p>运行Kylin的Linux账户要有访问 Hadoop 集群的权限，包括创建/写入 HDFS 文件夹，Hive表，HBase 表和提交MapReduce任务的权限。</p><h3 id="2-5-Kylin安装"><a href="#2-5-Kylin安装" class="headerlink" title="2.5 Kylin安装"></a>2.5 Kylin安装</h3><ol><li>从 <a href="https://kylin.apache.org/download/" target="_blank" rel="noopener">Apache Kylin下载网站</a> 下载一个适用于您 Hadoop 版本的二进制文件。例如，适用于 HBase 1.x 的 Kylin 2.5.0 可通过如下命令行下载得到：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/kylin/apache-kylin-2.5.0/apache-kylin-2.5.0-bin-hbase1x.tar.gz</span><br></pre></td></tr></table></figure><ol><li>解压 tar 包，配置环境变量 <code>$KYLIN_HOME</code> 指向 Kylin 文件夹。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-kylin-2.5.0-bin-hbase1x.tar.gzcd apache-kylin-2.5.0-bin-hbase1xexport KYLIN_HOME=`pwd`</span><br></pre></td></tr></table></figure><p>从 v2.6.1 开始， Kylin 不再包含 Spark 二进制包; 您需要另外下载 Spark，然后设置 <code>SPARK_HOME</code> 系统变量到 Spark 安装目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/path/to/spark</span><br></pre></td></tr></table></figure><p>或者使用脚本下载:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$KYLIN_HOME/bin/download-spark.sh</span><br></pre></td></tr></table></figure><h4 id="Kylin-tarball-目录"><a href="#Kylin-tarball-目录" class="headerlink" title="Kylin tarball 目录"></a>Kylin tarball 目录</h4><ul><li><code>bin</code>: shell 脚本，用于启动/停止 Kylin，备份/恢复 Kylin 元数据，以及一些检查端口、获取 Hive/HBase 依赖的方法等；</li><li><code>conf</code>: Hadoop 任务的 XML 配置文件</li><li><code>lib</code>: 供外面应用使用的 jar 文件，例如 Hadoop 任务 jar, JDBC 驱动, HBase coprocessor 等.</li><li><code>meta_backups</code>: 执行 <code>bin/metastore.sh     backup</code> 后的默认的备份目录;</li><li><code>sample_cube</code> 用于创建样例 Cube 和表的文件。</li><li><code>spark</code>: 自带的 spark。</li><li><code>tomcat</code>: 自带的 tomcat，用于启动 Kylin 服务。</li><li><code>tool</code>: 用于执行一些命令行的jar文件。</li></ul><h4 id="检查运行环境"><a href="#检查运行环境" class="headerlink" title="检查运行环境"></a>检查运行环境</h4><p>Kylin 运行在 Hadoop 集群上，对各个组件的版本、访问权限及 CLASSPATH 等都有一定的要求，为了避免遇到各种环境问题，您可以运行 <code>$KYLIN_HOME/bin/check-env.sh</code> 脚本来进行环境检测，如果您的环境存在任何的问题，脚本将打印出详细报错信息。如果没有报错信息，代表您的环境适合 Kylin 运行。</p><h4 id="启动-Kylin"><a href="#启动-Kylin" class="headerlink" title="启动 Kylin"></a>启动 Kylin</h4><p>运行 <code>$KYLIN_HOME/bin/kylin.sh start</code> 脚本来启动 Kylin，界面输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /usr/local/apache-kylin-2.5.0-bin-hbase1x</span><br><span class="line">......</span><br><span class="line">A new Kylin instance is started by root. To stop it, run &apos;kylin.sh stop&apos;</span><br><span class="line">Check the log at /usr/local/apache-kylin-2.5.0-bin-hbase1x/logs/kylin.log</span><br><span class="line">Web UI is at http://&lt;hostname&gt;:7070/kylin</span><br></pre></td></tr></table></figure><h4 id="使用-Kylin"><a href="#使用-Kylin" class="headerlink" title="使用 Kylin"></a>使用 Kylin</h4><p>Kylin 启动后您可以通过浏览器 <code>http://:7070/kylin</code> 进行访问。<br> 其中 <code></code> 为具体的机器名、IP 地址或域名，默认端口为 7070。<br> 初始用户名和密码是 <code>ADMIN/KYLIN</code>。<br> 服务器启动后，您可以通过查看 <code>$KYLIN_HOME/logs/kylin.log</code> 获得运行时日志。</p><h4 id="停止-Kylin"><a href="#停止-Kylin" class="headerlink" title="停止 Kylin"></a>停止 Kylin</h4><p>运行 <code>$KYLIN_HOME/bin/kylin.sh stop</code> 脚本来停止 Kylin，界面输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /usr/local/apache-kylin-2.5.0-bin-hbase1x</span><br><span class="line">Stopping Kylin: 25964</span><br><span class="line">Stopping in progress. Will check after 2 secs again...</span><br><span class="line">Kylin with pid 25964 has been stopped.</span><br></pre></td></tr></table></figure><p>您可以运行 <code>ps -ef | grep kylin</code> 来查看 Kylin 进程是否已停止。</p><h4 id="HDFS-目录结构"><a href="#HDFS-目录结构" class="headerlink" title="HDFS 目录结构"></a>HDFS 目录结构</h4><p>Kylin 会在 HDFS 上生成文件，根目录是 “/kylin/”, 然后会使用 Kylin 集群的元数据表名作为第二层目录名，默认为 “kylin_metadata” (可以在<code>conf/kylin.properties</code>中定制).</p><p>通常, <code>/kylin/kylin_metadata</code> 目录下会有这么几种子目录：<code>cardinality</code>, <code>coprocessor</code>, <code>kylin-job_id</code>, <code>resources</code>, <code>jdbc-resources</code>.<br> \1. <code>cardinality</code>: Kylin 加载 Hive 表时，会启动一个 MR 任务来计算各个列的基数，输出结果会暂存在此目录。此目录可以安全清除。<br> \2. <code>coprocessor</code>: Kylin 用于存放 HBase coprocessor jar 的目录；请勿删除。<br> \3. <code>kylin-job_id</code>: Cube 计算过程的数据存储目录，请勿删除。 如需要清理，请遵循 <a href="http://kylin.apache.org/docs/howto/howto_cleanup_storage.html" target="_blank" rel="noopener">storage cleanup guide</a>.<br> \4. <code>resources</code>: Kylin 默认会将元数据存放在 HBase，但对于太大的文件（如字典或快照），会转存到 HDFS 的该目录下，请勿删除。</p><p>\5. <code>jdbc-resources</code>：性质同上，只在使用 MySQL 做元数据存储时候出现。</p><h3 id="2-6-集群模式部署"><a href="#2-6-集群模式部署" class="headerlink" title="2.6 集群模式部署"></a>2.6 集群模式部署</h3><p>Kylin 实例是无状态的服务，运行时的状态信息存储在 HBase metastore 中。 出于负载均衡的考虑，可以启用多个共享一个 metastore 的 Kylin 实例，使得各个节点分担查询压力且互为备份，从而提高服务的可用性。</p><p>下图描绘了 Kylin 集群模式部署的一个典型场景：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8vblwtlk5j20iu0bltdn.jpg" alt="2.png"></p><p>将多个 Kylin 节点组成集群，先确保他们使用同一个 Hadoop 集群、HBase 集群。然后在每个节点的配置文件 <code>$KYLIN_HOME/conf/kylin.properties</code> 中执行下述操作：</p><ol><li>配置相同的 <code>kylin.metadata.url</code> 值，即配置所有的 Kylin 节点使用同一个 HBase metastore。</li><li>配置 Kylin 节点列表 <code>kylin.server.cluster-servers</code>，包括所有节点（包括当前节点），当事件变化时，接收变化的节点需要通知其他所有节点（包括当前节点）。</li><li>配置 Kylin 节点的运行模式 <code>kylin.server.mode</code>，参数值可选 <code>all</code>, <code>job</code>, <code>query</code> 中的一个，默认值为 <code>all</code>。<pre><code>`job` 模式代表该服务仅用于任务调度，不用于查询；`query` 模式代表该服务仅用于查询，不用于构建任务的调度；`all` 模式代表该服务同时用于任务调度和 SQL 查询。</code></pre></li></ol><p><strong>注意：</strong>默认情况下只有<strong>一个实例</strong>用于构建任务的调度 （即 <code>kylin.server.mode</code>设置为 <code>all</code> 或者 <code>job</code> 模式）。</p><h4 id="任务引擎高可用"><a href="#任务引擎高可用" class="headerlink" title="任务引擎高可用"></a>任务引擎高可用</h4><p>从 v2.0 开始, Kylin 支持多个任务引擎一起运行，相比于默认单任务引擎的配置，多引擎可以保证任务构建的高可用。</p><p>使用多任务引擎，可以在多个 Kylin 节点上配置它的角色为 <code>job</code> 或 <code>all</code>。为了避免它们之间产生竞争，需要启用分布式任务锁，需在 <code>kylin.properties</code> 里配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kylin.job.scheduler.default=2</span><br><span class="line">kylin.job.lock=org.apache.kylin.storage.hbase.util.ZookeeperJobLock</span><br></pre></td></tr></table></figure><p>并记得将所有任务和查询节点的地址注册到 <code>kylin.server.cluster-servers</code>。</p><h4 id="安装负载均衡器"><a href="#安装负载均衡器" class="headerlink" title="安装负载均衡器"></a>安装负载均衡器</h4><p>为了将查询请求发送给集群而非单个节点，可以部署一个负载均衡器，如<a href="http://nginx.org/en/" target="_blank" rel="noopener">Nginx</a>等，使得客户端和负载均衡器通信代替和特定的 Kylin 实例通信。</p><p>1.安装依赖包</p><p>yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel</p><p>2.下载并解压安装包</p><p>mkdir -p /usr/local/nginx</p><p>//下载tar包</p><p>wget <a href="http://nginx.org/download/nginx-1.13.7.tar.gz" target="_blank" rel="noopener">http://nginx.org/download/nginx-1.13.7.tar.gz</a></p><p>tar -zxvf nginx-1.13.7.tar.g</p><p>3.安装nginx</p><p>//进入nginx-1.13.7目录</p><p>//执行命令</p><p>./configure –prefix=/usr/local/nginx</p><p>//执行make命令</p><p>make</p><p>//执行make install命令</p><p>make install</p><p>4.配置nginx.conf</p><p># 打开配置文件</p><p>vi /usr/local/nginx/conf/nginx.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#gzip  on;</span><br><span class="line">   upstream kylinserver &#123;</span><br><span class="line">     server 172.16.0.125:7070 weight=5 max_fails=5 fail_timeout=600s;</span><br><span class="line">     server 172.16.0.127:7070 weight=5 max_fails=5 fail_timeout=600s;</span><br><span class="line">     server 172.16.0.128:7070 weight=5 max_fails=5 fail_timeout=600s;</span><br><span class="line">   &#125;</span><br><span class="line">   server &#123;</span><br><span class="line">       listen       8087;</span><br><span class="line">       server_name  172.16.0.125;</span><br><span class="line"></span><br><span class="line">       #charset koi8-r;</span><br><span class="line"></span><br><span class="line">       #access_log  logs/host.access.log  main;</span><br><span class="line"></span><br><span class="line">       location / &#123;</span><br><span class="line">           #root   html;</span><br><span class="line">           #index  index.html index.htm;</span><br><span class="line">          proxy_pass http://kylinserver;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       location /kylin &#123;</span><br><span class="line">           proxy_pass http://kylinserver;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><p><img src="https://tva1.sinaimg.cn/large/bec9bff2ly1g8vcz4iabpj20e20ckt8q.jpg" alt="3"></p><p>检查配置文件是否正确：/usr/local/nginx/sbin/nginx -t</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">启动：/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf</span><br></pre></td></tr></table></figure><p>重启：/usr/local/nginx/sbin/nginx -s reload</p><p>查看进程：ps -ef | grep nginx</p><p>从容停止：kill -QUIT 进程号</p><p>快速停止：kill -TERM 进程号</p><p>强制停止：pkill -9 nginx</p><h2 id="第三章-快速入门"><a href="#第三章-快速入门" class="headerlink" title="第三章 快速入门"></a>第三章 快速入门</h2><p><a href="https://www.jianshu.com/p/c49c61b654da" target="_blank" rel="noopener">参考链接</a></p><h2 id="第四章-Kylin实例调研"><a href="#第四章-Kylin实例调研" class="headerlink" title="第四章 Kylin实例调研"></a>第四章 Kylin实例调研</h2><p>从业务层面来讲，OLAP一般分为两个种类。</p><p><strong>即席查询</strong>： 即席查询（Ad Hoc）是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。 </p><p>就是用户通过手写SQL来完成一些临时的数据分析需求，这类需求的SQL形式多变，逻辑复杂，对相应时间没有严格的眼球。</p><p><strong>固化查询</strong>：对一些固化下来的取数，看数的需求，通过数据产品的形式提供给用户，从而提高数据分析和运行效率。这类SQL有固定的模式，对相应时间有较高要求。</p><h3 id="美团大规模使用Kylin的例子："><a href="#美团大规模使用Kylin的例子：" class="headerlink" title="美团大规模使用Kylin的例子："></a>美团大规模使用Kylin的例子：</h3><p>随着公司业务数据量和复杂度的不断提升 ，第二种方案会出现几个突出的问题：</p><ol><li><p>随着维度的不断增加，在数仓中维护各种维度组合的聚合表的成本越来越高，数据开发效率明显下降; </p></li><li><p>数据量超过千万行后，MySQL的导入和查询变得非常慢，经常把MySQL搞崩，DBA的抱怨很大;</p></li><li><p>由于大数据平台缺乏更高效率的查询引擎，查询需求都跑在Hive/Presto上，导致集群的计算压力大，跟不上业务需求的增长。</p></li></ol><p>目前OLAP引擎种类还有挺多，目前还没有一个西永能够满足各种场景的查询需求，本质是：没有一个能在数据量、性能、灵活性三个方面做到完美，每个西永在设计时都需要在这三者间做出取舍。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8we03d0ryj20hh06r41n.jpg" alt="1.png"></p><p>例如:</p><p>MPP架构的系统（Presto/Impala/SparkSQL/Drill等）有很好的数据量和灵活性支持，但是对响应时间是没有保证的。当数据量和计算复杂度增加后，响应时间会变慢，从秒级到分钟级，甚至小时级都有可能。</p><p>搜索引擎架构的系统（Elasticsearch等）相对比MPP系统，在入库时将数据转换为倒排索引，采用Scatter-Gather计算模型，牺牲了灵活性换取很好的性能，在搜索类查询上能做到亚秒级响应。但是对于扫描聚合为主的查询，随着处理数据量的增加，响应时间也会退化到分钟级。 </p><p>预计算系统 （Druid/Kylin等）则在入库时对数据进行预聚合，进一步牺牲灵活性换取性能，以实现对超大数据集的秒级响应。<br>对这些特点有了了解，针对不用的场合才知道选择什么。</p><p>典型的使用场景：</p><ul><li>数据以离线生产为主，数据量在千万到百亿之间级别。</li><li>需要多维度任意组合的。</li><li>指标中包含大量去重主表，要求结果精确的。</li><li>相应时间要求：3s内</li><li>可以提供SQL接口</li></ul><p>Kylin在生产环境经过了考研，2016年初美团开始推广Kylin解决方案。一年后覆盖了所有业务线，成为OLAP首选。</p><p>截止16年底，一年时间产生了214个Cube，包含的数据总行数2853亿行，Cube在HBase中的存储有59TB。日查询次数超过了50w次，TP50查询延迟87ms，TP99延迟1266ms，很好满足性能需求。</p><p>美团的硬件环境是：30个节点的Kylin专属Hbase集群，2台用于Cube构建的物理机，和8台8核16GVM用作Kylin的查询机。Cube的构建是运行在主计算机群的MR作业，各业务线的构建任务拆分到了他们各自的资源队列上。</p><p>Kylin对外是REST接口，我们接入了公司统一的http服务治理框架来实现负载均衡和平滑重启。 </p><p>调研的时候，看到Cube这个概念，一般都会担心“维度爆炸这个问题”，就是每增加一个维度，由于维度组合翻倍，可能会产生纬度爆炸，对预计算的算力和磁盘空间都产生很大考验，后来发现这个问题并没有想象的严重。这主要是因为：</p><p>Kylin支持Partial Cube，不需要对所有维度组合都进行预计算。</p><p>实际业务中，纬度之间往往存在衍生关系，而Kylin可以把衍生纬度的计算从预计算推迟到查询处理阶段。</p><p>以事实表上的衍生维度为例，业务中的很多维度都是(ID, NAME)成对出现的。查询时需要对ID列进行过滤，但显示时只需要取对应的NAME列。如果把这两列都作为维度，维度个数会翻倍。而在Kylin中，可以把NAME作为ID列的extendedcolumn指标，这样Cube中的维度个数就减半了。 </p><p>美团在采用衍生维度后，90%的场景可以把Cube中的维度个数（Rowkey列数）控制在20个以内。指标个数呈现长尾分布，小于10个指标的Cube是最多的，不过也有近一半的Cube指标数超过20。总共有382个去重指标，占到了总指标数的10%，绝大多数都是精确去重指标。49%的Cube膨胀率小于100%，即Cube存储量不超过上游Hive表。68%的Cube能够在1小时内完成构建，92%在2小时内完成构建。 </p><p>从美团的实践中能看出美团投入Kylin是看重的Kylin的海量数据超高查询性能的特点，虽然在磁盘空间上做了一部分牺牲，实践证明Kylin的空间换时间是可行的。</p><h3 id="斗鱼大规模使用Kylin的例子"><a href="#斗鱼大规模使用Kylin的例子" class="headerlink" title="斗鱼大规模使用Kylin的例子"></a>斗鱼大规模使用Kylin的例子</h3><p>斗鱼的这个例子，更有代表性，斗鱼随着业务的增长，在2019年Q2，平均MAU，达到1.6亿MAU，每天，超大量的用户使用斗鱼各客户端参与线上互动，斗鱼需要对客户端采集到的的性能数据进行统计和分析，开发出具有多维度分析图标和数据监控的APM（ Application Performance Monitoring，应用性能监控 ）平台。 最初，斗鱼采用了市面上非常流行的 Elasticsearch （简称 ES）实时聚合实现。运行一段时间后，基于 ES 的方案面临用户查询时间长、数据精度丢失等问题，斗鱼采用 Apache Kylin 替换 Elasticsearch， 对 APM 平台中存在的问题进行优化。不试不知道，一试吓一跳。</p><h4 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h4><p>斗鱼是一家面向大众用户的在线直播平台，每天都有超大量的终端用户在使用斗鱼各客户端参与线上互动。伴随业务的迅猛发展，斗鱼需要对客户端采集到的性能数据进行统计和分析，开发出具有多维度分析图表和数据监控的 APM （Application Performance Monitoring，应用性能监控） 平台。 </p><p>针对不同的客户端采集的不同数据，我们需要将各种维度之间相互组合并聚合，最终产出的数据变成指标在图表中展示。例如：对在时间、地域、网络环境、客户端以及 CDN 厂商等维度聚合下的各项指标情况进行<strong>多维度分析</strong>，包括客户端网络性能（包含完整请求耗时，请求耗时，响应耗时，DNS 耗时，TCP 耗时，TLS 耗时等等指标）各类错误时间段内的占比以及详细数量、状态码分布等等。图一和图二分别是两个示例： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xbb72l6hj20q607u0tt.jpg" alt="undefined"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xbbe37fxj20sg0aw43o.jpg" alt="undefined"></p><p>最初使用ES实施聚合，配合字眼多数据源统一接口（REST多数据源统一接口平台）框架，能够实现纬度指标的自由组合查询。数据采用strom实时消费kafka写入ES，醉倒了数据的事实展示，告警采用定时查询ES的方式。</p><p>运行一段时间后，ES的方案存在问题： <strong>采用 ES 实时聚合的方式，大多数时候对单个字段的聚合查询是非常快的，一旦遇到较为复杂的多维度组合查询并且聚合的数据量比较大（如数十亿），就可能会产生大量的分组，对 ES 的性能压力很大，查询时间很长（几十秒到数分钟）导致用户难以等待，还可能会遇到数据精度丢失的问题。</strong> 因此为了支撑业务， 考虑再三我们决定寻找替代方案，注意到 Apache Kylin 在大数据 OLAP 分析方面非常有优势，于是决定采用 <strong>Kylin 替换 Elasticsearch</strong>， 对斗鱼 APM 平台中存在的问题进行优化。不试不知道，一试吓一跳，效果还真的不错。 </p><h4 id="二、使用Kylin的挑战和解决方案"><a href="#二、使用Kylin的挑战和解决方案" class="headerlink" title="二、使用Kylin的挑战和解决方案"></a>二、使用Kylin的挑战和解决方案</h4><h5 id="Kylin集群的搭建"><a href="#Kylin集群的搭建" class="headerlink" title="Kylin集群的搭建"></a>Kylin集群的搭建</h5><p>斗鱼的这个需求是独立业务，所以搭建了独立集群，目前为止集群共17台机器，其中 CM 节点3台，角色包含 HDFS，YARN，Zookeeper，Hive，HBase，Kafka（主要是消费使用），Spark 2 等。其中 4 台机器上部署了 Kylin 服务，采用了 1 个 “all“ 节点，1 个 “job“ 节点，2个 “query“ 节点的模式，确保了查询节点和任务节点都互有备份，满足服务的高可用。</p><p>斗鱼客户端收集到的 APM 数据会先暂存于 Kafka 消息队列中，Kylin 支持直接从 Kafka topic 中摄入数据而不用先落 Hive，于是我们选择了这种直连 Kafka 的方式来构建实时 Cube。 </p><h5 id="构建实时Cube的问题"><a href="#构建实时Cube的问题" class="headerlink" title="构建实时Cube的问题"></a>构建实时Cube的问题</h5><p>斗鱼客户端收集到的 APM 数据会先暂存于 Kafka 消息队列中，Kylin 支持直接从 Kafka topic 中摄入数据而不用先落 Hive，于是我们选择了这种直连 Kafka 的方式来构建实时 Cube。 </p><p>1) Kafka数据格式要求：</p><p> Kylin 的实时 Cube 需要配置基于 Kafka topic 的 Streaming Table (将 Kafka topic 映射成一张普通表）。这一步不同于基于 Hive 的数据表（Kylin 可以直接从 Hive metastore 获取表的字段信息），需要管理员进行一定的手工配置，才能将 Kafka 中的 JSON 消息映射成表格中的行。这一步对 Kafka 中的数据格式和字段有一定的要求，起初因为不了解这些要求，配置的 Cube 在构建时经常失败，只有少数 Cube 构建成功。也有的 Cube 很多次都构建成功，但偶尔会有失败。针对这些问题我们进行了一系列的排查和改进。现总结如下：</p><p> a. 由于我们原始数据在 kafka 中的存放格式为数组格式（JSON 字符串），所以在创建 Streaming Table 的时候会遇到下面的问题： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xktgl70sj20sg0bjn0p.jpg" alt="undefined"></p><p>Kylin 会将数组中识别的字段默认加上数组下标，例如图中的 0_a，0_b 等，与我们的预期不符，所以需要对数组数据进行拆分。也就是说，Kylin 期望一条消息就是一个 JSON 对象（而非数组）。</p><p>b. 我们原始数据中还有嵌套的对象类型的字段，这种类型在 Kylin Streaming Table 识别的时候也可能会有问题，同样需要规整。如 Kylin 会把嵌套格式如 “{A: {B: value}}” 识别为 A_B 的字段，如下图，所以使用起来同样也可以，这个根据业务的不同可以自由选择，可以采用将嵌套字段铺平来规避后面可能出现的问题。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xl9j5qrcj20sg06ywfg.jpg" alt="undefined"></p><p>c. 这个是比较难发现的一个问题，就是在设计好 Cube 之后，有时会有 Cube 构建失败的情况，经过排查之后发现，是由于公司业务数据来源的特殊性（来自于客户端上报），所以可能会出现 Kafka 中字段不一致的情况。一旦出现极少数字段不一样的数据混在 Kafka 中，便极有可能让这一次的 Cube 构建失败。 </p><p>基于以上几点，我们总结，<strong>Kylin 在接入 Kafka 实时数据构建之前，一定要做好数据清洗和规整</strong>，这也是我们前期耗费大量时间踩坑的代价。数据的清洗和规整我们采用的是流处理（Storm/Flink）对 Kafka 中的数据进行对应的处理，再写入一个新的 Kafka topic 中供 Kylin 消费。 </p><p>2) 任务的定时调度</p><p>Cube 的构建任务需要调用 API，如何定时消费 kafka 的数据进行构建，以及消费 kafka 的机制究竟如何。由于对 Kylin 理解的不够，一开始建出来的 Cube 消耗性能十分严重，需要对所建的 Cube 配合业务进行剪枝和优化。</p><p>构建实时 Cube 和构建基于 Hive 的离线 Cube 有很多不一样的地方，我们在使用和摸索的过程中踩了很多坑，也有了一定的经验。</p><p>由于是近实时 Cube 构建，需要每隔一小段时间就要构建一次，采用服务器中 Kylin 主节点上部署 Crontab-job 的模式来实现。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xlqat61cj20jf05qt8z.jpg" alt="undefined"></p><p> 调度的时间间隔也经过了多次试验，调度的时间短了，上一个任务还没有执行完，下一个就开始了，容易产生堆积，在高峰时期容易造成滚雪球式的崩塌；调度的时间长了，一次处理的数据量就会特别大，导致任务执行的时间和消耗的资源也随之增长（Kylin 取 Kafka的数据，是比较简单粗暴的从上一次调度记录的 offset 直接取到当前最新的 offset，所以间隔时间越长，数据量越多），这是一个需要平衡的事情。经过反复测试使用，以及官方相应的介绍下，我们发现任务执行时间在 10~20 分钟为最优解，当然根据数据量的不同会有不同的调整。 </p><p> 3）Kylin 的剪枝与优化 </p><p>由于业务比较复杂，每个 Cube 的维度可能特别的多，随着维度数目的增加 Cuboid 的数量会爆炸式的增长。例如 WEB 端网络性能分析的 Cube 维度可能达到 47 个，如果采用全量构建，每一个可能情况都需要的话，最多可能构建 2 的 47 次方，也就是1.4 * 10^14 种组合，这肯定是不能接受的。所以在 Cube 设计的时候一定要结合业务进行优化和剪枝。</p><p>首先是筛选，将原始数据中根据不同的业务，选择不同的字段进行设计，以Ajax性能分析为例，选择出需要使用的 25 种维度。<strong>（2^47 -&gt; 2^25）</strong></p><p>接下来是分组，将 25 种维度按照不同的场景进行分组，例如，地域相关的可以放在一起，浏览器相关的也能分为一组。我们将场景分为了 4 组，将指数增长拆分为多个维度组之和。<strong>好的分组可以有效的减少计算复杂度，但是没有设计好的分组，很可能会由于设计问题没有覆盖好各种场景，导致查询的时候需要二次聚合，导致查询的性能很差，这里需要重点注意。（2^25 -&gt; 2^12 + 2^13 + 2^14 + 2^13）</strong></p><p> 然后是层级维度（Hierarchy Dimensions）、联合维度（Joint Dimensions）和必要维度（Mandatory Dimensions）的设置。这三个官网和网上都有大量的说明，这里不加赘述。最终实现 Kylin 的剪枝，来减少计算的成本。 </p><p>最后是 Kylin 本身一系列的配置上的优化，这些针对各自业务和集群可以参照官方文档进行调参优化。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xlrfg7plj20sg0b6ahm.jpg" alt="undefined"></p><h5 id="Kylin集群优化"><a href="#Kylin集群优化" class="headerlink" title="Kylin集群优化"></a>Kylin集群优化</h5><p>起初我们为 Kylin 集群申请的机器类型是计算密集型，没有足够的本地存储空间。Kylin 在运行的过程中磁盘经常满了，常常需要手动清理机器。同时在前期运行的过程中时不时会出现「Kylin 服务挂了（或者管理端登不上）」，「HBase 挂了」等等情况，针对遇到的这几个问题，我们有一些解决的措施。</p><p>1）<strong>磁盘不足。</strong>因为 Kylin 在构建 Cube 的时候，会产生大量的临时文件，而且其中有部分临时文件 Kylin 是不会主动删除的，所以机器经常会出现磁盘空间不足的问题（也跟我们计算型机器磁盘空间小有关）。</p><p>解决办法：采用定时自动清除，和手动调动 API 清除临时文件，扩容 2 台大容量机器调整 Reblance 比例（这才彻底解决这个问题）。</p><p>2）<strong>服务不稳定。</strong>刚开始的时候集群部分角色总是挂起（例如 HDFS、HBase 和 Kylin 等），排查发现是由于每台机器存在多个角色，角色分配的内存之和大于机器的可用内存，当构建任务多时，可能导致角色由于内存问题挂掉。</p><p>解决办法：对集群中各个角色重新分配，通过扩容可以解决一切资源问题。添加及时的监控，由于 Kylin 不在 CM 中管理，需要添加单独的监控来判断 Kylin 进程是否挂掉或者卡住，一旦发现需要重启 Kylin。要注意有 job 的节点重启时需要设置好 kafka 安装路径。</p><h5 id="HBase超时优化"><a href="#HBase超时优化" class="headerlink" title="HBase超时优化"></a>HBase超时优化</h5><p> Kylin 在后期维护中，经常会有任务由于 operationTimeout 导致任务失败。如图： </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8xlxz8a4ej20sg06taf5.jpg" alt="undefined"></p><p> 这个报错让 Cube 构建常常失败，且一旦构建失败超过一定的次数，该 Cube 就不会继续构建了，影响到了业务的使用，针对此问题也进行了相应的排查，发现是构建的时候，可能会由于 HBase 连接超时或者是连接数不够造成任务失败。需要在 CM 中调整 HBase 相关参数。包括调整 hbase.rpc.timeout 和 hbase.client.operation.timeout 到 50000 等（之前是 30000，可以根据业务不不同自行调整，如果还有超时可以优化或者继续调整）。 </p><h5 id="已有-Cube-的修改"><a href="#已有-Cube-的修改" class="headerlink" title="已有 Cube 的修改"></a>已有 Cube 的修改</h5><p>由于业务的迭代，新增了几个维度和指标需要增加在已存在的Cube上，又例如原先 Cube 设计上有一些不足需要修改。在这方面例如 DataSource 没有修改功能，新旧 Cube 如何切换，修改经常没有响应等等问题让我们十分为难。</p><p>已有 Cube 的修改是目前使用 Kylin 最为头疼的地方。虽然 Kylin 支持 Hybrid model 来支持一定程度的修改，但是在使用的过程中因为</p><p>各种各样的原因，例如 Streaming Table 无法修改来新增字段等，还是未能修改成功。</p><p>目前采用的修改模式为，重新设计一整套从 DataSource 到 Model 再到 Cube，停止之前 Cube 的构建任务，开始新 Cube 的构建调度。使用修改我们 Java 代码的方式动态的选择查询新 Cube 还是旧 Cube，等到一定的时间周期之后再废弃旧 Cube。目前这种方式的弊端在于查询时间段包含新旧时，需要在程序中拼接数据，十分麻烦且会造成统计数据不准。所以在设计之初就要多考虑一下后面的扩展，可以先预留几个扩展字段。</p><p>用SPA如今稳进死                                                                                                                                                                                                                                           </p><h4 id="三、效果对比"><a href="#三、效果对比" class="headerlink" title="三、效果对比"></a>三、效果对比</h4><h5 id="对比表格"><a href="#对比表格" class="headerlink" title="对比表格"></a>对比表格</h5><table><thead><tr><th>条件</th><th>Apache Kylin</th><th>ES实时聚合</th><th><strong>Hive</strong> 离线任务再入  MySQL</th></tr></thead><tbody><tr><td>查询速度</td><td>较快，一般在亚秒级别。从HBase中选择适合的纬度，Cube设计的好的话不存在二次聚合，也不会有速度方面的问题。</td><td><strong>慢，可能有几分钟。</strong>实时聚合，在复杂的情况下有严重的性能问题，查询的时间可能到几分钟。</td><td><strong>快，一般在毫秒级。</strong>计算好的数据基于  MySQL 查询，一般不会有性能问题。</td></tr><tr><td>时效性</td><td>近实时，一般在30分钟以内。延迟主要取决于任务调度的时间，但是一般都会在10~30分钟左右。</td><td><strong>实时，一般延迟在秒级。</strong>ES的延迟是取决于上游数据的写入延迟和数据刷新的时间，一般可以控制在秒级。</td><td><strong>离线，一般是T+1延迟。</strong>离线数据由于同步和计算的关系，一般都是+1小时延迟或者是+1天延迟。</td></tr><tr><td>开发难度</td><td>较简单。实时数据需要先进行一系列的清洗和规整，后面只需要配置即可，不过 Cube 设计有一定的难度。</td><td><strong>简单。</strong>只需要写入数据即可，配合已有的EST框架可以任意组合满足业务需要。</td><td><strong>工作量极大。</strong>针对每一种维度组合，都需要手动开发任务来进行计算和存储。</td></tr><tr><td>资源消耗</td><td>一般。单独搭建的集群，不会对其他业务造成影响，但是集群资源需求还是比较大。</td><td><strong>一般。</strong>查询和写入一旦量大复杂后对集群上其他的查询会带来影响。</td><td><strong>一般。</strong>在大集群上跑  YARN 任务，对集群整体影响不大。</td></tr><tr><td>可拓展性</td><td><strong>不太好扩展。</strong>针对已经建好的  DataSource、Model 和 Cube 的修改比较不友好，但是有解决的办法。</td><td><strong>可扩展性较强。</strong>所有的修改只需修改Index模板，下一周期生效即可。</td><td><strong>基本无法扩展。</strong>每次有新的业务需求需要重新开发任务。</td></tr><tr><td>容错性</td><td><strong>较差。</strong>对数据的格式和类型要求较为严格，容易导致构建失败。</td><td><strong>较差。</strong>字段不一致会带来冲突，导致字段无法聚合，且冲突一旦在索引中生成，该索引将无法解决，只有等待下一周期或删除索引。</td><td><strong>较好。</strong>对字段类型和数据字段有一定的容错性。</td></tr><tr><td>数据查询复杂度</td><td><strong>十分简单。</strong>Kylin 会根据条件自动识别就是在哪一个 Cuboid 中查询数据，只需要使用 SQL 即可，跨 Cuboid 的查询也可以自动二次聚合，SQL 也可以直接配合 EST 框架。</td><td><strong>较为容易。</strong>配合 EST 框架查询十分容易，但是由于索引有小时和天后缀，需要在程序中进行判断，才能有效降低查询量。</td><td><strong>十分困难。</strong>由于每个维度存储组合存储的表都不一样，导致存储结构十分复杂，查询的时候需要自己判断在那张表里面，难度很大。</td></tr></tbody></table><h5 id="Tips-Impala速度快的原因"><a href="#Tips-Impala速度快的原因" class="headerlink" title="Tips: Impala速度快的原因"></a>Tips: Impala速度快的原因</h5><p>impala采用了MPP查询引擎(Massively Parallel Processor)， 在数据库非共享集群中，每个节点都有独立的磁盘存储系统和内存系统，业务数据根据数据库模型和应用特点划分到各个节点上，每台数据节点通过专用网络或者商业通用网络互相连接，彼此协同计算，作为整体提供数据 库服务。非共享数据库集群有完全的可伸缩性、高可用、高性能、优秀的性价比、资源共享等优势。 </p><p>Elasticsearch也是一种MPP架构的数据库，Presto、Impala等都是MPP engine，各节点不共享资源，每个executor可以独自完成数据的读取和计算，缺点在于怕stragglers，遇到后整个engine的性能下降到该straggler的能力，所谓木桶的短板，这也是为什么MPP架构不适合异构的机器，要求各节点配置一样。 </p><h4 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h4><p>之前ES需要90s完成的查询，使用Kylin只需要2-3s，原来需要加载几分钟的仪表盘，现在只需要几秒钟就能加载完成，速度提升30多倍。</p><p>在开发效率上，切换至 Kylin 在前期不熟悉的情况下的确走了一些弯路，踩了不少坑（跟数据质量、对 Kylin 原理的掌握等都有关）。但是后面在熟悉之后便可以有不逊色于 ES 的开发效率，用起来非常不错。</p><p>目前版本的 Kylin 也有一些不足，例如数据的时效性，因为 Kylin 2.x 的流数据源只能达到准实时（Near Real-time），准实时延迟通常在十几到几十分钟的，对 APM 系统中的实时告警模块还不能满足业务要求。所以目前实时告警这一块走的还是ES，由于告警只需要对上个短暂周期（1~5 分钟）内的数据做聚合，数据量较小，ES 对此没有性能问题倒能承受。对于海量历史数据，通过 Kylin 来查询的效果更好。</p><p>新系统于 2018 年 11 月正式上线，目前已经稳定运行近一年。我们也注意到 Kylin 3.0 已经在实时统计上开始发力，能够做到 ES 这样的秒级延迟，我们会持续关注，希望 Kylin 可以发展的越来越好。</p><h2 id="第五章-Kylin在腾讯的平台化及Flink引擎实践"><a href="#第五章-Kylin在腾讯的平台化及Flink引擎实践" class="headerlink" title="第五章 Kylin在腾讯的平台化及Flink引擎实践"></a>第五章 Kylin在腾讯的平台化及Flink引擎实践</h2><p>Kylin 现有的用户管理、资源隔离机制并不能满足我们需求，基于此，腾讯对 Kylin 进行了平台化改造。希望平台化改造完成后，在下面这些层面，能够有一些改进：</p><ul><li>用户管理</li><li>资源隔离</li><li>易用性提升</li><li>方便运维</li></ul><h3 id="用户管理："><a href="#用户管理：" class="headerlink" title="用户管理："></a>用户管理：</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92byqbpinj20sg0enq3v.jpg" alt="undefined"></p><p>为了便于系统的管理及安全，公司内部有一套自己的认证系统，而且需要用个人账号去验证，所以 Kylin 作为一个平台对外提供服务的话，也需要接入到该系统。所以，我们新增了一个用户管理界面，该界面展示了 Kylin 平台内的所有用户。管理员可以新增任一用户到 Kylin 平台，新增用户时会填写企业微信名、用户角色以及是否激活用户。当用户登录系统时，会自动检测用户账号以及该账号是否在平台内注册，如果没有注册则无权限，反之自动登录系统。 </p><h3 id="内部Hive兼容"><a href="#内部Hive兼容" class="headerlink" title="内部Hive兼容"></a>内部Hive兼容</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92bzc39h9j20g60ajmxc.jpg" alt="undefined"></p><p>由于历史原因，我们部门内的 Hive 版本（THive）与 Kylin 不兼容，这就导致 Kylin 无法正常访问 Kylin 集群，所以我们采用了上图所示的兼容方案。首先，我们使用社区 Hive 版本搭建一个全新的 Hive，并作为 Kylin 的默认 Hive；其次，当 kylin 加载源表时，我们是通过内部的 UPS 系统读取 THive 的元数据信息；最后，在 Load 源表到 Kylin 时，我们根据表的元数据信息在 Kylin 的 Hive 上创建一张相同的表，但该表的存储路径依旧指向 THive 的路径，而用户在构建 cube 时，则访问新创建的表，至此就解决了 Kylin 访问 THive 的问题。 </p><h3 id="计算资源可配置化"><a href="#计算资源可配置化" class="headerlink" title="计算资源可配置化"></a>计算资源可配置化</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92bzqjxlmj20sg0b00td.jpg" alt="undefined"></p><p> 目前，Kylin 配置计算资源信息有两种方式：一是在 Kylin 配置文件中配置一个全局的计算集群及队列；二是在创建工程或者 Cube 时，在扩展参数中指定集群配置。这两种配置方式在灵活性及便捷性方面都比较差，而在我们内部是有接口可以获取到某一个用户有计算资源的计算集群及计算队列的，所以，在创建工程或者 Cube 时，我们使用了下拉框选择式的方式，让用户选择提交任务的计算资源及队列，从而大大简化了用户的使用流程。 </p><h3 id="通知机制"><a href="#通知机制" class="headerlink" title="通知机制"></a>通知机制</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92c01zg2dj20sg0cijs8.jpg" alt="undefined"></p><p>Kylin 只提供了发邮件通知的功能，而作为目前使用最广泛的工具，微信、企业微信在实时性及便捷性方面都远远胜于邮件，所以，我们提供了邮件、微信、企业微信三种方式，供用户选择。 </p><h3 id="定时调度"><a href="#定时调度" class="headerlink" title="定时调度"></a>定时调度</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92c0ds7dij20sg0augmf.jpg" alt="undefined"></p><p>Kylin 系统自身并没有提供定时调度功能，但基本上每家公司都有自己的统一调度平台，我们也不例外。我们通过 Kylin 提供的API接口，将 Cube 定时构建的功能作为一个插件集成到了公司内部的统一调度平台上。 </p><h3 id="业务接入"><a href="#业务接入" class="headerlink" title="业务接入"></a>业务接入</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92c0z1koyj20sg0bjjs2.jpg" alt="undefined"></p><p>做完以上平台化改造后，Kylin 平台基本具备了接入不同类型业务的能力，用户申请接入流程如上图所示。</p><p>业务使用情况：</p><p>我们团队是在今年初才开始引入 Kylin，目前已经在使用的业务主要有 QQ 音乐、腾讯视频、广点通、财付通等，Cube 的数量有 10 个，单份数据存储总量是 5 T，数据规模在 30 亿条左右。</p><h3 id="Flink-Cube-Engion："><a href="#Flink-Cube-Engion：" class="headerlink" title="Flink Cube Engion："></a>Flink Cube Engion：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">目前，Kylin 已经支持使用 MapReduce 和 Spark 作为构建引擎，而作为目前比较火的流批一体的大数据计算引擎怎能缺席？所以我使用 Flink 开发了一个高性能的构建引擎：Flink Cube Engine。</span><br><span class="line"></span><br><span class="line">Flink Cube Engine 是腾讯基于 Kylin 插件化的 Cube Engine 架构开发的一个高性能构建引擎，目前已具备了上线使用的能力，感兴趣的同学可以体验一下，目前该引擎已经在腾讯生产环境上线 1 个月+，非常稳定而且效果不错。</span><br><span class="line"></span><br><span class="line">Umbrella issue: </span><br><span class="line"></span><br><span class="line">Umbrella issue: </span><br><span class="line"></span><br><span class="line">https://issues.apache.org/jira/browse/KYLIN-3758</span><br><span class="line"></span><br><span class="line">分支：</span><br><span class="line"></span><br><span class="line">https://github.com/apache/kylin/tree/engine-flink</span><br></pre></td></tr></table></figure><p>上面</p><p>第一个链接可以看到flink cube的完成情况</p><p>第二个链接可以看到flink cube在github的代码</p><p>使用：<img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92cvbklxij20sg0bf0t6.jpg" alt="undefined"></p><p>Kylin 的一次 Cube 构建任务，包含了很多个子任务，而最重要的莫过于 Cube 构建这一步骤，所以，我们在 build 和 merge Cube 这两种任务中，优先实现了Cube 构建这一步骤，其他计算步骤依旧通过使用 MapReduce 来实现。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92cwad8r9j20m50f1aaq.jpg" alt="undefined"></p><p>选择使用 Flink Cube Engine 的方式也和选择 Map Reduce 和 Spark 任务类似，我们提供了前台可视化的界面，供用户选择。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92cwti90pj20sg0bygmq.jpg" alt="undefined"></p><p>上图是我们内部业务上线 Flink Cube Engine 之后的性能对比，从图中可见，该步骤的构建耗时从 49 分钟降到了 13 分钟，优化效果比较明显。两种情况的资源配置如下：</p><p>Flink 配置为：</p><p>-ytm 4G -yjm 2G -ys 1 -p 100 -yn 100</p><p>Spark 采用的动态分配资源如下：</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.enabled=true</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.minExecutors=2</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.maxExecutors=1000</p><p>kylin.engine.spark-conf.spark.dynamicAllocation.executorIdleTimeout=300</p><p>kylin.engine.spark-conf.spark.shuffle.service.enabled=true</p><p>kylin.engine.spark-conf.spark.shuffle.service.port=7337</p><p>虽然，Spark 采用的是动态分配资源，但在任务执行过程中，我们观察到 Spark实际分配的资源远比 Flink 要多的多。</p><p>那为什么性能提升会那么明显呢？</p><ol start="4"><li>Flink Cube Engine 的优化</li></ol><p>性能的提升，无非有两方面的原因，一是参数的优化，二是代码的优化。</p><p>1) 调参</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92d1xm7dbj20sg04qt9j.jpg" alt="undefined"></p><p>影响 Flink 任务性能主要有几个核心参数：并行度、单个 TM slot 数目、TM container 数目，其中单个 TM container 数目=并行度/单个 TM slot 数目。</p><p>我们调优的过程采用了控制变量法，即：固定并行度不变、固定 Job 总内存数不变。通过不断的调整单个 TM 的 slot 数目，我们发现如果单个 TM 的 slot 数目减少，拉起更多的 TM container 性能会更好。</p><p>此外，我们还使用了对象复用、内存预分配等方法，发现没有对性能提升起到太大的效果。</p><p>2) 代码优化(合并计算)</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92d2ii64cj20sg0bimyh.jpg" alt="undefined"></p><p> 在实现 Flink Cube Engine 的时候，一开始我们使用了 Map/Reduce 两个算子，发现性能很差，比 Spark 的性能还要差很多，后来我们通过调整使用了 Flink 的 mapPartition/reduceGroup 两个算子，性能就有了明显的提升。 </p><p>Flink Cube Engine 下一步的计划：</p><ol><li>全链路 Flink</li></ol><p>如上所述，目前 Cube 构建过程中，只有最关键的 cube 构建这一子任务使用了 Flink，而其他子任务仍然使用的是 MapReduce，我们下一步会继续完善 Flink Cube Engine，将所有的子任务都使用 Flink 来构建。</p><ol start="2"><li>Flink 升级到 1.9</li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g92d55mrpgj20sg09bdgg.jpg" alt="undefined"></p><p>Flink 最近发布了 1.9.0，该版本包含了很多重要特性且性能也有了一定提升，所以，我们会把 Flink Cube Engine 使用的 Flink 版本升级到1.9.0。 </p><h2 id="第六章-Kylin实操"><a href="#第六章-Kylin实操" class="headerlink" title="第六章 Kylin实操"></a>第六章 Kylin实操</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Kylin的安装相对简单，我们使用的Kylin是2.6.3版本的，安装并没有多少复杂的步骤，按照官网的步骤安装即可：</p><p><a href="http://kylin.apache.org/docs/install/index.html" target="_blank" rel="noopener">官方文档安装步骤</a></p><p>安装完成后，在bin目录下面执行check-env.sh，这个脚本会执行检查，如果仅仅显示KYLIN_HOME is set to… 说明一切配置正常，就可以启动了。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>回到Kylin的bin目录下执行sample.sh，成功执行后会有如下提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Retrieving hadoop conf dir...</span><br><span class="line">KYLIN_HOME is set to /data1/CM/kylin-2.6.3</span><br></pre></td></tr></table></figure><p>出现这个提示，就说明已经成功了。</p><p>根据日志提示信息，重新加载元数据，使kylin能读取到创建好的project “learn_kylin”.跳转至system ,点击reload metadata</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g94igifi2xj20zk0gxtc3.jpg" alt="undefined"></p><p> 然后buil cube</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g94ih39tw6j20zk0gxq4q.jpg" alt="undefined"></p><p>此时cube正在创建，可以在Monitor中监控到整个cube构造的过程以及可以查看到每一步构建的过程和日志。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g94ihodonej20zk0gx0un.jpg" alt="undefined"></p><p>然后就可以在SQL输入框输入相关的SQL，就可以做简单的可视化。</p><h2 id="第七章-Kylin的优化"><a href="#第七章-Kylin的优化" class="headerlink" title="第七章 Kylin的优化"></a>第七章 Kylin的优化</h2><h3 id="查询时间优化"><a href="#查询时间优化" class="headerlink" title="查询时间优化"></a>查询时间优化</h3><p>Kylin 的查询过程主要包含四个步骤：解析 SQL，从 HBase 获取数据，二次聚合运算，返回结果。显然优化的重点就落在如何加快 HBase 获取数据的速度和减少二次聚合预算。</p><ul><li>提高 HBase 响应时间：修改配置，修改 Cache 的策略，增加 Block Cache 的容量</li><li>减少二次聚合运算：合理设计纬度，使查询时尽量能精确命中 Cuboid。去重值使用有损算法。</li></ul><h3 id="预计算优化"><a href="#预计算优化" class="headerlink" title="预计算优化"></a>预计算优化</h3><p>预计算的优化，主要考虑有何缩短构建花费的时间，以及中间结果和最终结果占用的空间。每个业务单独一个 Cube，避免每个 Cube 大而全，减少不必要的计算。</p><h4 id="Cube优化"><a href="#Cube优化" class="headerlink" title="Cube优化"></a>Cube优化</h4><p>随着维度数目的增加，Cuboid 的数量成指数级增长。为了缓解 Cube 的构建压力，Kylin 提供了 Cube 的高级设置。这些高级设置包括聚合组（Aggregation Group）、联合维度（Joint Dimension）、层级维度（Hierarchy Dimension）和必要维度（Mandatory Dimension）等。</p><p>合理调整纬度配置，对需构建的 Cuboid 进行剪枝，刷选出真正需要的 Cuboid，优化构建性能，降低构建时间，大大提高了集群资源的利用效率。</p><p>如果Cube优化的好，效果可以非常明显。</p><h5 id="优化方法："><a href="#优化方法：" class="headerlink" title="优化方法："></a>优化方法：</h5><p>1.必须维度</p><p>查询时，经常使用的维度，以及低基数纬度。如该维度基数&lt;10，可以考虑作为必须维度。 </p><p>2.层级维度</p><p> 维度关系有一定层级性、基数有小到大情况可以使用层级维度。 </p><p>3.Joint维度</p><p>维度之间是同时出现的关系，及查询时，绝大部分情况都是同时出现的。可以使用 joint 维。 </p><p>4.维度组合组</p><p>将为维度进行分组，查询时组与组之间的维度不会同时出现。 </p><h5 id="配置优化："><a href="#配置优化：" class="headerlink" title="配置优化："></a>配置优化：</h5><p> 配置优化，包括 Kylin 资源的配置以及 Hadoop 集群配置相关修改。 </p><ol><li>构建资源</li></ol><p>每个 Cube 构建时，所需的资源不太一样，需要进行相应的资源调整。</p><ol start="2"><li>调整副本</li></ol><p>集群默认的文件副本数为 3，Cube 构建时，将副本数调为 2，个别中间任务还可以调整为 1，这样可以降低构建任务时集群 IO。为了保证查询的稳定性，HBase 副本数依然为 3。</p><ol start="3"><li>压缩格式</li></ol><p>在实验中发现，如果Hive和Hbase都设置了snappy格式，那么集群IO交互会小很多。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>Kylin在复杂业务的查询中可以用预计算的方式提前计算好预期结果存在Hive临时表中，然后写入Hbase，优点显而易见：</p><p>1.可以合理配置集群资源，预计算的时间可以提前设定。</p><p>2.对海量数据的提前预计算意味着用户查询时不用重复计算，直接从Hbase中获取结果即可。</p><p>缺点：</p><p>Kylin仅支持星型模型的数据集，对原数据提出了要求。</p><p>并且作为空间换时间的OLAP应用，需要占用HBase集群大量的空间。</p><p>维度的膨胀需要结合业务控制，不同的业务还需要研究不同Cube的优化，使用成本会比IMPALA 这种使用SQL语言的高不少。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Kylin是在Hadoop上的SQL层，最近对Phoenix调研完成之后，对Kylin产生了兴趣。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g8v1qog93xj209d08laa6.jpg&quot; alt=&quot;undefined&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Kylin" scheme="http://yoursite.com/categories/Apache/Kylin/"/>
    
    
      <category term="Kylin" scheme="http://yoursite.com/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>数据中台是什么</title>
    <link href="http://yoursite.com/2019/11/12/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
    <id>http://yoursite.com/2019/11/12/数据中台是什么？/</id>
    <published>2019-11-12T09:05:02.000Z</published>
    <updated>2020-04-10T17:04:03.313Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>数据中台是什么？能发挥什么作用？</p><p>对最近很火的数据中台一些思考。</p></blockquote><a id="more"></a> <h1 id="数据中台是什么？"><a href="#数据中台是什么？" class="headerlink" title="数据中台是什么？"></a>数据中台是什么？</h1><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><p>2015年全年产生的数据量等于历史上所有人类产生数据的总和，人类的数据增长正式从乘法型增长变成了指数型增长，海量数据处理成为了全人类的挑战。</p><p>阿里提出了DT时代已经到来：DataTech替代ITTech，强调数据驱动的重要性。</p><p>阿里走在了前面，阿里用几百人的团队支撑了几万亿的GMV，其中60%-70%来源于数据支持的机器决策，机器智能赋能业务，用更低的成本，更高的效率去服务顾客，提供个性化推荐。</p><p>阿里的数据处理经理了四个阶段，分别是：</p><p>一、数据库阶段，主要是OLTP（联机事务处理）的需求；</p><p>二、数据仓库阶段，OLAP（联机分析处理）成为主要需求；</p><p>三、数据平台阶段，主要解决BI和报表需求的技术问题；</p><p>四、数据中台阶段，通过系统来对接OLTP（事务处理）和OLAP（报表分析）的需求，强调数据业务化的能力。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v3ih8b6lj22ii1qi7aa.jpg" alt="undefined"></p><p>第一个阶段到第二阶段很好理解，数据库阶段，简单的OLTP（OLTP强调高并发，单条数据简单提取和战士，后者对并发的要求并不高，后者是地并发，大批量，面向分析。）</p><p>第一次转型就是从数据库阶段走到了数据仓库的阶段。互联网数据里面数据量最大的是网页日志，90%以上的数据是非结构化数据，数据量已经到了TB界别，针对分析需求，诞生了数据仓库（DW），阿里的第一个DW是Oracle RAC搭建了DW，这个阶段DW支持的主要久食BI和报表需求。数据库这是也在从传统CB转向分布式DB。</p><p>第二次转型就是从数据仓库阶段到数据平台阶段，这个阶段解决的 还是BI和报表需求，但是主要是在解决底层的技术问题。也即是数据库架构设计问题。</p><p>第二次转型是数据从TB阶段走向了PB级别， Oracle RAC是基于IOE架构的，所有数据用同一个EMC存储。在海量数据处理上，IOE架构有天然的限制，不适合未来的发展。阿里巴巴的第一个数据仓库就是建立在Oracle RAC上，由于数据量增长太快，所以很快就到达20个节点，当时是全亚洲最大的Oracle RAC集群，但阿里巴巴早年算过一笔账，如果仍然沿用IOE架构，那么几年后，阿里的预计营收还远远赶不上服务器的支出费用，就是说，如果不去IOE，阿里会破产。  Shared Nothing的代表就是Hadoop。Hadoop的各个处理单元都有自己私有的存储单元和处理单元， Shared Everything一般是针对单个主机，完全透明共享CPU/MEMORY/IO，并行处理能力是最差的，典型的代表SQLServer。</p><p>所以第二次转型关键词就是去IOE，建立Shared Nothing的海量数据平台来解决数据存储成本增长过快的问题，在阿里巴巴，前期是Hadoop，后期转向自研的ODPS。</p><p>第四阶段就是数据中台服务，这个阶段的特征是数据量的指数级增长，从PB级别到了EB级别，未来会到什么级别，还不好说。</p><p>目前互联网是主力，15年之后，视图声数据指数级增长，未来90%的数据可能都是非结构化数据，这些数据需要CV等技术的解析，5G技术发展，可能会进一步方法数据的体量。</p><p>另一方面，从业务来看，数据也好，数据分析也好，最终都是要为了业务服务，也就是说，要在系统层面能把OLAP和OLTP去做对接，这个对接不能靠人来完成，要靠智能算法。</p><p>目前的数据中台，最底下的数据平台还是偏技术，是中台技术方案中的一个组件，主要解决数据存储和计算，上面是数据服务层，数据服务层通过服务化API能够把数据平台和前台的业务层对接；数据中台里面就没有人的事情，直接系统去做对接，通过智能算法，能把前台的分析需求和交易需求去做对接，最终赋能业务。 </p><p>未来的数据中台，一定是「AI驱动的数据中台」，这个中台包括「计算平台+算法模型+智能硬件」，不仅要在端上具备视觉数据的收集和分析能力，而且还要能通过Face ID，帮助企业去打通业务数据，最终建立线上线下触达和服务消费者的能力。</p><p>真正做到「一切业务数据化，一切数据业务化」。</p><p>数据中台需要三种能力：</p><p>数据模型能力，AI算法模型能力，行业的应用能力。</p><h2 id="Ex"><a href="#Ex" class="headerlink" title="Ex."></a>Ex.</h2><p><strong>阿里中台全景图</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8v7cv7jsfj21b50ka121.jpg" alt="1.png"></p><p><strong>基础设施服务</strong>，即IAAS层，提供硬件底层支持。</p><p><strong>基础服务层</strong>，即PAAS层，包括分布式服务框架、分布式数据库、分布式消息、分布式存储、分布式事务、实时监控服务等等。</p><p><strong>互联网业务中台</strong>，包括各服务中心的抽象出来的各种业务能力，包括交易中心、支付中心、营销中心、结算中心、用户中心、账户中心等等。也包括非业务类服务，如日志分析中心、配置中心、序列中心、基础中心。</p><p><strong>业务应用</strong>，经过调取业务中台，组装形成独立业务服务能力的业务应用。</p><p><strong>交易来源</strong>，就是前台用户使用的各个端，如淘宝App、PC站等。</p><p>数据量超EB，表数量超过百万。</p><p>PPT小结：</p><p><strong>1、阿里业务中台架构图。</strong>阿里完整前后中台技术架构图。</p><p><strong>2、业务中台化-产品形态。</strong>将商业基础形态和逻辑梳理出来，解构成业务“积木块”。</p><p><strong>3、业务中台化-全局架构。</strong>建立中台的中心化控制单元，对中台有一个纵观全局的视图。</p><p><strong>4、业务中台化 - 业务创新和智能化。</strong>业务中台化，汇集和沉淀业务逻辑和数据，对快速创新提供支持。</p><p><strong>5、阿里核心业务架构。</strong>小前台、大中台、轻后台的相互支撑体系。</p><p><strong>6、阿里数据中台架构。</strong>数据中台建设理论、方法和实践。</p><p><strong>7、阿里技术全栈全景图。</strong>阿里的移动中台、业务中台、数据中台、技术中台。</p><p><strong>8、阿里技术平台底座。</strong>阿里多年技术积累和沉淀，构建在阿里云之上。</p><p><strong>9、阿里中台组织架构。</strong>阿里的中台战略，相匹配的组织架构升级。</p><p><strong>10、业务中台建设路径。</strong>企业中台建设应遵循的3个步骤：决心变革、成功试点、持续融合。</p><p><strong>11、企业中台战略4个升级。</strong>从战略、组织、流程、技术四个方面进行升级。</p><p><strong>12、阿里中台的能力开放。</strong>基于阿里云、ET大脑、业务&amp;数据双中台的能力开放。</p><p><strong>13、阿里业务中台建设方法论。</strong>中台建设和基础协议、中心化操控单元。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>中台这个概念被炒作的恨火，然而究竟什么是中台，似乎并没有人给出一个明确的定义。</p><p>有的人把技术集成平台叫中台，都感觉很片面。</p><p>我查阅了很多资料，中台像业务发展遇到某一瓶颈的时候，为了解决实际问题提出的解决方案。</p><p>2018年9月，腾讯宣布组织架构调整，在原有七大事业部重新组织机构， 新成立了云与智慧产业事业群（CSIG）、平台与内容事业群（PCG），调整为新的6大事业群。而6大事业群紧紧围绕的，正是技术委员会充当“技术中台”角色。 </p><p> 同年12月18日，百度集团进行了一次大的架构调整，由百度创始人、董事长李彦宏发信宣布：”百度将打造AI时代最领先的技术平台，实现前端业务和技术平台的资源高效统筹及组织全面协同。” </p><p> 3天后的12月21日，京东集团人力资源部发布关于京东商城组织架构调整的公告，公告内容称：“在新的组织架构下，京东商城将围绕以客户为中心，划分为前中后台。中台为前台业务运营和创新提供专业能力的共享平台职能。” </p><p>建设方法：</p><p>阿里：业务数据双中台；移动中台；技术中台。</p><p>腾讯：业务中台和数据中台。</p><p>百度：搜索中台</p><p>京东：数据中台</p><p>阿里数据中台概念提出这么久了，我看了这么多资料，对中台的定义大多是一家之言，我个人观点：中台是阿里为了两个目的提出来的概念，第一个目标是大企业的尾大不掉，第二点是数据驱动价值。</p><p>第二点在电商领域的价值已经不用多提，第一点可能是他觉得阿里必须要克服的问题。</p><p>说实话，具体的落地可能都还在摸索之中，先把这个概念拿出来炒作挺离谱的。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;数据中台是什么？能发挥什么作用？&lt;/p&gt;
&lt;p&gt;对最近很火的数据中台一些思考。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading Note" scheme="http://yoursite.com/categories/Reading-Note/"/>
    
    
      <category term="中台" scheme="http://yoursite.com/tags/%E4%B8%AD%E5%8F%B0/"/>
    
  </entry>
  
  <entry>
    <title>Phoenix测试</title>
    <link href="http://yoursite.com/2019/11/06/Phoenix/"/>
    <id>http://yoursite.com/2019/11/06/Phoenix/</id>
    <published>2019-11-06T09:59:16.000Z</published>
    <updated>2020-04-10T17:10:30.421Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h1><p>Phoenix实践</p><a id="more"></a> <h2 id="Phoenix启动安装基本操作"><a href="#Phoenix启动安装基本操作" class="headerlink" title="Phoenix启动安装基本操作"></a>Phoenix启动安装基本操作</h2><p>二级索引支持(gobal index + local index)</p><p>编译SQL成为原生HBase的可并行执行的Scan</p><h3 id="Phoenix结构"><a href="#Phoenix结构" class="headerlink" title="Phoenix结构"></a>Phoenix结构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oabze7naj20yg0judjv.jpg" alt="4.jpg"></p><p>Phoenix在Hadoop生态系统中的位置</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oacads47j20qy0fn769.jpg" alt="5.jpg"></p><p>HBase性能提升</p><p>hbase1.2性能能提：</p><p>1.2相对1.1，提升是十分显著的，在某些方面的额提升，延迟低了好几倍，</p><p>更别说Hive over HBase，Hive over Hbase的性能下，Phoenix的性能是这种的好多倍。</p><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>配置：配置相对来说比较简单</p><p>Download：<a href="http://phoenix.apache.org/download.html，下载hbase对应版本的phoenix；解压bin.tar.gz包，拷贝**phoenix" target="_blank" rel="noopener">http://phoenix.apache.org/download.html，下载hbase对应版本的phoenix；解压bin.tar.gz包，拷贝**phoenix</a> server jar**包到hbase集群的每个region server 的lib目录下，然后重启hbase 集群。</p><p>phoniex 的启动命令是sqlline.py</p><p>使用bin/sqlline.py 172.16.0.128:2181连接上zk，就可以连接上HBase</p><h4 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">!all               Execute the specified SQL against all the current connections</span><br><span class="line">!autocommit         Set autocommit mode on or off</span><br><span class="line">!batch             Start or execute a batch of statements</span><br><span class="line">!brief             Set verbose mode off</span><br><span class="line">!call               Execute a callable statement</span><br><span class="line">!close             Close the current connection to the database</span><br><span class="line">!closeall           Close all current open connections</span><br><span class="line">!columns           List all the columns for the specified table</span><br><span class="line">!commit             Commit the current transaction (if autocommit is off)</span><br><span class="line">!connect           Open a new connection to the database.</span><br><span class="line">!dbinfo             Give metadata information about the database</span><br><span class="line">!describe           Describe a table</span><br><span class="line">!dropall           Drop all tables in the current database</span><br><span class="line">!exportedkeys       List all the exported keys for the specified table</span><br><span class="line">!go                 Select the current connection</span><br><span class="line">!help               Print a summary of command usage</span><br><span class="line">!history           Display the command history</span><br><span class="line">!importedkeys       List all the imported keys for the specified table</span><br><span class="line">!indexes           List all the indexes for the specified table</span><br><span class="line">!isolation         Set the transaction isolation for this connection</span><br><span class="line">!list               List the current connections</span><br><span class="line">!manual             Display the SQLLine manual</span><br><span class="line">!metadata           Obtain metadata information</span><br><span class="line">!nativesql         Show the native SQL for the specified statement</span><br><span class="line">!outputformat       Set the output format for displaying results</span><br><span class="line">                  (table,vertical,csv,tsv,xmlattrs,xmlelements)</span><br><span class="line">!primarykeys       List all the primary keys for the specified table</span><br><span class="line">!procedures         List all the procedures</span><br><span class="line">!properties         Connect to the database specified in the properties file(s)</span><br><span class="line">!quit               Exits the program</span><br><span class="line">!reconnect         Reconnect to the database</span><br><span class="line">!record             Record all output to the specified file</span><br><span class="line">!rehash             Fetch table and column names for command completion</span><br><span class="line">!rollback           Roll back the current transaction (if autocommit is off)</span><br><span class="line">!run               Run a script from the specified file</span><br><span class="line">!save               Save the current variabes and aliases</span><br><span class="line">!scan               Scan for installed JDBC drivers</span><br><span class="line">!script             Start saving a script to a file</span><br><span class="line">!set               Set a sqlline variable</span><br><span class="line"></span><br><span class="line">Variable Value</span><br><span class="line">                  Description</span><br><span class="line">=============== ==========</span><br><span class="line">autoCommit true/false</span><br><span class="line">                  Enable/disable automatic</span><br><span class="line">transaction commit</span><br><span class="line">autoSave</span><br><span class="line">                   true/false Automatically save preferences</span><br><span class="line">color true/false</span><br><span class="line">                  Control whether color is used</span><br><span class="line">for display</span><br><span class="line">fastConnect</span><br><span class="line">                   true/false Skip building table/column list</span><br><span class="line">for</span><br><span class="line">                  tab-completion</span><br><span class="line">force true/false Continue running script</span><br><span class="line">                  even</span><br><span class="line">after errors</span><br><span class="line">headerInterval integer The interval between</span><br><span class="line">                  which</span><br><span class="line">headers are displayed</span><br><span class="line">historyFile path File in which to</span><br><span class="line">                  save command</span><br><span class="line">history. Default is</span><br><span class="line"><span class="meta">$</span>HOME/.sqlline/history</span><br><span class="line">                  (UNIX,</span><br><span class="line">Linux, Mac OS),</span><br><span class="line"><span class="meta">$</span>HOME/sqlline/history</span><br><span class="line">                  (Windows)</span><br><span class="line">incremental true/false Do not receive all rows</span><br><span class="line">                  from</span><br><span class="line">server before printing the first</span><br><span class="line">row. Uses fewer</span><br><span class="line">                  resources,</span><br><span class="line">especially for long-running</span><br><span class="line">queries, but column</span><br><span class="line">                  widths may</span><br><span class="line">be incorrect.</span><br><span class="line">isolation LEVEL Set transaction</span><br><span class="line">                  isolation level</span><br><span class="line">maxColumnWidth integer The maximum width to</span><br><span class="line">                  use when</span><br><span class="line">displaying columns</span><br><span class="line">maxHeight integer The maximum</span><br><span class="line">                  height of the</span><br><span class="line">terminal</span><br><span class="line">maxWidth integer The maximum width of</span><br><span class="line">                  the</span><br><span class="line">terminal</span><br><span class="line">numberFormat pattern Format numbers</span><br><span class="line">                  using</span><br><span class="line">DecimalFormat pattern</span><br><span class="line">outputFormat</span><br><span class="line">                  table/vertical/csv/tsv Format mode for</span><br><span class="line">result</span><br><span class="line">                  display</span><br><span class="line">propertiesFile path File from which SqlLine</span><br><span class="line">                  reads</span><br><span class="line">properties on startup; default</span><br><span class="line">                  is</span><br><span class="line"><span class="meta">$</span>HOME/.sqlline/sqlline.properties</span><br><span class="line">(UNIX, Linux, Mac</span><br><span class="line">                  OS),</span><br><span class="line"><span class="meta">$</span>HOME/sqlline/sqlline.properties</span><br><span class="line">(Windows)</span><br><span class="line">rowLimit</span><br><span class="line">                  integer Maximum number of rows returned</span><br><span class="line">from a query; zero</span><br><span class="line">                  means no</span><br><span class="line">limit</span><br><span class="line">showElapsedTime true/false Display execution</span><br><span class="line">                  time when</span><br><span class="line">verbose</span><br><span class="line">showHeader true/false Show column names in</span><br><span class="line">                  query</span><br><span class="line">results</span><br><span class="line">showNestedErrs true/false Display nested</span><br><span class="line">                  errors</span><br><span class="line">showWarnings true/false Display connection</span><br><span class="line">                  warnings</span><br><span class="line">silent true/false Be more silent</span><br><span class="line">timeout integer</span><br><span class="line">                  Query timeout in seconds; less</span><br><span class="line">than zero means no</span><br><span class="line">                  timeout</span><br><span class="line">trimScripts true/false Remove trailing spaces</span><br><span class="line">                  from</span><br><span class="line">lines read from script files</span><br><span class="line">verbose true/false Show</span><br><span class="line">                  verbose error messages and</span><br><span class="line">debug info</span><br><span class="line">!sql               Execute a SQL command</span><br><span class="line">!tables             List all the tables in the database</span><br><span class="line">!typeinfo           Display the type map for the current connection</span><br><span class="line">!verbose           Set verbose mode on</span><br><span class="line"></span><br><span class="line">Comments, bug reports, and patches go to ???</span><br></pre></td></tr></table></figure><p>这里面特别常见的有</p><p>!table 查看表</p><p>!quit 退出</p><p>平时命令行大多数都是输入SQL查看结果</p><p>对SQL的支持命令：</p><p>·         SELECT</p><p>·         UPSERT VALUES</p><p>·         UPSERT SELECT</p><p>·         DELETE</p><p>·         CREATE TABLE</p><p>·         DROP TABLE</p><p>·         CREATE FUNCTION</p><p>·         DROP FUNCTION</p><p>·         CREATE VIEW</p><p>·         DROP VIEW</p><p>·         CREATE SEQUENCE</p><p>·         DROP SEQUENCE</p><p>·         ALTER</p><p>·         CREATE INDEX</p><p>·         DROP SEQUENCE</p><p>·         ALTER</p><p>·         CREATE INDEX</p><p>·         DROP INDEX</p><p>·         ALTER INDEX</p><p>·         EXPLAIN</p><p>·         UPDATE STATISTICS</p><p>·         CREATE SCHEMA</p><p>·         USE</p><p>·         DROP SCHEMA</p><p>注意:在没有索引的情况下,针对大表使用非索引查询会非常耗时,很有可能会报超时错误.</p><h4 id="JDBC对Phoenix的基本操作"><a href="#JDBC对Phoenix的基本操作" class="headerlink" title="JDBC对Phoenix的基本操作"></a>JDBC对Phoenix的基本操作</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@Author</span> Administrator</span></span><br><span class="line"><span class="comment">* <span class="doctag">@create</span> 2019/9/9 17:13</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseDB</span> </span>&#123;</span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// load driver</span></span><br><span class="line">           Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           <span class="comment">// jdbc 的 url 类似为 jdbc:phoenix [ :&lt;zookeeper quorum&gt; [ :&lt;port number&gt; ] [ :&lt;root node&gt; ] ]，</span></span><br><span class="line">           <span class="comment">// 需要引用三个参数：hbase.zookeeper.quorum、hbase.zookeeper.property.clientPort、and zookeeper.znode.parent，</span></span><br><span class="line">           <span class="comment">// 这些参数可以缺省不填而在 hbase-site.xml 中定义。</span></span><br><span class="line">           <span class="keyword">return</span> DriverManager.getConnection(<span class="string">"jdbc:phoenix:172.16.0.128:2181"</span>);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check if the table exist</span></span><br><span class="line">           ResultSet rs = conn.getMetaData().getTables(<span class="keyword">null</span>, <span class="keyword">null</span>, <span class="string">"USER"</span>,</span><br><span class="line">                   <span class="keyword">null</span>);</span><br><span class="line">           <span class="keyword">if</span> (rs.next()) &#123;</span><br><span class="line">               System.out.println(<span class="string">"table user is exist..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"CREATE TABLE user (id varchar PRIMARY KEY,INFO.account varchar ,INFO.passwd varchar)"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute</span></span><br><span class="line">           ps.execute();</span><br><span class="line">           System.out.println(<span class="string">"create success..."</span>);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">upsert</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"upsert into user(id, INFO.account, INFO.passwd) values('001', 'admin', 'admin')"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute upsert</span></span><br><span class="line">           String msg = ps.executeUpdate() &gt; <span class="number">0</span> ? <span class="string">"insert success..."</span></span><br><span class="line">                  : <span class="string">"insert fail..."</span>;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// you must commit</span></span><br><span class="line">           conn.commit();</span><br><span class="line">           System.out.println(msg);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">query</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"select * from user"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           ResultSet rs = ps.executeQuery();</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"id"</span> + <span class="string">"\t"</span> + <span class="string">"account"</span> + <span class="string">"\t"</span> + <span class="string">"passwd"</span>);</span><br><span class="line">           System.out.println(<span class="string">"======================"</span>);</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> (rs != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">                   System.out.print(rs.getString(<span class="string">"id"</span>) + <span class="string">"\t"</span>);</span><br><span class="line">                   System.out.print(rs.getString(<span class="string">"account"</span>) + <span class="string">"\t"</span>);</span><br><span class="line">                   System.out.println(rs.getString(<span class="string">"passwd"</span>));</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"delete from user where id='001'"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute upsert</span></span><br><span class="line">           String msg = ps.executeUpdate() &gt; <span class="number">0</span> ? <span class="string">"delete success..."</span></span><br><span class="line">                  : <span class="string">"delete fail..."</span>;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// you must commit</span></span><br><span class="line">           conn.commit();</span><br><span class="line">           System.out.println(msg);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">drop</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       Connection conn = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// get connection</span></span><br><span class="line">           conn = BaseDB.getConnection();</span><br><span class="line"></span><br><span class="line">           <span class="comment">// check connection</span></span><br><span class="line">           <span class="keyword">if</span> (conn == <span class="keyword">null</span>) &#123;</span><br><span class="line">               System.out.println(<span class="string">"conn is null..."</span>);</span><br><span class="line">               <span class="keyword">return</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// create sql</span></span><br><span class="line">           String sql = <span class="string">"drop table user"</span>;</span><br><span class="line"></span><br><span class="line">           PreparedStatement ps = conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">           <span class="comment">// execute</span></span><br><span class="line">           ps.execute();</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"drop success..."</span>);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">           e.printStackTrace();</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">           <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">try</span> &#123;</span><br><span class="line">                   conn.close();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                   e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 创角标</span></span><br><span class="line">       create();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 注意更新用的是upsert 插入和更新一样的</span></span><br><span class="line">       upsert();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 查询 查询出结果并打印</span></span><br><span class="line">       query();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 删除表</span></span><br><span class="line"><span class="comment">//       drop();</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基本操作和JDBC基本一样，只要在Class里面修改成Phoenix就可以</p><h2 id="无索引简单测试"><a href="#无索引简单测试" class="headerlink" title="无索引简单测试"></a>无索引简单测试</h2><h3 id="首先上结论"><a href="#首先上结论" class="headerlink" title="首先上结论:"></a>首先上结论:</h3><p>针对主键根据单纬度查询,数据量对搜索结果影响非常小,如果仅仅是返回单条结果的查询,能够达到毫秒级反应速度,根据主键批量查询的话速度由表大小和返回数量决定,从目前数据量11亿左右响应速度也在秒级.</p><p>但是如果没有建立索引,想根据非主键查询,反应时间会非常久.</p><h3 id="响应时间测试"><a href="#响应时间测试" class="headerlink" title="响应时间测试"></a>响应时间测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">       Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line"></span><br><span class="line">       String url = <span class="string">"jdbc:phoenix:datanode128:2181"</span>;</span><br><span class="line"></span><br><span class="line">       Connection conn = DriverManager.getConnection(url);</span><br><span class="line"></span><br><span class="line">       Statement statement = conn.createStatement();</span><br><span class="line"></span><br><span class="line">       <span class="keyword">long</span> time = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">       ResultSet rs = statement.executeQuery(<span class="string">"select * from test"</span>);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">           String myKey = rs.getString(<span class="string">"MYKEY"</span>);</span><br><span class="line">           String myColumn = rs.getString(<span class="string">"MYCOLUMN"</span>);</span><br><span class="line"></span><br><span class="line">           System.out.println(<span class="string">"myKey="</span> + myKey + <span class="string">"myColumn="</span> + myColumn);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">long</span> timeUsed = System.currentTimeMillis() - time;</span><br><span class="line"></span><br><span class="line">       System.out.println(<span class="string">"time "</span> + timeUsed + <span class="string">"mm"</span>);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 关闭连接</span></span><br><span class="line">       rs.close();</span><br><span class="line">       statement.close();</span><br><span class="line">       conn.close();</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">       e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>结果：</strong></p><p>120ms</p><p>CREATE TABLE IF NOT EXISTS “employee” (“no” VARCHAR(10) NOT NULL PRIMARY KEY, “company”.”name” VARCHAR(30),”company”.”position” VARCHAR(20), “family”.”tel” VARCHAR(20), “family”.”age” INTEGER);</p><p>csv columns from database. CSV Upsert complete. 1000000 rows upserted </p><p><strong>测试结果：</strong></p><p>100w： insert 70s         count:1.032s     groupby PK: 0.025s </p><p>500w：insert 314s   count:1.246s   groupby PK:0.024s </p><p>从结果看，随着数量级的增加，查询时耗也随之增加，有一个例外，就是当用索引字段为主键时作聚合查询时，用时相差不大。总的来说，Phoenix在用到索引时查询性能会比较好。那对于Count来说，如果不用Phoenix,用HBase自带的Count耗时是怎样的呢，测了一下，HBase Count 100万需要33s, 500万需要139s，性能还是很差的。对于大表来说基本不能用Count来统计行数，还得依赖于基于Coprocessor机制来统计。</p><h2 id="JDBC模拟数据代码"><a href="#JDBC模拟数据代码" class="headerlink" title="JDBC模拟数据代码"></a>JDBC模拟数据代码</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Calendar;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.RandomUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.DateUtils;</span><br><span class="line"></span><br><span class="line"><span class="comment">//id，日期,号牌号码，车型，颜色</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BuildData</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] ys = <span class="keyword">new</span> String[] &#123; <span class="string">"红"</span>, <span class="string">"橙"</span>, <span class="string">"黄"</span>, <span class="string">"绿"</span>, <span class="string">"青"</span>, <span class="string">"蓝"</span>, <span class="string">"紫"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] cx = <span class="keyword">new</span> String[] &#123; <span class="string">"大众"</span>, <span class="string">"别克"</span>, <span class="string">"奥迪"</span>, <span class="string">"宝马"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] zm = <span class="keyword">new</span> String[] &#123; <span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>, <span class="string">"F"</span>, <span class="string">"G"</span>, <span class="string">"H"</span>, <span class="string">"I"</span>, <span class="string">"G"</span>, <span class="string">"K"</span>, <span class="string">"L"</span>, <span class="string">"M"</span>, <span class="string">"N"</span>,</span><br><span class="line"><span class="string">"O"</span>, <span class="string">"P"</span>, <span class="string">"Q"</span>, <span class="string">"R"</span>, <span class="string">"S"</span>, <span class="string">"T"</span>, <span class="string">"U"</span>, <span class="string">"V"</span>, <span class="string">"W"</span>, <span class="string">"X"</span>, <span class="string">"Y"</span>, <span class="string">"Z"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] zmSz = <span class="keyword">new</span> String[] &#123; <span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>, <span class="string">"F"</span>, <span class="string">"G"</span>, <span class="string">"H"</span>, <span class="string">"I"</span>, <span class="string">"G"</span>, <span class="string">"K"</span>, <span class="string">"L"</span>, <span class="string">"M"</span>, <span class="string">"N"</span>,</span><br><span class="line"><span class="string">"O"</span>, <span class="string">"P"</span>, <span class="string">"Q"</span>, <span class="string">"R"</span>, <span class="string">"S"</span>, <span class="string">"T"</span>, <span class="string">"U"</span>, <span class="string">"V"</span>, <span class="string">"W"</span>, <span class="string">"X"</span>, <span class="string">"Y"</span>, <span class="string">"Z"</span>, <span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>, <span class="string">"4"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>, <span class="string">"7"</span>, <span class="string">"8"</span>, <span class="string">"9"</span>,</span><br><span class="line"><span class="string">"0"</span> &#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String[] prv = <span class="keyword">new</span> String[] &#123; <span class="string">"京"</span>, <span class="string">"津"</span>, <span class="string">"沪"</span>, <span class="string">"渝"</span>, <span class="string">"冀"</span>, <span class="string">"晋"</span>, <span class="string">"辽"</span>, <span class="string">"吉"</span>, <span class="string">"黑"</span>, <span class="string">"苏"</span>, <span class="string">"浙"</span>, <span class="string">"皖"</span>, <span class="string">"闽"</span>, <span class="string">"赣"</span>,</span><br><span class="line"><span class="string">"鲁"</span>, <span class="string">"豫"</span>, <span class="string">"鄂"</span>, <span class="string">"湘"</span>, <span class="string">"粤"</span>, <span class="string">"琼"</span>, <span class="string">"川"</span>, <span class="string">"贵"</span>, <span class="string">"云"</span>, <span class="string">"陕"</span>, <span class="string">"甘"</span>, <span class="string">"青"</span>, <span class="string">"台"</span>, <span class="string">"蒙"</span>, <span class="string">"桂"</span>, <span class="string">"宁"</span>, <span class="string">"新"</span>, <span class="string">"藏"</span>, <span class="string">"港"</span>, <span class="string">"澳"</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Calendar start = Calendar.getInstance();</span><br><span class="line"><span class="keyword">private</span> Calendar end = Calendar.getInstance();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> dayOfSecond = <span class="number">250</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">BuildData</span><span class="params">(<span class="keyword">int</span> dayOfSecond, Date start, Date end)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.start.setTime(start);</span><br><span class="line"><span class="keyword">this</span>.end.setTime(end);</span><br><span class="line"><span class="keyword">this</span>.dayOfSecond = dayOfSecond;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成数据</span></span><br><span class="line"><span class="keyword">public</span> String[][] buildData() &#123;</span><br><span class="line"><span class="keyword">if</span> (dayOfSecond &gt; <span class="number">0</span>) &#123;</span><br><span class="line">String date = getDate();</span><br><span class="line"><span class="keyword">if</span> (date == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">String[][] result = <span class="keyword">new</span> String[dayOfSecond][<span class="number">5</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dayOfSecond; i++) &#123;</span><br><span class="line">result[i][<span class="number">0</span>] = getId();</span><br><span class="line">result[i][<span class="number">1</span>] = date;</span><br><span class="line">result[i][<span class="number">2</span>] = getHphm();</span><br><span class="line">result[i][<span class="number">3</span>] = getCx();</span><br><span class="line">result[i][<span class="number">4</span>] = getYs();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// id</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String string = UUID.randomUUID().toString().replaceAll(<span class="string">"-"</span>, <span class="string">""</span>);</span><br><span class="line"><span class="keyword">return</span> string;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 日期</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getDate</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (end.getTime().getTime() &gt;= start.getTime().getTime()) &#123;</span><br><span class="line">start.setTime(DateUtils.addSeconds(start.getTime(), <span class="number">1</span>));</span><br><span class="line"><span class="keyword">return</span> sdf.format(start.getTime());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 车牌 苏E3G02D</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getHphm</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String p = prv[RandomUtils.nextInt(<span class="number">0</span>, prv.length)];</span><br><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder(p);</span><br><span class="line">sb.append(zm[RandomUtils.nextInt(<span class="number">0</span>, zm.length)]);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">sb.append(zmSz[RandomUtils.nextInt(<span class="number">0</span>, zmSz.length)]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 车型</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getCx</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> nextInt = RandomUtils.nextInt(<span class="number">0</span>, cx.length);</span><br><span class="line"><span class="keyword">return</span> cx[nextInt];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 颜色</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getYs</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> nextInt = RandomUtils.nextInt(<span class="number">0</span>, ys.length);</span><br><span class="line"><span class="keyword">return</span> ys[nextInt];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CarVo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> String id;</span><br><span class="line"><span class="keyword">private</span> String date;</span><br><span class="line"><span class="keyword">private</span> String hphm;</span><br><span class="line"><span class="keyword">private</span> String cx;</span><br><span class="line"><span class="keyword">private</span> String ys;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.id = id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getDate</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> date;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDate</span><span class="params">(String date)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.date = date;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getHphm</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> hphm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHphm</span><span class="params">(String hphm)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.hphm = hphm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getCx</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> cx;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCx</span><span class="params">(String cx)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.cx = cx;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getYs</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> ys;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setYs</span><span class="params">(String ys)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.ys = ys;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.text.ParseException;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.dbutils.QueryRunner;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.time.DateUtils;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertData</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Logger log = LoggerFactory.getLogger(InsertData.class);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">args = <span class="keyword">new</span> String[] &#123; <span class="string">"20170103000000"</span>, <span class="string">"20170131000000"</span> &#125;;</span><br><span class="line">Date start = DateUtils.parseDate(args[<span class="number">0</span>], <span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line">Date end = DateUtils.parseDate(args[<span class="number">1</span>], <span class="string">"yyyyMMddHHmmss"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">Connection connection = <span class="keyword">null</span>;</span><br><span class="line">Class.forName(<span class="string">"org.apache.phoenix.jdbc.PhoenixDriver"</span>);</span><br><span class="line">connection = DriverManager.getConnection(<span class="string">"jdbc:phoenix:172.16.0.128:2181"</span>, <span class="string">""</span>, <span class="string">""</span>);</span><br><span class="line">QueryRunner queryRunner = <span class="keyword">new</span> QueryRunner();</span><br><span class="line"></span><br><span class="line">BuildData buildData = <span class="keyword">new</span> BuildData(<span class="number">250</span>, start, end);</span><br><span class="line">String[][] buildData2 = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">int</span> num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">long</span> all = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> ((buildData2 = buildData.buildData()) != <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">int</span>[] batch = queryRunner.batch(connection, <span class="string">"upsert into car_test values(?,?,?,?,?)"</span>, buildData2);</span><br><span class="line">num += batch.length;</span><br><span class="line">all += batch.length;</span><br><span class="line"><span class="keyword">if</span> (num &gt; <span class="number">1000</span>) &#123;</span><br><span class="line"><span class="keyword">long</span> time1 = System.currentTimeMillis();</span><br><span class="line">connection.commit();</span><br><span class="line"><span class="keyword">long</span> time2 = System.currentTimeMillis();</span><br><span class="line">num = <span class="number">0</span>;</span><br><span class="line">System.out.println(<span class="keyword">new</span> Date() + <span class="string">":"</span> + <span class="string">"-start:"</span> + args[<span class="number">0</span>] + <span class="string">"-end:"</span> + args[<span class="number">1</span>] + <span class="string">"--"</span> + all + <span class="string">"--"</span></span><br><span class="line">+ (time2 - time1));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// num++;</span></span><br><span class="line"><span class="comment">// all++;</span></span><br><span class="line"><span class="comment">// if (num &gt; 1000) &#123;</span></span><br><span class="line"><span class="comment">// num=0;</span></span><br><span class="line"><span class="comment">// System.out.println(new Date() + ":" + "-start:" + args[0] +</span></span><br><span class="line"><span class="comment">// "-end:" + args[1] + "--" + all);</span></span><br><span class="line"><span class="comment">// &#125;</span></span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Phoenix和Spark整合"><a href="#Phoenix和Spark整合" class="headerlink" title="Phoenix和Spark整合"></a>Phoenix和Spark整合</h2><p>数据格式：</p><table><thead><tr><th>imei</th><th>alarm_type</th><th>lat</th><th>lng</th><th>device_status</th><th>mc_type</th><th>read_status</th><th>speed</th><th>addr</th><th>index_name</th><th>user_id</th><th>user_parent_id</th></tr></thead><tbody><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>spark写入HBase(通过Phoenix)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparkPhoenixSave</span></span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">       <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"sparkToPhoenix"</span>)</span><br><span class="line">       <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="type">SparkConf</span>)</span><br><span class="line">       <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line">       <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// 从集合创建rdd</span></span><br><span class="line">       <span class="keyword">val</span> rdd = <span class="type">List</span>((<span class="number">1</span>L,<span class="string">"1"</span>,<span class="number">1</span>),(<span class="number">2</span>L,<span class="string">"2"</span>,<span class="number">2</span>),(<span class="number">3</span>L,<span class="string">"3"</span>,<span class="number">3</span>))</span><br><span class="line">       <span class="comment">// 从rdd创建DF</span></span><br><span class="line">       <span class="keyword">val</span> df = rdd.toDF(<span class="string">"id"</span>,<span class="string">"col1"</span>,<span class="string">"col2"</span>)</span><br><span class="line">       df.show()</span><br><span class="line">       </span><br><span class="line">       <span class="comment">// Save to OUTPUT_TABLE</span></span><br><span class="line">       df.save(<span class="string">"org.apache.phoenix.spark"</span>,<span class="type">SaveMode</span>.<span class="type">Overwrite</span>,<span class="type">Map</span>(<span class="string">"table"</span> -&gt; <span class="string">"GPS"</span>,<span class="string">"zkUrl"</span> -&gt; <span class="string">"172.16.0.126:2181/hbase-unsecure"</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>问题点：</p><p>依赖方面有几个问题：</p><p>集群升级后采用了Phoenix 4.14 - HBase 1.2.0版本。</p><p>这个版本的依赖经过一段时间的试验之后选择了如下的版本:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- phoenix spark--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-HBase-1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>理论上只要用</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.13.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>就能解决问题，但是这个依赖一直有问题，手动下载也没有解决加载问题</p><p>我们集群是5.13.3的，这边选择的是5.13.2的，应该能够兼容的，深入进去看里面的组件，其实HBase版本啥的都一样。</p><p>但是Phoenix4.14.0-CDH版本安装的时候在HBase-site.xml中已经有了两个改动，所以这边有两个选择，要么在代码中配置hbase的选项，</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbConfig.set(<span class="string">"phoenix.schema.isNamespaceMappingEnabled"</span>,<span class="string">"true"</span>);</span><br><span class="line">hbConfig.set(<span class="string">"phoenix.schema.mapSystemTablesToNamespace "</span>,<span class="string">"true"</span>);</span><br></pre></td></tr></table></figure><p>要么在resource文件夹中添加hbase-site.xml，后者更加方便一点。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">19</span>/<span class="number">10</span>/<span class="number">14</span> <span class="number">17</span>:<span class="number">07</span>:<span class="number">39</span> INFO ConnectionQueryServicesImpl: HConnection established. Stacktrace <span class="keyword">for</span> informational purposes: hconnection-<span class="number">0x1de9b505</span> java.lang.Thread.getStackTrace(Thread.java:<span class="number">1559</span>)</span><br><span class="line">org.apache.phoenix.util.LogUtil.getCallerStackTrace(LogUtil.java:<span class="number">55</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.openConnection(ConnectionQueryServicesImpl.java:<span class="number">427</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.access$<span class="number">400</span>(ConnectionQueryServicesImpl.java:<span class="number">267</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl$<span class="number">12</span>.call(ConnectionQueryServicesImpl.java:<span class="number">2515</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl$<span class="number">12</span>.call(ConnectionQueryServicesImpl.java:<span class="number">2491</span>)</span><br><span class="line">org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:<span class="number">76</span>)</span><br><span class="line">org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:<span class="number">2491</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:<span class="number">255</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:<span class="number">150</span>)</span><br><span class="line">org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:<span class="number">221</span>)</span><br><span class="line">java.sql.DriverManager.getConnection(DriverManager.java:<span class="number">664</span>)</span><br><span class="line">java.sql.DriverManager.getConnection(DriverManager.java:<span class="number">208</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.ConnectionUtil.getConnection(ConnectionUtil.java:<span class="number">113</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.ConnectionUtil.getInputConnection(ConnectionUtil.java:<span class="number">58</span>)</span><br><span class="line">org.apache.phoenix.mapreduce.util.PhoenixConfigurationUtil.getSelectColumnMetadataList(PhoenixConfigurationUtil.java:<span class="number">354</span>)</span><br><span class="line">org.apache.phoenix.spark.PhoenixRDD.toDataFrame(PhoenixRDD.scala:<span class="number">118</span>)</span><br><span class="line">org.apache.phoenix.spark.PhoenixRelation.schema(PhoenixRelation.scala:<span class="number">60</span>)</span><br><span class="line">org.apache.spark.sql.execution.datasources.LogicalRelation.&lt;init&gt;(LogicalRelation.scala:<span class="number">40</span>)</span><br><span class="line">org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:<span class="number">389</span>)</span><br><span class="line">org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:<span class="number">146</span>)</span><br><span class="line">org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:<span class="number">125</span>)</span><br><span class="line">phoenix.SparkPhoenixRead$.main(SparkPhoenixRead.scala:<span class="number">17</span>)</span><br><span class="line">phoenix.SparkPhoenixRead.main(SparkPhoenixRead.scala)</span><br></pre></td></tr></table></figure><p>配置完成之后，使用代码的时候，始终在报如上错误。</p><p>分析之后发现这并不是报错，而是长得像报错的日志格式。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ConnectionQueryServicesImpl.java</span></span><br><span class="line">logger.info(<span class="string">"HConnection established. Stacktrace for informational purposes: "</span> + connection + <span class="string">" "</span> +  LogUtil.getCallerStackTrace());</span><br><span class="line"><span class="comment">//LogUtil.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getCallerStackTrace</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   StackTraceElement[] st = Thread.currentThread().getStackTrace();</span><br><span class="line">   StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">   <span class="keyword">for</span> (StackTraceElement element : st) &#123;</span><br><span class="line">       sb.append(element.toString());</span><br><span class="line">       sb.append(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">   <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>获取调用栈并记入日记，不是bug，==</p><p>最终使用的代码是Spark2.0以上格式的SparkSession</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local[*]"</span>).appName(<span class="string">"sparkPhoenix"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> df = spark.read.format(<span class="string">"org.apache.phoenix.spark"</span>)</span><br><span class="line">    .option(<span class="string">"zkUrl"</span>,<span class="string">"172.16.0.127:2181"</span>)</span><br><span class="line">    .option(<span class="string">"table"</span>,<span class="string">"TABLE_NAME"</span>)</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line">   df.show()</span><br><span class="line"></span><br><span class="line">   spark.stop()</span><br></pre></td></tr></table></figure><p>Spark与Phoenix Jar包冲突说明：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.driver.extraClassPath=phoenix-spark-4.14.0-HBase-1.2.jar --conf spark.executor.extraClassPath=phoenix-spark-4.14.0-HBase-1.2.jar</span><br></pre></td></tr></table></figure><h3 id="Phoenix-Jar包冲突（大坑）"><a href="#Phoenix-Jar包冲突（大坑）" class="headerlink" title="Phoenix Jar包冲突（大坑）"></a>Phoenix Jar包冲突（大坑）</h3><p>Jar包冲突的原因是HBase…这个包和CDH…这个包加载顺序的问题</p><p>先加载CDH这个包的话就会导致问题Jar包冲突，必须先加载HBase包，要手动指定。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oad0y9u2j20dp0613yt.jpg" alt="8.png"></p><p>图上还缺少一个HBase包放在oozie的lib里面，这些是加载必备的。</p><p>不然会出现奇怪的错误，因为加载是随机加载的，如果先加载了CDH包就会报错。</p><h2 id="映射、索引分区"><a href="#映射、索引分区" class="headerlink" title="映射、索引分区"></a>映射、索引分区</h2><h3 id="映射："><a href="#映射：" class="headerlink" title="映射："></a>映射：</h3><p>默认情况下，直接在hbase中创建的表，通过phoenix是查看不到的</p><p>test是在hbase中直接创建的，默认情况下，在phoenix中是查看不到test的。</p><p>有两种映射方法，一种是视图映射，一种是表映射。</p><p>视图创建过后,直接删除,Hbase中的原表不会受影响,如果创建的是表映射,删除Phoenix中的映射表,会把原表也删除.</p><p><strong>一、基础知识</strong></p><p>Salted Table 是phoenix为了防止hbase表rowkey设计为自增序列而引发热点region读和热点region写而采取的一种表设计手段。通过在创建表的时候指定Salt_Buckets来实现pre-split，下面的建表语句建表的时候将会把表预分割到20个region里面。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SALT_TEST (A_KEY VARCHAR PRIMARY KEY,</span><br><span class="line"> A_col VARCHAR) SALT_BUCKETS = 20</span><br></pre></td></tr></table></figure><p>默认情况下，对salted table 创建耳机索引，二级索引表会随同原表进行salted切分，salt_buckets与原表保持一致，当然，在创建耳机索引表的时候也可以自定义salt_buckets的数量，phoenix没有强制数量必须和原表一致。</p><p><strong>二、实现原理</strong></p><p>讲一个散列取余后的byte值插入到rowkey的第一个字节里，通过定义每个region的start key和end key将数据分割到不同region，以此来防止自增序列引入的热点问题。从而达到平衡hbase集群的读写性能问题。</p><p>salted byte的计算方式大致如下</p><p>hash(rowkey) % SALT_BUCKETS</p><p>默认下salted byte将作为每个region的start key及 end key，以此分割数据到不同的region，</p><p>这样能做到具有相同的salted byte处在一个region。</p><p><strong>三、本质</strong></p><p>本质就是在hbase中</p><p>rowkey前面加上一个字节，在表中实际存储时，就可以自动分布到不同的region中去了。</p><p><strong>四、实例</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE SALT_TEST (a_key VARCHAR PRIMARY KEY, a_col VARCHAR) SALT_BUCKETS = 4;</span><br><span class="line"></span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_abc&apos;, &apos;col_abc&apos;);</span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_ABC&apos;, &apos;col_ABC&apos;);</span><br><span class="line"> UPSERT INTO SALT_TEST(a_key, a_col) VALUES(&apos;key_rowkey01&apos;, &apos;col01&apos;);</span><br></pre></td></tr></table></figure><p>从Phoenix sqlline.py查询数据 看不出区别，去hbase scan 就能看到phoenix是在rowkey的第一个字节插入一个byte字节。</p><p><strong>五、注意</strong></p><p>每条rowkey前面加一个Byte，这里显示为16进制，创建完成之后，应该使用Phoenix SQL来读取数据，不要混合使用Phoenix Sql插入数据，使得原始rowkey前面被自动加上一个byte。</p><p><strong>同步索引和异步索引</strong></p><p>一般我可以使用create index来创建一个索引，这是一种同步的方法，但是有时候我们创建索引的表非常大，</p><p>我们需要等很长时间，Phoenix 4.5 以后有一个异步创建索引的方式，使用关键字ASYNC来创建索引：</p><p>实际上在二级索引中，我们需要先将phoenix的client包放入hbase的lib中然后在启动，这个IndexTool底层走的是MR，MR任务运行完毕后，索引会被自动引导，当我们在phoenix命令行中使用!table，看到索引表已经处于enable状态就可以使用该索引了。</p><p>官网demo里面的建立索引，仅仅针对一个字段，这样涉及稍微复杂的业务，索引并不能起效，还是全表索引。</p><p>二级索引分为以下几种：</p><p>全局索引、本地索引、覆盖索引</p><p>全局索引：</p><p>全局索引是默认索引类型，适用于读多写少的场景，由于全局索引会拦截（DELETE，UPSERT VALUES and UPSERT SELECT）数据更新并更新索引表，而索引表十分不在不用数据节点上的，跨节点的数据传输带来了较大的性能损耗。</p><p>本地索引</p><p>本地索引适用于少读多写</p><p>必须通过下面两部才能完成异步索引的构建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CRAETE INDEX anync_index ON PHOENIX_ALARM_DIS(LAT,LNG,IMEI,CREATETIME) ASYNC</span><br></pre></td></tr></table></figure><p>创建异步索引，命令执行后一开始不会有数据，还必须使用单独的命令行工具来执行数据的创建，当语句给执行的时候，后端会启动mr任务，只有等到这个任务结束，数据都被生成在索引表中后，这个索引才能被使用，创建语句执行完后，还需要用工具导入数据。</p><p>官网说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一般我们可以使用CREATE INDEX来创建一个索引，这是一种同步的方法。但是有时候我们创建索引的表非常大，我们需要等很长时间。Phoenix 4.5以后有一个异步创建索引的方式，使用关键字ASYNC来创建索引：</span><br><span class="line"> ​</span><br><span class="line"> CREATE INDEX index1_c ON hao1 (age) INCLUDE(name) ASYNC;</span><br><span class="line"> 这时候创建的索引表中不会有数据。你还必须要单独的使用命令行工具来执行数据的创建。当语句给执行的时候，后端会启动一个map reduce任务，只有等到这个任务结束，数据都被生成在索引表中后，这个索引才能被使用。启动工具的方法：</span><br><span class="line"> ​</span><br><span class="line"> $&#123;HBASE_HOME&#125;/bin/hbase org.apache.phoenix.mapreduce.index.IndexTool</span><br><span class="line"> --schema MY_SCHEMA --data-table MY_TABLE --index-table ASYNC_IDX</span><br><span class="line"> --output-path ASYNC_IDX_HFILES</span><br><span class="line"> 这个任务不会因为客户端给关闭而结束，是在后台运行。你可以在指定的文件ASYNC_IDX_HFILES中找到最终实行的结果。</span><br></pre></td></tr></table></figure><h2 id="测试和遇到的一些问题"><a href="#测试和遇到的一些问题" class="headerlink" title="测试和遇到的一些问题"></a>测试和遇到的一些问题</h2><p>使用Spark写入11亿5千万测试数据,分为24个分区,使用异步索引针对time和imei字段各建立了一个索引.</p><p>这边有个问题，索引在创建完成之后，数据类型自动变化了，详细：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oad8uzidj20u105u0u5.jpg" alt="10.jpg"></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadcvtxwj20u10b8q5t.jpg" alt="11.jpg"></p><p>上面两张图分别是索引表和原数据表的数据类型，可以看到原表和索引表的数据类型并不相同，让人有点费解。</p><p>考虑到phoenix有很多时间种类和别的一些情况，于是提出了几种解决办法：</p><p>1.建表的时候不用timestamp作为时间类型</p><p>2.改二级索引表的字段类型，将decimal改为bigint</p><p>3.将seq_id建表时候，设置为not null，并属于联合主键</p><p>4.从搜索的角度考虑，能不能将搜索的数据类型更改</p><p>第一种方法尝试换了一种时间类型后，建立索引任然更改了为了decimal</p><p>第二种方法更改索引类型经过尝试后发现是不可行的</p><p>第三种方法和第四种方法是可行的，第四种方法更加简单</p><p>第四种方法，本来准备自己在代码中实现对时间的转换，后来发现phoenix SQL内置了timestamp转换</p><p>建立完成索引之后，直接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> CREATETIME,IMEI,LAT,LNG <span class="keyword">from</span> PHOENIX_ALARM_DIS <span class="keyword">where</span> CREATETIME &gt; TO_TIMESTAMP(<span class="string">'2017-08-30 06:21:46.732'</span>);</span><br></pre></td></tr></table></figure><p>这种方式会引导查询走索引</p><p>关于索引，这边还有需要一提的就是，创建索引之后</p><p>例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> INDEX_1 <span class="keyword">ON</span> <span class="keyword">TABLE</span>(A,B) <span class="keyword">INCLUDE</span> (C,D) ASYNC;</span><br></pre></td></tr></table></figure><p>创建索引之后并不能指定B为查询条件，explain会发现依然是在全局扫描，效率很低，这边稍微尝试了一下，在建立一个</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> INDEX_1 <span class="keyword">ON</span> <span class="keyword">TABLE</span>(B,A) <span class="keyword">INCLUDE</span> (C,D) ASYNC;</span><br></pre></td></tr></table></figure><p>建立了两个索引之后，两个索引就占了大概80G的硬盘空间，这个表格用parquet.snappy格式存储之后才120G，真正是用空间换时间。</p><p>测试结果</p><p>数据量：</p><p>11亿5000w 占用磁盘空间：120G</p><p>索引占用空间：40G</p><p>下面分别是50并发 100并发 200并发的测试结果,</p><p>测试中现在Phoenix建表,用Spark写入,随机IMEI和时间段,时间长度为1天,用对应的线程数至少跑30分钟以上.,测试过程中的CPU使用量因为测试集群性能较高,使用最多不超过40%,所以没有记录.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadgrbgpj20k50btgmf.jpg" alt="12.jpg"></p><p>50并发稳定后的查询大多在200ms以下.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadnpw9uj20h60adwf4.jpg" alt="13.jpg"></p><p>100并发的查询时间大多数在400ms以下</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8oadtjlhgj20h40avq3j.jpg" alt="14.jpg"></p><p>200并发的查询大多数在500ms以下.</p><p>针对云车金融数据的测试:</p><p>云车金融测试采用映射表,原数据在HBase中,在Phoenix中建立映射表,把原先设计好的ROWKEY作为主键,建立映射表.</p><h3 id="根据主键范围模糊查询"><a href="#根据主键范围模糊查询" class="headerlink" title="根据主键范围模糊查询"></a>根据主键范围模糊查询</h3><p>采用SQL模糊查询,因为没有并发要求,直接在命令行输入SQL查询.</p><p>针对某条IMEI单天的查询,反应时间在3-5s左右.</p><p>结论：模糊查询都是走全表，无法优化，效率比较低。</p><h3 id="关于映射表"><a href="#关于映射表" class="headerlink" title="关于映射表"></a>关于映射表</h3><p>官网有提及，映射表并不推荐用，数据从hbase写入，通过phoenix读取并不是个好主意，因为：</p><p>1.Phoenix创建的表有很多数据类型，但是从hbase映射表的话只能有一种类型：varchar，否则就会报错。字段只有varchar会给查询带来一定的麻烦。</p><p>2.大表的话创建映射表肯定会超时，需要根据版本修改配置信息，max超时时间拉大，拉到多少比较好，这个时间需要把握。</p><p>映射表如果因为这样或者那样的原因创建失败的话是不能直接删除的，直接删除会导致hbase原表也被删除，可以在SYSTEM.CATALOG中把和映射表有关的信息删除，比如：</p><p>delete from system.catalog where table_name = ‘MYTEST’;</p><p>就能把和关联表有关的信息全部删除了。</p><p>相比于HBase,性能上并没有优势</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Phoenix&quot;&gt;&lt;a href=&quot;#Phoenix&quot; class=&quot;headerlink&quot; title=&quot;Phoenix&quot;&gt;&lt;/a&gt;Phoenix&lt;/h1&gt;&lt;p&gt;Phoenix实践&lt;/p&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Phoenix" scheme="http://yoursite.com/categories/Apache/Phoenix/"/>
    
    
      <category term="Phoenix" scheme="http://yoursite.com/tags/Phoenix/"/>
    
  </entry>
  
  <entry>
    <title>WaterMark原理以及验证</title>
    <link href="http://yoursite.com/2019/11/06/WaterMark%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2019/11/06/WaterMark原理以及验证/</id>
    <published>2019-11-06T03:06:09.000Z</published>
    <updated>2020-04-10T17:12:41.006Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前Flink的Watermark原理老是不明白，并且在CSDN上找的一篇文章，似乎是因为版本的问题，两年前的博客，代码验证下来始终有问题，在网上和人谈论之后，重新用代码验证了，才有点清晰明了，在此记录一下。</p></blockquote><a id="more"></a> <h3 id="WaterMark"><a href="#WaterMark" class="headerlink" title="WaterMark"></a>WaterMark</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8o1sw7244j20xd0g4q9s.jpg" alt="1.png"></p><p>实时计算中，数据时间比较敏感。有<code>eventTime</code>和<code>processTime</code>区分，一般来说<code>eventTime</code>是从原始的消息中提取过来的，<code>processTime</code>是<code>Flink</code>自己提供的，<code>Flink</code>中一个亮点就是可以基于<code>eventTime</code>计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用<code>processTime</code>显然是不合理的。</p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><code>watermark</code>是一种衡量<code>Event Time</code>进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个<code>timestamp.watermark</code>是用于处理乱序事件的，而正确的处理乱序事件，通常用<code>watermark</code>机制结合<code>window</code>来实现。</p><p>流处理从事件产生，到流经<code>source</code>，再到<code>operator</code>，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（<code>out-of-order</code>或者说<code>late element</code>）。</p><p>但是对于<code>late element</code>，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发<code>window</code>去进行计算了。这个特别的机制，就是<code>watermark</code>。</p><h3 id="window划分"><a href="#window划分" class="headerlink" title="window划分"></a>window划分</h3><p><code>window</code>的设定无关数据本身，而是系统定义好了的。<br><code>window</code>是<code>flink</code>中划分数据一个基本单位，<code>window</code>的划分方式是固定的，默认会根据自然时间划分<code>window</code>，并且划分方式是前闭后开。</p><table><thead><tr><th>window划分</th><th>w1</th><th>w2</th><th>w3</th></tr></thead><tbody><tr><td>3s</td><td>[00:00:00~00:00:03)</td><td>[00:00:03~00:00:06)</td><td>[00:00:06~00:00:09)</td></tr><tr><td>5s</td><td>[00:00:00~00:00:05)</td><td>[00:00:05~00:00:10)</td><td>[00:00:10~00:00:15)</td></tr><tr><td>10s</td><td>[00:00:00~00:00:10)</td><td>[00:00:10~00:00:20)</td><td>[00:00:20~00:00:30)</td></tr><tr><td>1min</td><td>[00:00:00~00:01:00)</td><td>[00:01:00~00:02:00)</td><td>[00:02:00~00:03:00)</td></tr></tbody></table><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>如果设置最大允许的乱序时间是10s，滚动时间窗口为5s</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:24"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br><span class="line">//currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:24],currentMaxTimestamp:[2019-03-26 16:25:24],watermark:[2019-03-26 16:25:14]</span><br></pre></td></tr></table></figure><p>触达改记录的时间窗口应该为<code>2019-03-26 16:25:20~2019-03-26 16:25:25</code><br>即当有数据eventTime &gt;= 2019-03-26 16:25:35 时</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:35"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br><span class="line">//currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25]</span><br><span class="line">//(zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><h3 id="提取watermark"><a href="#提取watermark" class="headerlink" title="提取watermark"></a>提取watermark</h3><p>watermark的提取工作在taskManager中完成，意味着这项工作是并行进行的的，而watermark是一个全局的概念，就是一个整个Flink作业之后一个warkermark。</p><h3 id="AssignerWithPeriodicWatermarks"><a href="#AssignerWithPeriodicWatermarks" class="headerlink" title="AssignerWithPeriodicWatermarks"></a>AssignerWithPeriodicWatermarks</h3><p>定时提取watermark，这种方式会定时提取更新wartermark。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//默认200ms</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line">    <span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="AssignerWithPunctuatedWatermarks"><a href="#AssignerWithPunctuatedWatermarks" class="headerlink" title="AssignerWithPunctuatedWatermarks"></a>AssignerWithPunctuatedWatermarks</h3><p>伴随event的到来就提取watermark，就是每一个event到来的时候，就会提取一次Watermark。<br>这样的方式当然设置watermark更为精准，但是当数据量大的时候，频繁的更新wartermark会比较影响性能。<br>通常情况下采用定时提取就足够了。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h3 id="设置数据流时间特征"><a href="#设置数据流时间特征" class="headerlink" title="设置数据流时间特征"></a>设置数据流时间特征</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置为事件时间</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br></pre></td></tr></table></figure><p>默认为<code>TimeCharacteristic.ProcessingTime</code>,默认水位线更新每隔200ms</p><p>入口文件</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment">//便于测试，并行度设置为1</span></span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//env.getConfig.setAutoWatermarkInterval(9000)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//设置为事件时间</span></span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置source 本地socket</span></span><br><span class="line"><span class="keyword">val</span> text: <span class="type">DataStream</span>[<span class="type">String</span>] = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lateText = <span class="keyword">new</span> <span class="type">OutputTag</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Long</span>, <span class="type">Long</span>)](<span class="string">"late_data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> value = text.filter(<span class="keyword">new</span> <span class="type">MyFilterNullOrWhitespace</span>)</span><br><span class="line">.flatMap(<span class="keyword">new</span> <span class="type">MyFlatMap</span>)</span><br><span class="line">.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">MyWaterMark</span>)</span><br><span class="line">.map(x =&gt; (x.name, x.datetime, x.timestamp, <span class="number">1</span>L))</span><br><span class="line">.keyBy(_._1)</span><br><span class="line">.window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">.sideOutputLateData(lateText)</span><br><span class="line"><span class="comment">//.sum(2)</span></span><br><span class="line">.apply(<span class="keyword">new</span> <span class="type">MyWindow</span>)</span><br><span class="line"><span class="comment">//.window(TumblingEventTimeWindows.of(Time.seconds(3)))</span></span><br><span class="line"><span class="comment">//.apply(new MyWindow)</span></span><br><span class="line">value.getSideOutput(lateText).map(x =&gt; &#123;</span><br><span class="line"><span class="string">"延迟数据|name:"</span> + x._1 + <span class="string">"|datetime:"</span> + x._2</span><br><span class="line">&#125;).print()</span><br><span class="line"></span><br><span class="line">value.print()</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"watermark test"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWaterMark</span> <span class="keyword">extends</span> <span class="title">AssignerWithPeriodicWatermarks</span>[<span class="type">EventObj</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> maxOutOfOrderness = <span class="number">10000</span>L <span class="comment">// 3.0 seconds</span></span><br><span class="line">  <span class="keyword">var</span> currentMaxTimestamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 用于生成新的水位线，新的水位线只有大于当前水位线才是有效的</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * 通过生成水印的间隔（每n毫秒）定义 ExecutionConfig.setAutoWatermarkInterval(...)。</span></span><br><span class="line"><span class="comment">    * getCurrentWatermark()每次调用分配器的方法，如果返回的水印非空并且大于先前的水印，则将发出新的水印。</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCurrentWatermark</span></span>: <span class="type">Watermark</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Watermark</span>(<span class="keyword">this</span>.currentMaxTimestamp - <span class="keyword">this</span>.maxOutOfOrderness)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 用于从消息中提取事件时间</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param element                  EventObj</span></span><br><span class="line"><span class="comment">    * @param previousElementTimestamp Long</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">EventObj</span>, previousElementTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line"></span><br><span class="line">    currentMaxTimestamp = <span class="type">Math</span>.max(element.timestamp, currentMaxTimestamp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> id = <span class="type">Thread</span>.currentThread().getId</span><br><span class="line">    println(<span class="string">"currentThreadId:"</span> + id + <span class="string">",key:"</span> + element.name + <span class="string">",eventTime:["</span> + element.datetime + <span class="string">"],currentMaxTimestamp:["</span> + sdf.format(currentMaxTimestamp) + <span class="string">"],watermark:["</span> + sdf.format(getCurrentWatermark().getTimestamp) + <span class="string">"]"</span>)</span><br><span class="line"></span><br><span class="line">    element.timestamp</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h3><ol><li>设置为事件时间</li><li>接受本地socket数据</li><li>抽取timestamp生成watermark，打印(线程id,key,eventTime,currentMaxTimestamp,watermark）</li><li>event time每隔3秒触发一次窗口，打印（key,窗口内元素个数，窗口内最早元素的时间，窗口内最晚元素的时间，窗口自身开始时间，窗口自身结束时间）</li></ol><h3 id="试验"><a href="#试验" class="headerlink" title="试验"></a>试验</h3><h4 id="第一次"><a href="#第一次" class="headerlink" title="第一次"></a>第一次</h4><p>数据</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"datetime"</span>:<span class="string">"2019-03-26 16:25:24"</span>,<span class="attr">"name"</span>:<span class="string">"zhangsan"</span>&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:24],currentMaxTimestamp:[2019-03-26 16:25:24],watermark:[2019-03-26 16:25:14]</span><br></pre></td></tr></table></figure><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr></tbody></table><h4 id="第二次"><a href="#第二次" class="headerlink" title="第二次"></a>第二次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:27&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:27],currentMaxTimestamp:[2019-03-26 16:25:27],watermark:[2019-03-26 16:25:17]</span><br></pre></td></tr></table></figure><p>随着EventTime的升高，Watermark升高。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td></tr></tbody></table><h4 id="第三次"><a href="#第三次" class="headerlink" title="第三次"></a>第三次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:34&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:34],currentMaxTimestamp:[2019-03-26 16:25:34],watermark:[2019-03-26 16:25:24]</span><br></pre></td></tr></table></figure><p>到这里，window仍然没有被触发，此时watermark的时间已经等于了第一条数据的Event Time了。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td><strong>2019-03-26 16:25:24</strong></td></tr></tbody></table><h4 id="第四次"><a href="#第四次" class="headerlink" title="第四次"></a>第四次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:35&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25](zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:35],currentMaxTimestamp:[2019-03-26 16:25:35],watermark:[2019-03-26 16:25:25](zhangsan,1,2019-03-26 16:25:24,2019-03-26 16:25:24,2019-03-26 16:25:20,2019-03-26 16:25:25)</span><br></pre></td></tr></table></figure><p>直接证明了window的设定无关数据本身，而是系统定义好了的。<br>输入的数据中，根据自身的Event Time，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;=Event Time时，就符合了window触发的条件了，最终决定window触发，还是由数据本身的Event Time所属的window中的window_end_time决定。</p><p>当最后一条数据16:25:35到达是，Watermark提升到16:25:25，此时窗口16:25:20~16:25:25中有数据，Window被触发。</p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr></tbody></table><h4 id="第五次"><a href="#第五次" class="headerlink" title="第五次"></a>第五次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:37&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:37],currentMaxTimestamp:[2019-03-26 16:25:37],watermark:[2019-03-26 16:25:27]</span><br></pre></td></tr></table></figure><p>此时，watermark时间虽然已经达到了第二条数据的时间，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。</p><p>第二条数据所在的window时间是：<code>[2019-03-26 16:25:25,2019-03-26 16:25:30)</code></p><p>汇总</p><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:27</td><td></td></tr></tbody></table><h4 id="第六次"><a href="#第六次" class="headerlink" title="第六次"></a>第六次</h4><p>数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;datetime&quot;:&quot;2019-03-26 16:25:40&quot;,&quot;name&quot;:&quot;zhangsan&quot;&#125;</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">currentThreadId:38,key:zhangsan,eventTime:[2019-03-26 16:25:40],currentMaxTimestamp:[2019-03-26 16:25:40],watermark:[2019-03-26 16:25:30](zhangsan,1,2019-03-26 16:25:27,2019-03-26 16:25:27,2019-03-26 16:25:25,2019-03-26 16:25:30)</span><br></pre></td></tr></table></figure><table><thead><tr><th>Key</th><th>EventTime</th><th>currentMaxTimestamp</th><th>Watermark</th><th>WindowStartTime</th><th>WindowEndTime</th></tr></thead><tbody><tr><td>zhangsan</td><td><strong>2019-03-26 16:25:24</strong></td><td>2019-03-26 16:25:24</td><td>2019-03-26 16:25:14</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:27</td><td>2019-03-26 16:25:17</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:34</td><td>2019-03-26 16:25:24</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:35</td><td>2019-03-26 16:25:35</td><td><strong>2019-03-26 16:25:25</strong></td><td><strong>[2019-03-26 16:25:20</strong></td><td><strong>2019-03-26 16:25:25)</strong></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:37</td><td>2019-03-26 16:25:27</td><td></td><td></td></tr><tr><td>zhangsan</td><td>2019-03-26 16:25:40</td><td>2019-03-26 16:25:40</td><td><strong>2019-03-26 16:25:30</strong></td><td><strong>[2019-03-26 16:25:25</strong></td><td><strong>2019-03-26 16:25:30)</strong></td></tr></tbody></table><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>window的触发要符合以下几个条件：</p><ol><li>watermark时间 &gt;= window_end_time</li><li>在[window_start_time,window_end_time)中有数据存在</li></ol><p>同时满足了以上2个条件，window才会触发。<br>watermark是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加.</p><h3 id="多并行度"><a href="#多并行度" class="headerlink" title="多并行度"></a>多并行度</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8o5410mbdj20xd0h3wlo.jpg" alt="2.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h3 id="Flink如何处理乱序？"><a href="#Flink如何处理乱序？" class="headerlink" title="Flink如何处理乱序？"></a>Flink如何处理乱序？</h3><p>watermark+window机制。window中可以对input进行按照Event Time排序，使得完全按照Event Time发生的顺序去处理数据，以达到处理乱序数据的目的。</p><h3 id="Flink何时触发window？"><a href="#Flink何时触发window？" class="headerlink" title="Flink何时触发window？"></a>Flink何时触发window？</h3><p>对于late element太多的数据而言</p><ol><li>Event Time &lt; watermark时间</li></ol><p>对于out-of-order以及正常的数据而言</p><ol><li>watermark时间 &gt;= window_end_time</li><li>在[window_start_time,window_end_time)中有数据存在</li></ol><h3 id="Flink应该如何设置最大乱序时间？"><a href="#Flink应该如何设置最大乱序时间？" class="headerlink" title="Flink应该如何设置最大乱序时间？"></a>Flink应该如何设置最大乱序时间？</h3><p>结合自己的业务以及数据情况去设置。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8o549eyilj210b0i446b.jpg" alt="3.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前Flink的Watermark原理老是不明白，并且在CSDN上找的一篇文章，似乎是因为版本的问题，两年前的博客，代码验证下来始终有问题，在网上和人谈论之后，重新用代码验证了，才有点清晰明了，在此记录一下。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Apache/Flink/"/>
    
    
      <category term="Flink" scheme="http://yoursite.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Scala Note</title>
    <link href="http://yoursite.com/2019/08/16/Scala%20Note/"/>
    <id>http://yoursite.com/2019/08/16/Scala Note/</id>
    <published>2019-08-16T08:06:13.000Z</published>
    <updated>2019-08-16T08:06:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。</p></blockquote><a id="more"></a> <h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h2 id="Scala-介绍"><a href="#Scala-介绍" class="headerlink" title="Scala 介绍"></a>Scala 介绍</h2><p>Scala 是 Scalable Language 的简写，是一门多范式的编程语言</p><p>联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计Scala。</p><p>Funnel是把函数式编程思想和Petri网相结合的一种编程语言。</p><p>Odersky先前的工作是Generic Java和javac（Sun Java编译器）。Java平台的Scala于2003年底/2004年初发布。.NET平台的Scala发布于2004年6月。该语言第二个版本，v2.0，发布于2006年3月。</p><p>截至2009年9月，最新版本是版本2.7.6 。Scala 2.8预计的特性包括重写的Scala类库（Scala collections library）、方法的命名参数和默认参数、包对象（package object），以及Continuation。</p><p>2009年4月，Twitter宣布他们已经把大部分后端程序从Ruby迁移到Scala，其余部分也打算要迁移。此外， Wattzon已经公开宣称，其整个平台都已经是基于Scala基础设施编写的。</p><hr><h2 id="环境部分："><a href="#环境部分：" class="headerlink" title="环境部分："></a>环境部分：</h2><p>安装：和Java一样也要配置环境变量</p><p>配置IDEA：</p><p>先安装插件Scala</p><p>然后创建Maven项目</p><p>因为Maven默认不支持Scala</p><p>创建完毕之后</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izm7uublj20f30ch0t5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izmsgpxxj20pw0lnab5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iznzcv6ej20sw0o13zt.jpg" alt></p><p>Scala文件夹标记为Source</p><h2 id="语法部分"><a href="#语法部分" class="headerlink" title="语法部分"></a>语法部分</h2><h3 id="Hello-Scala"><a href="#Hello-Scala" class="headerlink" title="Hello Scala"></a>Hello Scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"hello Scala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令台执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala -cp C:\Users\61661\Desktop\scala-1.0-SNAPSHOT.jar HelloScala</span><br></pre></td></tr></table></figure><h3 id="声明值和变量"><a href="#声明值和变量" class="headerlink" title="声明值和变量"></a>声明值和变量</h3><p>Scala声明变量有两种方式：<code>val</code> 和 <code>var</code></p><p><code>val</code>定义的值是不可变的，它不是一个常量，是不可变量，或者称之为只读变量。</p><p>Tips：</p><ol><li>Scala的匿名变量（为了运行程序，系统自动添加的变量）分配<code>val</code>。</li><li><code>val</code>定义的变量虽然不能改变其引用的内存地址，但是可以改变其引用的对象的内部的其他属性值。</li><li>为了减少可变性引起的bug，应该尽可能地使用不可变变量。变量类型可以省略，解析器会根据值进行推断。<code>val</code>和<code>var</code>声明变量时都必须初始化。</li></ol><h3 id="常用类型"><a href="#常用类型" class="headerlink" title="常用类型"></a>常用类型</h3><p>8种常用类型</p><table><thead><tr><th>类型</th><th>属性</th></tr></thead><tbody><tr><td>Boolean</td><td><code>true</code> 或者 <code>false</code></td></tr><tr><td>Byte</td><td>8位， 有符号</td></tr><tr><td>Short</td><td>16位， 有符号</td></tr><tr><td>Int</td><td>32位， 有符号</td></tr><tr><td>Long</td><td>64位， 有符号</td></tr><tr><td>Char</td><td>16位， 无符号</td></tr><tr><td>Float</td><td>32位， 单精度浮点数</td></tr><tr><td>Double</td><td>64位， 双精度浮点数</td></tr><tr><td>String</td><td>由Char数组组成</td></tr></tbody></table><p>与Java中的数据类型不同，Scala并不区分基本类型和引用类型，所以这些类型<strong>都是对象</strong></p><p>可以调用相对应的方法，String直接使用的是<code>java.lang.String</code></p><p>由于String实际是一系列Char的不可变的集合，Scala中大部分针对集合的操作，都可以用于String，具体来说，String的这些方法存在于类<code>scala.collection.immutable.StringOps</code>中。</p><p>由于String在需要时能隐式转换为<code>StringOps</code>，因此不需要任何额外的转换，String就可以使用这些方法。</p><p>每一种数据类型都有对应的<code>Rich*</code>类型，如<code>RichInt</code>、<code>RichChar</code>等，为基本类型提供了更多的有用操作。</p><h3 id="常用类型结构图"><a href="#常用类型结构图" class="headerlink" title="常用类型结构图"></a>常用类型结构图</h3><p>Scala中，所有的值都是类对象，而所有的类，包括值类型，都最终继承自一个统一的根类型<code>Any</code>。统一类型，是Scala的又一大特点。更特别的是，Scala中还定义了几个底层类<code>Bottom Class</code>，比如<code>Null</code>和<code>Nothing</code>。</p><ol><li><code>Null</code>是所有引用类型的子类型，而<code>Nothing</code>是所有类型的子类型。<code>Null</code>类只有一个实例对象，<code>null</code>，类似于Java中的<code>null</code>引用。<code>null</code>可以赋值给任意引用类型，但是不能赋值给值类型。</li><li><code>Nothing</code>，可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于<code>Nothing</code>是其他任意类型的子类，他还能跟要求返回值的方法兼容。</li><li><code>Unit</code>类型用来标识过程，也就是没有明确返回值的函数。 由此可见，<code>Unit</code>类似于<code>Java</code>里的<code>void</code>。<code>Unit</code>只有一个实例，()，这个实例也没有实质的意义。</li></ol><p>关系图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3jcuo85e8j20of0gtta1.jpg" alt></p><h3 id="算数操作符重载"><a href="#算数操作符重载" class="headerlink" title="算数操作符重载"></a>算数操作符重载</h3><p><code>+</code> <code>-</code> <code>*</code> <code>/</code> <code>%</code>可以完成和Java中相同的工作，但是有一点区别，他们都是方法。你几乎可以用任何符号来为方法命名。</p><p><code>1 + 2</code> 等同于 <code>1.+(2)</code></p><p>Tips: Scala中没有++、–操作符，需要通过+=、-=来实现同样的效果。</p><h3 id="调用函数与方法"><a href="#调用函数与方法" class="headerlink" title="调用函数与方法"></a>调用函数与方法</h3><p>在Scala中，一般情况下我们不会刻意的去区分<code>函数</code>与<code>方法</code>的区别，但是他们确实是不同的东西。</p><p>后面我们再详细探讨。首先我们要学会使用Scala来调用函数与方法。</p><h4 id="1-调用函数，求方根"><a href="#1-调用函数，求方根" class="headerlink" title="1.调用函数，求方根"></a>1.调用函数，求方根</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.math</span><br><span class="line">sqrt(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h4 id="2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"><a href="#2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）" class="headerlink" title="2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"></a>2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">BigInt</span>.probablePrime(<span class="number">16</span>, scala.util.<span class="type">Random</span>)</span><br></pre></td></tr></table></figure><h4 id="3-调用方法，非静态方法，使用对象调用"><a href="#3-调用方法，非静态方法，使用对象调用" class="headerlink" title="3.调用方法，非静态方法，使用对象调用"></a>3.调用方法，非静态方法，使用对象调用</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"HelloWorld"</span>.distinct</span><br></pre></td></tr></table></figure><h4 id="4-apply与update方法"><a href="#4-apply与update方法" class="headerlink" title="4.apply与update方法"></a>4.apply与update方法</h4><p>apply方法是调用时可以省略方法名的方法。用于构造和获取元素：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Hello"</span>(<span class="number">4</span>)  等同于  <span class="string">"Hello"</span>.apply(<span class="number">4</span>)</span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) 等同于 <span class="type">Array</span>.apply(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">如：</span><br><span class="line">println(<span class="string">"Hello"</span>(<span class="number">4</span>))</span><br><span class="line">println(<span class="string">"Hello"</span>.apply(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>在<code>StringOps</code>中你会发现一个 <code>def apply(n: Int): Char</code>方法定义。<code>update</code>方法也是调用时可以省略方法名的方法，用于元素的更新：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr(<span class="number">4</span>) = <span class="number">5</span>  等同于  arr.update(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">如：</span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">5</span>)</span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">2</span></span><br><span class="line">arr1.update(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(arr1.mkString(<span class="string">","</span>))</span><br></pre></td></tr></table></figure><h4 id="Option类型"><a href="#Option类型" class="headerlink" title="Option类型"></a>Option类型</h4><p>Scala为单个值提供了对象的包装器，表示为那种可能存在也可能不存在的值。他只有两个有效的子类对象，一个是Some，表示某个值，另外一个是None，表示为空，通过Option的使用，避免了使用null、空字符串等方式来表示缺少某个值的做法。</p><p>如：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>(<span class="string">"Alice"</span> -&gt; <span class="number">20</span>, <span class="string">"Bob"</span> -&gt; <span class="number">30</span>)</span><br><span class="line">println(map1.get(<span class="string">"Alice"</span>))</span><br><span class="line">println(map1.get(<span class="string">"Jone"</span>))</span><br></pre></td></tr></table></figure><h3 id="控制结构和函数"><a href="#控制结构和函数" class="headerlink" title="控制结构和函数"></a>控制结构和函数</h3><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>Scala中没有三目运算符，因为根本不需要。Scala中if else表达式是有返回值的，如果if或者else返回的类型不一样，就返回Any类型（所有类型的公共超类型）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> a3 = <span class="number">10</span></span><br><span class="line">    <span class="keyword">val</span> a4 =</span><br><span class="line">      <span class="comment">//返回类型一样</span></span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">        <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="string">"a3小于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> a5 = </span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">          <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    println(a4)</span><br><span class="line">    <span class="comment">//a3小于20</span></span><br><span class="line">    println(a5)</span><br><span class="line">    <span class="comment">//()</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果缺少一个判断，什么都没有返回，但是Scala认为任何表达式都会有值，对于空值，使用Unit类，写做()，叫做无用占位符，相当于Java中的void。</p><p>Tips: 行尾的位置不需要分号，只要能够从上下文判断出语句的终止即可。但是如果在单行中写多个语句，则需要分号分割。在Scala中，{}块包含一系列表达式，其结果也是一个表达式。块中最后一个表达式的值就是块的值。</p><h4 id="while-表达式"><a href="#while-表达式" class="headerlink" title="while 表达式"></a>while 表达式</h4><p>Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> while1 = <span class="keyword">while</span>(n &lt;= <span class="number">10</span>)&#123;</span><br><span class="line">      n += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(while1) <span class="comment">//()</span></span><br><span class="line">    println(n) <span class="comment">//11</span></span><br><span class="line">    <span class="comment">//Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>while循环的中断</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span></span><br><span class="line"><span class="keyword">val</span> loop = <span class="keyword">new</span> <span class="type">Breaks</span></span><br><span class="line">loop.breakable&#123;</span><br><span class="line">  <span class="keyword">while</span>(n &lt;= <span class="number">20</span>)&#123;</span><br><span class="line">    n += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">18</span>)&#123;</span><br><span class="line">      loop.<span class="keyword">break</span>()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(n)</span><br></pre></td></tr></table></figure><p>Tips: Scala并没有提供break和continue语句来退出循环，如果需要break，可以通过几种方法来做1、使用Boolean型的控制变量 2、使用嵌套函数，从函数中return 3、使用Breaks对象的break方法。</p><h4 id="for表达式"><a href="#for表达式" class="headerlink" title="for表达式"></a>for表达式</h4><p>Scala也为for循环这一常见的控制结构提供了非常多的特性，这些for循环特性被称为for推导式(for comprehension)或for表达式(for expression).</p><h5 id="for示例1-to左右两边为前闭后闭的访问"><a href="#for示例1-to左右两边为前闭后闭的访问" class="headerlink" title="for示例1: to左右两边为前闭后闭的访问"></a>for示例1: to左右两边为前闭后闭的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span>)&#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例2：until左右两边为前闭后开的访问"><a href="#for示例2：until左右两边为前闭后开的访问" class="headerlink" title="for示例2：until左右两边为前闭后开的访问"></a>for示例2：until左右两边为前闭后开的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">3</span>; j &lt;- <span class="number">1</span> until <span class="number">3</span>) &#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"><a href="#for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue" class="headerlink" title="for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"></a>for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != <span class="number">2</span>) &#123;</span><br><span class="line">  print(i + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例4：引入变量"><a href="#for示例4：引入变量" class="headerlink" title="for示例4：引入变量"></a>for示例4：引入变量</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j = <span class="number">4</span> - i) &#123;</span><br><span class="line">  print(j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"><a href="#for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字" class="headerlink" title="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"></a>for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> for5 = <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i</span><br><span class="line">println(for5)</span><br></pre></td></tr></table></figure><h5 id="for示例6：使用花括号-代替小括号"><a href="#for示例6：使用花括号-代替小括号" class="headerlink" title="for示例6：使用花括号{}代替小括号()"></a>for示例6：使用花括号{}代替小括号()</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>&#123;</span><br><span class="line">  i &lt;- <span class="number">1</span> to <span class="number">3</span></span><br><span class="line">  j = <span class="number">4</span> - i&#125;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>Tips</strong>: {}和()对于for表达式来说都可以。for 推导式有一个不成文的约定：当for<br>推导式仅包含单一表达式时使用原括号，当其包含多个表达式时使用大括号。值得注意的是，使用原括号时，早前版本的Scala 要求表达式之间必须使用分号。</p><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><p>scala定义函数的标准格式为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">函数名</span></span>(参数名<span class="number">1</span>: 参数类型<span class="number">1</span>, 参数名<span class="number">2</span>: 参数类型<span class="number">2</span>) : 返回类型 = &#123;函数体&#125;</span><br></pre></td></tr></table></figure><p>函数示例1：返回Unit类型的函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">shout1</span><span class="params">(content: String)</span> : Unit </span>= &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例2：返回Unit类型的函数，但是没有显式指定返回类型。（当然也可以返回非Unit类型的值）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout2</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例3:返回值类型有多种可能，此时也可以省略Unit</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout3</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span>(content.length &gt;= <span class="number">3</span>)</span><br><span class="line">    content + <span class="string">"喵喵喵~"</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例4：带有默认值参数的函数，调用该函数时，可以只给无默认值的参数传递值，也可以都传递，新值会覆盖默认值；传递参数时如果不按照定义顺序，则可以通过参数名来指定。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout4</span></span>(content: <span class="type">String</span>, leg: <span class="type">Int</span> = <span class="number">4</span>) = &#123;</span><br><span class="line">  println(content + <span class="string">","</span> + leg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例5：变长参数（不确定个数参数，类似Java的…）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(args: <span class="type">Int</span>*) = &#123;</span><br><span class="line">  <span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span>(arg &lt;- args)</span><br><span class="line">    result += arg</span><br><span class="line">  result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>递归函数：递归函数在使用时必须有明确的返回值类型</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(n: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span>(n &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    n * factorial(n - <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Tips:</strong></p><p>1、Scala可以通过=右边的表达式  推断出函数的返回类型。如果函数体需要多个表达式，可以用代码块{}。</p><p>2、可以把return 当做  函数版本的break语句。</p><p>3、递归函数一定要指定返回类型。</p><p>4、变长参数通过* 来指定，所有参数会转化为一个seq序列。</p><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>我们将函数的返回类型为Unit的函数称之为过程。</p><h5 id="定义过程示例1："><a href="#定义过程示例1：" class="headerlink" title="定义过程示例1："></a>定义过程示例1：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) : <span class="type">Unit</span> = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例3：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>尖叫提示：这只是一个逻辑上的细分，如果因为该概念导致了理解上的混淆，可以暂时直接跳过过程这样的描述。毕竟过程，在某种意义上也是函数。</p><h4 id="懒值"><a href="#懒值" class="headerlink" title="懒值"></a>懒值</h4><p>当val被声明为lazy时，他的初始化将被推迟，直到我们首次对此取值，适用于初始化开销较大的场景。</p><p>lazy示例：通过lazy关键字的使用与否，来观察执行过程</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Lazy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    println(<span class="string">"init方法执行"</span>)</span><br><span class="line">    <span class="string">"嘿嘿嘿，我来了~"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> msg = init()</span><br><span class="line">    println(<span class="string">"lazy方法没有执行"</span>)</span><br><span class="line">    println(msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><p>当碰到异常情况时，方法抛出一个异常，终止方法本身的执行，异常传递到其调用者，调用者可以处理该异常，也可以升级到它的调用者。运行系统会一直这样升级异常，直到有调用者能处理它。 如果一直没有处理，则终止整个程序。</p><p>Scala的异常的工作机制和Java一样，但是Scala没有“checked”异常，你不需要声明说函数或者方法可能会抛出某种异常。受检异常在编译器被检查，java必须声明方法所会抛出的异常类型。</p><p><strong>抛出异常</strong>：用throw关键字，抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing，因为Nothing是所有类型的子类型，所以throw表达式可以用在需要类型的地方。</p><p><strong>捕捉异常：</strong>在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case字句。</p><p>异常捕捉的机制与其他语言中一样，如果有异常发生，catch字句是按次序捕捉的。因此，在catch字句中，越具体的异常越要靠前，越普遍的异常越靠后。 如果抛出的异常不在catch字句中，该异常则无法处理，会被升级到调用者处。</p><p>finally字句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作。</p><h5 id="异常示例："><a href="#异常示例：" class="headerlink" title="异常示例："></a>异常示例：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExceptionSyllabus</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">divider</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Float</span>= &#123;</span><br><span class="line">    <span class="keyword">if</span>(y == <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"0作为了除数"</span>)</span><br><span class="line">    <span class="keyword">else</span> x / y</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          println(divider(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; println(<span class="string">"捕获了异常："</span> + ex)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="数据结构特点"><a href="#数据结构特点" class="headerlink" title="数据结构特点"></a>数据结构特点</h4><p>Scala同时支持可变集合和不可变集合，不可变集合从不可变，可以安全的并发访问。</p><p>两个主要的包：</p><p>不可变集合：scala.collection.immutable</p><p>可变集合：  scala.collection.mutable</p><p>Scala优先采用不可变集合，对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本。</p><p>不可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hebn4l8j20qa0j63zo.jpg" alt></p><p>可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hf54tkwj20xc0l0q50.jpg" alt></p><h4 id="数组Array"><a href="#数组Array" class="headerlink" title="数组Array"></a>数组Array</h4><h5 id="1-定长数组"><a href="#1-定长数组" class="headerlink" title="1.定长数组"></a>1.定长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">7</span></span><br><span class="line">或：</span><br><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h5 id="2-变长数组"><a href="#2-变长数组" class="headerlink" title="2.变长数组"></a>2.变长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr2 = <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line"><span class="comment">//追加值</span></span><br><span class="line">arr2.append(<span class="number">7</span>)</span><br><span class="line"><span class="comment">//重新赋值</span></span><br><span class="line">arr2(<span class="number">0</span>) = <span class="number">7</span></span><br></pre></td></tr></table></figure><h5 id="3-定长数据与变长数据的装换"><a href="#3-定长数据与变长数据的装换" class="headerlink" title="3.定长数据与变长数据的装换"></a>3.定长数据与变长数据的装换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1.toBuffer</span><br><span class="line">arr2.toArray</span><br></pre></td></tr></table></figure><h5 id="4-多维数据"><a href="#4-多维数据" class="headerlink" title="4.多维数据"></a>4.多维数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr3 = <span class="type">Array</span>.ofDim[<span class="type">Double</span>](<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr3(<span class="number">1</span>)(<span class="number">1</span>) = <span class="number">11.11</span></span><br></pre></td></tr></table></figure><h5 id="5-与Java数组的互转"><a href="#5-与Java数组的互转" class="headerlink" title="5.与Java数组的互转"></a>5.与Java数组的互转</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala =&gt; Java</span></span><br><span class="line"><span class="keyword">val</span> arr4 = <span class="type">ArrayBuffer</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)</span><br><span class="line"><span class="comment">//Scala to Java</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.bufferAsJavaList</span><br><span class="line"><span class="keyword">val</span> javaArr = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(arr4)</span><br><span class="line">println(javaArr.command())</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java =&gt; scala</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.asScalaBuffer</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Buffer</span></span><br><span class="line"><span class="keyword">val</span> scalaArr: <span class="type">Buffer</span>[<span class="type">String</span>] = javaArr.command()</span><br><span class="line">println(scalaArr)</span><br></pre></td></tr></table></figure><p>6.数据的遍历</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(x &lt;- arr1) &#123;</span><br><span class="line">  println(x)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-元组-Tuple"><a href="#5-3-元组-Tuple" class="headerlink" title="5.3 元组 Tuple"></a>5.3 元组 Tuple</h4><p>元组可以理解为一个容器，可以存放各种相同或者不同类型的数据。</p><h5 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tuple1 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">"heiheihei"</span>)</span><br><span class="line">println(tuple1)</span><br></pre></td></tr></table></figure><h5 id="访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0"><a href="#访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0" class="headerlink" title="访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)"></a>访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val value1 = tuple1._4</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="元组的遍历"><a href="#元组的遍历" class="headerlink" title="元组的遍历"></a>元组的遍历</h5><p><strong>方式1</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (elem &lt;- tuple1.productIterator) &#123;</span><br><span class="line">  print(elem)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>方式2</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tuple1.productIterator.foreach(i =&gt; println(i))</span><br><span class="line">tuple1.productIterator.foreach(print(_))</span><br></pre></td></tr></table></figure><h4 id="列表List"><a href="#列表List" class="headerlink" title="列表List"></a>列表List</h4><p>如果List列表为空，则使用Nil来表示</p><h5 id="创建List"><a href="#创建List" class="headerlink" title="创建List"></a>创建List</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(list1)</span><br></pre></td></tr></table></figure><h5 id="访问List元素"><a href="#访问List元素" class="headerlink" title="访问List元素"></a>访问List元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = list1(<span class="number">1</span>)</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="List元素的追加"><a href="#List元素的追加" class="headerlink" title="List元素的追加"></a>List元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = list1 :+ <span class="number">99</span></span><br><span class="line">println(list2)</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="number">100</span> +: list1</span><br><span class="line">println(list3)</span><br></pre></td></tr></table></figure><p>List的创建与追加，符号“::”，注意观察去掉Nil和不去掉Nil的区别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list4 = <span class="number">1</span> :: <span class="number">2</span> :: <span class="number">3</span> :: list1 :: <span class="type">Nil</span></span><br><span class="line">println(list4)</span><br></pre></td></tr></table></figure><h4 id="队列Queue"><a href="#队列Queue" class="headerlink" title="队列Queue"></a>队列Queue</h4><p>队列数据存取符合先进先出的策略</p><h5 id="队列的创建"><a href="#队列的创建" class="headerlink" title="队列的创建"></a>队列的创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">val</span> q1 = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">Int</span>]</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="队列元素的追加"><a href="#队列元素的追加" class="headerlink" title="队列元素的追加"></a>队列元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1+=<span class="number">1</span></span><br><span class="line">print;n(q1)</span><br></pre></td></tr></table></figure><h5 id="队列的追加"><a href="#队列的追加" class="headerlink" title="队列的追加"></a>队列的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1 ++= <span class="type">List</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="按照进入队列的顺序删除元素"><a href="#按照进入队列的顺序删除元素" class="headerlink" title="按照进入队列的顺序删除元素"></a>按照进入队列的顺序删除元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1.dequeue()</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="塞入数据"><a href="#塞入数据" class="headerlink" title="塞入数据"></a>塞入数据</h5><h5 id="返回队列的第一个元素"><a href="#返回队列的第一个元素" class="headerlink" title="返回队列的第一个元素"></a>返回队列的第一个元素</h5><h5 id="返回队列的最后一个元素"><a href="#返回队列的最后一个元素" class="headerlink" title="返回队列的最后一个元素"></a>返回队列的最后一个元素</h5><h5 id="返回队列最后一个元素"><a href="#返回队列最后一个元素" class="headerlink" title="返回队列最后一个元素"></a>返回队列最后一个元素</h5><h5 id="返回除了第一个以外的元素"><a href="#返回除了第一个以外的元素" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h5 id="返回除了第一个以外的元素-1"><a href="#返回除了第一个以外的元素-1" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><h5 id="构造不可变映射"><a href="#构造不可变映射" class="headerlink" title="构造不可变映射"></a>构造不可变映射</h5><h5 id="构造可变映射"><a href="#构造可变映射" class="headerlink" title="构造可变映射"></a>构造可变映射</h5><h5 id="空的映射"><a href="#空的映射" class="headerlink" title="空的映射"></a>空的映射</h5><h5 id="对偶元组"><a href="#对偶元组" class="headerlink" title="对偶元组"></a>对偶元组</h5><h5 id="取值"><a href="#取值" class="headerlink" title="取值"></a>取值</h5><h5 id="更新值"><a href="#更新值" class="headerlink" title="更新值"></a>更新值</h5><h5 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h5><h4 id="集-Set"><a href="#集-Set" class="headerlink" title="集 Set"></a>集 Set</h4><h5 id="1-Set不可变集合的创建"><a href="#1-Set不可变集合的创建" class="headerlink" title="1.Set不可变集合的创建"></a>1.Set不可变集合的创建</h5><h5 id="2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"><a href="#2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合" class="headerlink" title="2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"></a>2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合</h5><h5 id="3-可变集合的元素添加"><a href="#3-可变集合的元素添加" class="headerlink" title="3.可变集合的元素添加"></a>3.可变集合的元素添加</h5><h5 id="4-可变集合的元素删除"><a href="#4-可变集合的元素删除" class="headerlink" title="4.可变集合的元素删除"></a>4.可变集合的元素删除</h5><h5 id="5-遍历"><a href="#5-遍历" class="headerlink" title="5.遍历"></a>5.遍历</h5><h5 id="6-Set更多常用操作"><a href="#6-Set更多常用操作" class="headerlink" title="6.Set更多常用操作"></a>6.Set更多常用操作</h5><h4 id="集合元素与函数的映射"><a href="#集合元素与函数的映射" class="headerlink" title="集合元素与函数的映射"></a>集合元素与函数的映射</h4><h5 id="map"><a href="#map" class="headerlink" title="map"></a>map</h5><h5 id="flatmap"><a href="#flatmap" class="headerlink" title="flatmap"></a>flatmap</h5><h4 id="化简、折叠、扫描"><a href="#化简、折叠、扫描" class="headerlink" title="化简、折叠、扫描"></a>化简、折叠、扫描</h4><h5 id="折叠，化简：将二次元函数引用于集合中的函数。"><a href="#折叠，化简：将二次元函数引用于集合中的函数。" class="headerlink" title="折叠，化简：将二次元函数引用于集合中的函数。"></a>折叠，化简：将二次元函数引用于集合中的函数。</h5><h5 id="折叠，化简：fold"><a href="#折叠，化简：fold" class="headerlink" title="折叠，化简：fold"></a>折叠，化简：fold</h5>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Language" scheme="http://yoursite.com/categories/Language/"/>
    
      <category term="Scala" scheme="http://yoursite.com/categories/Language/Scala/"/>
    
    
      <category term="Scala" scheme="http://yoursite.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Flink集群搭建</title>
    <link href="http://yoursite.com/2019/08/09/Flink%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/08/09/Flink集群搭建/</id>
    <published>2019-08-09T01:17:48.000Z</published>
    <updated>2020-04-10T17:07:43.553Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink集群搭建"><a href="#Flink集群搭建" class="headerlink" title="Flink集群搭建"></a>Flink集群搭建</h1><a id="more"></a> <h3 id="环境"><a href="#环境" class="headerlink" title="环境:"></a>环境:</h3><p>hadoop 2.6.0</p><p>centos 7</p><p>java 1.8.144</p><p>scala 2.11.8</p><p>机器: datanode 126 127 128(ssh)</p><h3 id="版本选择"><a href="#版本选择" class="headerlink" title="版本选择"></a>版本选择</h3><p>参考:<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/yarn_setup.html#flink-yarn-session" target="_blank" rel="noopener">Flink 官方文档</a></p><p>Flink 1.8</p><p>选择原因: Flink 还处在频繁更新的状态,较新的版本,特性和老版本特别较大,</p><p>Flink 1.8 有如下主要改变:</p><p>1.将会增量清除旧的State<br>2.编程方面TableEnvironment弃用<br>3.Flink1.8将不发布带有Hadoop的二进制安装包</p><p>其中编程方面的改变比较重要,会延续到以后的版本,综合考虑,不使用最新的1.9,使用1.8是较为稳妥的选择.</p><h3 id="Flink安装模式"><a href="#Flink安装模式" class="headerlink" title="Flink安装模式"></a>Flink安装模式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">local模式：适用于本地开发和测试环境，占用的资源较少，部署简单 ，只需要部署JDK和flink即可达到功能开发和测试的目的。只需要一台主机即可。</span><br><span class="line"> </span><br><span class="line">standalone cluster：可以在测试环境功能验证完毕到版本发布的时候使用，进行性能验证。搭建需要ssh</span><br><span class="line">jdk和flink。至少需要3台主机，一个master两个worker节点。</span><br><span class="line"> </span><br><span class="line">YARN：flink使用YARN进行调度。</span><br><span class="line"> </span><br><span class="line">Hadoop Integration：和hadoop生态进行整合，可以借用HDFS、YARN的功能，是用于整个大数据环境都用Hadoop全家桶的环境。</span><br><span class="line"> </span><br><span class="line">Docker： 在开发测试使用，docker方式很容易搭建。推荐的方式。</span><br><span class="line"> </span><br><span class="line">kubernetes：由于FLink使用的无状态模式，只需要kubernetes提供计算资源即可。会是Flink以后运行的主流方式，可以起到节约硬件资源和便于管理的效果。</span><br><span class="line"> </span><br><span class="line">HA模式：</span><br><span class="line">现在主流的方式有standalone cluster HA 和YARN cluster HA方式，适用于在生产上部署。</span><br><span class="line">standalone cluster HA：</span><br><span class="line">需要JDK、ssh、zookeeper HA、flink构建，至少需要三个物理机。</span><br><span class="line"> </span><br><span class="line">YARN cluster HA：</span><br><span class="line">需要JDK、ssh、zookeeper HA、Hadoop HA、flink，需要更多的资源。</span><br><span class="line"> </span><br><span class="line">若flink运行于k8s则可以借助于kubernetes的集群提供高可用，充分的利用资源。</span><br><span class="line"> </span><br><span class="line">当前大部分公司还是将Flink运行在物理机上。</span><br></pre></td></tr></table></figure><h3 id="多种安装模式尝试"><a href="#多种安装模式尝试" class="headerlink" title="多种安装模式尝试:"></a>多种安装模式尝试:</h3><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><p><a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">Flink官方链接</a></p><p>官方链接中可以选择使用scala 2.11 还是 2.12版本.这边选择2.11版本即可.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5ittrsyi7j20an03v3yh.jpg" alt></p><p>下载完成之后, <code>tar zvxf flink-1.8.1-bin-scala_2.11.tgz -C /usr</code>解压</p><h4 id="Yarn配置模式的选择"><a href="#Yarn配置模式的选择" class="headerlink" title="Yarn配置模式的选择:"></a>Yarn配置模式的选择:</h4><p>如果选择Flink on Yarn的话,就比较简单,因为测试集群上已经存在了CDH,所以直接尝试<code>Flink on yarn.</code></p><p>把 Flink 运行在 YARN 上有两种方式，第一种方式是建立一个长期运行的 Flink YARN Session，然后向这个 Session 提交 Flink Job，多个任务同时运行时会共享资源。第二种方式是为单个任务启动一个 Flink 集群，这个任务会独占 Flink 集群的所有资源，任务结束即代表集群被回收。</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>因为之前安装使用的CDH 所以系统中没有hadoop的环境变量,这边需要配置hadoop的环境变量才可以继续使用.</p><p>配置如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> hadoop</span><br><span class="line">export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> hadoop conf</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br></pre></td></tr></table></figure><p>配置完成之后  source 立即生效.</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>因为之前说过1.8版本有个新特性就是官方不再发布关联hadoop的二进制包,所以hadoop的依赖我们自己下载.</p><p><a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">下载地址</a></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5ivnayodoj20ob0dyaaz.jpg" alt></p><p>我们的hadoop是2.6的,下载这个2.6版本的就好.</p><p>下载完成后有添加到lib</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink-shaded-hadoop-2-uber-2.6.5-7.0.jar</span><br></pre></td></tr></table></figure><p>使用yarn session.sh启动yarn的Session模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">启动Session参数说明:</span><br><span class="line">-n(--container)taskmanager的数量 </span><br><span class="line">-s(--slots)用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为16～10</span><br><span class="line">-jmjobmanager的内存（单位MB)3072</span><br><span class="line">-tm每个taskmanager的内存（单位MB)根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</span><br><span class="line">-nmyarn 的appName(现在yarn的ui上的名字)｜ </span><br><span class="line">-d后台执行</span><br><span class="line">启动任务参数:</span><br><span class="line">-j运行flink 应用的jar所在的目录</span><br><span class="line">-a运行flink 应用的主方法的参数</span><br><span class="line">-p运行flink应用的并行度</span><br><span class="line">-c运行flink应用的主类, 可以通过在打包设置主类</span><br><span class="line">-nmflink 应用名字，在flink-ui 上面展示</span><br><span class="line">-d后台执行</span><br><span class="line">--fromsavepointflink 应用启动的状态恢复点</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode127 flink-1.8.1]# bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">2019-07-31 11:46:46,986 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:46:46,987 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class="line">2019-07-31 11:46:46,988 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-07-31 11:46:47,489 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to root (auth:SIMPLE)</span><br><span class="line">2019-07-31 11:46:47,548 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at datanode127/172.16.0.127:8032</span><br><span class="line">2019-07-31 11:46:47,766 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class="line">2019-07-31 11:46:48,018 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-07-31 11:46:48,033 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory ('/data2/flink/flink-1.8.1/conf') contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-07-31 11:46:49,540 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1564366526843_1818</span><br><span class="line">2019-07-31 11:46:49,560 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1564366526843_1818</span><br><span class="line">2019-07-31 11:46:49,560 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-07-31 11:46:49,561 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-07-31 11:46:52,573 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-07-31 11:46:52,928 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on datanode127:37898 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://datanode127:37898</span><br><span class="line">^C2019-07-31 11:54:04,390 INFO  org.apache.flink.runtime.rest.RestClient                      - Shutting down rest endpoint.</span><br><span class="line">2019-07-31 11:54:04,392 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest endpoint shutdown complete.</span><br><span class="line">2019-07-31 11:54:04,393 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Deleted Yarn properties file at /tmp/.yarn-properties-root</span><br><span class="line">[root@datanode127 flink-1.8.1]# ^C</span><br><span class="line">[root@datanode127 flink-1.8.1]# bin/yarn-session.sh run -n 2 -tm 2048 -s 4</span><br><span class="line">2019-07-31 11:58:44,326 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 1</span><br><span class="line">2019-07-31 11:58:44,327 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-07-31 11:58:44,779 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to root (auth:SIMPLE)</span><br><span class="line">2019-07-31 11:58:44,837 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at datanode127/172.16.0.127:8032</span><br><span class="line">2019-07-31 11:58:45,046 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=1&#125;</span><br><span class="line">2019-07-31 11:58:45,286 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-07-31 11:58:45,300 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory ('/data2/flink/flink-1.8.1/conf') contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-07-31 11:58:46,808 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1564366526843_1821</span><br><span class="line">2019-07-31 11:58:46,828 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1564366526843_1821</span><br><span class="line">2019-07-31 11:58:46,828 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-07-31 11:58:46,829 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-07-31 11:58:50,594 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-07-31 11:58:50,961 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on datanode127:35550 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://datanode127:35550</span><br></pre></td></tr></table></figure><p>开启之后 最后会给一个WebUI的地址,经过多次发现on yarn模式下,这个端口是随机分配的.</p><h3 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h3><p>使用IDEA创建新的Maven项目,写一个简单的wordcount</p><p>在Flink Web UI中创建一个简单的任务:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j27shu1lj210s0n7wg8.jpg" alt></p><p>选中Submit new Job,把打好的Jar包上传进去,可以在这个界面选择需要传入的args</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j29biz46j20sx0b0aac.jpg" alt></p><p>传完参数之后就可以运行了,任务的日志和记录也都可以在上面找到.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j2aapan1j20sq0393yj.jpg" alt></p><p>点进去就可以看到任务的记录.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5j2bhs8m3j210s0n7wh5.jpg" alt></p><p>这就是Flink on yarn的部署方式之Flink YARN Session.</p><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>目前大多数企业使用应该还是使用Standalone模式的,从官方发行版本不再包含yarn的jar包就可以看出,Flink团队应该也不是特别喜欢yarn对资源的调度,在Standalone模式里面,我们可以自己配置Flink的资源使用.</p><p>安装 解压都和上面一样,主要区别在与Standalone模式要修改flink配置文件.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: localhost1   --jobManager 的IP地址</span><br><span class="line">jobmanager.rpc.port: 6123   --jobManager 的端口，默认为6123</span><br><span class="line">jobmanager.heap.mb --jobManager 的JVM heap大小 </span><br><span class="line">taskmanager.heap.mb  --taskManager的jvm heap大小设置</span><br><span class="line">taskmanager.numberOfTaskSlots  --taskManager中taskSlots个数，最好设置成work节点的CPU个数相等</span><br><span class="line">parallelism.default  --并行计算数</span><br><span class="line">fs.default-scheme --文件系统来源</span><br><span class="line">fs.hdfs.hadoopconf:  --hdfs置文件路径</span><br><span class="line">jobmanager.web.port    -- jobmanager的页面监控端口</span><br></pre></td></tr></table></figure><p>配置完成后,就可以直接启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./start-cluster.sh</span><br></pre></td></tr></table></figure><p>停止脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./stop-cluster.sh</span><br></pre></td></tr></table></figure><p>直接浏览器访问页面＋管理web 端口</p><p>localhost:8081</p><p>这里涉及的配置文件比较多,上面只标出了几个比较重要的,需要使用的时候(如配置HA)还是要看最新的<a href="https://ci.apache.org/projects/flink/flink-docs-stable/release-notes/flink-1.8.html" target="_blank" rel="noopener">官方文档</a>.</p><hr><p>下图展示了 Flink 中目前支持的主要几种流的类型，以及它们之间的转换关系。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5t5ry2zvmj20nb0d70u3.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Flink集群搭建&quot;&gt;&lt;a href=&quot;#Flink集群搭建&quot; class=&quot;headerlink&quot; title=&quot;Flink集群搭建&quot;&gt;&lt;/a&gt;Flink集群搭建&lt;/h1&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Flink" scheme="http://yoursite.com/categories/Apache/Flink/"/>
    
    
  </entry>
  
  <entry>
    <title>Hexo Conf</title>
    <link href="http://yoursite.com/2019/08/02/%E6%96%B0%E7%94%B5%E8%84%91%E9%85%8D%E7%BD%AEHexo/"/>
    <id>http://yoursite.com/2019/08/02/新电脑配置Hexo/</id>
    <published>2019-08-02T03:58:57.000Z</published>
    <updated>2020-04-22T13:24:02.389Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>跨平台win/mac更新博文</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g5l74xxdnaj20o308umxm.jpg" alt></p><a id="more"></a> <p>After My download from OneDrive.</p><ol><li><p>Node.js</p></li><li><p>Git</p></li><li><p>Hexo</p></li><li><ol><li>Under the installation directory</li><li>Git Bash</li><li>Npm install hexo -cli -g</li><li>SSH Key to Github</li><li>OK</li></ol></li></ol><p>SSH Key to Github</p><p>git config –global user.name “FlyMeToTheMars”<br> git config –global user.email <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>ssh-keygen -t rsa -C  <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>(file in C:\Users\Administrator.ssh) </p><p>Then upload to github.com with website</p><p>Test:</p><p>ssh <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a></p><hr><p>更新于2020/4/11</p><p>macOS亲测 相同方法可用，自此Hexo在全平台OneDrive上同步，全平台可以实时更新日志</p><hr><p>第三次更新与2020/4/22</p><p>配置多个git源的办法</p><p>ssh-keygen -t rsa -f ~/.ssh/id_rsa_gitlab -C <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>ssh-keygen -t rsa -f ~/.ssh/id_rsa_github -C <a href="mailto:flyhobo@live.com" target="_blank" rel="noopener">flyhobo@live.com</a></p><p>给不同的公钥配置起不同的标志性id</p><p>在网站上配置完成之后</p><p>在 ~/.ssh目录下新建config文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Host github.com</span><br><span class="line">    HostName ssh.github.com</span><br><span class="line">    User flyhobo@live.com</span><br><span class="line">    PreferredAuthentications publickey</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa_github</span><br><span class="line"></span><br><span class="line">Host git.lug.ustc.edu.cn</span><br><span class="line">    HostName git.lug.ustc.edu.cn</span><br><span class="line">    User flyhobo@live.com</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa_gitlab</span><br></pre></td></tr></table></figure><p>锁进注意</p><p>内容如上就ok了</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;跨平台win/mac更新博文&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2ly1g5l74xxdnaj20o308umxm.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yoursite.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yoursite.com/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Compression Code</title>
    <link href="http://yoursite.com/2019/07/16/Snappy/"/>
    <id>http://yoursite.com/2019/07/16/Snappy/</id>
    <published>2019-07-16T06:27:57.000Z</published>
    <updated>2020-04-10T17:12:05.540Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Snappy"><a href="#Snappy" class="headerlink" title="Snappy"></a>Snappy</h2><p>Snappy使用C++ 开发的压缩和解压缩开发包，只在提供高速压缩速度和合理压缩率。</p><p>主要是用内存空间换压缩速度，2015年的i7大概能提供250-500M的压缩速度。</p><a id="more"></a> <h4 id="Spark取消CSV文件输出默认的Snappy压缩格式："><a href="#Spark取消CSV文件输出默认的Snappy压缩格式：" class="headerlink" title="Spark取消CSV文件输出默认的Snappy压缩格式："></a>Spark取消CSV文件输出默认的Snappy压缩格式：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"sparktoDisk"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark2之后都是直接用SparkSession,对于之前的Conf中的属性用下面的格式设置。 压缩选项可以设置两个，分别是map阶段和reduce阶段的.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//对map输出的内容进行压缩</span></span><br><span class="line">spark.conf.set(<span class="string">"mapred.compress.map.output"</span>,<span class="string">"true"</span>);</span><br><span class="line">spark.conf.set(<span class="string">"mapred.map.output.compression.codec"</span>,<span class="string">"org.apache.hadoop.io.compress.SnappyCodec"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//对reduce输出的内容进行压缩</span></span><br><span class="line">spark.conf.set(<span class="string">"mapred.output.compress"</span>,<span class="string">"true"</span>);</span><br><span class="line">spark.conf.set(<span class="string">"mapred.output.compression"</span>,<span class="string">"org.apache.hadoop.io.compress.SnappyCodec"</span>);</span><br></pre></td></tr></table></figure><h4 id="DF保存为CSV"><a href="#DF保存为CSV" class="headerlink" title="DF保存为CSV"></a>DF保存为CSV</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.coalesce(<span class="number">1</span>).write.option(<span class="string">"header"</span>,<span class="string">"true"</span>).csv(<span class="string">"sample_file.csv"</span>)</span><br></pre></td></tr></table></figure><h4 id="使用Lib包压索解压文件"><a href="#使用Lib包压索解压文件" class="headerlink" title="使用Lib包压索解压文件"></a>使用Lib包压索解压文件</h4><p><a href="https://blog.csdn.net/lucien_zong/article/details/17071401" target="_blank" rel="noopener">CSDN链接</a></p><h4 id="Python解压snappy文件"><a href="#Python解压snappy文件" class="headerlink" title="Python解压snappy文件"></a>Python解压snappy文件</h4><ol><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://bootstrap.pypa.io/get-pip.py</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python ./get-pip.py</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install python-snappy</span><br></pre></td></tr></table></figure></li></ol><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><h5 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m snappy -c uncompressed_file compressed_file.snappy</span><br></pre></td></tr></table></figure><h5 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m snappy -d compressed_file.snappy uncompressed_file</span><br></pre></td></tr></table></figure><h3 id="阿里云文档说明"><a href="#阿里云文档说明" class="headerlink" title="阿里云文档说明"></a>阿里云文档说明</h3><p>阿里云对这些整理的很细致啊，是个找资料的好地方</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.alibabacloud.com/help/zh/doc-detail/108942.htm</span><br></pre></td></tr></table></figure><p>同时还有别的压缩格式的介绍，很详细。</p><p>文档中心-&gt;数据投递-&gt;投递日志到OSS-&gt;Snappy</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Snappy&quot;&gt;&lt;a href=&quot;#Snappy&quot; class=&quot;headerlink&quot; title=&quot;Snappy&quot;&gt;&lt;/a&gt;Snappy&lt;/h2&gt;&lt;p&gt;Snappy使用C++ 开发的压缩和解压缩开发包，只在提供高速压缩速度和合理压缩率。&lt;/p&gt;
&lt;p&gt;主要是用内存空间换压缩速度，2015年的i7大概能提供250-500M的压缩速度。&lt;/p&gt;
    
    </summary>
    
      <category term="Compression" scheme="http://yoursite.com/categories/Compression/"/>
    
    
      <category term="Compression" scheme="http://yoursite.com/tags/Compression/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch工具之RESTClient</title>
    <link href="http://yoursite.com/2019/07/08/ES%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7RESTClient%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <id>http://yoursite.com/2019/07/08/ES测试工具RESTClient使用说明/</id>
    <published>2019-07-08T03:41:28.000Z</published>
    <updated>2020-04-10T17:07:28.934Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ES简单测试工具</p></blockquote><a id="more"></a> <h1 id="1-使用RESTClient前的准备工作"><a href="#1-使用RESTClient前的准备工作" class="headerlink" title="1. 使用RESTClient前的准备工作"></a>1. 使用RESTClient前的准备工作</h1><h2 id="1-1-下载RESTClient"><a href="#1-1-下载RESTClient" class="headerlink" title="1.1 下载RESTClient"></a>1.1 下载RESTClient</h2><p>JAR包： [<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 1.2 使用前安装Java</span><br><span class="line"></span><br><span class="line">支持的Java版本 **&gt;=1.7**</span><br><span class="line"></span><br><span class="line">## 1.3 启动RESTClient软件</span><br><span class="line"></span><br><span class="line">双击[```restclient.jar```](https://github.com/Wisdom-Projects/rest-client/blob/master/tools)，或者执行命令```java -jar restclient.jar```启动RESTClient软件。</span><br><span class="line"></span><br><span class="line">RESTClient主窗体包含： </span><br><span class="line"></span><br><span class="line">+ 请求视图（Request）</span><br><span class="line">+ 响应视图（Response）</span><br><span class="line">+ 历史视图（History）</span><br><span class="line">+ 菜单栏（File, Edit, Test, Apidoc, Help）</span><br><span class="line"></span><br><span class="line"># 2. 使用RESTClient测试REST API步骤</span><br><span class="line"></span><br><span class="line">## 2.1 请求视图中输入REST API所需的请求数据</span><br><span class="line"></span><br><span class="line">在请求视图中对所测试的REST API输入的数据详情如下：</span><br><span class="line"></span><br><span class="line">### 2.1.1 选择请求方法</span><br><span class="line"></span><br><span class="line">RESTClient支持请求方法详情如下： </span><br><span class="line"></span><br><span class="line">方法名 |操作|备注</span><br><span class="line">------|---|--------------</span><br><span class="line">GET   |查询|无需要填写请求体</span><br><span class="line">POST  |添加|</span><br><span class="line">PUT   |修改|</span><br><span class="line">DELETE|删除|无需要填写请求体</span><br><span class="line"></span><br><span class="line">### 2.1.2 输入访问REST API的URL</span><br><span class="line"></span><br><span class="line">+ URL格式： ```HTTP协议://主机名:端口号/路径</span><br></pre></td></tr></table></figure></p><ul><li>URL示例： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.3 输入请求体(Body)</span><br><span class="line"></span><br><span class="line">如果选择的请求方法是**POST**或者**PUT**则可以填写请求体，**其他方法则无需填写**。</span><br><span class="line"></span><br><span class="line">#### 2.1.3.1 选择请求体类型（Body-Type）</span><br><span class="line"></span><br><span class="line">+ **字符串(String)**</span><br><span class="line"></span><br><span class="line">  直接在请求体的文本框中填写字符串；</span><br><span class="line"></span><br><span class="line">+ **文件(File)**</span><br><span class="line">  </span><br><span class="line">  浏览并选择地文本文件，文件内容会被读取并作为请求体。</span><br><span class="line"></span><br><span class="line">#### 2.1.3.2 选择内容类型（Content-Type）</span><br><span class="line"></span><br><span class="line">根据REST API消息体类型，对照下表，选择跟API匹配的内容类型，如果表中的内容类型都不是API所需要的类型，可以直接在内容类型文本框中**输入所需类型**。</span><br><span class="line">常见的内容类型详情如下：</span><br><span class="line"></span><br><span class="line">内容类型（Content-Type）           |数据格式</span><br><span class="line">---------------------------------|-------</span><br><span class="line">application/json                 |JSON</span><br><span class="line">application/xml                  |XML</span><br><span class="line">application/x-www-form-urlencoded|Form表单</span><br><span class="line">text/plain                       |纯文本</span><br><span class="line">text/xml                         |XML文本</span><br><span class="line">text/html                        |HTML文本</span><br><span class="line">multipart/form-data              |用于上传文件</span><br><span class="line">application/xhtml+xml            |XHTML</span><br><span class="line"></span><br><span class="line">### 2.1.4 选择字符集(Charset）</span><br><span class="line"></span><br><span class="line">默认字符集是**UTF-8**，可以选择REST API所需要的字符集，如果下拉列表里的字符集都不是API所需要的，可以直接在字符集文本框中**输入所需的字符集**。</span><br><span class="line"></span><br><span class="line">### 2.1.5 填写消息头(Header）</span><br><span class="line"></span><br><span class="line">可以根据REST API定义要求，以键值对的形式添加相应的消息头。</span><br><span class="line">Header键值对示例：</span><br></pre></td></tr></table></figure></li></ul><p>Key   ： Accept<br>Value ： application/json<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.6 填写Cookie</span><br><span class="line"></span><br><span class="line">可以根据REST API定义要求，以键值对的形式添加相应的Cookie。</span><br><span class="line">如果API需要登录认证，请先使用浏览器完成API登录认证成功后，将浏览器生成的JSESSIONID填写到Cookie中，这样就可以无需登录认证，直接访问REST API了，免登陆使用详情[**参考资料**](http://blog.wdom.net/article/9)。</span><br><span class="line">Cookie键值对示例：</span><br></pre></td></tr></table></figure></p><p>Key   ：JSESSIONID<br>Value : MY0REST1COOKIE2DEMO3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2.1.7 完整的请求数据示例</span><br><span class="line"></span><br><span class="line">填写完请求数据后点击Start按钮会触发API请求，在请求视图中输入完整的请求数据如图所示：</span><br><span class="line">![请求视图数据](http://blog.wdom.net/upload/2018/12/5am0qalk2egv9o7f6bsiglck34.png)</span><br><span class="line"></span><br><span class="line">## 2.2 响应视图中返回REST API响应的数据</span><br><span class="line"></span><br><span class="line">REST API请求完成后得到响应数据如下：</span><br><span class="line"></span><br><span class="line">+ 响应状态码（Status）</span><br><span class="line">+ 响应消息体（Body）</span><br><span class="line">+ 响应消息头（Header）</span><br><span class="line">+ 原始的响应数据（Raw）</span><br><span class="line"></span><br><span class="line">响应数据如图所示：</span><br><span class="line">![响应视图数据](http://blog.wdom.net/upload/2018/12/o3g9ecc474hhepvtec3e81hp36.png)</span><br><span class="line"></span><br><span class="line">## 2.3 历史视图中记录测试过的REST API</span><br><span class="line"></span><br><span class="line">在历史视图中可以对API进行的可视化编辑如下：</span><br><span class="line"></span><br><span class="line">+ 刷新API</span><br><span class="line">+ 对选中的API进行顺序调整</span><br><span class="line">+ 删除选中的API或者清空全部历史API</span><br><span class="line">+ 可以编辑选中的API</span><br><span class="line"></span><br><span class="line">历史API可视化编辑的快捷菜单如图所示：</span><br><span class="line">![API可视化编辑的快捷菜](http://blog.wdom.net/upload/2018/12/6brouneb90gkfomk7l94pn1bk9.png)</span><br><span class="line"></span><br><span class="line">## 2.4 对历史REST API进行再测试</span><br><span class="line"></span><br><span class="line">如果需要对历史API进行再测试，在RESTClient菜单栏点击 ```Test =&gt; Start Test</span><br></pre></td></tr></table></figure></p><p><img src="http://blog.wdom.net/upload/2018/12/7off78b5naiuep54qr9s4jvfh7.png" alt="API再测试"></p><p>记录的历史API测试完成后，在Windows系统中会使用默认的浏览器打开测试报告。其他系统可以根据提示框中的报告路径，手动打开测试报告。<br>测试报告如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/sq7mku6a0uhvrqk2gecvcvcsgf.png" alt="API测试报告"></p><h2 id="2-5-对历史REST-API生成API文档"><a href="#2-5-对历史REST-API生成API文档" class="headerlink" title="2.5 对历史REST API生成API文档"></a>2.5 对历史REST API生成API文档</h2><p>如果需要生成API文档，在RESTClient菜单栏点击 <code>Apidoc =&gt; Create</code><br><img src="http://blog.wdom.net/upload/2018/12/10qf6lnph6jcpri7tfuvdmhvs3.png" alt="生成API文档"></p><p>API文档生成完成后，在Windows系统中会使用默认的浏览器打开API文档。其他系统可以根据提示框中的文档路径，手动打开API文档。<br>API文档如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/o07em8jbr6g6oqrtahs9f8f2ru.png" alt="API文档"></p><h2 id="2-6-对历史REST-API进行编辑"><a href="#2-6-对历史REST-API进行编辑" class="headerlink" title="2.6 对历史REST API进行编辑"></a>2.6 对历史REST API进行编辑</h2><p>为了满足API再测试要求或者满足API文档数据要求，可以对API进行如下操作：</p><ul><li>调整API顺序</li><li>删除冗余的、废弃的API</li><li>对API进行可视化编辑</li></ul><p>历史视图中选中API，快捷菜单中选择<code>Edit</code>打开API编辑窗体，如图所示：<br><img src="http://blog.wdom.net/upload/2018/12/3318v8o0dqhfrrj6p0ahtrdsvp.png" alt="API可视化编辑窗体"></p><p>在API编辑窗体中，可以编辑如下内容：</p><ul><li>请求方法</li><li>请求URL</li><li>请求头（Header）</li><li>请求体（Body）</li><li>响应状态码（Status）</li><li>响应的消息体（Text视图）</li><li>是否校验返回的消息体（Assert Body）</li></ul><p>默认勾选了<code>Assert Body</code>，API再测试会对返回的消息体进行完整匹配校验，如果不需要对返回的消息体进行匹配校验，可以去勾选。</p><p>如果返回的消息体中的某些JSON节点不需要进行再测试匹配校验，可以在<code>Viewer</code>视图上勾选排除这些节点，这样API再测试只对未排除的节点进行匹配校验。</p><h2 id="2-7-定制API文档"><a href="#2-7-定制API文档" class="headerlink" title="2.7 定制API文档"></a>2.7 定制API文档</h2><p>如果生成的API文档不能满足要求，需要改动，可以修改数据文件<code>work/apidoc/js/apidata.js</code>来定制API文档，API定制详情可以<a href="http://blog.wdom.net/article/10" target="_blank" rel="noopener"><strong>参考资料</strong></a>。</p><h2 id="2-8-通过命令行（CLI）方式使用RESTClient实现自动化测试REST-API"><a href="#2-8-通过命令行（CLI）方式使用RESTClient实现自动化测试REST-API" class="headerlink" title="2.8 通过命令行（CLI）方式使用RESTClient实现自动化测试REST API"></a>2.8 通过命令行（CLI）方式使用RESTClient实现自动化测试REST API</h2><p>RESTClient支持通过执行命令的方式启动和再测试API以及生成API文档，RESTClient CLI使用详情<a href="http://blog.wdom.net/article/6" target="_blank" rel="noopener"><strong>参考资料</strong></a>。</p><p>通过CLI方式，这样很容易在<strong>Jenkins</strong>中定时执行命令来调度RESTClient进行API再测试，从而实现<strong>自动化测试REST API</strong>和生成REST API文档。</p><h1 id="3-问题咨询与帮助"><a href="#3-问题咨询与帮助" class="headerlink" title="3. 问题咨询与帮助"></a>3. 问题咨询与帮助</h1><p>使用RESTClient过程中遇到问题可以查看RESTClient日志文件：<code>work/log/rest-client.log</code>，这样很容易排查出问题的具体原因。</p><p>更多的RESTClient使用示例，请参考<a href="http://blog.wdom.net/tag/RESTClient" target="_blank" rel="noopener"><strong>相关的技术资料</strong></a>来获得更多的使用示例和帮助。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ES简单测试工具&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="RESTClient" scheme="http://yoursite.com/categories/RESTClient/"/>
    
    
  </entry>
  
  <entry>
    <title>ElasticSearch</title>
    <link href="http://yoursite.com/2019/06/27/ElasticSearch/"/>
    <id>http://yoursite.com/2019/06/27/ElasticSearch/</id>
    <published>2019-06-27T05:54:59.000Z</published>
    <updated>2020-04-10T17:07:12.470Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ES这个组件其实挺重要的，早就应该了解的组件，借着这次机会，我尽量把这个组件摸摸透</p></blockquote><a id="more"></a> <h3 id="ES和结构型数据库的横向对比"><a href="#ES和结构型数据库的横向对比" class="headerlink" title="ES和结构型数据库的横向对比"></a>ES和结构型数据库的横向对比</h3><table><thead><tr><th>MySQL</th><th>ElasticSearch</th></tr></thead><tbody><tr><td>Database</td><td>Index</td></tr><tr><td>Table</td><td>Type</td></tr><tr><td>Row</td><td>Document</td></tr><tr><td>Column</td><td>Field</td></tr><tr><td>Schema</td><td>Mapping</td></tr><tr><td>Index</td><td>Everything Indexed by default</td></tr><tr><td>SQL</td><td>Query DSL(查询专用语言)</td></tr></tbody></table><p>这个表格对ES介绍的很好，从概念上很容易就能把ES和MySQL联系到一起。</p><h3 id="ElasticSearch-版本新特性"><a href="#ElasticSearch-版本新特性" class="headerlink" title="ElasticSearch 版本新特性"></a>ElasticSearch 版本新特性</h3><h3 id="ElasticSearch-Java-API"><a href="#ElasticSearch-Java-API" class="headerlink" title="ElasticSearch Java API"></a>ElasticSearch Java API</h3><p>ES的Java REST Client有两种风格：</p><p>Java Low Level REST Client: 用于Elasticsearch的官方低级客户端。它允许通过http与Elasticsearch集群通信。将请求编排和响应反编排留给用户自己处理。它兼容所有的Elasticsearch版本。</p><p>Java High Level REST Client: 用于Elasticsearch的官方高级客户端。它是基于低级客户端的，它提供很多API，并负责请求的编排与响应的反编排。</p><p>在 Elasticsearch 7.0 中不建议使用TransportClient，并且在8.0中会完全删除TransportClient。因此，官方更建议我们用Java High Level REST Client，它执行HTTP请求，而不是序列号的Java请求。</p><h3 id="ElasticSearch-Query-DSL"><a href="#ElasticSearch-Query-DSL" class="headerlink" title="ElasticSearch Query DSL"></a>ElasticSearch Query DSL</h3><h4 id="查询与过滤"><a href="#查询与过滤" class="headerlink" title="查询与过滤"></a>查询与过滤</h4><p>数据检索分为两种情况：<strong>查询</strong>和<strong>过滤</strong></p><p>Query会对检索结果进行<strong>评分</strong>，注重的点是匹配程度，计算的是查询与文档的相关程度，计算完成之后会酸醋一个评分，记录在<code>_score</code>中，最终按照<code>_score</code>进行排序。</p><p>Filter过滤不会对检索结果进行评分，注重的点是是否匹配，，所以速度 要快一点，并且过滤的结果会被缓存到内存中，性能要比Query高很多。</p><h4 id="简单查询"><a href="#简单查询" class="headerlink" title="简单查询"></a>简单查询</h4><p>最简单的DSL查询表达式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GET /_search //查找整个ES中所有索引的内容</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>/_search</strong> 查找整个ES中所有索引的内容</p><p><strong>query</strong> 为查询关键字，类似的还有<code>aggs</code>为聚合关键字</p><p><strong>match_all</strong> 匹配所有的文档，也可以写<code>match_none</code>不匹配任何文档</p><p>返回结果：</p><p><img src="http://ws4.sinaimg.cn/large/bec9bff2gy1g47rpbap0aj20n30a6gls.jpg" alt="TIM截图20190620175052"></p><p><strong>took：</strong> 表示我们执行整个搜索请求消耗了多少毫秒</p><p><strong>timed_out：</strong> 表示本次查询是否超时</p><p>这里需要注意当<code>timed_out</code>为True时也会返回结果，这个结果是在请求超时时ES已经获取到的数据，所以返回的这个数据可能不完整。</p><p>且当你收到<code>timed_out</code>为True之后，虽然这个连接已经关闭，但在后台这个查询并没有结束，而是会继续执行</p><p><strong>_shards：</strong> 显示查询中参与的分片信息，成功多少分片失败多少分片等</p><p><strong>hits：</strong> 匹配到的文档的信息，其中<code>total</code>表示匹配到的文档总数，<code>max_score</code>为文档中所有<code>_score</code>的最大值</p><p>hits中的<code>hits</code>数组为查询到的文档结果，默认包含查询结果的前十个文档，每个文档都包含文档的<code>_index</code>、<code>_type</code>、<code>_id</code>、<code>_score</code>和<code>_source</code>数据</p><p>结果文档默认情况下是按照相关度（_score）进行降序排列，也就是说最先返回的是相关度最高的文档，文档相关度意思是文档内容与查询条件的匹配程度，上边的查询与过滤中有介绍</p><h4 id="指定索引"><a href="#指定索引" class="headerlink" title="指定索引"></a>指定索引</h4><ol><li><p>指定固定索引：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index1/_search</span><br></pre></td></tr></table></figure></li><li><p>指定多个索引：</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index1,index2/_search</span><br></pre></td></tr></table></figure><ol start="3"><li>用*号匹配，在匹配到的所有索引下查找数据</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /index-*/_search</span><br></pre></td></tr></table></figure><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>因为<code>hits</code>默认只展示10个文档，那我们如何查询10个以后的文档呢？ES中给力size和from两个参数。</p><p>size: 设置一次返回的结果数量，也就是<code>hits</code>中文档的数量，默认是10</p><p>from:  设置从第几个结果开始往后查询，默认值是0</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 5,</span><br><span class="line">  &quot;from&quot;: 10,</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这条命令的意义是：显示第11条到15个文档的数据。</p><p><code>match_all</code>为查询所有记录，常用的查询关键字在ES中还有<code>match</code>、<code>multi_match</code>、<code>query_string</code>、<code>term</code>、<code>range</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;host&quot;:&quot;ops-coffee.cn&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;multi_match&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;ops-coffee.cn&quot;,</span><br><span class="line">      &quot;fields&quot;:[&quot;host&quot;,&quot;http_referer&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//在多个字段上搜索时用multi_match</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;(a.ops-coffee.cn) OR (b.ops-coffee.cn)&quot;,</span><br><span class="line">      &quot;fields&quot;:[&quot;host&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//可以在查询里边使用AND或者OR来完成复杂的查询</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;:&quot;host:a.ops-coffee.cn OR (host:b.ops-coffee.cn AND status:403)&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//以上表示查询（host为a.ops-coffee.cn）或者是（host为b.ops-coffee.cn且status为403）的所有记录\</span><br><span class="line">与其像类似的还有个simple_query_string的关键字，可以将query_string中的AND或OR用+或|这样的符号替换掉</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//term表示了精确匹配，精确匹配的可以是数字，时间，布尔值或者是设置了not_analyzed不分词的字符串</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;term&quot;: &#123;</span><br><span class="line">      &quot;status&quot;: &#123;</span><br><span class="line">        &quot;value&quot;: 404</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//匹配多个值</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;status&quot;:[403,404]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">//range用来查询落在指定区间里的数字或者时间</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;:&#123;</span><br><span class="line">      &quot;status&quot;:&#123;</span><br><span class="line">        &quot;gte&quot;: 400,</span><br><span class="line">        &quot;lte&quot;: 599</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//range用来查询落在指定区间内的数字或者时间</span><br><span class="line">范围关键字主要有四个：</span><br><span class="line">gt: 大于</span><br><span class="line">gte: 大于等于</span><br><span class="line">lt: 小于</span><br><span class="line">lte: 小于等于</span><br><span class="line"></span><br><span class="line">并且：当range把日期作为查询范围时，我们需注意下日期的格式，官方支持的日期格式主要有两种</span><br><span class="line">1.时间戳（ms）</span><br><span class="line">get /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;: &#123;</span><br><span class="line">      &quot;@timestamp&quot;: &#123;</span><br><span class="line">        &quot;gte&quot;: 1557676800000,</span><br><span class="line">        &quot;lte&quot;: 1557680400000,</span><br><span class="line">        &quot;format&quot;:&quot;epoch_millis&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">2.日期字符串</span><br><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;:&#123;</span><br><span class="line">      &quot;@timestamp&quot;:&#123;</span><br><span class="line">        &quot;gte&quot;: &quot;2019-05-13 18:30:00&quot;,</span><br><span class="line">        &quot;lte&quot;: &quot;2019-05-14&quot;,</span><br><span class="line">        &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd&quot;,</span><br><span class="line">        &quot;time_zone&quot;: &quot;+08:00&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">日期格式可以按照自己的习惯输入，只需要format字段指定匹配的格式，如果格式有多个就用||分开，不过推荐用相同的日期格式。</span><br><span class="line">如果日期中缺少年月日这些内容，那么缺少的部分会用unix的开始时间（即1970年1月1日）填充，当你将&quot;format&quot;:&quot;dd&quot;指定为格式时，那么&quot;gte&quot;:10将被转换成1970-01-10T00:00:00.000Z</span><br><span class="line">elasticsearch中默认使用的是UTC时间，所以我们在使用时要通过time_zone来设置好时区，以免出错</span><br></pre></td></tr></table></figure><h3 id="组合查询"><a href="#组合查询" class="headerlink" title="组合查询"></a>组合查询</h3><p>通常我们可能需要将很多个条件组合在一起查处最后的结果，这个时候就需要使用es提供的<code>bool</code>来实现。</p><p>Ex. 要查询<code>host</code>为<code>ops-coffee.cn</code>且<code>http_x_forworded_for</code>为<code>111.18.78.128</code>且<code>status</code>不为200的所有数据就可以使用下边的语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line"> &quot;query&quot;:&#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: [</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;host&quot;: &quot;ops-coffee.cn&quot;</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;http_x_forwarded_for&quot;: &quot;111.18.78.128&quot;</span><br><span class="line">        &#125;&#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;must_not&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">          &quot;status&quot;: 200</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">组合查询设计四个关键字组合设计查询之间的关系。分别为：</span><br><span class="line">must: 类似于SQL中的AND，必须包含</span><br><span class="line">must_not: 类似于SQL中的NOT，必须不包含</span><br><span class="line">should: 满足这些条件中的任何条件都会增加评分_score，不满足也不影响，should只会影响查询结果的_score值，并不会影响结果的内容</span><br><span class="line">filter: 与must相似，但不会对结果进行相关性评分_score，大多数情况下我们对于日志的需求都无相关性的要求，所以建议查询的过程中多用filter</span><br></pre></td></tr></table></figure><h3 id="全文检索"><a href="#全文检索" class="headerlink" title="全文检索"></a>全文检索</h3><p>全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。 </p><p>全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。</p><h3 id="ES调优"><a href="#ES调优" class="headerlink" title="ES调优"></a>ES调优</h3><p><a href="https://juejin.im/post/5c3e9813518825552880084a" target="_blank" rel="noopener">调优</a></p><h3 id="Lucene"><a href="#Lucene" class="headerlink" title="Lucene"></a>Lucene</h3><p><strong>介绍</strong>: Lucene是apache软件基金会发布的一个开放源代码的全文检索引擎工具包，由资深全文检索专家Doug Cutting所撰写,它是一个<strong>全文检索引擎的架构</strong>，提供了完整的创建索引和查询索引，以及部分文本分析的引擎。</p><p><strong>特色</strong>: Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎，Lucene在全文检索领域是一个经典的祖先，现在很多检索引擎都是在其基础上创建的，思想是相通的。</p><p><strong>Lucene是根据关健字来搜索的文本搜索工具，只能在某个网站内部搜索文本内容，不能跨网站搜索</strong></p><p>为什么有了数据库还要使用Lucene:</p><ul><li>（1）SQL只能针对数据库表搜索，<strong>不能直接针对硬盘上的文本搜索</strong></li><li>（2）<strong>SQL没有相关度排名</strong></li><li>（3）<strong>SQL搜索结果没有关健字高亮显示</strong></li><li>（4）<strong>SQL需要数据库的支持</strong>，数据库本身需要内存开销较大，例如：Oracle</li><li>（5）<strong>SQL搜索有时较慢</strong>，尤其是数据库不在本地时，超慢，例如：Oracle</li></ul><p>以上所说的，我们如果使用SQL的话，是做不到的。因此我们就学习<strong>Lucene来帮我们在站内根据文本关键字来进行搜索数据</strong>！</p><p>我们如果网站需要根据关键字来进行搜索，可以使用SQL，也可以使用Lucene…那么我们<strong>Lucene和SQL是一样的，都是在持久层中编写代码的</strong>。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fiswh248j20so0d7gs3.jpg" alt></p><p>Lucene中存的就是<strong>一系列的二进制压缩文件和一些控制文件</strong>，它们位于计算机的硬盘上，<br><strong>这些内容统称为索引库</strong>，索引库有二部份组成：</p><ul><li><p>（1）<strong>原始记录</strong></p></li><li><ul><li>存入到索引库中的原始文本，例如：我是钟福成</li></ul></li><li><p>（2）<strong>词汇表</strong></p></li><li><ul><li>按照一定的拆分策略（即分词器）将原始记录中的每个字符拆开后，存入一个供将来搜索的表</li></ul></li></ul><p>也就是说：<strong>Lucene存放数据的地方我们通常称之为索引库，索引库又分为两部分组成：原始记录和词汇表</strong>….</p><h4 id="原始记录和词汇表"><a href="#原始记录和词汇表" class="headerlink" title="原始记录和词汇表"></a>原始记录和词汇表</h4><p>当我们想要把数据存到索引库的时候，我们首先存入的是将数据存到原始记录上面去….</p><p>又由于我们给用户使用的时候，用户<strong>使用的是关键字来进行查询我们的具体记录</strong>。因此，我们需要把我们<strong>原始存进的数据进行拆分</strong>！将<strong>拆分出来的数据存进词汇表中</strong>。</p><p>词汇表就是类似于我们在学Oracle中的索引表，<strong>拆分的时候会给出对应的索引值。</strong></p><p>一旦用户根据关键字来进行搜索，那<strong>么程序就先去查询词汇表中有没有该关键字，如果有该关键字就定位到原始记录表中，将符合条件的原始记录返回给用户查看</strong>。</p><p>我们查看以下的图方便理解：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fiusyxfzj20sy0dcqct.jpg" alt></p><p>到了这里，有人可能就会疑问：难道原始记录拆分的数据都是一个一个汉字进行拆分的吗？？然后在词汇表中不就有很多的关键字了？？？</p><p>其实，我们在存到原始记录表中的时候，可以指定我们使用哪种算法来将数据拆分，存到词汇表中…..我们的<strong>图是Lucene的标准分词算法，一个一个汉字进行拆分</strong>。我们可以使用别的分词算法，两个两个拆分或者其他的算法。</p><h4 id="Lucene程序："><a href="#Lucene程序：" class="headerlink" title="Lucene程序："></a>Lucene程序：</h4><p>首先要导入必要的Lucene的必要开发包</p><ul><li><strong>lucene-core-3.0.2.jar【Lucene核心】</strong></li><li><strong>lucene-analyzers-3.0.2.jar【分词器】</strong></li><li><strong>lucene-highlighter-3.0.2.jar【Lucene会将搜索出来的字，高亮显示，提示用户】</strong></li><li><strong>lucene-memory-3.0.2.jar【索引库优化策略】</strong></li></ul><p>创建User对象，User对象封装了数据….</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by ozc on 2017/7/12.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id ;</span><br><span class="line">    <span class="keyword">private</span> String userName;</span><br><span class="line">    <span class="keyword">private</span> String sal;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(String id, String userName, String sal)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.userName = userName;</span><br><span class="line">        <span class="keyword">this</span>.sal = sal;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getUserName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> userName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUserName</span><span class="params">(String userName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userName = userName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getSal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSal</span><span class="params">(String sal)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sal = sal;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们想要使用Lucene来查询出站内的数据，首先我们得要有个索引库吧！于是<strong>我们先创建索引库，将我们的数据存到索引库中</strong>。</p><p>创建索引库的步骤：</p><ul><li>1）<strong>创建JavaBean对象</strong></li><li>2）<strong>创建Docment对象</strong></li><li>3）<strong>将JavaBean对象所有的属性值，均放到Document对象中去，属性名可以和JavaBean相同或不同</strong></li><li>4）<strong>创建IndexWriter对象</strong></li><li>5）<strong>将Document对象通过IndexWriter对象写入索引库中</strong></li><li>6）<strong>关闭IndexWriter对象</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把数据填充到JavaBean对象中</span></span><br><span class="line">    User user = <span class="keyword">new</span> User(<span class="string">"1"</span>, <span class="string">"钟福成"</span>, <span class="string">"未来的程序员"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建Document对象【导入的是Lucene包下的Document对象】</span></span><br><span class="line">    Document document = <span class="keyword">new</span> Document();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将JavaBean对象所有的属性值，均放到Document对象中去，属性名可以和JavaBean相同或不同</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 向Document对象加入一个字段</span></span><br><span class="line"><span class="comment">     * 参数一：字段的关键字</span></span><br><span class="line"><span class="comment">     * 参数二：字符的值</span></span><br><span class="line"><span class="comment">     * 参数三：是否要存储到原始记录表中</span></span><br><span class="line"><span class="comment">     *      YES表示是</span></span><br><span class="line"><span class="comment">     *      NO表示否</span></span><br><span class="line"><span class="comment">     * 参数四：是否需要将存储的数据拆分到词汇表中</span></span><br><span class="line"><span class="comment">     *      ANALYZED表示拆分</span></span><br><span class="line"><span class="comment">     *      NOT_ANALYZED表示不拆分</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"id"</span>, user.getId(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"userName"</span>, user.getUserName(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">    document.add(<span class="keyword">new</span> Field(<span class="string">"sal"</span>, user.getSal(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建IndexWriter对象</span></span><br><span class="line">    <span class="comment">//目录指定为E:/createIndexDB</span></span><br><span class="line">    Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用标准的分词算法对原始记录表进行拆分</span></span><br><span class="line">    Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//LIMITED默认是1W个</span></span><br><span class="line">    IndexWriter.MaxFieldLength maxFieldLength = IndexWriter.MaxFieldLength.LIMITED;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * IndexWriter将我们的document对象写到硬盘中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 参数一：Directory d,写到硬盘中的目录路径是什么</span></span><br><span class="line"><span class="comment">     * 参数二：Analyzer a, 以何种算法来对document中的原始记录表数据进行拆分成词汇表</span></span><br><span class="line"><span class="comment">     * 参数三：MaxFieldLength mfl 最多将文本拆分出多少个词汇</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    IndexWriter indexWriter = <span class="keyword">new</span> IndexWriter(directory, analyzer, maxFieldLength);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将Document对象通过IndexWriter对象写入索引库中</span></span><br><span class="line">    indexWriter.addDocument(document);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭IndexWriter对象</span></span><br><span class="line">    indexWriter.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>于是，我们现在用一个关键字，把索引库的数据读取。看看读取数据是否成功。</p><p>根据关键字查询索引库中的内容：</p><ul><li>1）<strong>创建IndexSearcher对象</strong></li><li>2）<strong>创建QueryParser对象</strong></li><li>3）<strong>创建Query对象来封装关键字</strong></li><li>4）<strong>用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</strong></li><li>5）<strong>获取符合条件的编号</strong></li><li>6）<strong>用indexSearcher对象去索引库中查询编号对应的Document对象</strong></li><li>7）<strong>将Document对象中的所有属性取出，再封装回JavaBean对象中去，并加入到集合中保存，以备将之用</strong></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 参数一： IndexSearcher(Directory path)查询以xxx目录的索引库</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">     Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line">     <span class="comment">//创建IndexSearcher对象</span></span><br><span class="line">     IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(directory);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//创建QueryParser对象</span></span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 参数一： Version matchVersion 版本号【和上面是一样的】</span></span><br><span class="line"><span class="comment">      * 参数二：String f,【要查询的字段】</span></span><br><span class="line"><span class="comment">      * 参数三：Analyzer a【使用的拆词算法】</span></span><br><span class="line"><span class="comment">      * */</span></span><br><span class="line">     Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line">     QueryParser queryParser = <span class="keyword">new</span> QueryParser(Version.LUCENE_30, <span class="string">"userName"</span>, analyzer);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//给出要查询的关键字</span></span><br><span class="line">     String keyWords = <span class="string">"钟"</span>;</span><br><span class="line"></span><br><span class="line">     <span class="comment">//创建Query对象来封装关键字</span></span><br><span class="line">     Query query = queryParser.parse(keyWords);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</span></span><br><span class="line">     TopDocs topDocs = indexSearcher.search(query, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">     <span class="comment">//获取符合条件的编号</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; topDocs.scoreDocs.length; i++) &#123;</span><br><span class="line"></span><br><span class="line">         ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">         <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">         <span class="comment">//用indexSearcher对象去索引库中查询编号对应的Document对象</span></span><br><span class="line">         Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">         <span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">         String id = document.get(<span class="string">"id"</span>);</span><br><span class="line">         String userName = document.get(<span class="string">"userName"</span>);</span><br><span class="line">         String sal = document.get(<span class="string">"sal"</span>);</span><br><span class="line"></span><br><span class="line">         User user = <span class="keyword">new</span> User(id, userName, sal);</span><br><span class="line">         System.out.println(user);</span><br><span class="line"></span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><h4 id="代码说明："><a href="#代码说明：" class="headerlink" title="代码说明："></a>代码说明：</h4><p>我们的Lucene程序就是大概这么一个思路：<strong>将JavaBean对象封装到Document对象中，然后通过IndexWriter把document写入到索引库中。当用户需要查询的时候，就使用IndexSearcher从索引库中读取数据，找到对应的Document对象，从而解析里边的内容，再封装到JavaBean对象中让我们使用</strong>。</p><h4 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h4><p>我们再次看回我们上一篇快速入门写过的代码，我来截取一些有代表性的：</p><p>以下代码在把数据填充到索引库，和从索引库查询数据的时候，都出现了。<strong>是重复代码</strong>！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Directory directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用标准的分词算法对原始记录表进行拆分</span></span><br><span class="line">Analyzer analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br></pre></td></tr></table></figure><p>以下的代码其实就是<strong>将JavaBean的数据封装到Document对象中，我们是可以通过反射来对其进行封装</strong>….如果不封装的话，我们如果有很多JavaBean都要添加到Document对象中，就会出现很多类似的代码.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"id"</span>, user.getId(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"userName"</span>, user.getUserName(), Field.Store.YES, Field.Index.ANALYZED));</span><br><span class="line">document.add(<span class="keyword">new</span> Field(<span class="string">"sal"</span>, user.getSal(), Field.Store.YES, Field.Index.ANALYZED));</span><br></pre></td></tr></table></figure><p>以下代码就是从Document对象中把数据取出来，封装到JavaBean去。如果JavaBean中有很多属性，也是需要我们写很多次类似代码….</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">String id = document.get(<span class="string">"id"</span>);</span><br><span class="line">String userName = document.get(<span class="string">"userName"</span>);</span><br><span class="line">String sal = document.get(<span class="string">"sal"</span>);</span><br><span class="line">User user = <span class="keyword">new</span> User(id, userName, sal);</span><br></pre></td></tr></table></figure><h4 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h4><p>编写工具类的时候，值得注意的地方：</p><ul><li>当我们得到了对象的属性的时候，就可以把属性的get方法封装起来</li><li>得到get方法，就可以调用它，得到对应的值</li><li>在操作对象的属性时，我们要使用暴力访问</li><li>如果有属性，值，对象这三个变量，我们记得使用BeanUtils组件</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.analysis.Analyzer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.document.Document;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.index.IndexWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.store.Directory;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.store.FSDirectory;</span><br><span class="line"><span class="keyword">import</span> org.apache.lucene.util.Version;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by ozc on 2017/7/12.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用单例事例模式</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LuceneUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Directory directory;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Analyzer analyzer;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> IndexWriter.MaxFieldLength maxFieldLength;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">LuceneUtils</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            directory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/createIndexDB"</span>));</span><br><span class="line">            analyzer = <span class="keyword">new</span> StandardAnalyzer(Version.LUCENE_30);</span><br><span class="line">            maxFieldLength = IndexWriter.MaxFieldLength.LIMITED;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Directory <span class="title">getDirectory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> directory;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Analyzer <span class="title">getAnalyzer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> analyzer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> IndexWriter.<span class="function">MaxFieldLength <span class="title">getMaxFieldLength</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> maxFieldLength;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> object 传入的JavaBean类型</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回Document对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Document <span class="title">javaBean2Document</span><span class="params">(Object object)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Document document = <span class="keyword">new</span> Document();</span><br><span class="line">            <span class="comment">//得到JavaBean的字节码文件对象</span></span><br><span class="line">            Class&lt;?&gt; aClass = object.getClass();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//通过字节码文件对象得到对应的属性【全部的属性，不能仅仅调用getFields()】</span></span><br><span class="line">            Field[] fields = aClass.getDeclaredFields();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//得到每个属性的名字</span></span><br><span class="line">            <span class="keyword">for</span> (Field field : fields) &#123;</span><br><span class="line">                String name = field.getName();</span><br><span class="line">                <span class="comment">//得到属性的值【也就是调用getter方法获取对应的值】</span></span><br><span class="line">                String method = <span class="string">"get"</span> + name.substring(<span class="number">0</span>, <span class="number">1</span>).toUpperCase() + name.substring(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">//得到对应的值【就是得到具体的方法，然后调用就行了。因为是get方法，没有参数】</span></span><br><span class="line">                Method aClassMethod = aClass.getDeclaredMethod(method, <span class="keyword">null</span>);</span><br><span class="line">                String value = aClassMethod.invoke(object).toString();</span><br><span class="line">                System.out.println(value);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment">//把数据封装到Document对象中。</span></span><br><span class="line">                document.add(<span class="keyword">new</span> org.apache.lucene.document.Field(name, value, org.apache.lucene.document.Field.Store.YES, org.apache.lucene.document.Field.Index.ANALYZED));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> document;</span><br><span class="line">        &#125;  <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> aClass   要解析的对象类型，要用户传入进来</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> document 将Document对象传入进来</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 返回一个JavaBean</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Object <span class="title">Document2JavaBean</span><span class="params">(Document document, Class aClass)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//创建该JavaBean对象</span></span><br><span class="line">            Object obj = aClass.newInstance();</span><br><span class="line">            <span class="comment">//得到该JavaBean所有的成员变量</span></span><br><span class="line">            Field[] fields = aClass.getDeclaredFields();</span><br><span class="line">            <span class="keyword">for</span> (Field field : fields) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">//设置允许暴力访问</span></span><br><span class="line">                field.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">                String name = field.getName();</span><br><span class="line">                String value = document.get(name);</span><br><span class="line">                <span class="comment">//使用BeanUtils把数据封装到Bean中</span></span><br><span class="line">                BeanUtils.setProperty(obj, name, value);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> obj;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        User user = <span class="keyword">new</span> User();</span><br><span class="line">        LuceneUtils.javaBean2Document(user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用LuceneUtils改造程序"><a href="#使用LuceneUtils改造程序" class="headerlink" title="使用LuceneUtils改造程序"></a>使用LuceneUtils改造程序</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">//把数据填充到JavaBean对象中</span></span><br><span class="line">    User user = <span class="keyword">new</span> User(<span class="string">"2"</span>, <span class="string">"钟福成2"</span>, <span class="string">"未来的程序员2"</span>);</span><br><span class="line">    Document document = LuceneUtils.javaBean2Document(user);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * IndexWriter将我们的document对象写到硬盘中</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 参数一：Directory d,写到硬盘中的目录路径是什么</span></span><br><span class="line"><span class="comment">     * 参数二：Analyzer a, 以何种算法来对document中的原始记录表数据进行拆分成词汇表</span></span><br><span class="line"><span class="comment">     * 参数三：MaxFieldLength mfl 最多将文本拆分出多少个词汇</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    IndexWriter indexWriter = <span class="keyword">new</span> IndexWriter(LuceneUtils.getDirectory(), LuceneUtils.getAnalyzer(), LuceneUtils.getMaxFieldLength());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将Document对象通过IndexWriter对象写入索引库中</span></span><br><span class="line">    indexWriter.addDocument(document);</span><br><span class="line">    <span class="comment">//关闭IndexWriter对象</span></span><br><span class="line">    indexWriter.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">findIndexDB</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建IndexSearcher对象</span></span><br><span class="line">    IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtils.getDirectory());</span><br><span class="line">    <span class="comment">//创建QueryParser对象</span></span><br><span class="line">    QueryParser queryParser = <span class="keyword">new</span> QueryParser(Version.LUCENE_30, <span class="string">"userName"</span>, LuceneUtils.getAnalyzer());</span><br><span class="line">    <span class="comment">//给出要查询的关键字</span></span><br><span class="line">    String keyWords = <span class="string">"钟"</span>;</span><br><span class="line">    <span class="comment">//创建Query对象来封装关键字</span></span><br><span class="line">    Query query = queryParser.parse(keyWords);</span><br><span class="line">    <span class="comment">//用IndexSearcher对象去索引库中查询符合条件的前100条记录，不足100条记录的以实际为准</span></span><br><span class="line">    TopDocs topDocs = indexSearcher.search(query, <span class="number">100</span>);</span><br><span class="line">    <span class="comment">//获取符合条件的编号</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; topDocs.scoreDocs.length; i++) &#123;</span><br><span class="line">        ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">        <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">        <span class="comment">//用indexSearcher对象去索引库中查询编号对应的Document对象</span></span><br><span class="line">        Document document = indexSearcher.doc(no);</span><br><span class="line">        <span class="comment">//将Document对象中的所有属性取出，再封装回JavaBean对象中去</span></span><br><span class="line">        User user = (User) LuceneUtils.Document2JavaBean(document, User.class);</span><br><span class="line">        System.out.println(user);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="索引库优化："><a href="#索引库优化：" class="headerlink" title="索引库优化："></a>索引库优化：</h4><p>我们已经可以创建索引库并且从索引库读取对象的数据了。其实索引库还有地方可以优化的…</p><h4 id="合并文件"><a href="#合并文件" class="headerlink" title="合并文件"></a>合并文件</h4><p>我们把数据添加到索引库中的时候，<strong>每添加一次，都会帮我们自动创建一个cfs文件</strong>…</p><p>这样其实不好，因为如果数据量一大，我们的硬盘就有非常非常多的cfs文件了…..其实<strong>索引库会帮我们自动合并文件的，默认是10个</strong>。</p><p>如果，我们想要修改默认的值，我们可以通过以下的代码修改：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//索引库优化</span></span><br><span class="line">indexWriter.optimize();</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置合并因子为3，每当有3个cfs文件，就合并</span></span><br><span class="line">indexWriter.setMergeFactor(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>我们的目前的程序是直接与文件进行操作，这样对IO的开销其实是比较大的。而且速度相对较慢….我们可以使用内存索引库来提高我们的读写效率…</p><p>对于内存索引库而言，它的速度是很快的，因为我们直接操作内存…但是呢，<strong>我们要将内存索引库是要到硬盘索引库中保存起来的。当我们读取数据的时候，先要把硬盘索引库的数据同步到内存索引库中去的。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Article article = <span class="keyword">new</span> Article(<span class="number">1</span>,<span class="string">"培训"</span>,<span class="string">"传智是一家Java培训机构"</span>);</span><br><span class="line">Document document = LuceneUtil.javabean2document(article);</span><br><span class="line"></span><br><span class="line">Directory fsDirectory = FSDirectory.open(<span class="keyword">new</span> File(<span class="string">"E:/indexDBDBDBDBDBDBDBDB"</span>));</span><br><span class="line">Directory ramDirectory = <span class="keyword">new</span> RAMDirectory(fsDirectory);</span><br><span class="line"></span><br><span class="line">IndexWriter fsIndexWriter = <span class="keyword">new</span> IndexWriter(fsDirectory,LuceneUtil.getAnalyzer(),<span class="keyword">true</span>,LuceneUtil.getMaxFieldLength());</span><br><span class="line">IndexWriter ramIndexWriter = <span class="keyword">new</span> IndexWriter(ramDirectory,LuceneUtil.getAnalyzer(),LuceneUtil.getMaxFieldLength());</span><br><span class="line"></span><br><span class="line">ramIndexWriter.addDocument(document);</span><br><span class="line">ramIndexWriter.close();</span><br><span class="line"></span><br><span class="line">fsIndexWriter.addIndexesNoOptimize(ramDirectory);</span><br><span class="line">fsIndexWriter.close();</span><br></pre></td></tr></table></figure><h4 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h4><p>我们在前面中就已经说过了，在把数据存到索引库的时候，我们会使用某些算法，将原始记录表的数据存到词汇表中…..那么<strong>这些算法总和我们可以称之为分词器</strong></p><p>分词器： <strong> 采用一种算法，将中英文本中的字符拆分开来，形成词汇，以待用户输入关健字后搜索</strong></p><p>对于为什么要使用分词器，我们也明确地说过：由于用户不可能把我们的原始记录数据完完整整地记录下来，于是他们在搜索的时候，是通过关键字进行对原始记录表的查询….此时，我们就采用<strong>分词器来最大限度地匹配相关的数据</strong></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g4fklfh67fj20ka08kdhl.jpg" alt></p><h4 id="分词器-1"><a href="#分词器-1" class="headerlink" title="分词器"></a>分词器</h4><ul><li>步一：按分词器拆分出词汇</li><li>步二：去除停用词和禁用词</li><li>步三：如果有英文，把英文字母转为小写，即搜索不分大小写</li></ul><p>API：</p><p>我们在选择分词算法的时候，我们会发现有非常非常多地分词器API，我们可以用以下代码来看看该<strong>分词器是怎么将数据分割的</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">testAnalyzer</span><span class="params">(Analyzer analyzer, String text)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"当前使用的分词器："</span> + analyzer.getClass());</span><br><span class="line">    TokenStream tokenStream = analyzer.tokenStream(<span class="string">"content"</span>,<span class="keyword">new</span> StringReader(text));</span><br><span class="line">    tokenStream.addAttribute(TermAttribute.class);</span><br><span class="line">    <span class="keyword">while</span> (tokenStream.incrementToken()) &#123;</span><br><span class="line">        TermAttribute termAttribute = tokenStream.getAttribute(TermAttribute.class);</span><br><span class="line">        System.out.println(termAttribute.term());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在实验完之后，我们就可以选择恰当的分词算法了….</p><h4 id="IKAnalyzer分词器"><a href="#IKAnalyzer分词器" class="headerlink" title="IKAnalyzer分词器"></a>IKAnalyzer分词器</h4><p>这是一个第三方的分词器，我们如果要使用的话需要导入对应的jar包</p><ul><li><strong>IKAnalyzer3.2.0Stable.jar</strong></li><li><strong>步二：将IKAnalyzer.cfg.xml和stopword.dic和xxx.dic文件复制到MyEclipse的src目录下，再进行配置，在配置时，首行需要一个空行</strong></li></ul><p>这个第三方的分词器有什么好呢？？？？他是<strong>中文首选的分词器</strong>…也就是说：他是按照中文的词语来进行拆分的!</p><h3 id="对搜索结果进行处理"><a href="#对搜索结果进行处理" class="headerlink" title="对搜索结果进行处理"></a>对搜索结果进行处理</h3><h4 id="搜索结果高亮"><a href="#搜索结果高亮" class="headerlink" title="搜索结果高亮"></a>搜索结果高亮</h4><p>我们在使用SQL时，搜索出来的数据是没有高亮的…而我们使用<strong>Lucene，搜索出来的内容我们可以设置关键字为高亮</strong>…这样一来就更加注重用户体验了！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">    String keywords = <span class="string">"钟福成"</span>;</span><br><span class="line">    List&lt;Article&gt; articleList = <span class="keyword">new</span> ArrayList&lt;Article&gt;();</span><br><span class="line">    QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br><span class="line">    Query query = queryParser.parse(keywords);</span><br><span class="line">    IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtil.getDirectory());</span><br><span class="line">    TopDocs topDocs = indexSearcher.search(query,<span class="number">1000000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置关键字高亮</span></span><br><span class="line">    Formatter formatter = <span class="keyword">new</span> SimpleHTMLFormatter(<span class="string">"&lt;font color='red'&gt;"</span>,<span class="string">"&lt;/font&gt;"</span>);</span><br><span class="line">    Scorer scorer = <span class="keyword">new</span> QueryScorer(query);</span><br><span class="line">    Highlighter highlighter = <span class="keyword">new</span> Highlighter(formatter,scorer);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;topDocs.scoreDocs.length;i++)&#123;</span><br><span class="line">        ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">        <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">        Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置内容高亮</span></span><br><span class="line">        String highlighterContent = highlighter.getBestFragment(LuceneUtil.getAnalyzer(),<span class="string">"content"</span>,document.get(<span class="string">"content"</span>));</span><br><span class="line">        document.getField(<span class="string">"content"</span>).setValue(highlighterContent);</span><br><span class="line"></span><br><span class="line">        Article article = (Article) LuceneUtil.document2javabean(document,Article.class);</span><br><span class="line">        articleList.add(article);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(Article article : articleList)&#123;</span><br><span class="line">        System.out.println(article);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="搜索结果摘要"><a href="#搜索结果摘要" class="headerlink" title="搜索结果摘要"></a>搜索结果摘要</h4><p>如果我们搜索出来的文章内容太大了，而我们只想显示部分的内容，那么我们可以对其进行摘要…</p><p>值得注意的是：搜索结果摘要需要与设置高亮一起使用</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">String keywords = <span class="string">"钟福成"</span>;</span><br><span class="line">        List&lt;Article&gt; articleList = <span class="keyword">new</span> ArrayList&lt;Article&gt;();</span><br><span class="line">        QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br><span class="line">        Query query = queryParser.parse(keywords);</span><br><span class="line">        IndexSearcher indexSearcher = <span class="keyword">new</span> IndexSearcher(LuceneUtil.getDirectory());</span><br><span class="line">        TopDocs topDocs = indexSearcher.search(query,<span class="number">1000000</span>);</span><br><span class="line"></span><br><span class="line">        Formatter formatter = <span class="keyword">new</span> SimpleHTMLFormatter(<span class="string">"&lt;font color='red'&gt;"</span>,<span class="string">"&lt;/font&gt;"</span>);</span><br><span class="line">        Scorer scorer = <span class="keyword">new</span> QueryScorer(query);</span><br><span class="line">        Highlighter highlighter = <span class="keyword">new</span> Highlighter(formatter,scorer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置摘要</span></span><br><span class="line">        Fragmenter fragmenter  = <span class="keyword">new</span> SimpleFragmenter(<span class="number">4</span>);</span><br><span class="line">        highlighter.setTextFragmenter(fragmenter);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;topDocs.scoreDocs.length;i++)&#123;</span><br><span class="line">            ScoreDoc scoreDoc = topDocs.scoreDocs[i];</span><br><span class="line">            <span class="keyword">int</span> no = scoreDoc.doc;</span><br><span class="line">            Document document = indexSearcher.doc(no);</span><br><span class="line"></span><br><span class="line">            String highlighterContent = highlighter.getBestFragment(LuceneUtil.getAnalyzer(),<span class="string">"content"</span>,document.get(<span class="string">"content"</span>));</span><br><span class="line">            document.getField(<span class="string">"content"</span>).setValue(highlighterContent);</span><br><span class="line"></span><br><span class="line">            Article article = (Article) LuceneUtil.document2javabean(document,Article.class);</span><br><span class="line">            articleList.add(article);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(Article article : articleList)&#123;</span><br><span class="line">            System.out.println(article);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="搜索结果排序"><a href="#搜索结果排序" class="headerlink" title="搜索结果排序"></a>搜索结果排序</h4><p>我们搜索引擎肯定用得也不少，使用不同的搜索引擎来搜索相同的内容。他们首页的排行顺序也会不同…这就是它们内部用了搜索结果排序….</p><p>影响网页的排序有非常多种：</p><ul><li>head/meta/【keywords关键字】</li><li>网页的标签整洁</li><li>网页执行速度</li><li>采用div+css</li><li>等等等等</li></ul><p>而在Lucene中我们就可以设置相关度得分来使不同的结果对其进行排序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter indexWriter = new IndexWriter(LuceneUtil.getDirectory(),LuceneUtil.getAnalyzer(),LuceneUtil.getMaxFieldLength());</span><br><span class="line">//为结果设置得分</span><br><span class="line">document.setBoost(20F);</span><br><span class="line">indexWriter.addDocument(document);</span><br><span class="line">indexWriter.close();</span><br></pre></td></tr></table></figure><p>当然了，我们也可以按单个字段排序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//true表示降序</span></span><br><span class="line">Sort sort = <span class="keyword">new</span> Sort(<span class="keyword">new</span> SortField(<span class="string">"id"</span>,SortField.INT,<span class="keyword">true</span>));</span><br><span class="line">TopDocs topDocs = indexSearcher.search(query,<span class="keyword">null</span>,<span class="number">1000000</span>,sort);</span><br></pre></td></tr></table></figure><p>也可以按多个字段排序：在多字段排序中，<strong>只有第一个字段排序结果相同时，第二个字段排序才有作用 提倡用数值型排序</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sort sort = <span class="keyword">new</span> Sort(<span class="keyword">new</span> SortField(<span class="string">"count"</span>,SortField.INT,<span class="keyword">true</span>),<span class="keyword">new</span> SortField(<span class="string">"id"</span>,SortField.INT,<span class="keyword">true</span>));</span><br><span class="line">TopDocs topDocs = indexSearcher.search(query,<span class="keyword">null</span>,<span class="number">1000000</span>,sort);</span><br></pre></td></tr></table></figure><h4 id="条件搜索"><a href="#条件搜索" class="headerlink" title="条件搜索"></a>条件搜索</h4><p>在我们的例子中，我们使用的是根据一个关键字来对某个字段的内容进行搜索。语法类似于下面：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QueryParser queryParser = <span class="keyword">new</span> QueryParser(LuceneUtil.getVersion(),<span class="string">"content"</span>,LuceneUtil.getAnalyzer());</span><br></pre></td></tr></table></figure><p>其实，我们也可以使用关键字来对多个字段进行搜索，也就是多条件搜索。<strong>我们实际中常常用到的是多条件搜索，多条件搜索可以使用我们最大限度匹配对应的数据</strong>！</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">QueryParser queryParser = <span class="keyword">new</span> MultiFieldQueryParser(LuceneUtil.getVersion(),<span class="keyword">new</span> String[]&#123;<span class="string">"content"</span>,<span class="string">"title"</span>&#125;,LuceneUtil.getAnalyzer());</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>Lucene是全文索引引擎的祖先</strong>，后面的Solr、Elasticsearch都是基于Lucene的(后面会有一篇讲Elasticsearch的，敬请期待～)</li><li><p>Lucene中存的就是一系列的<strong>二进制压缩文件和一些控制文件</strong>,这些内容统称为<strong>索引库</strong>,索引库又分了两个部分：词汇表、词汇表</p></li><li><p>了解索引库的优化方式：1、合并文件  2、设置内存索引库</p></li><li>Lucene的分词器有非常多种，选择自己适合的一种进行分词</li><li>查询出来的结果可对其设置高亮、摘要、排序</li><li></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ES这个组件其实挺重要的，早就应该了解的组件，借着这次机会，我尽量把这个组件摸摸透&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Lucene" scheme="http://yoursite.com/categories/Apache/Lucene/"/>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/Apache/Lucene/ElasticSearch/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>Kudu测试报告</title>
    <link href="http://yoursite.com/2019/06/21/kudu_test/"/>
    <id>http://yoursite.com/2019/06/21/kudu_test/</id>
    <published>2019-06-21T08:46:24.000Z</published>
    <updated>2020-04-10T17:09:21.513Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>数据模拟完成后的 Kudu 对比性能测试</p></blockquote><a id="more"></a> <h3 id="测试介绍"><a href="#测试介绍" class="headerlink" title="测试介绍"></a>测试介绍</h3><p>TPCDS 和 TPCH 是专门为超大数据量设置的测试项目，下面的是Kudu官网上官方用TPCH测试的结果：</p><p><img src="https://o59mpa.by.files.1drv.com/y4mA8w9Pf83M66l791AicbzEiuNzwVX4g4cBec-VP___6-UAnjPtWMl-GL4worC01W_SVpnXxhqIib-CTLsjnNi0DAjav_B5FpF9JVfWq1dgF0YZwb8SPSGgZl88X96ZhvR9-AysqpK3a6Wbt5L_oFf1L-JabZic5epZcecBgL_Hs937A5vMe_Y9HnWoepnsnctIat0QIijPkQsXI8aulx7yg?width=574&amp;height=258&amp;cropmode=none" alt></p><p>我自己做的表格也是模仿的<a href="https://kudu.apache.org/overview.html" target="_blank" rel="noopener">官方的图表</a>格式。</p><p>介绍一下这个图表的含义，TPCH测试中含有很多超复杂SQL，用不同的数据库在相同数据下运行了这些语句之后，然后对比运行时间，图表横坐标是SQL的序号，纵坐标是运行时间，官方的运行环境是75节点的，单位是毫秒。</p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>下面是我做的测试，三节点下的kudu，机器配置都是64G内存，16核心，系统是Centos 7.4，Kudu版本是1.5，SQL是用impala+Kudu的形式运行的，测试数据量是用TPCDS生成的100G数据。</p><p><img src="https://oj9gpa.by.files.1drv.com/y4mMCX7UM5aW2Q-AcVklQJrf5ZZ72ypNIIrCkO1UakfnMLQ7gfq3KudD80TWG0CpUUJ2zR_aAEiMTPKznwaIcKpLDxRnoYpgnnuyQ-uMGT0zuVrrOvpG3zyPP2CZKVXcS-v1bPxU3xZ1etv2t_9e-lDnftN2cYqM29n_OIZPWOx6NF7jo5EPXx9FOLyipipqNZdR2eVhoHutBQSIMrW9xyXVw?width=1443&amp;height=691&amp;cropmode=none" alt></p><p>我的测试相比官方的添加了文本格式的HiveSQL测试，测试下来确实是速度最慢的。</p><p>Kudu和parquet各有千秋，和官方的测试结果相差不大。</p><h3 id="写入性能"><a href="#写入性能" class="headerlink" title="写入性能"></a>写入性能</h3><p>Kudu的写入性能测试的时候没有能够完全发挥出来，短板不在kudu的写入，这边的写入测试是我用脚本统计5分钟内表多出来的行数然后除以600s得到的，这边截取一段（因为是5分钟统计一次，图表的表述可能不够准确）</p><p><img src="https://o59lpa.by.files.1drv.com/y4mTSkFgZzq__Ts2m5ZTdUr9MBUPBEGXJnEzp3ss_3cDbmVjyxh3gMkQnD0ZBVa5AGdeACWzycw83MmAoszRmiMgdFZgFrbviTB7gckxqw0fTDGuohEWxSo2npNc90L6oRZA1b7l5EizbDLEjJTlpTnWmLtu6dPnJeNxOwdKdPgYziqEisqyfE7rLS0Ekk3yEOgii45aCJ_hfZ2QU99_71yOw?width=476&amp;height=286&amp;cropmode=none" alt></p><p>监测过程中得到的最大写入速度在9.5w/s左右，大部分时候速度在3.5w条到5w条之间。</p><p>后来在开启多线程写入kudu的时候，kudu的写入速度很轻松就能达到20w/s，但是可能因为资源不足，kudu的tablet Server容易挂掉，因为impala也是非常吃内存的组件，64G有点捉襟见肘。</p><h4 id="kudu写入速度和表格大小的关系"><a href="#kudu写入速度和表格大小的关系" class="headerlink" title="kudu写入速度和表格大小的关系"></a>kudu写入速度和表格大小的关系</h4><p>kudu的这个写入速度和表格大小有直接关系。</p><p>结论：</p><p><strong>kudu导入小表的速度十分快，但是导入大表的时候性能会严重下降。</strong></p><p>为了避免测试误差，在用tpcds生成不同大小的测试样本中取相同表格不同大小来测试，避免阻断等等误差，测试结果如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g47cbzp1cuj20d407vdfp.jpg" alt></p><p>根据测试结果基本可以判断，我们测试集群环境下，小表的导入速度明显要比大表大很多。</p><p>类似的，我们根据TPCDS生成的不同大小的数据样本，分析数据量大小对kudu的影响（这边的数据来源网易的报告，1T的因为测试集群环境原因没有测试）：</p><p>100G:</p><p><img src="http://wx3.sinaimg.cn/large/bec9bff2gy1g47lubfzqyj20r80hzacf.jpg" alt></p><p>1T:</p><p><img src="http://wx3.sinaimg.cn/large/bec9bff2gy1g47lu7zq04j20qy0hbq3l.jpg" alt></p><p>根据上面两张折线图可以看出，随着表的数据量变成巨大，kudu和parquet的之间的性能也被拉开了。</p><p>10T:</p><p><img src="http://wx2.sinaimg.cn/large/bec9bff2gy1g47m9zhf2dj20p20h6gm3.jpg" alt></p><p>1T的和10T的相差不是特别大</p><p>结论：<strong>kudu表目前看来不太适合大表，分区能否解决这个问题，还要靠实验</strong></p><h4 id="资源使用情况"><a href="#资源使用情况" class="headerlink" title="资源使用情况"></a>资源使用情况</h4><p>Impala使用的资源整体上少于Spark，磁盘的读取少于Spark，这对于速度的提高至关重要，这与其语句的优化有关。Impala的CPU一直维持在较低的水平，说明其C++的实现比Java高效。</p><h4 id="kudu写入速度"><a href="#kudu写入速度" class="headerlink" title="kudu写入速度"></a>kudu写入速度</h4><p>Spark的CPU占用较高，但是维持在50%的水平，可见CPU并没有成为其瓶颈，在使用Oozie多线程写入的时候可能遇到了kudu的瓶颈，Kudu的写入瓶颈是可以通过一些参数进行简单调整的。</p><p>目前从集群的写入速度上来分析，初步判断硬盘的写入速度已经成为了瓶颈。</p><p>下图以Master节点为例，列出的Kudu TS以6个小时为一个窗口使用磁盘的峰值和平均值：</p><p><img src="https://arbgdq.by.files.1drv.com/y4mcprp_tVDZU3zFzgcmpoJtNJKwwr5PEVDcLXL_K-4D9X9ST4Vru8dYseDZ8sUPMTd7LwyKYe7MQI1fkaeXZGZG6ZyHTMGE1OvqLVCIJOBpK1NFAbatJZ6fmTTlvZjc6fFYT_8cCFOLjYhbWKeqbalRJ70sWjBHvRgqcnyyKKvhFltneMaGeQx4iunOQC-yk5_p-KmD3H_8tko7WCCj45ZDw?width=451&amp;height=289&amp;cropmode=none" alt></p><p>MiB单位换算成Mbit/s是Mbit/s = MiB/s * 0.1192，图中所示峰值几乎达到了机械硬盘写入极限。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;数据模拟完成后的 Kudu 对比性能测试&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/categories/Apache/Kudu/"/>
    
    
      <category term="Kudu" scheme="http://yoursite.com/tags/Kudu/"/>
    
  </entry>
  
  <entry>
    <title>SQL积累</title>
    <link href="http://yoursite.com/2019/06/12/SQL/"/>
    <id>http://yoursite.com/2019/06/12/SQL/</id>
    <published>2019-06-12T08:50:43.000Z</published>
    <updated>2019-06-12T08:50:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>SQL看似简单其实也包含了相当多的内容</p><p>慢慢积累吧，最近状态不咋好，一点点来</p><a id="more"></a> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找最晚入职员工的所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">--有个答案是</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">0</span>,<span class="number">1</span></span><br><span class="line"><span class="comment">--但是这个答案有个问题，当一天由多个同事入职的时候会出现歧义</span></span><br><span class="line"><span class="comment">--所以用下面的方法是绝对正确的</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees <span class="keyword">where</span> hire_date = (<span class="keyword">select</span> <span class="keyword">max</span>(hire_date) <span class="keyword">from</span> employees)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找入职员工时间排名倒数第三的员工所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--LIMIT m,n : 表示从第m+1条开始，取n条数据；</span></span><br><span class="line"><span class="comment">--LIMIT n ： 表示从第0条开始，取n条数据，是limit(0,n)的缩写。</span></span><br><span class="line"><span class="comment">--考察点是limit的用法</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--与此同时还有一种想法觉得入职日期，只要是同一天的也就不分前后，也就是说，题目转换为了倒数三天前入职的所有同事。</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees </span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> hire_date </span><br><span class="line">     <span class="keyword">from</span> employees </span><br><span class="line">     <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">DESC</span> </span><br><span class="line">     <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">--同时考虑到distinct效率问题还可以改用group by</span></span><br><span class="line"><span class="comment">--经过测试，这种写法确实比上面的效率要高一点，同时应该要注意到这个应该和数据量也有关系</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">(<span class="keyword">select</span> hire_date</span><br><span class="line">    <span class="keyword">from</span> employees</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> hire_date</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找各个部门当前(to_date='9999-01-01')领导当前薪水详情以及其对应部门编号dept_no</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_manager`</span> (</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br><span class="line"><span class="comment">--要求输出格式：</span></span><br><span class="line"><span class="comment">--emp_nosalaryfrom_dateto_datedept_no</span></span><br><span class="line"><span class="comment">--答案一：先在两个表里用where过滤出现任的人选，然后用相等简单相等关联即可。</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.*, dm.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s,</span><br><span class="line">    dept_manager <span class="keyword">as</span> dm</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    dm.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    s.emp_no = dm.emp_no;</span><br><span class="line"><span class="comment">--答案二：</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.* , d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">    dept_manager <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    s.emp_no = d.emp_no</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    d.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="comment">--此题比较坑，限制了两个to_date，是因为薪水可能会变，人员也可能会变。</span></span><br><span class="line">然后两个表的前后位置不能动，否则和输出不符，姑且理解为必须小表<span class="keyword">join</span>大表吧。</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有已经分配部门的员工的last_name和first_name</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="comment">--我首先考虑的是没有使用join的情况</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d, employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    d.emp_no = e.emp_no</span><br><span class="line"><span class="comment">--其实从效率方面考虑，使用join会不会好一点，好像使用自然连接不用on就可以</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">natural</span> <span class="keyword">join</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--下面这道还是类似的</span></span><br><span class="line"><span class="comment">--查找所有员工的last_name和first_name以及对应部门编号dept_no，也包括展示没有分配具体部门的员工</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    e.emp_no = d.emp_no</span><br><span class="line"><span class="comment">--简单的left join</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有员工入职时候的薪水情况，给出emp_no以及salary， 并按照emp_no进行逆序</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL看似简单其实也包含了相当多的内容&lt;/p&gt;
&lt;p&gt;慢慢积累吧，最近状态不咋好，一点点来&lt;/p&gt;
    
    </summary>
    
      <category term="SQL" scheme="http://yoursite.com/categories/SQL/"/>
    
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>Analog Data With TPCDS &amp; TPCH</title>
    <link href="http://yoursite.com/2019/06/06/analog_data/"/>
    <id>http://yoursite.com/2019/06/06/analog_data/</id>
    <published>2019-06-06T06:39:01.000Z</published>
    <updated>2020-04-10T17:05:22.559Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段<br>本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同</p></blockquote><a id="more"></a> <h3 id="大神fayson的日志："><a href="#大神fayson的日志：" class="headerlink" title="大神fayson的日志："></a>大神<code>fayson</code>的日志：</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488108&amp;idx=1&amp;sn=8f34c674bc12990d61a8f4de4ca3c728&amp;chksm=ec2ac265db5d4b731b93c4b7da0b3a24f0bf200274dd763531873bb4dd205e37d704ea2719b6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用TPC-DS生成测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488190&amp;idx=1&amp;sn=3f34824bdadbfa0823823121f86cafd4&amp;chksm=ec2ac2b7db5d4ba1484d6a6cf3161fdb90798d2605d2e79fa8bf633d32766c42cc8e2b41e206&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用hive-testbench生成Hive基准测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247489095&amp;idx=1&amp;sn=5af481742664f79146c58f425c9429d3&amp;chksm=ec2ac64edb5d4f58860db96ae4b452fda70b108527e4cc78c1978461ed62e67fcf997631d270&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Impala TPC-DS基准测试</a></p><h3 id="一、遇到的问题"><a href="#一、遇到的问题" class="headerlink" title="一、遇到的问题"></a>一、遇到的问题</h3><h4 id="1-源码无法编译"><a href="#1-源码无法编译" class="headerlink" title="1.源码无法编译"></a>1.源码无法编译</h4><p>源码下载下来之后build，需要的组件根本下载不了</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330ii05j5j20tt0bt75b.jpg" alt></p><p>这里Google到了一个办法</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330lkku7yj20tn0a7wf6.jpg" alt></p><p>先把包下载下来，放进对应的文件夹里然后编译</p><h4 id="2-安装遇到的问题"><a href="#2-安装遇到的问题" class="headerlink" title="2.安装遇到的问题"></a>2.安装遇到的问题</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330stw7enj20of0ebdgx.jpg" alt></p><p>类似配置冲突的问题 不知道为什么</p><p>yes和no我都分别选过，但是都不对，配置完成之后执行都有问题</p><p>我初步怀疑可能是版本问题，我下一个旧版本的试一试</p><p><a href="http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp" target="_blank" rel="noopener">TPC下载地址</a></p><p>之前用的是V 2.11的，现在下载一个V 2.10.1的试一下</p><p>执行完毕之后，首先报错</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g335c98sa0j20ns0g2tac.jpg" alt></p><p>权限不够，我重新使用hdfs用户来创建目录</p><p><code>hdfs</code>用户没有办法<code>git clone</code></p><p>我使用了<code>root</code>用户<code>clone</code>之后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown -R hdfs:hdfs hive-testbench/</span><br><span class="line">chmod -R 777 hive-testbench/</span><br></pre></td></tr></table></figure><p>将权限开放</p><p>其余操作使用HDFS完成</p><p>。。。</p><p>等了一段时间</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g336c9kby8j20rx05ydfz.jpg" alt></p><p>MR正常运行没有问题，可以MR运行完毕之后还是报错，不知道为什么</p><p>中间又做了很多尝试，失败的尝试在这不做记录</p><p>重点记录一下我在BUG日志中发现HiveServer2有一些问题</p><p>Google之后发现了是因为配置里面出现了问题<br>Java 8里面用原先的配置代码已经被舍弃了，更改完毕之后解决了这个问题，</p><p>但是<code>TPCDS</code>的问题还是没有解决，吐出一口老血</p><p>验证<code>HiveServer2</code>正确开启的代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> /usr/lib/hive/bin/beeline</span><br><span class="line"><span class="meta">beeline&gt;</span> !connect jdbc:hive2://localhost:10000 username password org.apache.hive.jdbc.HiveDriver</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt; SHOW TABLES;</span><br><span class="line">show tables;</span><br><span class="line">+-----------+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+</span><br><span class="line">+-----------+</span><br><span class="line">No rows selected (0.238 seconds)</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt;</span><br></pre></td></tr></table></figure><p>现在我解决问题的点还在于是不是<code>CDH</code>的配置还有一些问题</p><p>但是<code>TPCH</code>明明又能够生成数据的，难顶了</p><h4 id="3-数据从Hive转入Kudu速度过慢"><a href="#3-数据从Hive转入Kudu速度过慢" class="headerlink" title="3.数据从Hive转入Kudu速度过慢"></a>3.数据从Hive转入Kudu速度过慢</h4><p>从周五下班的点开始到周一上班，1000个Tasks，仅仅完成了210个，速度十分之慢。</p><hr><h3 id="二、解决办法"><a href="#二、解决办法" class="headerlink" title="二、解决办法"></a>二、解决办法</h3><h4 id="1-总结问题"><a href="#1-总结问题" class="headerlink" title="1.总结问题"></a>1.总结问题</h4><p>好好想了下我自己遇到的错误，有几个点，第一个点是<code>TPCH</code>是可以生成数据的，第二个点是我在编译<code>TPCDS</code>源码的过程中，报出了奇怪的提示，我一直怀疑可能是我编译的时候除了问题，但是重新编译了好几遍，一直没有找到解决办法。</p><p>这边在<a href="https://blog.csdn.net/sinat_36300982/article/details/89556220" target="_blank" rel="noopener">另一个技术博客上</a>找到了解决方案，可以在本地编译完成之后再上传到服务器，但是我看了一下这篇博客，他是用的官方原版的<code>hive-testbench</code>，里面会有一些错误，我直接下载了别人使用的版本hive14.zip(可以在TIM上下载)，然后<a href="http://dev.hortonworks.com.s3.amazonaws.com/hive-testbench/tpcds/TPCDS_Tools.zip" target="_blank" rel="noopener">下载TPCDS_Tools.zip</a>改名<code>tpcds_kit.zip</code>放进<code>tpcds</code>对应的文件夹就可以了，最后编译成功。</p><p>编译完成之后，数据在Hive上面，Hive上面生成了两个库，一个是<code>ORC</code>库，还有一个是TEXT库，<code>ORC</code>文件<code>impala</code>用不了就算了，迁移TEXT就行，代码可以在下面的<code>github</code>中找到，然后要注意的事情是最后<code>package</code>的代码，因为是<code>scala</code>，打包的代码和别的并不一样</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">finalName</span>&gt;</span>anlogSparkSQL<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 设置项目编译版本--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 用于编译scala代码到class --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>kuduimport.hiveToKudu<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用HUE里面的<code>Oozie</code>调用Spark程序的时候，如果想要在spark提交里面出现任务记录，应该添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.shuffle.memoryFraction=0.3</span><br><span class="line">--conf spark.yarn.historyServer.address=http://datanode127:18089</span><br><span class="line">--conf spark.eventLog.dir=hdfs://master126:8020/user/spark/spark2ApplicationHistory</span><br><span class="line">--conf spark.eventLog.enabled=true</span><br></pre></td></tr></table></figure><p><a href="https://github.com/YunKillerE/kudu-learning" target="_blank" rel="noopener">github/kudu-learning</a></p><hr><h4 id="2-自动生成Kudu表格脚本"><a href="#2-自动生成Kudu表格脚本" class="headerlink" title="2.自动生成Kudu表格脚本"></a>2.自动生成Kudu表格脚本</h4><p>脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists call_center;</span><br><span class="line"></span><br><span class="line">create table call_center(</span><br><span class="line">      cc_call_center_sk         bigint               </span><br><span class="line">,     cc_call_center_id         string              </span><br><span class="line">,     cc_rec_start_date        string                         </span><br><span class="line">,     cc_rec_end_date          string                         </span><br><span class="line">,     cc_closed_date_sk         bigint                       </span><br><span class="line">,     cc_open_date_sk           bigint                       </span><br><span class="line">,     cc_name                   string                   </span><br><span class="line">,     cc_class                  string                   </span><br><span class="line">,     cc_employees              int                       </span><br><span class="line">,     cc_sq_ft                  int                       </span><br><span class="line">,     cc_hours                  string                      </span><br><span class="line">,     cc_manager                string                   </span><br><span class="line">,     cc_mkt_id                 int                       </span><br><span class="line">,     cc_mkt_class              string                      </span><br><span class="line">,     cc_mkt_desc               string                  </span><br><span class="line">,     cc_market_manager         string                   </span><br><span class="line">,     cc_division               int                       </span><br><span class="line">,     cc_division_name          string                   </span><br><span class="line">,     cc_company                int                       </span><br><span class="line">,     cc_company_name           string                      </span><br><span class="line">,     cc_street_number          string                      </span><br><span class="line">,     cc_street_name            string                   </span><br><span class="line">,     cc_street_type            string                      </span><br><span class="line">,     cc_suite_number           string                      </span><br><span class="line">,     cc_city                   string                   </span><br><span class="line">,     cc_county                 string                   </span><br><span class="line">,     cc_state                  string                       </span><br><span class="line">,     cc_zip                    string                      </span><br><span class="line">,     cc_country                string                   </span><br><span class="line">,     cc_gmt_offset             double                  </span><br><span class="line">,     cc_tax_percentage         double</span><br><span class="line">,PRIMARY KEY(cc_call_center_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_page;</span><br><span class="line"></span><br><span class="line">create table catalog_page(</span><br><span class="line">      cp_catalog_page_sk        bigint               </span><br><span class="line">,     cp_catalog_page_id        string              </span><br><span class="line">,     cp_start_date_sk          bigint                       </span><br><span class="line">,     cp_end_date_sk            bigint                       </span><br><span class="line">,     cp_department             string                   </span><br><span class="line">,     cp_catalog_number         int                       </span><br><span class="line">,     cp_catalog_page_number    int                       </span><br><span class="line">,     cp_description            string                  </span><br><span class="line">,     cp_type                   string</span><br><span class="line">,PRIMARY KEY(cp_catalog_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_returns;</span><br><span class="line"></span><br><span class="line">create table catalog_returns</span><br><span class="line">(</span><br><span class="line">    cr_item_sk                bigint,</span><br><span class="line">cr_order_number           bigint,</span><br><span class="line">    cr_returned_date_sk       bigint,</span><br><span class="line">    cr_returned_time_sk       bigint,</span><br><span class="line">    cr_refunded_customer_sk   bigint,</span><br><span class="line">    cr_refunded_cdemo_sk      bigint,</span><br><span class="line">    cr_refunded_hdemo_sk      bigint,</span><br><span class="line">    cr_refunded_addr_sk       bigint,</span><br><span class="line">    cr_returning_customer_sk  bigint,</span><br><span class="line">    cr_returning_cdemo_sk     bigint,</span><br><span class="line">    cr_returning_hdemo_sk     bigint,</span><br><span class="line">    cr_returning_addr_sk      bigint,</span><br><span class="line">    cr_call_center_sk         bigint,</span><br><span class="line">    cr_catalog_page_sk        bigint,</span><br><span class="line">    cr_ship_mode_sk           bigint,</span><br><span class="line">    cr_warehouse_sk           bigint,</span><br><span class="line">    cr_reason_sk              bigint,</span><br><span class="line">    cr_return_quantity        int,</span><br><span class="line">    cr_return_amount          double,</span><br><span class="line">    cr_return_tax             double,</span><br><span class="line">    cr_return_amt_inc_tax     double,</span><br><span class="line">    cr_fee                    double,</span><br><span class="line">    cr_return_ship_cost       double,</span><br><span class="line">    cr_refunded_cash          double,</span><br><span class="line">    cr_reversed_charge        double,</span><br><span class="line">    cr_store_credit           double,</span><br><span class="line">    cr_net_loss               double</span><br><span class="line">,PRIMARY KEY(cr_item_sk,cr_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cr_item_sk) PARTITIONS 16</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_sales;</span><br><span class="line"></span><br><span class="line">create table catalog_sales</span><br><span class="line">(</span><br><span class="line">    cs_item_sk                bigint,</span><br><span class="line">    cs_order_number           bigint,</span><br><span class="line">    cs_sold_date_sk           bigint,</span><br><span class="line">    cs_sold_time_sk           bigint,</span><br><span class="line">    cs_ship_date_sk           bigint,</span><br><span class="line">    cs_bill_customer_sk       bigint,</span><br><span class="line">    cs_bill_cdemo_sk          bigint,</span><br><span class="line">    cs_bill_hdemo_sk          bigint,</span><br><span class="line">    cs_bill_addr_sk           bigint,</span><br><span class="line">    cs_ship_customer_sk       bigint,</span><br><span class="line">    cs_ship_cdemo_sk          bigint,</span><br><span class="line">    cs_ship_hdemo_sk          bigint,</span><br><span class="line">    cs_ship_addr_sk           bigint,</span><br><span class="line">    cs_call_center_sk         bigint,</span><br><span class="line">    cs_catalog_page_sk        bigint,</span><br><span class="line">    cs_ship_mode_sk           bigint,</span><br><span class="line">    cs_warehouse_sk           bigint,</span><br><span class="line">    cs_promo_sk               bigint,</span><br><span class="line">    cs_quantity               int,</span><br><span class="line">    cs_wholesale_cost         double,</span><br><span class="line">    cs_list_price             double,</span><br><span class="line">    cs_sales_price            double,</span><br><span class="line">    cs_ext_discount_amt       double,</span><br><span class="line">    cs_ext_sales_price        double,</span><br><span class="line">    cs_ext_wholesale_cost     double,</span><br><span class="line">    cs_ext_list_price         double,</span><br><span class="line">    cs_ext_tax                double,</span><br><span class="line">    cs_coupon_amt             double,</span><br><span class="line">    cs_ext_ship_cost          double,</span><br><span class="line">    cs_net_paid               double,</span><br><span class="line">    cs_net_paid_inc_tax       double,</span><br><span class="line">    cs_net_paid_inc_ship      double,</span><br><span class="line">    cs_net_paid_inc_ship_tax  double,</span><br><span class="line">    cs_net_profit             double</span><br><span class="line">,PRIMARY KEY(cs_item_sk,cs_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cs_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_address;</span><br><span class="line"></span><br><span class="line">create table customer_address</span><br><span class="line">(</span><br><span class="line">    ca_address_sk             bigint,</span><br><span class="line">    ca_address_id             string,</span><br><span class="line">    ca_street_number          string,</span><br><span class="line">    ca_street_name            string,</span><br><span class="line">    ca_street_type            string,</span><br><span class="line">    ca_suite_number           string,</span><br><span class="line">    ca_city                   string,</span><br><span class="line">    ca_county                 string,</span><br><span class="line">    ca_state                  string,</span><br><span class="line">    ca_zip                    string,</span><br><span class="line">    ca_country                string,</span><br><span class="line">    ca_gmt_offset             double,</span><br><span class="line">    ca_location_type          string</span><br><span class="line">,PRIMARY KEY(ca_address_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ca_address_sk) PARTITIONS 6</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_demographics;</span><br><span class="line"></span><br><span class="line">create table customer_demographics</span><br><span class="line">(</span><br><span class="line">    cd_demo_sk                bigint,</span><br><span class="line">    cd_gender                 string,</span><br><span class="line">    cd_marital_status         string,</span><br><span class="line">    cd_education_status       string,</span><br><span class="line">    cd_purchase_estimate      int,</span><br><span class="line">    cd_credit_rating          string,</span><br><span class="line">    cd_dep_count              int,</span><br><span class="line">    cd_dep_employed_count     int,</span><br><span class="line">    cd_dep_college_count      int </span><br><span class="line">,PRIMARY KEY(cd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer;</span><br><span class="line"></span><br><span class="line">create table customer</span><br><span class="line">(</span><br><span class="line">    c_customer_sk             bigint,</span><br><span class="line">    c_customer_id             string,</span><br><span class="line">    c_current_cdemo_sk        bigint,</span><br><span class="line">    c_current_hdemo_sk        bigint,</span><br><span class="line">    c_current_addr_sk         bigint,</span><br><span class="line">    c_first_shipto_date_sk    bigint,</span><br><span class="line">    c_first_sales_date_sk     bigint,</span><br><span class="line">    c_salutation              string,</span><br><span class="line">    c_first_name              string,</span><br><span class="line">    c_last_name               string,</span><br><span class="line">    c_preferred_cust_flag     string,</span><br><span class="line">    c_birth_day               int,</span><br><span class="line">    c_birth_month             int,</span><br><span class="line">    c_birth_year              int,</span><br><span class="line">    c_birth_country           string,</span><br><span class="line">    c_login                   string,</span><br><span class="line">    c_email_address           string,</span><br><span class="line">    c_last_review_date        string</span><br><span class="line">,PRIMARY KEY(c_customer_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (c_customer_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists date_dim;</span><br><span class="line"></span><br><span class="line">create table date_dim</span><br><span class="line">(</span><br><span class="line">    d_date_sk                 bigint,</span><br><span class="line">    d_date_id                 string,</span><br><span class="line">    d_date                    string,</span><br><span class="line">    d_month_seq               int,</span><br><span class="line">    d_week_seq                int,</span><br><span class="line">    d_quarter_seq             int,</span><br><span class="line">    d_year                    int,</span><br><span class="line">    d_dow                     int,</span><br><span class="line">    d_moy                     int,</span><br><span class="line">    d_dom                     int,</span><br><span class="line">    d_qoy                     int,</span><br><span class="line">    d_fy_year                 int,</span><br><span class="line">    d_fy_quarter_seq          int,</span><br><span class="line">    d_fy_week_seq             int,</span><br><span class="line">    d_day_name                string,</span><br><span class="line">    d_quarter_name            string,</span><br><span class="line">    d_holiday                 string,</span><br><span class="line">    d_weekend                 string,</span><br><span class="line">    d_following_holiday       string,</span><br><span class="line">    d_first_dom               int,</span><br><span class="line">    d_last_dom                int,</span><br><span class="line">    d_same_day_ly             int,</span><br><span class="line">    d_same_day_lq             int,</span><br><span class="line">    d_current_day             string,</span><br><span class="line">    d_current_week            string,</span><br><span class="line">    d_current_month           string,</span><br><span class="line">    d_current_quarter         string,</span><br><span class="line">    d_current_year            string </span><br><span class="line">,PRIMARY KEY(d_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (d_date_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists household_demographics;</span><br><span class="line"></span><br><span class="line">create table household_demographics</span><br><span class="line">(</span><br><span class="line">    hd_demo_sk                bigint,</span><br><span class="line">    hd_income_band_sk         bigint,</span><br><span class="line">    hd_buy_potential          string,</span><br><span class="line">    hd_dep_count              int,</span><br><span class="line">    hd_vehicle_count          int</span><br><span class="line">,PRIMARY KEY(hd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (hd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists income_band;</span><br><span class="line"></span><br><span class="line">create table income_band(</span><br><span class="line">      ib_income_band_sk         bigint               </span><br><span class="line">,     ib_lower_bound            int                       </span><br><span class="line">,     ib_upper_bound            int</span><br><span class="line">,PRIMARY KEY(ib_income_band_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ib_income_band_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists inventory;</span><br><span class="line"></span><br><span class="line">create table inventory</span><br><span class="line">(</span><br><span class="line">    inv_date_skbigint,</span><br><span class="line">    inv_item_skbigint,</span><br><span class="line">    inv_warehouse_skbigint,</span><br><span class="line">    inv_quantity_on_handint</span><br><span class="line">,PRIMARY KEY(inv_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (inv_date_sk) PARTITIONS 12</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists item;</span><br><span class="line"></span><br><span class="line">create table item</span><br><span class="line">(</span><br><span class="line">    i_item_sk                 bigint,</span><br><span class="line">    i_item_id                 string,</span><br><span class="line">    i_rec_start_date          string,</span><br><span class="line">    i_rec_end_date            string,</span><br><span class="line">    i_item_desc               string,</span><br><span class="line">    i_current_price           double,</span><br><span class="line">    i_wholesale_cost          double,</span><br><span class="line">    i_brand_id                int,</span><br><span class="line">    i_brand                   string,</span><br><span class="line">    i_class_id                int,</span><br><span class="line">    i_class                   string,</span><br><span class="line">    i_category_id             int,</span><br><span class="line">    i_category                string,</span><br><span class="line">    i_manufact_id             int,</span><br><span class="line">    i_manufact                string,</span><br><span class="line">    i_size                    string,</span><br><span class="line">    i_formulation             string,</span><br><span class="line">    i_color                   string,</span><br><span class="line">    i_units                   string,</span><br><span class="line">    i_container               string,</span><br><span class="line">    i_manager_id              int,</span><br><span class="line">    i_product_name            string</span><br><span class="line">,PRIMARY KEY(i_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (i_item_sk) PARTITIONS 4</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists promotion;</span><br><span class="line"></span><br><span class="line">create table promotion</span><br><span class="line">(</span><br><span class="line">    p_promo_sk                bigint,</span><br><span class="line">    p_promo_id                string,</span><br><span class="line">    p_start_date_sk           bigint,</span><br><span class="line">    p_end_date_sk             bigint,</span><br><span class="line">    p_item_sk                 bigint,</span><br><span class="line">    p_cost                    double,</span><br><span class="line">    p_response_target         int,</span><br><span class="line">    p_promo_name              string,</span><br><span class="line">    p_channel_dmail           string,</span><br><span class="line">    p_channel_email           string,</span><br><span class="line">    p_channel_catalog         string,</span><br><span class="line">    p_channel_tv              string,</span><br><span class="line">    p_channel_radio           string,</span><br><span class="line">    p_channel_press           string,</span><br><span class="line">    p_channel_event           string,</span><br><span class="line">    p_channel_demo            string,</span><br><span class="line">    p_channel_details         string,</span><br><span class="line">    p_purpose                 string,</span><br><span class="line">    p_discount_active         string </span><br><span class="line">,PRIMARY KEY(p_promo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (p_promo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists reason;</span><br><span class="line"></span><br><span class="line">create table reason(</span><br><span class="line">      r_reason_sk               bigint               </span><br><span class="line">,     r_reason_id               string              </span><br><span class="line">,     r_reason_desc             string                </span><br><span class="line">,PRIMARY KEY(r_reason_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (r_reason_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists ship_mode;</span><br><span class="line"></span><br><span class="line">create table ship_mode(</span><br><span class="line">      sm_ship_mode_sk           bigint               </span><br><span class="line">,     sm_ship_mode_id           string              </span><br><span class="line">,     sm_type                   string                      </span><br><span class="line">,     sm_code                   string                      </span><br><span class="line">,     sm_carrier                string                      </span><br><span class="line">,     sm_contract               string                      </span><br><span class="line">,PRIMARY KEY(sm_ship_mode_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (sm_ship_mode_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_returns;</span><br><span class="line"></span><br><span class="line">create table store_returns</span><br><span class="line">(</span><br><span class="line">    sr_item_sk                bigint,</span><br><span class="line">    sr_returned_date_sk       bigint,</span><br><span class="line">    sr_return_time_sk         bigint,</span><br><span class="line">    sr_customer_sk            bigint,</span><br><span class="line">    sr_cdemo_sk               bigint,</span><br><span class="line">    sr_hdemo_sk               bigint,</span><br><span class="line">    sr_addr_sk                bigint,</span><br><span class="line">    sr_store_sk               bigint,</span><br><span class="line">    sr_reason_sk              bigint,</span><br><span class="line">    sr_ticket_number          bigint,</span><br><span class="line">    sr_return_quantity        int,</span><br><span class="line">    sr_return_amt             double,</span><br><span class="line">    sr_return_tax             double,</span><br><span class="line">    sr_return_amt_inc_tax     double,</span><br><span class="line">    sr_fee                    double,</span><br><span class="line">    sr_return_ship_cost       double,</span><br><span class="line">    sr_refunded_cash          double,</span><br><span class="line">    sr_reversed_charge        double,</span><br><span class="line">    sr_store_credit           double,</span><br><span class="line">    sr_net_loss               double,</span><br><span class="line">PRIMARY KEY(sr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 32</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_sales;</span><br><span class="line"></span><br><span class="line">create table store_sales</span><br><span class="line">(</span><br><span class="line">    ss_item_sk                bigint,</span><br><span class="line">    ss_sold_date_sk           bigint,</span><br><span class="line">    ss_sold_time_sk           bigint,</span><br><span class="line">    ss_customer_sk            bigint,</span><br><span class="line">    ss_cdemo_sk               bigint,</span><br><span class="line">    ss_hdemo_sk               bigint,</span><br><span class="line">    ss_addr_sk                bigint,</span><br><span class="line">    ss_store_sk               bigint,</span><br><span class="line">    ss_promo_sk               bigint,</span><br><span class="line">    ss_ticket_number          bigint,</span><br><span class="line">    ss_quantity               int,</span><br><span class="line">    ss_wholesale_cost         double,</span><br><span class="line">    ss_list_price             double,</span><br><span class="line">    ss_sales_price            double,</span><br><span class="line">    ss_ext_discount_amt       double,</span><br><span class="line">    ss_ext_sales_price        double,</span><br><span class="line">    ss_ext_wholesale_cost     double,</span><br><span class="line">    ss_ext_list_price         double,</span><br><span class="line">    ss_ext_tax                double,</span><br><span class="line">    ss_coupon_amt             double,</span><br><span class="line">    ss_net_paid               double,</span><br><span class="line">    ss_net_paid_inc_tax       double,</span><br><span class="line">    ss_net_profit             double                  </span><br><span class="line">,PRIMARY KEY(ss_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ss_item_sk) PARTITIONS 96</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store;</span><br><span class="line"></span><br><span class="line">create table store</span><br><span class="line">(</span><br><span class="line">    s_store_sk                bigint,</span><br><span class="line">    s_store_id                string,</span><br><span class="line">    s_rec_start_date          string,</span><br><span class="line">    s_rec_end_date            string,</span><br><span class="line">    s_closed_date_sk          bigint,</span><br><span class="line">    s_store_name              string,</span><br><span class="line">    s_number_employees        int,</span><br><span class="line">    s_floor_space             int,</span><br><span class="line">    s_hours                   string,</span><br><span class="line">    s_manager                 string,</span><br><span class="line">    s_market_id               int,</span><br><span class="line">    s_geography_class         string,</span><br><span class="line">    s_market_desc             string,</span><br><span class="line">    s_market_manager          string,</span><br><span class="line">    s_division_id             int,</span><br><span class="line">    s_division_name           string,</span><br><span class="line">    s_company_id              int,</span><br><span class="line">    s_company_name            string,</span><br><span class="line">    s_street_number           string,</span><br><span class="line">    s_street_name             string,</span><br><span class="line">    s_street_type             string,</span><br><span class="line">    s_suite_number            string,</span><br><span class="line">    s_city                    string,</span><br><span class="line">    s_county                  string,</span><br><span class="line">    s_state                   string,</span><br><span class="line">    s_zip                     string,</span><br><span class="line">    s_country                 string,</span><br><span class="line">    s_gmt_offset              double,</span><br><span class="line">    s_tax_precentage          double                  </span><br><span class="line">,PRIMARY KEY(s_store_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (s_store_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists time_dim;</span><br><span class="line"></span><br><span class="line">create table time_dim</span><br><span class="line">(</span><br><span class="line">    t_time_sk                 bigint,</span><br><span class="line">    t_time_id                 string,</span><br><span class="line">    t_time                    int,</span><br><span class="line">    t_hour                    int,</span><br><span class="line">    t_minute                  int,</span><br><span class="line">    t_second                  int,</span><br><span class="line">    t_am_pm                   string,</span><br><span class="line">    t_shift                   string,</span><br><span class="line">    t_sub_shift               string,</span><br><span class="line">    t_meal_time               string</span><br><span class="line">,PRIMARY KEY(t_time_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (t_time_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists warehouse;</span><br><span class="line"></span><br><span class="line">create table warehouse(</span><br><span class="line">      w_warehouse_sk            bigint               </span><br><span class="line">,     w_warehouse_id            string              </span><br><span class="line">,     w_warehouse_name          string                   </span><br><span class="line">,     w_warehouse_sq_ft         int                       </span><br><span class="line">,     w_street_number           string                      </span><br><span class="line">,     w_street_name             string                   </span><br><span class="line">,     w_street_type             string                      </span><br><span class="line">,     w_suite_number            string                      </span><br><span class="line">,     w_city                    string                   </span><br><span class="line">,     w_county                  string                   </span><br><span class="line">,     w_state                   string                       </span><br><span class="line">,     w_zip                     string                      </span><br><span class="line">,     w_country                 string                   </span><br><span class="line">,     w_gmt_offset              double                  </span><br><span class="line">,PRIMARY KEY(w_warehouse_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (w_warehouse_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_page;</span><br><span class="line"></span><br><span class="line">create table web_page(</span><br><span class="line">      wp_web_page_sk            bigint               </span><br><span class="line">,     wp_web_page_id            string              </span><br><span class="line">,     wp_rec_start_date        string                         </span><br><span class="line">,     wp_rec_end_date          string                         </span><br><span class="line">,     wp_creation_date_sk       bigint                       </span><br><span class="line">,     wp_access_date_sk         bigint                       </span><br><span class="line">,     wp_autogen_flag           string                       </span><br><span class="line">,     wp_customer_sk            bigint                       </span><br><span class="line">,     wp_url                    string                  </span><br><span class="line">,     wp_type                   string                      </span><br><span class="line">,     wp_char_count             int                       </span><br><span class="line">,     wp_link_count             int                       </span><br><span class="line">,     wp_image_count            int                       </span><br><span class="line">,     wp_max_ad_count           int</span><br><span class="line">,PRIMARY KEY(wp_web_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wp_web_page_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_returns;</span><br><span class="line"></span><br><span class="line">create table web_returns</span><br><span class="line">(</span><br><span class="line">    wr_item_sk                bigint,</span><br><span class="line">    wr_returned_date_sk       bigint,</span><br><span class="line">    wr_returned_time_sk       bigint,</span><br><span class="line">    wr_refunded_customer_sk   bigint,</span><br><span class="line">    wr_refunded_cdemo_sk      bigint,</span><br><span class="line">    wr_refunded_hdemo_sk      bigint,</span><br><span class="line">    wr_refunded_addr_sk       bigint,</span><br><span class="line">    wr_returning_customer_sk  bigint,</span><br><span class="line">    wr_returning_cdemo_sk     bigint,</span><br><span class="line">    wr_returning_hdemo_sk     bigint,</span><br><span class="line">    wr_returning_addr_sk      bigint,</span><br><span class="line">    wr_web_page_sk            bigint,</span><br><span class="line">    wr_reason_sk              bigint,</span><br><span class="line">    wr_order_number           bigint,</span><br><span class="line">    wr_return_quantity        int,</span><br><span class="line">    wr_return_amt             double,</span><br><span class="line">    wr_return_tax             double,</span><br><span class="line">    wr_return_amt_inc_tax     double,</span><br><span class="line">    wr_fee                    double,</span><br><span class="line">    wr_return_ship_cost       double,</span><br><span class="line">    wr_refunded_cash          double,</span><br><span class="line">    wr_reversed_charge        double,</span><br><span class="line">    wr_account_credit         double,</span><br><span class="line">    wr_net_loss               double</span><br><span class="line">,PRIMARY KEY(wr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wr_item_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_sales;</span><br><span class="line"></span><br><span class="line">create table web_sales</span><br><span class="line">(</span><br><span class="line">    ws_item_sk                bigint,</span><br><span class="line">    ws_sold_date_sk           bigint,</span><br><span class="line">    ws_sold_time_sk           bigint,</span><br><span class="line">    ws_ship_date_sk           bigint,</span><br><span class="line">    ws_bill_customer_sk       bigint,</span><br><span class="line">    ws_bill_cdemo_sk          bigint,</span><br><span class="line">    ws_bill_hdemo_sk          bigint,</span><br><span class="line">    ws_bill_addr_sk           bigint,</span><br><span class="line">    ws_ship_customer_sk       bigint,</span><br><span class="line">    ws_ship_cdemo_sk          bigint,</span><br><span class="line">    ws_ship_hdemo_sk          bigint,</span><br><span class="line">    ws_ship_addr_sk           bigint,</span><br><span class="line">    ws_web_page_sk            bigint,</span><br><span class="line">    ws_web_site_sk            bigint,</span><br><span class="line">    ws_ship_mode_sk           bigint,</span><br><span class="line">    ws_warehouse_sk           bigint,</span><br><span class="line">    ws_promo_sk               bigint,</span><br><span class="line">    ws_order_number           bigint,</span><br><span class="line">    ws_quantity               int,</span><br><span class="line">    ws_wholesale_cost         double,</span><br><span class="line">    ws_list_price             double,</span><br><span class="line">    ws_sales_price            double,</span><br><span class="line">    ws_ext_discount_amt       double,</span><br><span class="line">    ws_ext_sales_price        double,</span><br><span class="line">    ws_ext_wholesale_cost     double,</span><br><span class="line">    ws_ext_list_price         double,</span><br><span class="line">    ws_ext_tax                double,</span><br><span class="line">    ws_coupon_amt             double,</span><br><span class="line">    ws_ext_ship_cost          double,</span><br><span class="line">    ws_net_paid               double,</span><br><span class="line">    ws_net_paid_inc_tax       double,</span><br><span class="line">    ws_net_paid_inc_ship      double,</span><br><span class="line">    ws_net_paid_inc_ship_tax  double,</span><br><span class="line">    ws_net_profit             double</span><br><span class="line">,PRIMARY KEY(ws_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ws_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_site;</span><br><span class="line"></span><br><span class="line">create table web_site</span><br><span class="line">(</span><br><span class="line">    web_site_sk           bigint,</span><br><span class="line">    web_site_id           string,</span><br><span class="line">    web_rec_start_date    string,</span><br><span class="line">    web_rec_end_date      string,</span><br><span class="line">    web_name              string,</span><br><span class="line">    web_open_date_sk      bigint,</span><br><span class="line">    web_close_date_sk     bigint,</span><br><span class="line">    web_class             string,</span><br><span class="line">    web_manager           string,</span><br><span class="line">    web_mkt_id            int,</span><br><span class="line">    web_mkt_class         string,</span><br><span class="line">    web_mkt_desc          string,</span><br><span class="line">    web_market_manager    string,</span><br><span class="line">    web_company_id        int,</span><br><span class="line">    web_company_name      string,</span><br><span class="line">    web_street_number     string,</span><br><span class="line">    web_street_name       string,</span><br><span class="line">    web_street_type       string,</span><br><span class="line">    web_suite_number      string,</span><br><span class="line">    web_city              string,</span><br><span class="line">    web_county            string,</span><br><span class="line">    web_state             string,</span><br><span class="line">    web_zip               string,</span><br><span class="line">    web_country           string,</span><br><span class="line">    web_gmt_offset        double,</span><br><span class="line">    web_tax_percentage    double</span><br><span class="line">,PRIMARY KEY(web_site_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (web_site_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br></pre></td></tr></table></figure><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">impala-shell -f impala-shell</span><br></pre></td></tr></table></figure><p>Tips：可能会遇到这样的错误</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR: ImpalaRuntimeException: Error creating Kudu table 'impala::kudu_spark_tpcds_2.catalog_sales'</span><br><span class="line">CAUSED BY: NonRecoverableException: The requested number of tablets is over the maximum permitted at creation time (60). Additional tablets may be added by adding range partitions to the table post-creation.</span><br></pre></td></tr></table></figure><p>原因：</p><p><code>Kudu</code>默认配置最多分区被限制了，需要配置</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3azpwvi62j21c10k9ta6.jpg" alt></p><p>如图栏目里，配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--max_create_tablets_per_ts=30</span><br></pre></td></tr></table></figure><p>生成日志后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail q* -n 1 &gt;&gt; kudu_time_2.log</span><br></pre></td></tr></table></figure><p>–</p><p>–</p><p>–</p><p>不知道为什么，<code>impala+kudu</code>对内存的管理存在一些问题，明明物理内存足够使用，却老是会用上交换内存。</p><p>测试性能下降，这里取消交换内存再试一次</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 取消交换内存：</span><br><span class="line">swapoff -a</span><br><span class="line">swapon -a</span><br></pre></td></tr></table></figure><p>parquet表格生成脚本  <code>alltables_parquet.sql</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> $&#123;<span class="keyword">VAR</span>:DB&#125; <span class="keyword">cascade</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">use</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">set</span> parquet_file_size=<span class="number">512</span>M;</span><br><span class="line"><span class="keyword">set</span> COMPRESSION_CODEC=snappy;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> call_center;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.call_center</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.call_center;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_page</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_page;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_address;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_address</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_address;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_demographics</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_demographics;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> date_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.date_dim</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.date_dim;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> household_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.household_demographics</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.household_demographics;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> income_band;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.income_band</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.income_band;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> inventory;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.inventory</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.inventory;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> item;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.item</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.item;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> promotion;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.promotion</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.promotion;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.reason</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.reason;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> ship_mode;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.ship_mode</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.ship_mode;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> <span class="keyword">store</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> time_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.time_dim</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.time_dim;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> warehouse;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.warehouse</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.warehouse;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_page</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_page;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_site;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_site</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_site;</span><br></pre></td></tr></table></figure><p>然后用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">impala-shell -i datanode127 --var=DB=tpcds_parquet_500 --var=HIVE_DB=tpcds_text_500 -f alltables_parquet.sql</span><br></pre></td></tr></table></figure><p>理论上也能这么生成<code>Kudu</code>表，只要用下面的语句。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> $&#123;<span class="keyword">VAR</span>:DB&#125; <span class="keyword">cascade</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">use</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> call_center;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.call_center</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cc_call_center_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.call_center;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_page</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cp_catalog_page_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_page;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cr_returned_date_sk,cr_returned_time_sk,cr_item_sk,cr_refunded_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(cr_returned_date_sk,cr_returned_time_sk,cr_item_sk,cr_refunded_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cs_sold_date_sk,cs_sold_time_sk,cs_ship_date_sk,cs_bill_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(cs_sold_date_sk,cs_sold_time_sk,cs_ship_date_sk,cs_bill_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_address;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_address</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ca_address_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_address;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_demographics</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cd_demo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_demographics;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (c_customer_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> date_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.date_dim</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (d_date_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.date_dim;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> household_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.household_demographics</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (hd_demo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.household_demographics;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> income_band;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.income_band</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ib_income_band_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.income_band;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> inventory;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.inventory</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (inv_date_sk,inv_item_sk,inv_warehouse_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(inv_date_sk,inv_item_sk,inv_warehouse_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.inventory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> item;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.item</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (i_item_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> promotion;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.promotion</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (p_promo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.promotion;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.reason</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (r_reason_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.reason;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> ship_mode;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.ship_mode</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (sm_ship_mode_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.ship_mode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (sr_returned_date_sk,sr_return_time_sk,sr_item_sk,sr_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(sr_returned_date_sk,sr_return_time_sk,sr_item_sk,sr_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> <span class="keyword">store</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (s_store_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> time_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.time_dim</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (t_time_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.time_dim;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> warehouse;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.warehouse</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (w_warehouse_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.warehouse;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_page</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (wp_web_page_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_page;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (wr_returned_date_sk,wr_returned_time_sk,wr_item_sk,wr_refunded_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(wr_returned_date_sk,wr_returned_time_sk,wr_item_sk,wr_refunded_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ws_sold_date_sk,ws_sold_time_sk,ws_ship_date_sk,ws_item_sk,ws_bill_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(ws_sold_date_sk,ws_sold_time_sk,ws_ship_date_sk,ws_item_sk,ws_bill_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_site;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_site</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (web_site_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_site;</span><br></pre></td></tr></table></figure><p>但是存在问题就是<code>Kudu</code>的表需要主键，并且主键需要放置在最前面，但是<code>tpcds</code>默认生成的表格无法把主键放在最前面，所以这样创建的表格主主键包含很多个key，所以还是用上面的方法。</p><hr><p><a href="https://blog.csdn.net/weixin_39478115/article/details/78469837" target="_blank" rel="noopener">kudu性能调优</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段&lt;br&gt;本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Apache" scheme="http://yoursite.com/categories/Apache/"/>
    
      <category term="Kudu" scheme="http://yoursite.com/categories/Apache/Kudu/"/>
    
    
      <category term="Analog Data" scheme="http://yoursite.com/tags/Analog-Data/"/>
    
  </entry>
  
  <entry>
    <title>Grokking Algorithms</title>
    <link href="http://yoursite.com/2019/05/30/Grokking%20Algorithms/"/>
    <id>http://yoursite.com/2019/05/30/Grokking Algorithms/</id>
    <published>2019-05-29T22:59:14.000Z</published>
    <updated>2020-04-10T17:08:33.561Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。</p><p>这边值得一提的是作者推荐了一个网站，可汗学院，<code>khanacademy.org</code>  mark一下。</p><p>看完40%来总结一下，非常好，文盲也能看懂的算法入门。</p><p>这本书看完应该会扫一眼结城浩的《图解密码学》</p></blockquote><a id="more"></a> <h2 id="第一章-算法简介"><a href="#第一章-算法简介" class="headerlink" title="第一章 算法简介"></a>第一章 算法简介</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w8rt2sg9j20fk026mxh.jpg" alt></p><p>好的，我具备了</p><h3 id="1-2-二分查找"><a href="#1-2-二分查找" class="headerlink" title="1.2 二分查找"></a>1.2 二分查找</h3><p>二分查找(binary search)又叫折半搜索(half-interval search)、对数搜索(logarithmic search)，是一种在<strong>有序数组</strong>中查找某一特定元素的搜索算法。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。</p><p>对数：幂运算的逆运算</p><p>假设你要在字典中查找一个单词，而该字典包含240000个单词，<br>你认为每种查找最多需要多少步？</p><p>log<sub>2</sub> n步，本题中就是18步</p><p>给定一个有序数组和一个需要定位的数字，先创建两个变量 low 和 high，low和high一开始分别是数组的第一个和最后一个坐标，划定一个取中间元素的空间，然后取出中间元素和目标元素比较，如果不是的话，就更改low或者high中某一个的坐标为(low + high)/2，将查找空间缩小为原来的二分之一，然后继续。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(list, item)</span>:</span></span><br><span class="line">  low = <span class="number">0</span></span><br><span class="line">  high = len(list) - <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">    mid = (low + high) // <span class="number">2</span></span><br><span class="line">    guess = list[mid]</span><br><span class="line">    <span class="keyword">if</span> guess == item:</span><br><span class="line">      <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">if</span> guess &gt; item:</span><br><span class="line">      high = mid <span class="number">-1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      low = mid + <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">my_list = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/notebook#create=true&amp;language=python3" target="_blank" rel="noopener">运行环境</a></p><p>Tips：关于为什么更换搜索区域的时候没有直接用high = mid 或者low = mid</p><p>注意while的条件，如果没有这一条，范围缩小到两个数的时候，会无限循环</p><h4 id="运行时间"><a href="#运行时间" class="headerlink" title="运行时间"></a>运行时间</h4><p>最多猜测次数与列表长度相同被称为线性时间(linear time).</p><p>二分查找的运行时间为对数时间(log time).</p><h3 id="1-3-大-O-表示法"><a href="#1-3-大-O-表示法" class="headerlink" title="1.3 大 O 表示法"></a>1.3 大 O 表示法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">O(1):             常量时间，哈希</span><br><span class="line">O(log2(n)):       对数时间，二分，</span><br><span class="line">O(n):             线性时间，简单</span><br><span class="line">O(nlog2(n)):              快速排序</span><br><span class="line">O(n2):                    选择排序（冒泡）</span><br><span class="line">O(n!):                    旅行商问题</span><br></pre></td></tr></table></figure><p>算法的速度指的并非时间，而是操作数的增速。</p><p>谈论算法的速度时，我们说的是随着输入的增加，其运行时间将以什么样的速度增加。</p><p>算法的运行时间用大O表示法表示。</p><p>O(log n)比O(n)快，当需要搜索的元素越多时，前者比后者快得越多。</p><h4 id="旅行商问题"><a href="#旅行商问题" class="headerlink" title="旅行商问题"></a>旅行商问题</h4><p>行商问题（最短路径问题）（英语：travelling salesman problem, TSP）是这样一个问题：给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。它是组合优化中的一个NP困难问题，在运筹学和理论计算机科学中非常重要。</p><hr><h2 id="第二章-选择排序"><a href="#第二章-选择排序" class="headerlink" title="第二章 选择排序"></a>第二章 选择排序</h2><h3 id="2-1-内存工作原理"><a href="#2-1-内存工作原理" class="headerlink" title="2.1 内存工作原理"></a>2.1 内存工作原理</h3><h3 id="2-2-数组和链表"><a href="#2-2-数组和链表" class="headerlink" title="2.2 数组和链表"></a>2.2 数组和链表</h3><p><strong>链表</strong>：不需要移动元素，优势在插入元素</p><p>使用链表在中间插入元素只需要修改前面一个元素指向的地址，因此当需要在中间插入的时候，链表是更好的选择。</p><p>删除也是一样</p><p>数组和链表的运行时间：</p><table><thead><tr><th></th><th>数组</th><th>链表</th></tr></thead><tbody><tr><td>读取</td><td>O(1)</td><td>O(n)</td></tr><tr><td>插入</td><td>O(n)</td><td>O(1)</td></tr><tr><td>删除</td><td>O(n)</td><td>O(1)</td></tr></tbody></table><p>有两种访问方式：随机访问和顺序访问。</p><p>顺序访问意味着从第一个元素开始逐个读取元素，链表只能顺序访问，数组支持随机访问，所以数组在需要随机访问的情况下用得很多。</p><h3 id="2-3-选择排序"><a href="#2-3-选择排序" class="headerlink" title="2.3 选择排序"></a>2.3 选择排序</h3><p>时间复杂度的Tips</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g39qnoufddj20oz08a0v8.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findSmallest</span><span class="params">(arr)</span>:</span></span><br><span class="line">    smallest = arr[<span class="number">0</span>]</span><br><span class="line">    smallest_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(arr)):</span><br><span class="line">        <span class="keyword">if</span> arr[i] &lt; smallest:</span><br><span class="line">            smallest = arr[i]</span><br><span class="line">            smallest_index = i</span><br><span class="line">    <span class="keyword">return</span> smallest_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection</span><span class="params">(arr)</span>:</span></span><br><span class="line">    newArr = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        smallest = findSmallest(arr)</span><br><span class="line">        newArr.append(arr.pop(smallest))</span><br><span class="line">    <span class="keyword">return</span> newArr</span><br><span class="line"></span><br><span class="line">print(selection( [<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">10</span>] ))</span><br></pre></td></tr></table></figure><p>Tips：</p><p>python之间的语法不兼容是很蛋疼的事情</p><p>py2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Pyhon 2 can use print string without ()"</span>;</span><br></pre></td></tr></table></figure><p>py3:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Python3, print must use () to output string"</span>);</span><br></pre></td></tr></table></figure><p>py3中，print作为函数必须要带括号</p><h2 id="第三章-递归"><a href="#第三章-递归" class="headerlink" title="第三章 递归"></a>第三章 递归</h2><p>递归：优雅的问题解决办法</p><h3 id="3-1-递归"><a href="#3-1-递归" class="headerlink" title="3.1 递归"></a>3.1 递归</h3><p>“如果使用循环，程序的性能可能更高；如果使用递归，程序可能 更容易理解。如何选择要看什么对你来说更重要“</p><h3 id="3-2-基线条件和递归条件"><a href="#3-2-基线条件和递归条件" class="headerlink" title="3.2 基线条件和递归条件"></a>3.2 基线条件和递归条件</h3><p>递归条件(base case)是指函数调用自己，基线条件(recursive case)是指函数不再调用自己，从而表面形成无限循环。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countdown</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(i)</span><br><span class="line">    <span class="keyword">if</span> i &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        countdown(i<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">countdown(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="3-3-栈"><a href="#3-3-栈" class="headerlink" title="3.3 栈"></a>3.3 栈</h3><p>调用栈（call stack）</p><p>python虽然不是用的JVM 但是 对于栈内存的调用 好像都差不多</p><p>递归函数factorial(5)写作5!</p><p>意义是5! = 5 <em> 4 </em> 3 <em> 2 </em> 1</p><p>使用栈虽然很方便但是也要付出代价：占用大量内存</p><h2 id="第四章-快速排序"><a href="#第四章-快速排序" class="headerlink" title="第四章 快速排序"></a>第四章 快速排序</h2><h3 id="4-1-分而治之"><a href="#4-1-分而治之" class="headerlink" title="4.1 分而治之"></a>4.1 分而治之</h3><p>一种著名的递归式问题解决方法—divide and conquer,D&amp;C</p><p>重要的D&amp;C是算法：快排，优雅代码的典范</p><p>欧几里得算法(辗转相除法)：gcd(a.b) = gcd(b, a%b)</p><table><thead><tr><th>大的那个数</th><th>小的那个数</th><th>余数</th><th>商</th></tr></thead><tbody><tr><td>a</td><td>b</td><td>r0 = a%b</td><td>q0</td></tr><tr><td>b</td><td>r0</td><td>r1 = b% r0</td><td>q1</td></tr><tr><td>r0</td><td>r1</td><td>r2 = r0 % r1</td><td>q2</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>rN-4</td><td>rN-3</td><td>rN-2 = rN-4 % rN-3</td><td>qN-2</td></tr><tr><td>rN-3</td><td>rN-2</td><td>rN-1 = rN-3 % rN-2</td><td>qN-1</td></tr><tr><td>rN-2</td><td>rN-1</td><td>rN = rN-2 % rN-1</td><td>qN</td></tr><tr><td>rN-1</td><td>rN == 0</td><td>rN-1 = 1 <em> rN-1 - 0 </em> rN</td><td>0</td></tr></tbody></table><p>得到的最大公约数就是rN-1</p><p>欧几里得算法的证明：</p><p>我个人觉得反证法比较好理解：</p><p> 要证欧几里德算法成立，即证: gcd(a,b)=gcd(b,r),其中 gcd是取最大公约数的意思，r=a mod b<br>    下面证 gcd（a，b）=gcd（b，r）<br>    设  c是a，b的最大公约数，即c=gcd（a，b），则有 a=mc，b=nc，其中m，n为正整数，且m，n互为质数<br>    由 r= a mod b可知，r= a- qb 其中，q是正整数，<br>    则 r=a-qb=mc-qnc=（m-qn）c<br>    b=nc,r=(m-qn)c，且n，（m-qn）互质（假设n，m-qn不互质，则n=xd, m-qn=yd 其中x,y,d都是正整数，且d&gt;1</p><p>​    则a=mc=(qx+y)dc, b=xdc,这时a,b 的最大公约数变成dc，与前提矛盾，所以n ，m-qn一定互质）<br>​    则gcd（b,r）=c=gcd（a,b）<br>​    得证。</p><p>编写涉及数组的递归函数时，基线条件通常是数组为空或只包含一个元素。陷入困境时， 请检查基线条件是不是这样的。 </p><h3 id="4-2-快排"><a href="#4-2-快排" class="headerlink" title="4.2 快排"></a>4.2 快排</h3><p>快排使用了D&amp;C</p><p>思路：</p><p>基线条件：数组为空或者只包含一个元素。这种情况下，只需要原样返回。</p><p>对于两个元素的数组：如果第一个元素比第二个元素小，直接返回，如果不是，就交换位置。</p><p>三个元素的数组：</p><p>从数组中选择一个元素，这个元素被称为基准值(pivot)，</p><p>我们暂时先将数组的第一个元素作为基准值。</p><p>接下来找出比基准值小的元素以及比他大的元素。这个过程被称为分区（partition）</p><p>这里两个分区出来的数组时无需的，但是如果这两个数组是有序的，对整个数组进行排序将非常容易。</p><p>那么问题就转化成了如何对子数组进行排序，</p><p>这里我们讨论的是特定情况（三个元素），无论选用哪个元素作为pivot，剩下的情况总能用上面两个元素数组的排序方法代入。</p><p>于是就得到了解决办法</p><p>接下来四个元素的情况，类似的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="comment"># 基线条件</span></span><br><span class="line">    <span class="keyword">if</span> len(array) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 递归条件</span></span><br><span class="line">        pivot = array[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 分为两个数组</span></span><br><span class="line">        less = [ i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &lt;= pivot]</span><br><span class="line">        greater = [i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; pivot]</span><br><span class="line">        <span class="keyword">return</span> quicksort(less) + [pivot] + quicksort(greater)</span><br><span class="line"></span><br><span class="line">print(quicksort([<span class="number">1</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">45645</span>,<span class="number">34</span>,<span class="number">23</span>,<span class="number">65</span>,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这边可以对比一下时间复杂度</p><p>选择排序的时间复杂度是O(n<sup>2</sup>)</p><p>快速排序的时间复杂度最差是O(n<sup>2</sup>)，平均情况是O(n log n)</p><p>还有一种合并排序(merge sort)运行时间是O(n log n)</p><p>现在做出一个有趣的假设，假设简单查找每次需要10ms，二分查找的常量是1s，现在我们假设查找的元素个数是10个，简单查找需要100ms，二分查找却需要log 10 * 1s，可以二分查找的时间远大于简单查找，但是我们查找的元素很大时，比如40亿，这个时候我们使用简单查找需要463天，但是二分查找只要32s。</p><p>通过这个例子，我们可以看到常量的影响可能会很大。</p><p>我们再来看快排，快排的效率取决于选择的pivot，当pivot是最小值的时候，我们其实只用到的一个数组，要递归很多次才能递归结束，这种情况是最坏的情况</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3c25jnmi2j20jx0ga0v4.jpg" alt></p><p>如果我们选择的是中间值，是最佳情况，这种情况下根本不要这么多递归，因此调用栈就短得多。</p><p>需要注意的是，我们并不是如图这么简单的+n次调用，作为递归二次每次调用栈都设计O(n)，这是递归的性质决定的。</p><p>因此，实际上最佳情况是O(n log n)</p><p>最佳情况也是平均情况（和最佳情况在同一数量级所以忽略掉前面的参数，剩下的相同），快排是D&amp;G的典范。</p><h2 id="第五章-散列表-Hash-Table"><a href="#第五章-散列表-Hash-Table" class="headerlink" title="第五章 散列表 Hash Table"></a>第五章 散列表 Hash Table</h2><blockquote><p>散列表是足有用的基本数据结构之一。</p></blockquote><p>虽然二分法的效率已经可以了，但是能不能有一种查找方法的查找时间是O(1)呢——任意给出一个查找内容，都能立即给出答案。</p><h3 id="5-1-散列函数"><a href="#5-1-散列函数" class="headerlink" title="5.1 散列函数"></a>5.1 散列函数</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3eicbpqf2j20ar07y3z4.jpg" alt></p><p>散列函数应该满足的要求：</p><ul><li>他必须是一致的。例如：假设你输入apple时得到的是4，那么每次输入apple的时候，得到的都必须是4，如果不是这样，散列表将毫无用处。</li><li>它应该将不同的输入映射到不同的数字，如果一个散列函数不管输入是什么都返回1就不可以。最理想的情况是，将不同的输入映射到不同的数字。</li></ul><p>原理：</p><ul><li>散列函数总是将同样的输入映射到相同的索引。</li><li>不同输入映射到不同的索引。</li><li>散列函数知道数组有多大。</li></ul><p>散列表是一种包含额外逻辑的数据结构。</p><p>散列表又被称为散列映射、映射、字典和关联数组。（Hash Table）</p><p>python提供的散列表实现为字典，可以使用<code>dictt</code>来创建散列表。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ej8m95ydj20hm09omx2.jpg" alt></p><p>python语法中</p><p><code>book = dict()</code>和<code>book = {}</code>等价。</p><h3 id="5-2-应用案例"><a href="#5-2-应用案例" class="headerlink" title="5.2 应用案例"></a>5.2 应用案例</h3><p>电话簿</p><p>DNS解析（域名关联IP）DNS resolution</p><p>防止重复（比如抽奖、投票）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ejoaiscwj20gr0743yd.jpg" alt></p><p>将案列表用作缓存</p><p>Facebook将主页、about页面，Contact页面、Terms 和 Conditions页面等众多页面通过页面URL映射到页面数据。</p><h3 id="5-3-冲突-collision"><a href="#5-3-冲突-collision" class="headerlink" title="5.3 冲突 collision"></a>5.3 冲突 collision</h3><p>大多数语言都提供了散列实现，冲突是指，两个键分配的数组位置相同，这是个问题。</p><p>解决办法：如果两个键映射到了同一个位置，就在这个位置存储一个链表。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi55lqjgj20l606i74w.jpg" alt></p><p>但是，如果A开头的物品过多，散列表的效率将激素下降，然而：如果散列函数用的很好，这些列表就不会很长。</p><h3 id="5-4-性能"><a href="#5-4-性能" class="headerlink" title="5.4 性能"></a>5.4 性能</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi9iv10gj20d0071my4.jpg" alt></p><p>平均情况下，散列表的查找速度和数组一样快，而插入和删除速度与链表一样快，因此它兼具两者的优点！但是最糟的情况下，散列表的各种操作都很慢。</p><p>因此，为了避免冲突，需要有：</p><p>较低的填装因子。</p><p>良好的散列函数。</p><p>装填因子：</p><p>散列表包含的元素数目/位置总数</p><p>假设再散列表中存储100种商品的价格，散列表包含100个位置名最佳情况下，每个商品都将有自己的位置。</p><p>装填因子在大于1的情况下，需要在散列表中添加位置，这个操作被称为<strong>调整长度(resizing)</strong>。</p><p>一般操作是：<strong>数组增加一倍</strong>。</p><p>接下来，将所有元素用hash函数插入到新的散列表中。</p><p>平均而言，即便考虑到调整长度所需的时间，散列表操作所需的 时间也为O(1)。 </p><p>良好的散列函数让数组中的值呈均匀分布。 </p><p>糟糕的散列函数让值扎堆，导致大量的冲突。 </p><h2 id="第六章-广度优先搜索"><a href="#第六章-广度优先搜索" class="headerlink" title="第六章 广度优先搜索"></a>第六章 广度优先搜索</h2><blockquote><p>图算法之<em>广度优先搜索</em> (breadth-first search)</p></blockquote><h3 id="6-1-图简介"><a href="#6-1-图简介" class="headerlink" title="6.1 图简介"></a>6.1 图简介</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3finx3k1jj20jb07yq40.jpg" alt></p><p>如图用来解决从A点到B点最短路径问题的办法叫图计算方法。</p><p>这种最短路径既可能是最短路径，也有可能是国际象棋中将对方将死的最少步数。</p><p>解决最短路径问题的算法被称为<strong>广度优先搜索</strong>。</p><h3 id="6-2-图是什么"><a href="#6-2-图是什么" class="headerlink" title="6.2 图是什么"></a>6.2 图是什么</h3><p>图用于模拟不同的东西是如何相连的。</p><h3 id="6-3-广度优先搜索"><a href="#6-3-广度优先搜索" class="headerlink" title="6.3 广度优先搜索"></a>6.3 广度优先搜索</h3><p>书中的例计较简单，在朋友圈中找A，先遍历朋友，查找是否有A，有的话结束，没有的话，依次遍历朋友的朋友。（和之前找芒果经销商是一样的）</p><p>能够实现这种目的的数据结构叫做<strong>队列（queue）</strong></p><p>队列的工作原理：你不能随机访问队列中的元素。队列只支持两种操作：入队和出队。</p><p>队列是一种<strong>先进先出（First In First Out，FIFO）</strong>的数据结构，而栈是一种<strong>后进先出（Last In First Out，LIFO）</strong>的数据结构。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj0qapdpj20cd04ddg7.jpg" alt></p><h3 id="6-4-实现图"><a href="#6-4-实现图" class="headerlink" title="6.4 实现图"></a>6.4 实现图</h3><p>python实现一个简单的图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj7iusshj20f70azab9.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;&#125; </span><br><span class="line">graph[<span class="string">"you"</span>] = [<span class="string">"alice"</span>, <span class="string">"bob"</span>, <span class="string">"claire"</span>] </span><br><span class="line">graph[<span class="string">"bob"</span>] = [<span class="string">"anuj"</span>, <span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"alice"</span>] = [<span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"claire"</span>] = [<span class="string">"thom"</span>, <span class="string">"jonny"</span>] </span><br><span class="line">graph[<span class="string">"anuj"</span>] = [] </span><br><span class="line">graph[<span class="string">"peggy"</span>] = [] </span><br><span class="line">graph[<span class="string">"thom"</span>] = [] </span><br><span class="line">graph[<span class="string">"jonny"</span>] = []</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj9dzn13j20mt04pwf5.jpg" alt></p><p>上图中的有向图和无向图是等价的。</p><h3 id="6-5-实现算法"><a href="#6-5-实现算法" class="headerlink" title="6.5 实现算法"></a>6.5 实现算法</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fjb7w4rwj20jv0ls77b.jpg" alt></p><p>在Python中，可以使用函数deque来创建一个双端队列</p><p>这边需要考虑一个情况：就是朋友是朋友的朋友，循环调用会造成无限循环。</p><p>所以需要添加容错判断。用一个列表来记录检查过的人。</p><p>图的特殊情况：指针只往一个方向，比如说：族谱。</p><h2 id="第七章-狄克斯特拉算法-Dijkstra’s-Algorithm"><a href="#第七章-狄克斯特拉算法-Dijkstra’s-Algorithm" class="headerlink" title="第七章 狄克斯特拉算法 Dijkstra’s Algorithm"></a>第七章 狄克斯特拉算法 Dijkstra’s Algorithm</h2><h3 id="7-1-狄克斯特拉算法介绍"><a href="#7-1-狄克斯特拉算法介绍" class="headerlink" title="7.1 狄克斯特拉算法介绍"></a>7.1 狄克斯特拉算法介绍</h3><p>依旧图的讨论。</p><p>如果之前的路径有了权重（节点到节点之间花费的时间不等价），重新计算最短路径，就应该使用狄克斯特拉算法。</p><p>狄克斯特拉算法包含四个步骤：</p><ul><li>找出最便宜的节点，即可在最短时间内前往的节点。</li><li>对于该节点的邻居，检查是否有前往他们的最短路径，如果有，就更新其开销。</li><li>重复这个过程，知道对图中的每个节点都这样做了。</li><li>计算最终路径。</li></ul><h3 id="7-3-术语"><a href="#7-3-术语" class="headerlink" title="7.3 术语"></a>7.3 术语</h3><p>每条边关联的数字叫做权重（weight）。</p><p>带权重的图称为加权图（weighted graph），不带权重的图称为非加权图（unweighted graph）。</p><p>要计算非加权图中的最短路径，可以使用<strong>广度优先搜索</strong>。</p><p>如果是为了计算加权图中的最短路径，可以使用<strong>迪克斯特拉算法</strong>。</p><p>图还可能有环，这意味着你可以从一个节点出发，走一圈后又回到这个节点</p><p>在无向图中，每条边都是一个环，狄克斯特拉算法只使用于有向无环图（DAG）</p><h3 id="7-4-负权边"><a href="#7-4-负权边" class="headerlink" title="7.4 负权边"></a>7.4 负权边</h3><p>如果有负权边就不能用，就不能使用狄克斯特拉算法，因为负权边，就不能使用狄克斯特拉算法。</p><p>因为负权边会导致这种算法不管用。</p><p>因为：根据狄克斯特拉算法，没有比不支付任何费用获得海报更便宜的方式。</p><p>因此：不能将狄克斯特拉算法用于包含负权边的图。</p><p>要在包含负权边的图中，找出最短路径，可以使用另一种算法： 贝尔曼—福德算法（Bellman-Ford algorithm）.</p><h3 id="7-5-实现"><a href="#7-5-实现" class="headerlink" title="7.5 实现"></a>7.5 实现</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gntz3sx0j20aa06fq38.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gobfcl77j20fz0i4wek.jpg" alt></p><p>可以用以上代码表示上图的散列表。</p><p>上面代码表达的表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goeygswbj20b00a6dgw.jpg" alt></p><p>接下来需要一个散列表来粗春<strong>每个节点的开销</strong>。</p><p>节点的开销：从起点出发前往该节点需要的时间。</p><p>用表表示的话如图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gofc9opgj205805zdg6.jpg" alt></p><p>表中的无穷大可以这么表示：</p><p><a href="https://www.cnblogs.com/lvye-song/p/4029691.html" target="_blank" rel="noopener">python正负无穷</a></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goiypuf7j20e205z0sk.jpg" alt></p><p>除了上面两张表，还需要一个存储父节点的散列表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gox0oz11j209g096wf7.jpg" alt></p><p>创建这个散列表的代码如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gozpliz7j20aj05mjr7.jpg" alt></p><p>最后，需要一个数组用于记录处理过的节点，因为对于同一个节点，你不用处理多次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">processd = &#123;&#125;</span><br></pre></td></tr></table></figure><p>动图表示整个认证过程：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3hv4x3xeqg207v066jrc.gif" alt></p><p>整体过程：</p><p>本书介绍的python 迪克斯特拉算法：</p><p>使用了三个散列表和一个数组，三个散列表的作用分别是：</p><p>第一个：Graph散列表</p><p>用来记录每个节点到指向节点的权重</p><p>第二个：Costs散列表</p><p>指的起点到某个节点的消耗</p><p>第三个：Parents散列表</p><p>指的是父节点的散列表</p><p>数组的作用是记录用于处理过的节点。</p><p>处理过程是，</p><p>找出一个未处理的节点（规则定位开销最小的）</p><p>然后在表一获得该节点的开销和邻居。</p><p>遍历邻居，</p><p>接着计算从起点到X再到邻居节点的距离，然后在表一中对比这样的开销和原先的开销大小，如果这样效率更高，那么在表二中替换掉（或者更新掉原先的数字），然后在表三中改变其父节点为X</p><p>（表二记载的开销是经过父节点的最短开销）</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line"><span class="keyword">while</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">cost = costs[node]</span><br><span class="line">neighbors = graph[node]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> neighbors.keys():</span><br><span class="line">new_cost = cost + neighbors[n]</span><br><span class="line"><span class="keyword">if</span> costs[n] &gt; new_cost:</span><br><span class="line">costs[n] = new_cost</span><br><span class="line">parents[n] = node</span><br><span class="line">processed.append(node)</span><br><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_lowest_cost_node</span><span class="params">(costs)</span>:</span></span><br><span class="line">    lowest_cost = float(<span class="string">"inf"</span>)</span><br><span class="line">    lowest_cost_node = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> costs:</span><br><span class="line">        cost = costs[node]</span><br><span class="line">        <span class="keyword">if</span> cost &lt; lowest_cost <span class="keyword">and</span> node <span class="keyword">not</span> <span class="keyword">in</span> processed:</span><br><span class="line">        lowest_cost = cost</span><br><span class="line">            lowest_cost_node = node</span><br><span class="line"><span class="keyword">return</span> lowest_cost_node</span><br></pre></td></tr></table></figure><p>书上对这个过程的描述还可以，但是我觉得如果能增加一个循环就更好了。</p><h2 id="第八章-贪婪算法"><a href="#第八章-贪婪算法" class="headerlink" title="第八章 贪婪算法"></a>第八章 贪婪算法</h2><h3 id="8-1-教室调度问题"><a href="#8-1-教室调度问题" class="headerlink" title="8.1 教室调度问题"></a>8.1 教室调度问题</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iys53c86j20m50b140f.jpg" alt></p><p>解决方法：</p><p>(1) 选出结束最早的课，它就是要在这间教室上的第一堂课。 </p><p>(2) 接下来，必须选择第一堂课结束后才开始的课。同样，你选择结束最早的课，这将是要 在这间教室上的第二堂课。 </p><p>重读这样做就能找出答案。</p><p>即：每步都选择局部最优解，最终得到的就是全局最优解。</p><p>此方法并非万能！但是行之有效，并且<strong>简单</strong>！</p><h3 id="8-2-背包问题"><a href="#8-2-背包问题" class="headerlink" title="8.2 背包问题"></a>8.2 背包问题</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。&lt;/p&gt;
&lt;p&gt;这边值得一提的是作者推荐了一个网站，可汗学院，&lt;code&gt;khanacademy.org&lt;/code&gt;  mark一下。&lt;/p&gt;
&lt;p&gt;看完40%来总结一下，非常好，文盲也能看懂的算法入门。&lt;/p&gt;
&lt;p&gt;这本书看完应该会扫一眼结城浩的《图解密码学》&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading Note" scheme="http://yoursite.com/categories/Reading-Note/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/categories/Reading-Note/Grokking-Algorithms/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/tags/Grokking-Algorithms/"/>
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
      <category term="欧几里得算法" scheme="http://yoursite.com/tags/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/"/>
    
      <category term="快排" scheme="http://yoursite.com/tags/%E5%BF%AB%E6%8E%92/"/>
    
  </entry>
  
</feed>
