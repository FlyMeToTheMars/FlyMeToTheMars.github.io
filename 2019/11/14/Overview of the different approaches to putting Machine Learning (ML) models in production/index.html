<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/xingqiushangcheng.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/xingqiushangcheng.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-mac-osx.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="原文 &amp;lt;Overview of the different approaches to putting Machine Learning (ML) models in production&amp;gt;">
<meta name="keywords" content="Mechine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="将机器学习模型应用入生产的几种策略">
<meta property="og:url" content="http://yoursite.com/2019/11/14/Overview of the different approaches to putting Machine Learning (ML) models in production/index.html">
<meta property="og:site_name" content="Mars">
<meta property="og:description" content="原文 &amp;lt;Overview of the different approaches to putting Machine Learning (ML) models in production&amp;gt;">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wn761fzj209m09tq63.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wnpy6yjj20m80gntax.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wou2v4sj20ja08d74l.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wp3or7dj20em02k3yg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpd8zz1j20f10b5q39.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpl246sj20m8058aah.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpsew6sj20fg08x74h.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wq0vsywj20ix0b5mxt.jpg">
<meta property="og:updated_time" content="2020-04-10T17:10:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="将机器学习模型应用入生产的几种策略">
<meta name="twitter:description" content="原文 &amp;lt;Overview of the different approaches to putting Machine Learning (ML) models in production&amp;gt;">
<meta name="twitter:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg">

<link rel="canonical" href="http://yoursite.com/2019/11/14/Overview of the different approaches to putting Machine Learning (ML) models in production/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>将机器学习模型应用入生产的几种策略 | Mars</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Mars" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mars</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/14/Overview of the different approaches to putting Machine Learning (ML) models in production/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Fly Hugh">
      <meta itemprop="description" content="WE CHOOSE TO  GO TO THE MARS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mars">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          将机器学习模型应用入生产的几种策略
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-14 04:40:50" itemprop="dateCreated datePublished" datetime="2019-11-14T04:40:50+08:00">2019-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-11 01:10:05" itemprop="dateModified" datetime="2020-04-11T01:10:05+08:00">2020-04-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mechine-Learning/" itemprop="url" rel="index"><span itemprop="name">Mechine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>原文 <a href="https://medium.com/analytics-and-data/overview-of-the-different-approaches-to-putting-machinelearning-ml-models-in-production-c699b34abf86" target="_blank" rel="noopener">&lt;Overview of the different approaches to putting Machine Learning (ML) models in production&gt;</a></p>
</blockquote>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wmih501j20m80fwmzi.jpg"></p>
<a id="more"></a> 

<p>There are different approaches to putting models into productions, with benefits that can vary dependent on the specific use case. Take for example the use case of churn prediction, there is value in having a static value already that can easily be looked up when someone call a customer service, but there is some extra value that could be gained if for specific events, the model could be re-run with the newly acquired information.</p>
<p>There is generally different ways to both train and server models into production:</p>
<ul>
<li><strong>Train</strong>: one off, batch and real-time/online training</li>
<li><strong>Serve:</strong> Batch, Realtime (Database Trigger, Pub/Sub, web-service, inApp)</li>
</ul>
<p>Each approach having its own set of benefits and tradeoffs that need to be considered.</p>
<h3 id="One-off-Training"><a href="#One-off-Training" class="headerlink" title="One off Training"></a>One off Training</h3><p>Models don’t necessarily need to be continuously trained in order to be pushed to production. Quite often a model can be just trained ad-hoc by a data-scientist, and pushed to production until its performance deteriorates enough that they are called upon to refresh it.</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wn761fzj209m09tq63.jpg"></p>
<blockquote>
<p>From Jupyter to Prod</p>
</blockquote>
<p>DataScientists prototyping and doing machine learning tend to operate in their environment of choice <a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter</a> Notebooks. Essentially an advanced GUI on a <a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop" target="_blank" rel="noopener">repl</a>, that allows you to save both code and command outputs.</p>
<p>Using that approach it is more than feasible to push an ad-hoc trained model from some piece of code in Jupyter to production. Different types of libraries and other notebook providers help further tie the link between the data-scientist workbench and production.</p>
<h4 id="Model-Format"><a href="#Model-Format" class="headerlink" title="Model Format"></a>Model Format</h4><p><a href="https://docs.python.org/3/library/pickle.html" target="_blank" rel="noopener">Pickle</a> converts a python object to to a bitstream and allows it to be stored to disk and reloaded at a later time. It is provides a good format to store machine learning models provided that their intended applications is also built in python.</p>
<p><a href="https://github.com/onnx" target="_blank" rel="noopener">ONNX</a> the Open Neural Network Exchange format, is an open format that supports the storing and porting of predictive model across libraries and languages. Most deep learning libraries support it and sklearn also has a library extension to convert their model to <a href="https://github.com/onnx/sklearn-onnx/blob/master/docs/tutorial.rst" target="_blank" rel="noopener">ONNX’s format</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language" target="_blank" rel="noopener">PMML</a> or Predictive model markup language, is another interchange format for predictive models. Like for ONNX sklearn also has another library extension for converting the models to <a href="https://github.com/jpmml/sklearn2pmml" target="_blank" rel="noopener">PMML format</a>. It has the drawback however of only supporting certain type of prediction models.PMML has been around since 1997 and so has a large footprint of applications leveraging the format. Applications such as [SAP](<a href="https://archive.sap.com/kmuuid2/a07faefd-61d7-2c10-bba6-89ac5ffc302c/Integrating" target="_blank" rel="noopener">https://archive.sap.com/kmuuid2/a07faefd-61d7-2c10-bba6-89ac5ffc302c/Integrating</a> Real-time Predictive Analytics into SAP Applications.pdf) for instance is able to leverage certain versions of the PMML standard, likewise for CRM applications such as <a href="https://community.pega.com/knowledgebase/supported-pmml-model-types" target="_blank" rel="noopener">PEGA</a>.</p>
<p><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#about-pojos-and-mojos" target="_blank" rel="noopener">POJO and MOJO </a>are <a href="https://www.h2o.ai/" target="_blank" rel="noopener">H2O.ai</a>’s export format, that intendeds to offers an easily embeddable model into java application. They are however very specific to using the H2O’s platform.</p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>For one off training of models, the model can either be trained and fine tune adhoc by a data-scientists or training through AutoML libraries. Having an easily reproducible setup, however helps pushing into the next stage of productionalization, ie: batch training.</p>
<h3 id="Batch-Training"><a href="#Batch-Training" class="headerlink" title="Batch Training"></a>Batch Training</h3><p>While not fully necessary to implement a model in production, batch training allows to have a constantly refreshed version of your model based on the latest train.</p>
<p>Batch training can benefit a-lot from AutoML type of frameworks, AutoML enables you to perform/automate activities such as feature processing, feature selection, model selections and parameter optimization. Their recent performance has been on par or bested the most diligent data-scientists.</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wnpy6yjj20m80gntax.jpg"></p>
<p>Using them allows for a more comprehensive model training than what was typically done prior to their ascent: simply retraining the model weights.</p>
<p>Different technologies exists that are made to support this continuous batch training, these could for instance be setup through a mix of <a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">airflow</a> to manage the different workflow and an AutoML library such as <a href="https://epistasislab.github.io/tpot/" target="_blank" rel="noopener">tpot</a>, Different cloud providers offer their solutions for AutoML that can be put in a data workflow. Azure for instance integrates machine learning prediction and model training with their <a href="https://azure.microsoft.com/es-es/blog/retraining-and-updating-azure-machine-learning-models-with-azure-data-factory/" target="_blank" rel="noopener">data factory offering</a>.</p>
<h3 id="Real-time-training"><a href="#Real-time-training" class="headerlink" title="Real time training"></a>Real time training</h3><p>Real-time training is possible with ‘Online Machine Learning’ models, algorithms supporting this method of training includes K-means (through mini-batch), Linear and Logistic Regression (through Stochastic Gradient Descent) as well as Naive Bayes classifier.</p>
<p>Spark has StreamingLinearAlgorithm/StreamingLinearRegressionWithSGD to perform these operations, sklearn has SGDRegressor and SGDClassifier that can be incrementally trained. In sklearn, the incremental training is done through the partial_fit method as shown below:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_0 = pd.DataFrame([[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">0</span>]] )</span><br><span class="line">y_0 = pd.DataFrame([[<span class="number">0</span>], [<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">X_1 = pd.DataFrame([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">y_1 = pd.DataFrame([[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">clf = linear_model.SGDClassifier()</span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_0, y_0, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line"></span><br><span class="line">clf.partial_fit(X_1, y_1, classes=[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">0</span>]])) <span class="comment"># -&gt; 0</span></span><br><span class="line">print(clf.predict([[<span class="number">0</span>,<span class="number">1</span>]])) <span class="comment"># -&gt; 1</span></span><br></pre></td></tr></table></figure>

<p>When deploying this type of models there needs to be serious operational support and monitoring as the model can be sensitive to new data and noise, and model performance needs to be monitored on the fly. In offline training, you can filter points of <a href="https://en.wikipedia.org/wiki/Leverage_(statistics)" target="_blank" rel="noopener">high leverage</a> and correct for this type of incoming data. This is much harder to do when you are constantly updating your model training based on a stream of new data points.</p>
<p>Another challenge that occurs with training online model is that they don’t decay historical information. This means that, on case there are structural changes in your datasets, the model will need to be anyway re-trained and that there will be a big onus in model lifecycle management.</p>
<h3 id="Batch-vs-Real-time-Prediction"><a href="#Batch-vs-Real-time-Prediction" class="headerlink" title="Batch vs. Real-time Prediction"></a>Batch vs. Real-time Prediction</h3><p>When looking at whether to setup a batch or real-time prediction, it is important to get an understanding of why doing real-time prediction would be important. It can potentially be for getting a new score when significant event happen, for instance what would be the churn score of customer when they call a contact center. These benefits needs to be weighted against the complexity and cost implications that arise from doing real-time predictions.</p>
<p><strong>Load implications</strong></p>
<p>Catering to real time prediction, requires a way to handle peak load. Depending on the approach taken and how the prediction ends up being used, choosing a real-time approach, might also require to have machine with extra computing power available in order to provide a prediction within a certain SLA. This contrasts with a batch approach where the predictions computing can be spread out throughout the day based on available capacity.</p>
<p><strong>Infrastructure Implications</strong></p>
<p>Going for real-time, put a much higher operational responsibility. People need to be able to monitor how the system is working, be alerted when there is issue as well as take some consideration with respect to failover responsibility. For batch prediction, the operational obligation is much lower, some monitoring is definitely needed, and altering is desired but the need to be able to know of issues arising directly is much lower.</p>
<p><strong>Cost Implications</strong></p>
<p>Going for real-time predictions also has costs implications, going for more computing power, not being able to spread the load throughout the day can force into purchasing more computing capacity than you would need or to pay for spot price increase. Depending on the approach and requirements taken there might also be extra cost due to needing more powerful compute capacity in order to meet SLAs. Furthermore, there would tend to be a higher infrastructure footprint when choosing for real time predictions. One potential caveat there is where the choice is made to rely on in app prediction, for that specific scenario the cost might actually end up being cheaper than going for a batch approach.</p>
<p><strong>Evaluation Implications</strong></p>
<p>Evaluating the prediction performance in real-time manner can be more challenging than for batch predictions. How do you evaluate performance when you are faced with a succession of actions in a short burst producing multiple predictions for a given customer for instance? Evaluating and debugging real-time prediction models are significantly more complex to manage. They also require a log collection mechanism that allows to both collect the different predictions and features that yielded the score for further evaluation.</p>
<h3 id="Batch-Prediction-Integration"><a href="#Batch-Prediction-Integration" class="headerlink" title="Batch Prediction Integration"></a>Batch Prediction Integration</h3><p>Batch predictions rely on two different set of information, one is the predictive model and the other one is the features that we will feed the model. In most type of batch prediction architecture, ETL is performed to either fetch pre-calculated features from a specific datastore (feature-store) or performing some type of transformation across multiple datasets to provide the input to the prediction model. The prediction model then iterates over all the rows in the datasets providing the different score.</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wou2v4sj20ja08d74l.jpg"></p>
<p>Once all the predictions have been computed, we can then “serve” the score to the different systems wanting to consume the information. This can be done in different manner depending on thee use case for which we want to consume the score, for instance if we wanted to consume the score on a front-end application, we would most likely push the data to a “cache” or NoSQL database such as Redis so that we can offer milliseconds responses, while for certain use cases such as the creation of an email journey, we might just be relying on a CSV SFTP export or a data load to a more traditional RDBMS.</p>
<h3 id="Real-time-Prediction-integration"><a href="#Real-time-Prediction-integration" class="headerlink" title="Real-time Prediction integration"></a><strong>Real-time Prediction integration</strong></h3><p>Being able to push model into production for real-time applications require 3 base components. A customer/user profile, a set of triggers and predictive models.</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wp3or7dj20em02k3yg.jpg"></p>
<p><strong>Profile:</strong> The customer profile contains all the related attribute to the customer as well as the different attributes (eg: counters) necessary in order to make a given prediction. This is required for customer level prediction in order to reduce the latency of pulling the information from multiple places as well as to simplify the integration of machine learning models in productions. In most cases a similar type of data store would be needed in order to effectively fetch the data needed to power the prediction model.</p>
<p><strong>Triggers:</strong> Triggers are events causing the initiation of process, they can be for churn for instance, call to a customer service center, checking information within your order history, etc …</p>
<p><strong>Models:</strong> models need to have been pre-trained and typically exported to one of the 3 formats previously mentioned (pickle, ONNX or PMML) to be something that we could easily port to production.</p>
<p>There are quite a few different approach to putting models for scoring purpose in production:</p>
<ul>
<li><em>Relying on in Database integration:</em> a lot of database vendors have made a significant effort to tie up advanced analytics use cases within the database. Be it by direct integration of Python or R code, to the import of PMML model.</li>
<li><em>Exploiting a Pub/Sub model</em>: The prediction model is essentially an application feeding of a data-stream and performing certain operations, such as pulling customer profile information.</li>
<li><em>Webservice:</em> Setting up an API wrapper around the model prediction and deploying it as a web-service. Depending on the way the web-service is setup it might or might not do the pull or data needed to power the model.</li>
<li><em>inApp:</em> it is also possible to deploy the model directly into a native or web application and have the model be run on local or external datasources.</li>
</ul>
<h4 id="Database-integrations"><a href="#Database-integrations" class="headerlink" title="Database integrations"></a><em>Database integrations</em></h4><p>If the overall size of your database is fairly small (&lt; 1M user profile) and the update frequency is occasional it can make sense to integrate some of the real-time update process directly within the database.</p>
<p>Postgres possess an integration that allows to run Python code as functions or stored procedure called <a href="http://pl/Python" target="_blank" rel="noopener">PL/Python</a>. This implementation has access to all the libraries that are part of the <strong>PYTHONPATH</strong>, and as such are able to use libraries such as Pandas and SKlearn to run some operations.</p>
<p>This can be coupled with Postgres’ <a href="https://www.tutorialspoint.com/postgresql/postgresql_triggers.htm" target="_blank" rel="noopener">Triggers</a> Mechanism to perform a run of the database and update the churn score. For instance if a new entry is made to a complaint table, it would be valuable to have the model be re-run in real-time.</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpd8zz1j20f10b5q39.jpg"></p>
<p><strong>Sequence flow</strong></p>
<p>The flow could be setup in the following way:</p>
<p><em>New Event:</em> When a new row is inserted in the complain table, an event trigger is generated.</p>
<p><em>Trigger:</em> The trigger function would update the number of complaint made by this customer in the customer profile table and fetch the updated record for the customer.</p>
<p><em>Prediction Request:</em> Based on that it would re-run the churn model through PL/Python and retrieve the prediction.</p>
<p><em>Customer Profile Update:</em> It can then re-update the customer profile with the updated prediction. Downstream flows can then happen upon checking if the customer profile has been updated with new churn prediction value.</p>
<p><strong>Technologies</strong></p>
<p>Different databases are able to support the running of Python script, this is the case of PostGres which has a native Python integration as previosuly mentioned, but also of Ms SQL Server through its’ <a href="https://www.sqlshack.com/how-to-use-python-in-sql-server-2017-to-obtain-advanced-data-analytics/" target="_blank" rel="noopener">Machine Learning Service (in Database)</a>, other databases such as Teradata, are able to run R/Python script through an external script command. While Oracle supports <a href="https://docs.oracle.com/database/121/DMPRG/GUID-55C6ADBF-DA64-48B6-A424-5F0A59CD406D.htm#DMPRG701" target="_blank" rel="noopener">PMML model</a> through its data mining extension.</p>
<h4 id="Pub-Sub"><a href="#Pub-Sub" class="headerlink" title="Pub/Sub"></a>Pub/Sub</h4><p>Implementing real-time prediction through a pub/sub model allows to be able to properly handle the load through throttling. For engineers, it also means that they can just feed the event data through a single “logging” feed, to which different application can subscribe.</p>
<p>An example, of how this could be setup is shown below:</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpl246sj20m8058aah.jpg"></p>
<p>The page view event is fired to a specific event topic, on which two application subscribe a page view counter, and a prediction. Both of these application filter out specific relevant event from the topic for their purpose and consume the different messages in the topics. The page view counter app, provides data to power a dashboard, while the prediction app, updates the customer profile.</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wpsew6sj20fg08x74h.jpg"></p>
<p><strong>Sequence flow:</strong></p>
<p>Event messages are pushed to the pub/sub topic as they occur, the prediction app poll the topic for new messages. When a new message is retrieved by the prediction app, it will request and retrieve the customer profile and use the message and the profile information to make a prediction. which it will ultimately push back to the customer profile for further use.</p>
<p>A slightly different flow can be setup where the data is first consumed by an “enrichment app” that adds the profile information to the message and then pushes it back to a new topic to finally be consumed by the prediction app and pushed onto the customer profile.</p>
<p><strong>Technologies:</strong></p>
<p>The typical open source combination that you would find that support this kind of use case in the data ecosystem is a combination of Kafka and Spark streaming, but a different setup is possible on the cloud. On google notably a google pub-sub/dataflow (Beam) provides a good alternative to that combination, on azure a combination of Azure-Service Bus or Eventhub and Azure Functions can serve as a good way to consume the mesages and generate these predictions.</p>
<p><em>Web Service</em></p>
<p>We can implement models into productions as web-services. Implementing predictions model as web-services are particularly useful in engineering teams that are fragmented and that need to handle multiple different interfaces such as web, desktop and mobile.</p>
<p>Interfacing with the web-service could be setup in different way:</p>
<ul>
<li>either providing an identifier and having the web-service pull the required information, compute the prediction and return its’ value</li>
<li>Or by accepting a payload, converting it to a data-frame, making the prediction and returning its’ value.</li>
</ul>
<p>The second approach is usually recommended in cases, when there is a lot of interaction happening and a local cache is used to essentially buffer the synchronization with the backend systems, or when needing to make prediction at a different grain than a customer id, for instance when doing session based predictions.</p>
<p>The systems making use of local storage, tend to have a reducer function, which role is to calculate what would be the customer profile, should the event in local storage be integrated back. As such it provides an approximation of the customer profile based on local data.</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1g37wq0vsywj20ix0b5mxt.jpg"></p>
<p><strong>Sequence Flow</strong></p>
<p>The flow for handling the prediction using a mobile app, with local storage can be described in 4 phases.</p>
<p><em>Application Initialization (1 to 3)**</em>:** The application initializes, and makes a request to the customer profile, and retrieve its initial value back, and initialize the profile in local storage.</p>
<p><em>Applications (4):</em> The application stores the different events happening with the application into an array in local storage.</p>
<p><em>Prediction Preparation (5 to 8)**</em>:** The application wants to retrieve a new churn prediction, and therefore needs to prepare the information it needs to provide to the Churn Web-service. For that, it makes an initial request to local storage to retrieve the values of the profile and the array of events it has stored. Once they are retrieve, it makes a request to a reducer function providing these values as arguments, the reducer function outputs an updated* profile with the local events incorporated back into this profile.</p>
<p><em>Web-service Prediction (9 to 10):</em> The application makes a request to the churn prediction web-service, providing the different the updated*/reduced customer profile from step 8 as part of the payload. The web-service can then used the information provided by the payload to generate the prediction and output its value, back to the application.</p>
<p><strong>Technologies</strong></p>
<p>There are quite a few technologies that can be used to power a prediction web-service:</p>
<p><em>Functions</em></p>
<p>AWS Lambda functions, Google Cloud functions and Microsoft Azure Functions (although Python support is currently in Beta) offer an easy to setup interface to easily deploy scalable web-services.</p>
<p>For instance on Azure a prediction web-service could be implemented through a function looking roughly like this:</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> azure.functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(req: func.HttpRequest)</span> -&gt; func.HttpResponse:</span></span><br><span class="line">    logging.info(<span class="string">'Python HTTP trigger function processed a request.'</span>)</span><br><span class="line">    <span class="keyword">if</span> req.body:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            logging.info(<span class="string">"Converting Request to DataFrame"</span>)</span><br><span class="line">            req_body = req.get_json()</span><br><span class="line">            df_body  = pd.DataFrame([req_body])</span><br><span class="line"></span><br><span class="line">            logging.info(<span class="string">"Loadding the Prediction Model"</span>)</span><br><span class="line">            filename = <span class="string">"model.pckl"</span></span><br><span class="line">            loaded_model = joblib.load(filename)</span><br><span class="line">            <span class="comment"># Features names need to have been added to the pickled model</span></span><br><span class="line">            feature_names = loaded_model.feature_names</span><br><span class="line">            <span class="comment"># subselect only the feature names </span></span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Subselecting the dataframe"</span>)</span><br><span class="line">            df_subselect = df_body[feature_names]</span><br><span class="line">            </span><br><span class="line">            logging.info(<span class="string">"Predicting the Probability"</span>)</span><br><span class="line">            result = loaded_model.predict_proba(df_subselect)</span><br><span class="line">            <span class="comment"># We are looking at the probba prediction for class 1</span></span><br><span class="line">            prediction = result[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> func.HttpResponse(<span class="string">"&#123;prediction&#125;"</span>.format(prediction=prediction), status_code=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> func.HttpResponse(</span><br><span class="line">             <span class="string">"Please pass a name on the query string or in the request body"</span>,</span><br><span class="line">             status_code=<span class="number">400</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p><em>Container</em></p>
<p>An alternative to functions, is to deploy a flask or django application through a docker container (Amazon ECS, Azure Container Instance or Google Kubernetes Engine). Azure for instance provides an easy way to setup prediction containers through its’ <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where" target="_blank" rel="noopener">Azure Machine Learning service</a>.</p>
<p><em>Notebooks</em></p>
<p>Different notebooks providers such as <a href="https://docs.databricks.com/applications/mlflow/models.html" target="_blank" rel="noopener">databricks</a> and <a href="https://www.dataiku.com/dss/features/model-deployment/" target="_blank" rel="noopener">dataiku</a> have notably worked on simplifying the model deployment from their environments. These have the feature of setting up a webservice to a local environment or deploying to external systems such as Azure ML Service, Kubernetes engine etc…</p>
<h4 id="in-App"><a href="#in-App" class="headerlink" title="in App"></a>in App</h4><p>In certain situations when there are legal or privacy requirements that do not allow for data to be stored outside of an application, or there exists constraints such as having to upload a large amount of files, leveraging a model within the application tend to be the right approach.</p>
<p>Android-ML Kit or the likes of Caffe2 allows to leverage models within native applications, while <a href="https://www.tensorflow.org/js" target="_blank" rel="noopener">Tensorflow.js</a> and <a href="https://github.com/Microsoft/onnxjs" target="_blank" rel="noopener">ONNXJS</a> allow for running models directly in the browser or in apps leveraging javascripts.</p>
<h3 id="Considerations"><a href="#Considerations" class="headerlink" title="Considerations"></a>Considerations</h3><p>Beside the method of deployments of the models, they are quite a few important considerations to have when deploying to production.</p>
<p><strong>Model Complexity</strong></p>
<p>The complexity of the model itself, is the first considerations to have. Models such as a linear regressions and logistic regression are fairly easy to apply and do not usually take much space to store. Using more complex model such as a neural network or complex ensemble decision tree, will end up taking more time to compute, more time to load into memory on cold start and will prove more expensive to run</p>
<p><strong>Data Sources</strong></p>
<p>It is important to consider the difference that could occur between the datasource in productions and the one used for training. While it is important for the data used for the training to be in sync with the context it would be used for in production, it is often impractical to recalculate every value so that it becomes perfectly in-sync.</p>
<p><strong>Experimentation framework</strong></p>
<p>Setting up an experimentation framework, A/B testing the performance of different models versus objective metrics. And ensuring that there is sufficient tracking to accurately debug and evaluate models performance a posteriori.</p>
<h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>Choosing how to deploy a predictive models into production is quite a complex affair, there are different way to handle the lifecycle management of the predictive models, different formats to stores them, multiple ways to deploy them and very vast technical landscape to pick from.</p>
<p>Understanding specific use cases, the team’s technical and analytics maturity, the overall organization structure and its’ interactions, help come to the the right approach for deploying predictive models to production.</p>
<hr>
<p>More from me on <a href="https://medium.com/analytics-and-data" target="_blank" rel="noopener">Hacking Analytics</a>:</p>
<ul>
<li><a href="https://medium.com/analytics-and-data/on-the-evolution-of-data-engineering-c5e56d273e37" target="_blank" rel="noopener">One the evolution of Data Engineering</a></li>
<li><a href="https://medium.com/analytics-and-data/airflow-the-easy-way-f1c26859ee21" target="_blank" rel="noopener">Airflow, the easy way</a></li>
<li><a href="https://medium.com/analytics-and-data/e-commerce-analysis-data-structures-and-applications-6420c4fa65e7" target="_blank" rel="noopener">E-commerce Analysis: Data-Structures and Applications</a></li>
<li><a href="https://medium.com/analytics-and-data/setting-up-airflow-on-azure-connecting-to-ms-sql-server-8c06784a7e2b" target="_blank" rel="noopener">Setting up Airflow on Azure &amp; connecting to MS SQL Server</a></li>
<li><a href="https://medium.com/analytics-and-data/3-simple-rules-to-build-machine-learning-models-that-add-value-61106db88461" target="_blank" rel="noopener">3 simple rules to build machine learning Models that add value</a></li>
</ul>
<blockquote>
<p>简单看了下，没有深入纠结，本文主要从离线和实时两个方面介绍了ML的应用，给了一些简单的例子。</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Mechine-Learning/" rel="tag"># Mechine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/11/14/Scala Note/" rel="prev" title="Scala Note">
      <i class="fa fa-chevron-left"></i> Scala Note
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/11/14/Machine Learning/" rel="next" title="Machine Learning">
      Machine Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80Mzk4NC8yMDUyMA=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#One-off-Training"><span class="nav-number">1.</span> <span class="nav-text">One off Training</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-Format"><span class="nav-number">1.1.</span> <span class="nav-text">Model Format</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training"><span class="nav-number">1.2.</span> <span class="nav-text">Training</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Training"><span class="nav-number">2.</span> <span class="nav-text">Batch Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Real-time-training"><span class="nav-number">3.</span> <span class="nav-text">Real time training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-vs-Real-time-Prediction"><span class="nav-number">4.</span> <span class="nav-text">Batch vs. Real-time Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Prediction-Integration"><span class="nav-number">5.</span> <span class="nav-text">Batch Prediction Integration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Real-time-Prediction-integration"><span class="nav-number">6.</span> <span class="nav-text">Real-time Prediction integration</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Database-integrations"><span class="nav-number">6.1.</span> <span class="nav-text">Database integrations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pub-Sub"><span class="nav-number">6.2.</span> <span class="nav-text">Pub/Sub</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#in-App"><span class="nav-number">6.3.</span> <span class="nav-text">in App</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Considerations"><span class="nav-number">7.</span> <span class="nav-text">Considerations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrapping-Up"><span class="nav-number">8.</span> <span class="nav-text">Wrapping Up</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fly Hugh"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Fly Hugh</p>
  <div class="site-description" itemprop="description">WE CHOOSE TO  GO TO THE MARS</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">62</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/FlyMeToTheMars" title="GitHub → https://github.com/FlyMeToTheMars" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/flyhobo@live.com" title="E-Mail → flyhobo@live.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/3200892914" title="Weibo → https://weibo.com/u/3200892914" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Fly__HoBo" title="Twitter → https://twitter.com/Fly__HoBo" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fly Hugh</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
