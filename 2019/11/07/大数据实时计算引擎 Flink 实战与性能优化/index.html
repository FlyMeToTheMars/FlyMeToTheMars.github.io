<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/xingqiushangcheng.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/xingqiushangcheng.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Flink,">





  <link rel="alternate" href="/atom.xml" title="Mars" type="application/atom+xml">



  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "bfb40164"
    });
  daovoice('update');
  </script>





<meta name="description" content="大数据实时计算引擎 Flink 实战与性能优化 Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会">
<meta name="keywords" content="Flink">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据实时计算引擎 Flink 实战与性能优化">
<meta property="og:url" content="http://yoursite.com/2019/11/07/大数据实时计算引擎 Flink 实战与性能优化/index.html">
<meta property="og:site_name" content="Mars">
<meta property="og:description" content="大数据实时计算引擎 Flink 实战与性能优化 Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g9a0z5j7i6j20bp04tdgk.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plho80ksj20z50u042l.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plhzw2umj210c0dsdh7.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pli7o9s7j20gm12540a.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plijpowcj214o0u0jv0.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plio676aj21em0u0gqt.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pliw2d21j217f0u0tdh.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plj8y8wdj20p00goacz.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pljh92jpj216l0u00wg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkbpo1mj20u00ymq6n.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkidal4j21as0ps0va.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pllqzkewj219e0q0gnf.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm1lerpj219s0e6759.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm92vsuj219m0fudgu.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plmoolb2j219s0ts41e.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr0788dj214a0gy755.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr9ox1fj21gt0u0gp2.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plrjlud5j20lp0cuabw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pls0gqd0j22vw17qn0l.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plt1pm8wj21880kot9g.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pltiz3r3j20ph0lbwfh.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plu1ecy0j20op0fpad9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plud1dkhj218i0iw78b.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pluob9z7j21a00us11g.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plutdjgyj21q20h6q3c.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plv10mhgj215218utau.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plwkg73tj21li0u075w.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plx9wf35j22kg1bs0uc.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plxlthw1j20jb0crace.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plzy3s4ej215e1g045u.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm0elnw7j20mn0d7t9i.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm107pqzj217k104jw6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2ghwbrj20sg0g0gmg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2mmmyoj20wk0c6js2.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2t6k21j20tr06naad.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm30f5cqj20ub06nweq.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3b0i1qj20ub0asmxp.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3imkjuj20rm0asdg9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3y2va0j214w0m03zf.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5gkz7aj21810r9dgp.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5nafczj21980n940t.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn93djtcj20g205umx6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9bggknj21qo0yc407.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9ibun2j21fe0u040j.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9pgxz7j21nm0qwq48.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9wb7zgj22y81eedne.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pndeozfuj20u0130dll.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pne4yctlj24m02d04qp.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnejkllpj20t60qsdhg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnl2lqdzj21y417i0wa.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnmiv3uoj21fi106q8b.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnneh4l3j228o1j6myu.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0un83lnzj21r20tqk0e.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0unee0gmj21c00u0dn8.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uocicqoj21xw1aatf4.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uokn3kbj227w0pawfm.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uosbkccj22640mq3zx.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uoz4xbmj226w12e0zj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0up8brt7j20p20k0dg6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0upjwdhnj20yw080q2z.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0upq3ibrj21n8188dia.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uq05u0wj21pc0vy74y.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uq6lppkj226o1awdhc.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uqcmgspj224u1ewq50.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ur4cqi4j21b60aeq36.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urc5j0wj21ni14egno.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urirdxgj21s20legma.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urqumhzj22760mqgmr.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ury0iq8j225w1au3zz.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0us5qqsej227819wgnp.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uwc85x5j21li0u075w.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uwqobe4j227414ogn1.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uzyaa4aj21mm08mmx1.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v0en8wuj21m80hudfw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v0wadrcj21my0gmaa6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v1b9o6jj21mu0o2wet.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v50qmevj21sc0w2wgo.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v9hcl5wj21sa0xq410.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vazxmyfj21ek0ny769.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vbqi86ej225m0wmac7.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vcsdq1kj22gu0xaq5w.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vdsn6xyj21nc12qq3x.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vi0o960j20y20as3yg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vlvjsgdj22ro0p0wey.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vpz366lj20iv08gn0z.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vqk5hxnj22ua0is3ys.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vqqwlzej227017sdnt.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vrfwkxpj215m0f8q30.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vro7oi3j225y0ts0vk.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vsgous8j21ri0dcmxj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0xjwko8fj21200aqaa3.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0xkegqp2j213s0ju3yu.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0y9pd8g3j210u0heq35.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ybr9m9lj212e0h4mxg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yg0tundj21gg0ggq3e.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yi4jstvj215m17ejss.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yppinynj21cy0fodfz.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yvb90dej21em0rygnv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yvm1xntj21hq0hujsj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zf4f9lvj217e0om74r.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zfc2xczj20s20hsaaj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zfu71ysj21e20p0dgv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqks7btj21ik0pqdgr.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqrrk6kj21bi0nkgmc.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqyn66nj21cm0lsq46.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zrpl1ivj21xk0i674j.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zrx0blbj21hs1gsaci.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zs9dj5oj20r10a8t9f.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1164fu1kj21ri0dcmxj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga116diy51j21ji16aads.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga117rgkt8j21kw0yeakr.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1181fv12j20w20ri42r.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga118a05luj21kw0xgtk0.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga118gxfh0j22tc0mijrw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11hb4lnmj229g0l43z0.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11hipsabj218s0kq40p.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11ho1mg2j21di090754.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11id4k1dj22iy166q6l.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11j9hshgj22gq1acte2.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11jkx2dlj21po15y76t.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11jv4exfj22660qm470.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11k5j2xkj21i90u0alj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kelpl3j221a0lyguc.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kmeontj22ie14oad2.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kuirzfj21ha0lk74e.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11l1xtxpj21gw0cimx4.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11la4kjqj217m0g0gln.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11lhi3j5j21as0jq0su.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1svkzhh7j21kw0zknbx.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1svvcw3jj21h811ct9j.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sw3v5pzj22a011gtao.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sy3run1j21kw0zkti9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sypiutnj21ka10o0tl.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tmjwgh3j213s05wmx9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tmzgh4vj232e1jyjyg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1toe3bzdj21et0u0wqw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1toma7ioj21en0u0agv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tp0gu4uj21ec10utb6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpbszamj22h628igqp.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpnrm0zj21yh0k6qqj.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpvtgmwj21tm18gtcn.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tq2ra8kj21hu0o4abf.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tsa2sfwj22800a4abv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1ttq07q5j21xk05sjrz.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tu0f06kj216m0f4wfe.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tu7vqa2j227y10cdke.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tuhmykdj21qu0co75v.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tuqtg8pj224k172gud.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tv440c3j227k0okn03.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tvgngwkj21v40fw41q.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tvrecrwj22xc0p4n5q.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tw2katwj2230114wmp.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u105kjbj21980em75o.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u1cqotwj20r605tzk9.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u1psyytj225g0pstcr.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2170ijj214e0mcjsd.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2a7aouj215o0pg75r.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2jipebj224g12owj7.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1vh26in0j21k20qg752.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w0k0t1xj216a1kyq4n.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w14qx84j20pg0bw3ys.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w1dvbnrj21l40e4jrv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w1nxwk9j215w0z8gmm.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w20ksxwj21160aoq2y.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w2cze3oj20pl0e1jrv.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w2kwwogj20mn0bvjrw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3663xoj216a08qt8v.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3h8lofj212w0x776i.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3pegglj21tg1aaqbx.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w44qz7bj212w0x7wgo.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22tyfs6dj212w0dwaa2.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22u88wmij21k60z0jsl.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22uh2q3hj21120luweq.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22urjlpjj213y0jet9o.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22v2xtrmj248s16mtc3.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga234u9p5rj25eg1oyhaw.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga2355t4kzj23i52i3du7.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga235ip3bjj23op2j1qr6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga235uyrs8j23sr3937pe.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga2364ybmqj24lp1jdan6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga236iyh3zj21p00vwn3t.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga236rp2l1j21de0j4mz0.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25c4l92ij21ig0pudi6.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25drrn61j21iu0wm0w4.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25e8yjstj217e0luaak.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25eljzv0j21gg0cmdhq.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga3besz7gfj21b00uuq4h.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/bec9bff2gy1ga3bf4ctclj21qc27uthw.jpg">
<meta property="og:updated_time" content="2019-12-20T09:28:02.933Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大数据实时计算引擎 Flink 实战与性能优化">
<meta name="twitter:description" content="大数据实时计算引擎 Flink 实战与性能优化 Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会">
<meta name="twitter:image" content="http://ww1.sinaimg.cn/large/bec9bff2ly1g9a0z5j7i6j20bp04tdgk.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/11/07/大数据实时计算引擎 Flink 实战与性能优化/">





  <title>大数据实时计算引擎 Flink 实战与性能优化 | Mars</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'f6a8160e9467bb9adc80f36030e1c37b', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Mars</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/07/大数据实时计算引擎 Flink 实战与性能优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Fly Hugh">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mars">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">大数据实时计算引擎 Flink 实战与性能优化</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-07T17:07:24+08:00">
                2019-11-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Flink/" itemprop="url" rel="index">
                    <span itemprop="name">Flink</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2019/11/07/大数据实时计算引擎 Flink 实战与性能优化/" class="leancloud_visitors" data-flag-title="大数据实时计算引擎 Flink 实战与性能优化">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  76.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  315
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="大数据实时计算引擎-Flink-实战与性能优化"><a href="#大数据实时计算引擎-Flink-实战与性能优化" class="headerlink" title="大数据实时计算引擎 Flink 实战与性能优化"></a>大数据实时计算引擎 Flink 实战与性能优化</h1><blockquote>
<p>Flink作为流处理方案的最佳选择，还有流处理 批处理大一统之势，可谓必知必会</p>
</blockquote>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g9a0z5j7i6j20bp04tdgk.jpg" alt="undefined"></p>
<a id="more"></a> 
<h2 id="一、公司到底需不需要引入实时计算引擎？"><a href="#一、公司到底需不需要引入实时计算引擎？" class="headerlink" title="一、公司到底需不需要引入实时计算引擎？"></a>一、公司到底需不需要引入实时计算引擎？</h2><h3 id="实时计算需求"><a href="#实时计算需求" class="headerlink" title="实时计算需求"></a>实时计算需求</h3><p>大数据发展至今，数据呈指数倍的增长，对实效性的要求也越来越高，所以你可能接触到下面这类需求会越来越多。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">小田，你看能不能做个监控大屏实时查看促销活动销售额（GMV）？</span><br><span class="line"></span><br><span class="line">小朱，搞促销活动的时候能不能实时统计下网站的 PV/UV 啊？</span><br><span class="line"></span><br><span class="line">小鹏，我们现在搞促销活动能不能实时统计销量 Top5 啊？</span><br><span class="line"></span><br><span class="line">小李，怎么回事啊？现在搞促销活动结果服务器宕机了都没告警，能不能加一个？</span><br><span class="line"></span><br><span class="line">小刘，服务器这会好卡，是不是出了什么问题啊，你看能不能做个监控大屏实时查看机器的运行情况？</span><br><span class="line"></span><br><span class="line">小赵，我们线上的应用频繁出现 Error 日志，但是只有靠人肉上机器查看才知道情况，能不能在出现错误的时候及时告警通知？</span><br><span class="line"></span><br><span class="line">小夏，我们 1 元秒杀促销活动中有件商品被某个用户薅了 100 件，怎么都没有风控啊？</span><br><span class="line"></span><br><span class="line">小宋，你看我们搞促销活动能不能根据每个顾客的浏览记录实时推荐不同的商品啊？</span><br><span class="line"></span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>那这些场景对应着什么业务需求呢？我们来总结下，大概如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plho80ksj20z50u042l.jpg" alt="undefined"></p>
<p>初看这些需求，是不是感觉很难？那么我们接下来来分析一下该怎么去实现？</p>
<p>从这些需求来看，最根本的业务都是需要<strong>实时查看数据信息</strong>，那么首先我们得想想如何去采集这些实时数据，然后将采集的实时数据进行实时的计算，最后将计算后的结果下发到第三方。</p>
<h3 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h3><p>就上面这些需求，我们需要采集些什么数据呢？</p>
<ol>
<li>买家搜索记录信息</li>
<li>买家浏览的商品信息</li>
<li>买家下单订单信息</li>
<li>网站的所有浏览记录</li>
<li>机器 CPU/MEM/IO 信息</li>
<li>应用日志信息</li>
</ol>
<h3 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h3><p>采集后的数据实时上报后，需要做实时的计算，那我们怎么实现计算呢？</p>
<ol>
<li>计算所有商品的总销售额</li>
<li>统计单个商品的销量，最后求 Top5</li>
<li>关联用户信息和浏览信息、下单信息</li>
<li>统计网站所有的请求 IP 并统计每个 IP 的请求数量</li>
<li>计算一分钟内机器 CPU/MEM/IO 的平均值、75 分位数值</li>
<li>过滤出 Error 级别的日志信息</li>
</ol>
<h3 id="数据实时下发"><a href="#数据实时下发" class="headerlink" title="数据实时下发"></a>数据实时下发</h3><p>实时计算后的数据，需要及时的下发到下游，这里说的下游代表可能是：</p>
<ol>
<li>告警方式（邮件、短信、钉钉、微信）</li>
</ol>
<p>在计算层会将计算结果与阈值进行比较，超过阈值触发告警，让运维提前收到通知，及时做好应对措施，减少故障的损失大小。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plhzw2umj210c0dsdh7.jpg" alt="undefined"></p>
<ol>
<li>存储（消息队列、DB、文件系统等）</li>
</ol>
<p>数据存储后，监控大盘（Dashboard）从存储（ElasticSearch、HBase 等）里面查询对应指标的数据就可以查看实时的监控信息，做到对促销活动的商品销量、销售额，机器 CPU、MEM 等有实时监控，运营、运维、开发、领导都可以实时查看并作出对应的措施。</p>
<ul>
<li>让运营知道哪些商品是爆款，哪些店铺成交额最多，哪些商品成交额最高，哪些商品浏览量最多；</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pli7o9s7j20gm12540a.jpg" alt="undefined"></p>
<ul>
<li>让运维可以时刻了解机器的运行状况，出现宕机或者其他不稳定情况可以及时处理；</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plijpowcj214o0u0jv0.jpg" alt="undefined"></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plio676aj21em0u0gqt.jpg" alt="undefined"></p>
<ul>
<li>让开发知道自己项目运行的情况，从 Error 日志知道出现了哪些 Bug；</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pliw2d21j217f0u0tdh.jpg" alt="undefined"></p>
<ul>
<li>让领导知道这次促销赚了多少 money。</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plj8y8wdj20p00goacz.jpg" alt="undefined"></p>
<p><strong>从数据采集到数据计算再到数据下发，整个流程在上面的场景对实时性要求还是很高的，任何一个地方出现问题都将影响最后的效果！</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pljh92jpj216l0u00wg.jpg" alt="undefined"></p>
<h3 id="实时计算场景"><a href="#实时计算场景" class="headerlink" title="实时计算场景"></a>实时计算场景</h3><p>前面说了这么多场景，这里我们总结一下实时计算常用的场景有哪些呢？</p>
<ol>
<li>交通信号灯数据</li>
<li>道路上车流量统计（拥堵状况）</li>
<li>公安视频监控</li>
<li>服务器运行状态监控</li>
<li>金融证券公司实时跟踪股市波动，计算风险价值</li>
<li>数据实时 ETL</li>
<li>银行或者支付公司涉及金融盗窃的预警</li>
</ol>
<p>……</p>
<p>另外自己还做过调研，实时计算框架的使用场景有如下这些：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkbpo1mj20u00ymq6n.jpg" alt="undefined"></p>
<p>总结一下大概有下面这四类：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plkidal4j21as0ps0va.jpg" alt="undefined"></p>
<ol>
<li>实时数据存储</li>
</ol>
<p>实时数据存储的时候做一些微聚合、过滤某些字段、数据脱敏，组建数据仓库，实时 ETL。</p>
<ol start="2">
<li>实时数据分析</li>
</ol>
<p>实时数据接入机器学习框架（TensorFlow）或者一些算法进行数据建模、分析，然后动态的给出商品推荐、广告推荐</p>
<ol start="3">
<li>实时监控告警</li>
</ol>
<p>金融相关涉及交易、实时风控、车流量预警、服务器监控告警、应用日志告警</p>
<ol start="4">
<li>实时数据报表</li>
</ol>
<p>活动营销时销售额/销售量大屏，TopN 商品</p>
<p>说到实时计算，这里不得不讲一下和传统的离线计算的区别！</p>
<h3 id="离线计算-vs-实时计算"><a href="#离线计算-vs-实时计算" class="headerlink" title="离线计算 vs 实时计算"></a>离线计算 vs 实时计算</h3><p>再讲这两个区别之前，我们先来看看流处理和批处理的区别：</p>
<h4 id="流处理与批处理"><a href="#流处理与批处理" class="headerlink" title="流处理与批处理"></a>流处理与批处理</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pllqzkewj219e0q0gnf.jpg" alt="undefined"></p>
<p>看完流处理与批处理这两者的区别之后，我们来抽象一下前面文章的场景需求（<strong>实时计算</strong>）：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm1lerpj219s0e6759.jpg" alt="undefined"></p>
<p>实时计算需要不断的从 MQ 中读取采集的数据，然后处理计算后往 DB 里存储，在计算这层你无法感知到会有多少数据量过来、要做一些简单的操作（过滤、聚合等）、及时将数据下发。</p>
<p>相比传统的<strong>离线计算</strong>，它却是这样的：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plm92vsuj219m0fudgu.jpg" alt="undefined"></p>
<p>在计算这层，它从 DB（不限 MySQL，还有其他的存储介质）里面读取数据，该数据一般就是固定的（前一天、前一星期、前一个月），然后再做一些复杂的计算或者统计分析，最后生成可供直观查看的报表（dashboard）。</p>
<h4 id="离线计算的特点"><a href="#离线计算的特点" class="headerlink" title="离线计算的特点"></a>离线计算的特点</h4><ol>
<li>数据量大且时间周期长（一天、一星期、一个月、半年、一年）</li>
<li>在大量数据上进行复杂的批量运算</li>
<li>数据在计算之前已经固定，不再会发生变化</li>
<li>能够方便的查询批量计算的结果</li>
</ol>
<h4 id="实时计算的特点"><a href="#实时计算的特点" class="headerlink" title="实时计算的特点"></a>实时计算的特点</h4><p>在大数据中与离线计算对应的则是实时计算，那么实时计算有什么特点呢？由于应用场景的各不相同，所以这两种计算引擎接收数据的方式也不太一样：离线计算的数据是固定的（不再会发生变化），通常离线计算的任务都是定时的，如：每天晚上 0 点的时候定时计算前一天的数据，生成报表；然而实时计算的数据源却是流式的。</p>
<p>这里我不得不讲讲什么是流式数据呢？我的理解是比如你在淘宝上下单了某个商品或者点击浏览了某件商品，你就会发现你的页面立马就会给你推荐这种商品的广告和类似商品的店铺，这种就是属于实时数据处理然后作出相关推荐，这类数据需要不断的从你在网页上的点击动作中获取数据，之后进行实时分析然后给出推荐。</p>
<h4 id="流式数据的特点"><a href="#流式数据的特点" class="headerlink" title="流式数据的特点"></a>流式数据的特点</h4><ol>
<li>数据实时到达</li>
<li>数据到达次序独立，不受应用系统所控制</li>
<li>数据规模大且无法预知容量</li>
<li>原始数据一经处理，除非特意保存，否则不能被再次取出处理，或者再次提取数据代价昂贵</li>
</ol>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plmoolb2j219s0ts41e.jpg" alt="undefined"></p>
<h4 id="实时计算的优势"><a href="#实时计算的优势" class="headerlink" title="实时计算的优势"></a>实时计算的优势</h4><p><strong>实时计算一时爽，一直实时计算一直爽</strong>，对于持续生成最新数据的场景，采用流数据处理是非常有利的。例如，再监控服务器的一些运行指标的时候，能根据采集上来的实时数据进行判断，当超出一定阈值的时候发出警报，进行提醒作用。再如通过处理流数据生成简单的报告，如五分钟的窗口聚合数据平均值。复杂的事情还有在流数据中进行数据多维度关联、聚合、塞选，从而找到复杂事件中的根因。更为复杂的是做一些复杂的数据分析操作，如应用机器学习算法，然后根据算法处理后的数据结果提取出有效的信息，作出、给出不一样的推荐内容，让不同的人可以看见不同的网页（千人千面）。</p>
<h3 id="实时计算面临的挑战"><a href="#实时计算面临的挑战" class="headerlink" title="实时计算面临的挑战"></a>实时计算面临的挑战</h3><ol>
<li>数据处理唯一性（如何保证数据只处理一次？至少一次？最多一次？）</li>
<li>数据处理的及时性（采集的实时数据量太大的话可能会导致短时间内处理不过来，如何保证数据能够及时的处理，不出现数据堆积？）</li>
<li>数据处理层和存储层的可扩展性（如何根据采集的实时数据量的大小提供动态扩缩容？）</li>
<li>数据处理层和存储层的容错性（如何保证数据处理层和存储层高可用，出现故障时数据处理层和存储层服务依旧可用？）</li>
</ol>
<p>因为各种需求，也就造就了现在不断出现实时计算框架，在 1.2 节中将重磅介绍如今最火的实时计算框架 —— Flink，在 1.3 节中会对比介绍 Spark Streaming、Structured Streaming 和 Storm 之间的区别。</p>
<h3 id="小结与反思"><a href="#小结与反思" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节从实时计算的需求作为切入点，然后分析该如何去完成这种实时计算的需求，从而得知整个过程包括数据采集、数据计算、数据存储等，接着总结了实时计算场景的类型。最后开始介绍离线计算与实时计算的区别，并提出了实时计算可能带来的挑战。你们公司有文中所讲的类似需求吗？你是怎么解决的呢？</p>
<hr>
<h2 id="二、彻底了解大数据实时计算框架-Flink"><a href="#二、彻底了解大数据实时计算框架-Flink" class="headerlink" title="二、彻底了解大数据实时计算框架 Flink"></a>二、彻底了解大数据实时计算框架 Flink</h2><p>在 1.1 节中讲解了日常开发常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。</p>
<ul>
<li>第一代：Hadoop 承载的 MapReduce</li>
<li>第二代：支持 DAG（有向无环图）框架的计算引擎 Tez 和 Oozie，主要还是批处理任务</li>
<li>第三代：支持 Job 内部的 DAG（有向无环图），以 Spark 为代表</li>
<li>第四代：大数据统一计算引擎，包括流处理、批处理、AI、Machine Learning、图计算等，以 Flink 为代表</li>
</ul>
<p>或许会有人不同意以上的分类，笔者觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个。</p>
<p>本文将对 Flink 的整体架构和 Flink 的多种特性做个详细的介绍！在讲 Flink 之前的话，我们先来看看<strong>数据集类型</strong>和<strong>数据运算模型</strong>的种类。</p>
<h4 id="数据集类型"><a href="#数据集类型" class="headerlink" title="数据集类型"></a>数据集类型</h4><ul>
<li>无穷数据集：无穷的持续集成的数据集合</li>
<li>有界数据集：有限不会改变的数据集合</li>
</ul>
<p>那么那些常见的无穷数据集有哪些呢？</p>
<ul>
<li>用户与客户端的实时交互数据</li>
<li>应用实时产生的日志</li>
<li>金融市场的实时交易记录</li>
<li>…</li>
</ul>
<h4 id="数据运算模型"><a href="#数据运算模型" class="headerlink" title="数据运算模型"></a>数据运算模型</h4><ul>
<li>流式：只要数据一直在产生，计算就持续地进行</li>
<li>批处理：在预先定义的时间内运行计算，当计算完成时释放计算机资源</li>
</ul>
<p>那么我们再来看看 Flink 它是什么呢？</p>
<h3 id="Flink-是什么？"><a href="#Flink-是什么？" class="headerlink" title="Flink 是什么？"></a>Flink 是什么？</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr0788dj214a0gy755.jpg" alt="undefined"></p>
<p>Flink 是一个针对流数据和批数据的分布式处理引擎，代码主要是由 Java 实现，部分代码是 Scala。它可以处理有界的批量数据集、也可以处理无界的实时数据集。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已，所以 Flink 也是一款真正的流批统一的计算引擎。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plr9ox1fj21gt0u0gp2.jpg" alt="undefined"></p>
<p>Flink 提供了 State、Checkpoint、Time、Window 等，它们为 Flink 提供了基石，本篇文章下面会稍作讲解，具体深度分析后面会有专门的文章来讲解。</p>
<h3 id="Flink-整体架构"><a href="#Flink-整体架构" class="headerlink" title="Flink 整体架构"></a>Flink 整体架构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plrjlud5j20lp0cuabw.jpg" alt="undefined"></p>
<p>从下至上：</p>
<ol>
<li>部署：Flink 支持本地运行（IDE 中直接运行程序）、能在独立集群（Standalone 模式）或者在被 YARN、Mesos、K8s 管理的集群上运行，也能部署在云上。</li>
<li>运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。</li>
<li>API：DataStream、DataSet、Table、SQL API。</li>
<li>扩展库：Flink 还包括用于 CEP（复杂事件处理）、机器学习、图形处理等场景。</li>
</ol>
<h3 id="Flink-支持多种方式部署"><a href="#Flink-支持多种方式部署" class="headerlink" title="Flink 支持多种方式部署"></a>Flink 支持多种方式部署</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pls0gqd0j22vw17qn0l.jpg" alt="undefined"></p>
<p>作为一个计算引擎，如果要做的足够完善，除了它自身的各种特点要包含，还得支持各种生态圈，比如部署的情况，Flink 是支持以 Standalone、YARN、Kubernetes、Mesos 等形式部署的。</p>
<ul>
<li>Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群</li>
<li>Standalone：在 Flink 目录下执行 <code>bin/start-cluster.sh</code> 脚本则会启动一个 Standalone 模式的集群</li>
<li>YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下：</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plt1pm8wj21880kot9g.jpg" alt="undefined"></p>
<ul>
<li>Kubernetes：Kubernetes 是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性，Flink 也支持部署在 Kubernetes 上，在 <a href="https://github.com/Aleksandr-Filichkin/flink-k8s/blob/master/flow.jpg" target="_blank" rel="noopener">GitHub</a> 看到有下面这种运行架构的。</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pltiz3r3j20ph0lbwfh.jpg" alt="undefined"></p>
<p>通常上面四种居多，另外还支持 AWS、MapR、Aliyun OSS 等。</p>
<h3 id="Flink-分布式运行"><a href="#Flink-分布式运行" class="headerlink" title="Flink 分布式运行"></a>Flink 分布式运行</h3><p>Flink 作业提交架构流程可见下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plu1ecy0j20op0fpad9.jpg" alt="undefined"></p>
<p>1、Program Code：我们编写的 Flink 应用程序代码</p>
<p>2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户</p>
<p>3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理 checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件</p>
<p>4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽（Slot 个数）决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plud1dkhj218i0iw78b.jpg" alt="undefined"></p>
<p>Flink 提供了不同的抽象级别的 API 以开发流式或批处理应用。</p>
<ul>
<li>最底层提供了有状态流。它将通过 Process Function 嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致性、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。</li>
<li>DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换或者计算。</li>
<li>Table API 是以表为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。 你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。</li>
<li>Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。</li>
</ul>
<p>Flink 除了 DataStream 和 DataSet API，它还支持 Table/SQL API，Flink 也将通过 SQL API 来构建统一的大数据流批处理引擎，因为在公司中通常会有那种每天定时生成报表的需求（批处理的场景，每晚定时跑一遍昨天的数据生成一个结果报表），但是也是会有流处理的场景（比如采用 Flink 来做实时性要求很高的需求），于是慢慢的整个公司的技术选型就变得越来越多了，这样开发人员也就要面临着学习两套不一样的技术框架，运维人员也需要对两种不一样的框架进行环境搭建和作业部署，平时还要维护作业的稳定性。</p>
<p>当我们的系统变得越来越复杂了，作业越来越多了，这对于开发人员和运维来说简直就是噩梦，没准哪天凌晨晚上就被生产环境的告警电话给叫醒。所以 Flink 系统能通过 SQL API 来解决批流统一的痛点，这样不管是开发还是运维，他们只需要关注一个计算框架就行，从而减少企业的用人成本和后期开发运维成本。</p>
<h3 id="Flink-程序与数据流结构"><a href="#Flink-程序与数据流结构" class="headerlink" title="Flink 程序与数据流结构"></a>Flink 程序与数据流结构</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pluob9z7j21a00us11g.jpg" alt="undefined"></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plutdjgyj21q20h6q3c.jpg" alt="undefined"></p>
<p>一个完整的 Flink 应用程序结构就是如上两图所示：</p>
<p>1、Source：数据输入，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</p>
<p>2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</p>
<p>3、Sink：数据输出，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。</p>
<h3 id="Flink-支持丰富的-Connector"><a href="#Flink-支持丰富的-Connector" class="headerlink" title="Flink 支持丰富的 Connector"></a>Flink 支持丰富的 Connector</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plv10mhgj215218utau.jpg" alt="undefined"></p>
<p>通过源码可以发现不同版本的 Kafka、不同版本的 ElasticSearch、Cassandra、HBase、Hive、HDFS、RabbitMQ 都是支持的，除了流应用的 Connector 是支持的，另外还支持 SQL。</p>
<p>再就是要考虑计算的数据来源和数据最终存储，因为 Flink 在大数据领域的的定位就是实时计算，它不做存储（虽然 Flink 中也有 State 去存储状态数据，这里说的存储类似于 MySQL、ElasticSearch 等存储），所以在计算的时候其实你需要考虑的是数据源来自哪里，计算后的结果又存储到哪里去。庆幸的是 Flink 目前已经支持大部分常用的组件了，比如在 Flink 中已经支持了如下这些 Connector：</p>
<ul>
<li>不同版本的 Kafka</li>
<li>不同版本的 ElasticSearch</li>
<li>Redis</li>
<li>MySQL</li>
<li>Cassandra</li>
<li>RabbitMQ</li>
<li>HBase</li>
<li>HDFS</li>
<li>…</li>
</ul>
<p>这些 Connector 除了支持流作业外，目前还有还有支持 SQL 作业的，除了这些自带的 Connector 外，还可以通过 Flink 提供的接口做自定义 Source 和 Sink（在 3.8 节中）。</p>
<h3 id="Flink-提供事件时间-amp-处理时间语义"><a href="#Flink-提供事件时间-amp-处理时间语义" class="headerlink" title="Flink 提供事件时间&amp;处理时间语义"></a>Flink 提供事件时间&amp;处理时间语义</h3><p>Flink 支持多种 Time，比如 Event time、Ingestion Time、Processing Time，后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析</a> 中会很详细的讲解 Flink 中 Time 的概念。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plwkg73tj21li0u075w.jpg" alt="undefined"></p>
<h3 id="Flink-提供灵活的窗口机制"><a href="#Flink-提供灵活的窗口机制" class="headerlink" title="Flink 提供灵活的窗口机制"></a>Flink 提供灵活的窗口机制</h3><p>Flink 支持多种 Window，比如 Time Window、Count Window、Session Window，还支持自定义 Window。后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">如何使用 Flink Window 及 Window 基本概念与实现原理</a> 中会很详细的讲解 Flink 中 Window 的概念。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plx9wf35j22kg1bs0uc.jpg" alt="undefined"></p>
<h3 id="Flink-并行的执行任务"><a href="#Flink-并行的执行任务" class="headerlink" title="Flink 并行的执行任务"></a>Flink 并行的执行任务</h3><p>Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为 operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行； operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为 1：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plxlthw1j20jb0crace.jpg" alt="undefined"></p>
<h3 id="Flink-支持状态存储和容错"><a href="#Flink-支持状态存储和容错" class="headerlink" title="Flink 支持状态存储和容错"></a>Flink 支持状态存储和容错</h3><p>Flink 是一款有状态的流处理框架，它提供了丰富的状态访问接口，按照数据的划分方式，可以分为 Keyed State 和 Operator State，在 Keyed State 中又提供了多种数据结构：</p>
<ul>
<li>ValueState</li>
<li>MapState</li>
<li>ListState</li>
<li>ReducingState</li>
<li>AggregatingState</li>
</ul>
<p>另外状态存储也支持多种方式：</p>
<ul>
<li>MemoryStateBackend：存储在内存中</li>
<li>FsStateBackend：存储在文件中</li>
<li>RocksDBStateBackend：存储在 RocksDB 中</li>
</ul>
<p>Flink 中支持使用 Checkpoint 来提高程序的可靠性，开启了 Checkpoint 之后，Flink 会按照一定的时间间隔对程序的运行状态进行备份，当发生故障时，Flink 会将所有任务的状态恢复至最后一次发生 Checkpoint 中的状态，并从那里开始重新开始执行。</p>
<p>另外 Flink 还支持根据 Savepoint 从已停止作业的运行状态进行恢复，这种方式需要通过命令进行触发。</p>
<h3 id="Flink-实现了自己的内存管理机制"><a href="#Flink-实现了自己的内存管理机制" class="headerlink" title="Flink 实现了自己的内存管理机制"></a>Flink 实现了自己的内存管理机制</h3><p>//todo:深入内存到底要不要在第九章讲？ Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。我们在后面的文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db69938f6a6211cb96164da" target="_blank" rel="noopener">深入探索 Flink 内存管理机制</a> 会深入讲解 Flink 里面的内存管理机制。</p>
<h3 id="Flink-支持多种扩展库"><a href="#Flink-支持多种扩展库" class="headerlink" title="Flink 支持多种扩展库"></a>Flink 支持多种扩展库</h3><p>Flink 扩展库中含有机器学习、Gelly 图形处理、CEP 复杂事件处理、State Processing API 等，关于这块内容可以在第六章查看。</p>
<h3 id="小结与反思-1"><a href="#小结与反思-1" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节在开始介绍 Flink 之前先讲解了下数据集类型和数据运算模型，接着开始介绍 Flink 的各种特性，</p>
<hr>
<h2 id="三、大数据框架-Flink、Blink、Spark-Streaming、Structured-Streaming和-Storm-的区别。"><a href="#三、大数据框架-Flink、Blink、Spark-Streaming、Structured-Streaming和-Storm-的区别。" class="headerlink" title="三、大数据框架 Flink、Blink、Spark Streaming、Structured Streaming和 Storm 的区别。"></a>三、大数据框架 Flink、Blink、Spark Streaming、Structured Streaming和 Storm 的区别。</h2><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>Flink 是一个针对流数据和批数据分布式处理的引擎，在某些对实时性要求非常高的场景，基本上都是采用 Flink 来作为计算引擎，它不仅可以处理有界的批数据，还可以处理无界的流数据，在 Flink 的设计愿想就是将批处理当成是流处理的一种特例。</p>
<p>在 Flink 的母公司 <a href="https://www.eu-startups.com/2019/01/alibaba-takes-over-berlin-based-streaming-analytics-startup-data-artisans/" target="_blank" rel="noopener">Data Artisans 被阿里收购</a>之后，阿里也在开始逐步将内部的 Blink 代码开源出来并合并在 Flink 主分支上。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8plzy3s4ej215e1g045u.jpg" alt="undefined"></p>
<p>而 Blink 一个很强大的特点就是它的 SQL API 很强大，社区也在 Flink 1.9 版本将 Blink 开源版本大部分代码合进了 Flink 主分支。</p>
<h3 id="Blink"><a href="#Blink" class="headerlink" title="Blink"></a>Blink</h3><p>Blink 是早期阿里在 Flink 的基础上开始修改和完善后在内部创建的分支，然后 Blink 目前在阿里服务于阿里集团内部搜索、推荐、广告、菜鸟物流等大量核心实时业务。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm0elnw7j20mn0d7t9i.jpg" alt="undefined"></p>
<p>Blink 在阿里内部错综复杂的业务场景中锻炼成长着，经历了内部这么多用户的反馈（各种性能、资源使用率、易用性等诸多方面的问题），Blink 都做了针对性的改进。在 Flink Forward China 峰会上，阿里巴巴集团副总裁周靖人宣布 Blink 在 2019 年 1 月正式开源，同时阿里也希望 Blink 开源后能进一步加深与 Flink 社区的联动，</p>
<p>Blink 开源地址：<a href="https://github.com/apache/flink/tree/blink" target="_blank" rel="noopener">https://github.com/apache/flink/tree/blink</a></p>
<p>开源版本 Blink 的主要功能和优化点：</p>
<p>1、Runtime 层引入 Pluggable Shuffle Architecture，开发者可以根据不同的计算模型或者新硬件的需要实现不同的 shuffle 策略进行适配；为了性能优化，Blink 可以让算子更加灵活的 chain 在一起，避免了不必要的数据传输开销；在 BroadCast Shuffle 模式中，Blink 优化掉了大量的不必要的序列化和反序列化开销；Blink 提供了全新的 JM FailOver 机制，JM 发生错误之后，新的 JM 会重新接管整个 JOB 而不是重启 JOB，从而大大减少了 JM FailOver 对 JOB 的影响；Blink 支持运行在 Kubernetes 上。</p>
<p>2、SQL/Table API 架构上的重构和性能的优化是 Blink 开源版本的一个重大贡献。</p>
<p>3、Hive 的兼容性，可以直接用 Flink SQL 去查询 Hive 的数据，Blink 重构了 Flink catalog 的实现，并且增加了两种 catalog，一个是基于内存存储的 FlinkInMemoryCatalog，另外一个是能够桥接 Hive metaStore 的 HiveCatalog。</p>
<p>4、Zeppelin for Flink</p>
<p>5、Flink Web，更美观的 UI 界面，查看日志和监控 Job 都变得更加方便</p>
<p>对于开源那会看到一个对话让笔者感到很震撼：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Blink 开源后，两个开源项目之间的关系会是怎样的？未来 Flink 和 Blink 也会由不同的团队各自维护吗？</span><br><span class="line"></span><br><span class="line">Blink 永远不会成为另外一个项目，如果后续进入 Apache 一定是成为 Flink 的一部分</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm107pqzj217k104jw6.jpg" alt="undefined"></p>
<p>在 Blink 开源那会，笔者就将源码自己编译了一份，然后自己在本地一直运行着，感兴趣的可以看看文章 <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/" target="_blank" rel="noopener">阿里巴巴开源的 Blink 实时计算框架真香</a> ，你会发现 Blink 的 UI 还是比较美观和实用的。</p>
<p>如果你还对 Blink 有什么疑问，可以看看下面两篇文章：</p>
<p><a href="https://www.infoq.cn/article/wZ_b7Hw9polQWp3mTwVh" target="_blank" rel="noopener">阿里重磅开源 Blink：为什么我们等了这么久？</a></p>
<p><a href="https://www.infoq.cn/article/ZkOGAl6_vkZDTk8tfbbg" target="_blank" rel="noopener">重磅！阿里巴巴 Blink 正式开源，重要优化点解读</a></p>
<h3 id="1-3-3-Spark"><a href="#1-3-3-Spark" class="headerlink" title="1.3.3 Spark"></a>1.3.3 Spark</h3><p>Apache Spark 是一种包含流处理能力的下一代批处理框架。与 Hadoop 的 MapReduce 引擎基于各种相同原则开发而来的 Spark 主要侧重于通过完善的内存计算和处理优化机制加快批处理工作负载的运行速度。</p>
<p>Spark 可作为独立集群部署（需要相应存储层的配合），或可与 Hadoop 集成并取代 MapReduce 引擎。</p>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2ghwbrj20sg0g0gmg.jpg" alt="undefined"></p>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener">Spark Streaming</a> 是 Spark API 核心的扩展，可实现实时数据的快速扩展，高吞吐量，容错处理。数据可以从很多来源（如 Kafka、Flume、Kinesis 等）中提取，并且可以通过很多函数来处理这些数据，处理完后的数据可以直接存入数据库或者 Dashboard 等。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2mmmyoj20wk0c6js2.jpg" alt="undefined"></p>
<p><strong>Spark Streaming 的内部实现原理</strong>是接收实时输入数据流并将数据分成批处理，然后由 Spark 引擎处理以批量生成最终结果流，也就是常说的 micro-batch 模式。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm2t6k21j20tr06naad.jpg" alt="undefined"></p>
<p>DStreams 是 Spark Streaming 提供的基本的抽象，它代表一个连续的数据流。。它要么是从源中获取的输入流，要么是输入流通过转换算子生成的处理后的数据流。在内部实现上，DStream 由连续的序列化 RDD 来表示，每个 RDD 含有一段时间间隔内的数据：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm30f5cqj20ub06nweq.jpg" alt="undefined"></p>
<p>任何对 DStreams 的操作都转换成了对 DStreams 隐含的 RDD 的操作。例如 flatMap 操作应用于 lines 这个 DStreams 的每个 RDD，生成 words 这个 DStreams 的 RDD 过程如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3b0i1qj20ub0asmxp.jpg" alt="undefined"></p>
<p>通过 Spark 引擎计算这些隐含 RDD 的转换算子。DStreams 操作隐藏了大部分的细节，并且为了更便捷，为开发者提供了更高层的 API。</p>
<p><strong>Spark 支持的滑动窗口</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3imkjuj20rm0asdg9.jpg" alt="undefined"></p>
<p>它和 Flink 的滑动窗口类似，支持传入两个参数，一个代表窗口长度，一个代表滑动间隔。</p>
<p><strong>Spark 支持更多的 API</strong></p>
<p>因为 Spark 是使用 Scala 开发的居多，所以从官方文档就可以看得到对 Scala 的 API 支持的很好，而 Flink 源码实现主要以 Java 为主，因此也对 Java API 更友好，从两者目前支持的 API 友好程度，应该是 Spark 更好，它目前也支持 Python API，但是 Flink 新版本也在不断的支持 Python API。</p>
<p><strong>Spark 支持更多的 Machine Learning Lib</strong></p>
<p>你可以很轻松的使用 Spark MLlib 提供的机器学习算法，然后将这些这些机器学习算法模型应用在流数据中，目前 Flink Machine Learning 这块的内容还较少，不过阿里宣称会开源些 Flink Machine Learning 算法，保持和 Spark 目前已有的算法一致，我自己在 GitHub 上看到一个阿里开源的仓库，感兴趣的可以看看 <a href="https://github.com/alibaba/flink-ai-extended" target="_blank" rel="noopener">flink-ai-extended</a>。</p>
<p><strong>Spark Checkpoint</strong></p>
<p>Spark 和 Flink 一样都支持 Checkpoint，但是 Flink 还支持 Savepoint，你可以在停止 Flink 作业的时候使用 Savepoint 将作业的状态保存下来，当作业重启的时候再从 Savepoint 中将停止作业那个时刻的状态恢复起来，保持作业的状态和之前一致。</p>
<p><strong>Spark SQL</strong></p>
<p>Spark 除了 DataFrames 和 Datasets 外，也还有 SQL API，这样你就可以通过 SQL 查询数据，另外 Spark SQL 还可以用于从 Hive 中读取数据。</p>
<p>从 Spark 官网也可以看到很多比较好的特性，这里就不一一介绍了，如果对 Spark 感兴趣的话也可以去<a href="https://spark.apache.org/docs/latest/index.html" target="_blank" rel="noopener">官网</a>了解一下具体的使用方法和实现原理。</p>
<p><strong>Spark Streaming 优缺点</strong></p>
<p>1、优点</p>
<ul>
<li>Spark Streaming 内部的实现和调度方式高度依赖 Spark 的 DAG 调度器和 RDD，这就决定了 Spark Streaming 的设计初衷必须是粗粒度方式的，也就无法做到真正的实时处理</li>
<li>Spark Streaming 的粗粒度执行方式使其确保“处理且仅处理一次”的特性，同时也可以更方便地实现容错恢复机制。</li>
<li>由于 Spark Streaming 的 DStream 本质是 RDD 在流式数据上的抽象，因此基于 RDD 的各种操作也有相应的基于 DStream 的版本，这样就大大降低了用户对于新框架的学习成本，在了解 Spark 的情况下用户将很容易使用 Spark Streaming。</li>
</ul>
<p>2、缺点</p>
<ul>
<li>Spark Streaming 的粗粒度处理方式也造成了不可避免的数据延迟。在细粒度处理方式下，理想情况下每一条记录都会被实时处理，而在 Spark Streaming 中，数据需要汇总到一定的量后再一次性处理，这就增加了数据处理的延迟，这种延迟是由框架的设计引入的，并不是由网络或其他情况造成的。</li>
<li>使用的是 Processing Time 而不是 Event Time</li>
</ul>
<h3 id="Structured-Streaming"><a href="#Structured-Streaming" class="headerlink" title="Structured Streaming"></a>Structured Streaming</h3><p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">Structured Streaming</a> 是一种基于 Spark SQL 引擎的可扩展且容错的流处理引擎，它最关键的思想是将实时数据流视为一个不断增加的表，从而就可以像操作批的静态数据一样来操作流数据了。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm3y2va0j214w0m03zf.jpg" alt="undefined"></p>
<p>会对输入的查询生成“结果表”，每个触发间隔（例如，每 1 秒）新行将附加到输入表，最终更新结果表，每当结果表更新时，我们希望能够将更改后的结果写入外部接收器去。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5gkz7aj21810r9dgp.jpg" alt="undefined"></p>
<p>终于支持事件时间的窗口操作：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pm5nafczj21980n940t.jpg" alt="undefined"></p>
<p>对比你会发现这个 Structured Streaming 怎么和 Flink 这么像，哈哈哈哈，不过这确实是未来的正确之路，两者的功能也会越来越相像的，期待它们出现更加令人兴奋的功能。</p>
<p>如果你对 Structured Streaming 感兴趣的话，可以去<a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">官网</a>做更深一步的了解，顺带附上 <a href="https://cs.stanford.edu/~matei/papers/2018/sigmod_structured_streaming.pdf" target="_blank" rel="noopener">Structured Streaming</a> 的 Paper，同时也附上一位阿里小哥的 PPT —— <a href="https://www.slidestalk.com/s/FromSparkStreamingtoStructuredStreaming58639" target="_blank" rel="noopener">From Spark Streaming to Structured Streaming</a>。</p>
<h3 id="Flink-VS-Spark"><a href="#Flink-VS-Spark" class="headerlink" title="Flink VS Spark"></a>Flink VS Spark</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn93djtcj20g205umx6.jpg" alt="undefined"></p>
<p>通过上面你应该可以了解到 Flink 对比 Spark Streaming 的微批处理来说是有一定的优势，并且 Flink 还有一些特别的优点，比如灵活的时间语义、多种时间窗口、结合水印处理延迟数据等，但是 Spark 也有自己的一些优势，功能在早期来说是很完善的，并且新版本的 Spark 还添加了 Structured Streaming，它和 Flink 的功能很相近，两个还是值得更深入的对比，期待后面官方的测试对比报告。</p>
<h3 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h3><p>Storm 是一个开源的分布式实时计算系统，可以简单、可靠的处理大量的数据流。Storm 支持水平扩展，具有高容错性，保证每个消息都会得到处理，Strom 本身是无状态的，通过 ZooKeeper 管理分布式集群环境和集群状态。</p>
<h4 id="Storm-核心组件"><a href="#Storm-核心组件" class="headerlink" title="Storm 核心组件"></a>Storm 核心组件</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9bggknj21qo0yc407.jpg" alt="undefined"></p>
<p>Nimbus：负责资源分配和任务调度，Nimbus 对任务的分配信息会存储在 Zookeeper 上面的目录下。</p>
<p>Supervisor：负责去 Zookeeper 上的指定目录接受 Nimbus 分配的任务，启动和停止属于自己管理的 Worker 进程。它是当前物理机器上的管理者 —— 通过配置文件设置当前 Supervisor 上启动多少个 Worker。</p>
<p>Worker：运行具体处理组件逻辑的进程，Worker 运行的任务类型只有两种，一种是 Spout 任务，一种是 Bolt 任务。</p>
<p>Task：Worker 中每一个 Spout/Bolt 的线程称为一个 Task. 在 Storm0.8 之后，Task 不再与物理线程对应，不同 Spout/Bolt 的 Task 可能会共享一个物理线程，该线程称为 Executor。</p>
<p>Worker、Task、Executor 三者之间的关系:</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9ibun2j21fe0u040j.jpg" alt="undefined"></p>
<h4 id="Storm-核心概念"><a href="#Storm-核心概念" class="headerlink" title="Storm 核心概念"></a>Storm 核心概念</h4><ul>
<li>Nimbus：Storm 集群主节点，负责资源分配和任务调度，任务的提交和停止都是在 Nimbus 上操作的，一个 Storm 集群只有一个 Nimbus 节点。</li>
<li>Supervisor：Storm 集群工作节点，接受 Nimbus 分配任务，管理所有 Worker。</li>
<li>Worker：工作进程，每个工作进程中都有多个 Task。</li>
<li>Executor：产生于 Worker 进程内部的线程，会执行同一个组件的一个或者多个 Task。</li>
<li>Task：任务，每个 Spout 和 Bolt 都是一个任务，每个任务都是一个线程。</li>
<li>Topology：计算拓扑，包含了应用程序的逻辑。</li>
<li>Stream：消息流，关键抽象，是没有边界的 Tuple 序列。</li>
<li>Spout：消息流的源头，Topology 的消息生产者。</li>
<li>Bolt：消息处理单元，可以过滤、聚合、查询数据库。</li>
<li>Tuple：数据单元，数据流中就是一个个 Tuple。</li>
<li>Stream grouping：消息分发策略，一共 6 种，控制 Tuple 的路由，定义 Tuple 在 Topology 中如何流动。</li>
<li>Reliability：可靠性，Storm 保证每个 Tuple 都会被处理。</li>
</ul>
<h4 id="Storm-数据处理流程图"><a href="#Storm-数据处理流程图" class="headerlink" title="Storm 数据处理流程图"></a>Storm 数据处理流程图</h4><p>Storm 处理数据的特点：数据源源不断，不断处理，数据都是 Tuple。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9pgxz7j21nm0qwq48.jpg" alt="undefined"></p>
<h3 id="Flink-VS-Storm"><a href="#Flink-VS-Storm" class="headerlink" title="Flink VS Storm"></a>Flink VS Storm</h3><p>可以参考的文章有：</p>
<p><a href="https://tech.meituan.com/2017/11/17/flink-benchmark.html" target="_blank" rel="noopener">流计算框架 Flink 与 Storm 的性能对比</a></p>
<p><a href="https://mp.weixin.qq.com/s/E7pM5XKb_QH225nl0JKFkg" target="_blank" rel="noopener">360 深度实践：Flink 与 Storm 协议级对比</a></p>
<p>两篇文章都从不同场景、不同数据压力下对比 Flink 和 Storm 两个实时计算框架的性能表现，最终结果都表明 Flink 比 Storm 的吞吐量和性能远超 Storm。</p>
<h3 id="全部对比结果"><a href="#全部对比结果" class="headerlink" title="全部对比结果"></a>全部对比结果</h3><p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pn9wb7zgj22y81eedne.jpg" alt="undefined"></p>
<p>如果对延迟要求不高的情况下，可以使用 Spark Streaming，它拥有丰富的高级 API，使用简单，并且 Spark 生态也比较成熟，吞吐量大，部署简单，社区活跃度较高，从 GitHub 的 star 数量也可以看得出来现在公司用 Spark 还是居多的，并且在新版本还引入了 Structured Streaming，这也会让 Spark 的体系更加完善。</p>
<p>如果对延迟性要求非常高的话，可以使用当下最火的流处理框架 Flink，采用原生的流处理系统，保证了低延迟性，在 API 和容错性方面做的也比较完善，使用和部署相对来说也是比较简单的，加上国内阿里贡献的 Blink，相信接下来 Flink 的功能将会更加完善，发展也会更加好，社区问题的响应速度也是非常快的，另外还有专门的钉钉大群和中文列表供大家提问，每周还会有专家进行直播讲解和答疑。</p>
<h3 id="小结与反思-2"><a href="#小结与反思-2" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>因在 1.2 节中已经对 Flink 的特性做了很详细的讲解，所以本篇主要介绍其他几种计算框架（Blink、Spark、Spark Streaming、Structured Streaming、Storm），并对比分析了这几种框架的特点与不同。你对这几种计算框架中的哪个最熟悉呢？了解过它们之间的差异吗？你有压测过它们的处理数据的性能吗？</p>
<hr>
<h2 id="四、Flink-环境准备"><a href="#四、Flink-环境准备" class="headerlink" title="四、Flink 环境准备"></a>四、Flink 环境准备</h2><p>通过前面几篇文章，相信你已经对 Flink 的基础概念等知识已经有一定了解，现在是不是迫切的想把 Flink 给用起来？先别急，我们先把电脑的准备环境给安装好，这样后面才能更愉快地玩耍。</p>
<p>废话不多说了，直奔主题。因为后面可能用到的有：Kafka、MySQL、ElasticSearch 等，另外像 Flink 编写程序还需要依赖 Java，还有就是我们项目是用 Maven 来管理依赖的，所以这篇文章我们先来安装下这个几个，准备好本地的环境，后面如果还要安装其他的组件我们到时在新文章中补充，如果你的操作系统已经中已经安装过 JDK、Maven、MySQL、IDEA 等，那么你可以跳过对应的内容，直接看你未安装过的。</p>
<p>这里我再说下我自己电脑的系统环境：macOS High Sierra 10.13.5，后面文章的演示环境不作特别说明的话就是都在这个系统环境中。</p>
<h3 id="JDK-安装与配置"><a href="#JDK-安装与配置" class="headerlink" title="JDK 安装与配置"></a>JDK 安装与配置</h3><p>虽然现在 JDK 已经更新到 12 了，但是为了稳定我们还是安装 JDK 8，如果没有安装过的话，可以去<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">官网</a> 的<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">下载页面</a>下载对应自己操作系统的最新 JDK8 就行。</p>
<p>Mac 系统的是 jdk-8u211-macosx-x64.dmg 格式、Linux 系统的是 jdk-8u211-linux-x64.tar.gz 格式。</p>
<p>Mac 系统安装的话直接双击然后一直按照提示就行了，最后 JDK 的安装目录在 <code>/Library/Java/JavaVirtualMachines/</code> ，然后在 <code>/etc/hosts</code> 中配置好环境变量（注意：替换你自己电脑本地的路径）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home</span><br><span class="line">export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>
<p>Linux 系统的话就是在某个目录下直接解压就行了，然后在 <code>/etc/profile</code> 添加一下上面的环境变量（注意：替换你自己电脑的路径）。</p>
<p>然后执行 <code>java -version</code> 命令可以查看是否安装成功！</p>
<p> zhisheng@zhisheng ~  java -version<br>java version “1.8.0_152”<br>Java(TM) SE Runtime Environment (build 1.8.0_152-b16)<br>Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)</p>
<h3 id="Maven-安装与配置"><a href="#Maven-安装与配置" class="headerlink" title="Maven 安装与配置"></a>Maven 安装与配置</h3><p>安装好 JDK 后我们就可以安装 Maven 了，我们在<a href="http://maven.apache.org/download.cgi" target="_blank" rel="noopener">官网</a>下载二进制包就行，然后在自己本地软件安装目录解压压缩包就行。</p>
<p>接下来你需要配置一下环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export M2_HOME=/Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">export PATH=$PATH:$M2_HOME/bin</span><br></pre></td></tr></table></figure>
<p>然后执行命令 <code>mvn -v</code> 可以验证是否安装成功，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng ~ /Users  mvn -v</span><br><span class="line">Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T15:58:13+08:00)</span><br><span class="line">Maven home: /Users/zhisheng/Documents/maven-3.5.2</span><br><span class="line">Java version: 1.8.0_152, vendor: Oracle Corporation</span><br><span class="line">Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/jre</span><br><span class="line">Default locale: zh_CN, platform encoding: UTF-8</span><br><span class="line">OS name: &quot;mac os x&quot;, version: &quot;10.13.5&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;</span><br></pre></td></tr></table></figure>
<h3 id="IDE-安装与配置"><a href="#IDE-安装与配置" class="headerlink" title="IDE 安装与配置"></a>IDE 安装与配置</h3><p>安装完 JDK 和 Maven 后，就可以安装 IDE 了，大家可以选择你熟练的 IDE 就行，我后面演示的代码都是在 IDEA 中运行的，如果想为了后面不出其他的 问题的话，建议尽量和我的环境保持一致。</p>
<p>IDEA 官网下载地址：<a href="https://www.jetbrains.com/idea/download/#section=mac" target="_blank" rel="noopener">下载页面的地址</a></p>
<p>下载后可以双击后然后按照提示一步步安装，安装完成后需要在 IDEA 中配置 JDK 路径和 Maven 的路径，后面我们开发也都是靠 Maven 来管理项目的依赖。</p>
<h3 id="MySQL-安装与配置"><a href="#MySQL-安装与配置" class="headerlink" title="MySQL 安装与配置"></a>MySQL 安装与配置</h3><p>因为后面文章有用到 MySQL，所以这里也讲一下如何安装与配置，首先去官网下载 MySQL 5.7，<a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">下载页面的地址</a>，根据你们到系统安装对应的版本，Mac 的话双击 dmg 安装包就可以按照提示一步步执行到安装成功。</p>
<p>启动 MySQL，如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pndeozfuj20u0130dll.jpg" alt="undefined"></p>
<p>出现绿色就证明 MySQL 服务启动成功了。后面我们操作数据库不会通过本地命令行来，而是会通过图形化软件，比如：Navicat、Sequel pro，这些图形化软件可比命令行的效率高太多，读者可以自行下载安装一下。</p>
<h3 id="Kafka-安装与配置"><a href="#Kafka-安装与配置" class="headerlink" title="Kafka 安装与配置"></a>Kafka 安装与配置</h3><p>后面我们文章中会大量用到 Kafka，所以 Kakfa 一定要安装好。官网下载地址：<a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">下载页面的地址</a></p>
<p>同样，我自己下载的版本是 1.1.0 （保持和我公司的生产环境一致），如果你对 Kafka 还不太熟悉，可以参考我以前写的一篇入门文章：<a href="http://www.54tianzhisheng.cn/2018/01/04/Kafka/" target="_blank" rel="noopener">Kafka 安装及快速入门</a>。</p>
<p>在这篇文章里面教大家怎么安装 Kafka、启动 Zookeeper、启动 Kafka 服务、创建 Topic、使用 producer 创建消息、使用 consumer 消费消息、查看 Topic 的信息，另外还有提供集群配置的方案。</p>
<h3 id="ElasticSearch-安装与配置"><a href="#ElasticSearch-安装与配置" class="headerlink" title="ElasticSearch 安装与配置"></a>ElasticSearch 安装与配置</h3><p>因为后面有文章介绍连接器 (connector) —— Elasticsearch 介绍和整和使用，并且最后面的案例文章也会把数据存储在 Elasticsearch 中的，所以这里就简单的讲解一下 Elasticsearch 的安装，在我以前的博客中写过一篇搭建 Elasticsearch 集群的：<a href="http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/" target="_blank" rel="noopener">Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程</a>。</p>
<p>这里我在本地安装个单机的 Elasticsearch 就行了，首先在官网 <a href="https://www.elastic.co/cn/downloads/past-releases" target="_blank" rel="noopener">下载页面</a> 找到 Elasticsearch 产品，我下载的版本是 elasticsearch-6.3.2 版本，同样和我们公司的线上环境版本保持一致，因为 Flink Elasticsearch connector 有分好几个版本：2.x、5.x、6.x 版本，不同版本到时候写数据存入到 Elasticsearch 的 Job 代码也是有点区别的，如果你们公司的 Elasticsearch 版本比较低的话，到时候后面版本的学习代码还得找官网的资料对比学习一下。</p>
<p>另外就是写这篇文章的时候 Elasticsearch 7.x 就早已经发布了，Flink 我暂时还没看到支持 Elasticsearch 7 的连接器，自己也没测试过，所以暂不清楚如果用 6.x 版本的 connector 去连接 7.x 的 Elasticsearch 会不会出现问题？建议还是跟着我的安装版本来操作！</p>
<p>除了这样下载 Elasticsearch 的话，你如果电脑安装了 Homebrew，也可以通过 Homebrew 来安装 Elasticsearch，都还挺方便的，包括你还可以通过 Docker 的方式快速启动一个 Elasticsearch 来。</p>
<p>下载好了 Elasticsearch 的压缩包，在你的安装目录下解压就行了，然后进入 Elasticsearch 的安装目录执行下面命令就可以启动 Elasticsearch 了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure>
<p>执行命令后的结果：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pne4yctlj24m02d04qp.jpg" alt="undefined"></p>
<p>从浏览器端打开地址：<code>http://localhost:9200/</code> 即可验证是否安装成功：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnejkllpj20t60qsdhg.jpg" alt="undefined"></p>
<p>如果出现了如上图这样就代表 Elasticsearch 环境已经安装好了。</p>
<h3 id="小结与反思-3"><a href="#小结与反思-3" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲解了下 JDK、Maven、IDE、MySQL、Kafka、ElasticSearch 的安装与配置，因为这些都是后面要用的，所以这里单独抽一篇文章来讲解环境准备的安装步骤，当然这里还并不涉及全，因为后面我们还可能会涉及到 HBase、HDFS 等知识，后面我们用到再看，我们本系列的文章更多的还是讲解 Flink，所以更多的环境准备还是得靠大家自己独立完成。</p>
<p>这里我说下笔者自己一般安装环境的选择：</p>
<ol>
<li>组件尽量和公司的生产环境保持版本一致，不追求太新，够用就行，这样如果生产出现问题，本机还可以看是否可以复现出来</li>
<li>安装环境的时候先搜下类似的安装教程，提前知道要踩的坑，避免自己再次踩到</li>
</ol>
<p>下面文章我们就正式进入 Flink 专题了！</p>
<h2 id="五、Flink环境搭建"><a href="#五、Flink环境搭建" class="headerlink" title="五、Flink环境搭建"></a>五、Flink环境搭建</h2><p>在 2.1 节中已经将 Flink 的准备环境已经讲完了，本篇文章将带大家正式开始接触 Flink，那么我们得先安装一下 Flink。Flink 是可以在多个平台（Windows、Linux、Mac）上安装的。在开始写本书的时候最新版本是 1.8 版本，但是写到一半后更新到 1.9 了（合并了大量 Blink 的新特性），所以笔者又全部更新版本到 1.9，书籍后面也都是基于最新的版本讲解与演示。</p>
<p>Flink 的官网地址是：<a href="https://flink.apache.org/" target="_blank" rel="noopener">https://flink.apache.org/</a></p>
<h3 id="Flink-下载与安装"><a href="#Flink-下载与安装" class="headerlink" title="Flink 下载与安装"></a>Flink 下载与安装</h3><h4 id="Mac-amp-Linux-安装"><a href="#Mac-amp-Linux-安装" class="headerlink" title="Mac &amp; Linux 安装"></a>Mac &amp; Linux 安装</h4><p>你可以通过该地址 <a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">https://flink.apache.org/downloads.html</a> 下载到最新版本的 Flink。</p>
<p>这里我们选择 <code>Apache Flink 1.9.0 for Scala 2.11</code> 版本，点击跳转到了一个镜像下载选择的地址，随便选择哪个就行，只是下载速度不一致而已。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnl2lqdzj21y417i0wa.jpg" alt="undefined"></p>
<p>下载完后，你就可以直接解压下载的 Flink 压缩包了。</p>
<p>接下来我们可以启动一下 Flink，我们进入到 Flink 的安装目录下执行命令 <code>./bin/start-cluster.sh</code> 即可，产生的日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng /usr/local/flink-1.9.0  ./bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure>
<p>如果你的电脑是 Mac 的话，那么你也可以通过 Homebrew 命令进行安装。先通过命令 <code>brew search flink</code> 查找一下包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng  ~  brew search flink</span><br><span class="line">==&gt; Formulae</span><br><span class="line">apache-flink ✔       homebrew/linuxbrew-core/apache-flink</span><br></pre></td></tr></table></figure>
<p>可以发现找得到 Flink 的安装包，但是这样安装的版本可能不是最新的，如果你要安装的话，则使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install apache-flink</span><br></pre></td></tr></table></figure>
<p>那么它就会开始进行下载并安装好，安装后的目录应该是在 <code>/usr/local/Cellar/apache-flink</code> 下。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnmiv3uoj21fi106q8b.jpg" alt="undefined"></p>
<p>你可以通过下面命令检查安装的 Flink 到底是什么版本的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink --version</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Version: 1.9.0, Commit ID: ff472b4</span><br></pre></td></tr></table></figure>
<p>这种的话运行是得进入 <code>/usr/local/Cellar/apache-flink/1.9.0/libexec/bin</code> 目录下执行命令 <code>./start-cluster.sh</code> 才可以启动 Flink 的。</p>
<p>启动后产生的日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure>
<h4 id="Windows-安装"><a href="#Windows-安装" class="headerlink" title="Windows 安装"></a>Windows 安装</h4><p>如果你的电脑系统是 Windows 的话，那么你就直接双击 Flink 安装目录下面 bin 文件夹里面的 <code>start-cluster.bat</code> 就行，同样可以将 Flink 起动成功。</p>
<h3 id="Flink-启动与运行"><a href="#Flink-启动与运行" class="headerlink" title="Flink 启动与运行"></a>Flink 启动与运行</h3><p>启动成功后的话，我们可以通过访问地址<code>http://localhost:8081/</code> 查看 UI 长啥样了，如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2ly1g8pnneh4l3j228o1j6myu.jpg" alt="undefined"></p>
<p>你在通过 jps 命令可以查看到运行的进程有：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  jps</span><br><span class="line">73937 StandaloneSessionClusterEntrypoint</span><br><span class="line">74391 Jps</span><br><span class="line">520</span><br><span class="line">74362 TaskManagerRunner</span><br></pre></td></tr></table></figure>
<h3 id="Flink-目录配置文件解读"><a href="#Flink-目录配置文件解读" class="headerlink" title="Flink 目录配置文件解读"></a>Flink 目录配置文件解读</h3><p>Flink 安装好后，我们也运行启动看了效果了，接下来我们来看下它的目录结构吧：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> ✘ zhisheng@zhisheng  /usr/local/flink-1.9.0  ll</span><br><span class="line">total 1200</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff    11K  3  5 16:32 LICENSE</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff   582K  4  4 00:01 NOTICE</span><br><span class="line">-rw-r--r--@  1 zhisheng  staff   1.3K  3  5 16:32 README.txt</span><br><span class="line">drwxr-xr-x@ 26 zhisheng  staff   832B  3  5 16:32 bin</span><br><span class="line">drwxr-xr-x@ 14 zhisheng  staff   448B  4  4 14:06 conf</span><br><span class="line">drwxr-xr-x@  6 zhisheng  staff   192B  4  4 14:06 examples</span><br><span class="line">drwxr-xr-x@  5 zhisheng  staff   160B  4  4 14:06 lib</span><br><span class="line">drwxr-xr-x@ 47 zhisheng  staff   1.5K  3  6 23:21 licenses</span><br><span class="line">drwxr-xr-x@  2 zhisheng  staff    64B  3  5 19:50 log</span><br><span class="line">drwxr-xr-x@ 22 zhisheng  staff   704B  4  4 14:06 opt</span><br></pre></td></tr></table></figure>
<p>上面目录：</p>
<ul>
<li><strong>bin</strong> 存放一些启动脚本</li>
<li><strong>conf</strong> 存放配置文件</li>
<li><strong>examples</strong> 存放一些案例的 Job Jar 包</li>
<li><strong>lib</strong> Flink 依赖的 Jar 包</li>
<li><strong>log</strong> 存放产生的日志文件</li>
<li><strong>opt</strong> 存放的是一些可选择的 Jar 包，后面可能会用到</li>
</ul>
<p>在 bin 目录里面有如下这些脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll bin</span><br><span class="line">total 256</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff    28K  3  5 16:32 config.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.2K  3  5 16:32 flink</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.7K  3  5 16:32 flink-console.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   6.2K  3  5 16:32 flink-daemon.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.2K  3  5 16:32 flink.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.5K  3  5 16:32 historyserver.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.8K  3  5 16:32 jobmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-appmaster-job.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-appmaster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 mesos-taskmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.2K  3  5 16:32 pyflink-stream.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.1K  3  5 16:32 pyflink.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.1K  3  5 16:32 pyflink.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.4K  3  5 16:32 sql-client.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.5K  3  5 16:32 standalone-job.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.3K  3  5 16:32 start-cluster.bat</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 start-cluster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.3K  3  5 16:32 start-scala-shell.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 start-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.6K  3  5 16:32 stop-cluster.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.8K  3  5 16:32 stop-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   3.8K  3  5 16:32 taskmanager.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   1.6K  3  5 16:32 yarn-session.sh</span><br><span class="line">-rwxr-xr-x@ 1 zhisheng  staff   2.2K  3  5 16:32 zookeeper.sh</span><br></pre></td></tr></table></figure>
<p>脚本包括了配置启动脚本、historyserver、Job Manager、Task Manager、启动集群和停止集群等脚本。</p>
<p>在 conf 目录下面有如下这些配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll conf</span><br><span class="line">total 112</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   9.8K  4  4 00:01 flink-conf.yaml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.1K  3  5 16:32 log4j-cli.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.8K  3  5 16:32 log4j-console.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.7K  3  5 16:32 log4j-yarn-session.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.9K  3  5 16:32 log4j.properties</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.2K  3  5 16:32 logback-console.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.5K  3  5 16:32 logback-yarn.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   2.3K  3  5 16:32 logback.xml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff    15B  3  5 16:32 masters</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff    10B  3  5 16:32 slaves</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   3.8K  3  5 16:32 sql-client-defaults.yaml</span><br><span class="line">-rw-r--r--@ 1 zhisheng  staff   1.4K  3  5 16:32 zoo.cfg</span><br></pre></td></tr></table></figure>
<p>配置包含了 Flink 的自身配置、日志配置、masters、slaves、sql-client、zoo 等配置。</p>
<p>在 examples 目录里面可以看到有如下这些案例的目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll examples</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x@ 10 zhisheng  staff   320B  4  4 14:06 batch</span><br><span class="line">drwxr-xr-x@  3 zhisheng  staff    96B  4  4 14:06 gelly</span><br><span class="line">drwxr-xr-x@  4 zhisheng  staff   128B  4  4 14:06 python</span><br><span class="line">drwxr-xr-x@ 11 zhisheng  staff   352B  4  4 14:06 streaming</span><br></pre></td></tr></table></figure>
<p>这个目录下面有批、gelly、python、流的 demo，后面我们可以直接用上面的案例做些简单的测试。</p>
<p>在 log 目录里面存着 Task Manager &amp; Job manager 的日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/flink-1.9.0  ll log</span><br><span class="line">total 144</span><br><span class="line">-rw-r--r--  1 zhisheng  staff    11K  4 25 20:10 flink-zhisheng-standalonesession-0-zhisheng.log</span><br><span class="line">-rw-r--r--  1 zhisheng  staff     0B  4 25 20:10 flink-zhisheng-standalonesession-0-zhisheng.out</span><br><span class="line">-rw-r--r--  1 zhisheng  staff    11K  4 25 20:10 flink-zhisheng-taskexecutor-0-zhisheng.log</span><br><span class="line">-rw-r--r--  1 zhisheng  staff     0B  4 25 20:10 flink-zhisheng-taskexecutor-0-zhisheng.out</span><br></pre></td></tr></table></figure>
<p>一般我们如果要深入了解一个知识点，最根本的方法就是看其源码实现，源码下面无秘密，所以我这里也讲一下如何将源码下载编译并运行，然后将代码工程导入到 IDEA 中去，方便自己查阅和 debug 代码。</p>
<h3 id="Flink-源码下载"><a href="#Flink-源码下载" class="headerlink" title="Flink 源码下载"></a>Flink 源码下载</h3><p>Flink GitHub 仓库地址：<a href="https://github.com/apache/flink" target="_blank" rel="noopener">https://github.com/apache/flink</a></p>
<p>执行下面命令将源码下载到本地：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:apache/flink.git</span><br></pre></td></tr></table></figure>
<p>拉取的时候找个网络好点的地方，这样速度可能会更快点。</p>
<p>然后你可以切换到项目的不同分支，比如 release-1.9、blink（阿里巴巴开源贡献的） ，执行下面命令将代码切换到 release-1.9 分支：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout release-1.9</span><br></pre></td></tr></table></figure>
<p>或者你也想去看看 Blink 的代码实现，你也可以执行下面命令切换到 blink 分支来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout blink</span><br></pre></td></tr></table></figure>
<h3 id="Flink-源码编译"><a href="#Flink-源码编译" class="headerlink" title="Flink 源码编译"></a>Flink 源码编译</h3><p>编译源码的话，你需要执行如下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true</span><br></pre></td></tr></table></figure>
<ul>
<li>-Dmaven.test.skip：跳过测试代码</li>
<li>-Dmaven.javadoc.skip：跳过 javadoc 检查</li>
<li>-Dcheckstyle.skip：跳过代码风格检查</li>
</ul>
<p>maven 编译的时候跳过这些检查，这样可以减少很多时间，还可能会减少错误的发生。</p>
<p>注意：你的 maven 的 settings.xml 文件的 mirror 添加下面这个(这样才能下载到某些下载不了的依赖)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;*,!jeecg,!jeecg-snapshots,!mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br><span class="line"></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;mapr-public&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;mapr-releases&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;https://maven.aliyun.com/repository/mapr-public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure>
<p>如果还遇到什么其他的问题的话，可以去看看我之前在我博客分享的一篇源码编译的文章（附视频）：<a href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/" target="_blank" rel="noopener">Flink 源码解析 —— 源码编译运行</a>。</p>
<h3 id="Flink-源码导入到-IDE"><a href="#Flink-源码导入到-IDE" class="headerlink" title="Flink 源码导入到 IDE"></a>Flink 源码导入到 IDE</h3><p>看下图，因为我们已经下载好了源码，直接在 IDEA 里面 open 这个 maven 项目就行了：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0un83lnzj21r20tqk0e.jpg" alt="undefined"></p>
<p>导入后大概就是下面这样子：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0unee0gmj21c00u0dn8.jpg" alt="undefined"></p>
<p>很顺利，没多少报错，这里我已经把一些代码风格检查相关的 Maven 插件给注释掉了。</p>
<h3 id="小结与反思-4"><a href="#小结与反思-4" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节主要讲了 FLink 在不同系统下的安装和运行方法，然后讲了下怎么去下载源码和将源码导入到 IDE 中。不知道你在将源码导入到 IDE 中是否有遇到什么问题呢？</p>
<h2 id="六、FlinkWordCount"><a href="#六、FlinkWordCount" class="headerlink" title="六、FlinkWordCount"></a>六、FlinkWordCount</h2><p>在 2.2 中带大家讲解了下 Flink 的环境安装，这篇文章就开始我们的第一个 Flink 案例实战，也方便大家快速开始自己的第一个 Flink 应用。大数据里学习一门技术一般都是从 WordCount 开始入门的，那么我还是不打破常规了，所以这篇文章我也将带大家通过 WordCount 程序来初步了解 Flink。</p>
<h3 id="Maven-创建项目"><a href="#Maven-创建项目" class="headerlink" title="Maven 创建项目"></a>Maven 创建项目</h3><p>Flink 支持 Maven 直接构建模版项目，你在终端使用该命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate                               \</span><br><span class="line">      -DarchetypeGroupId=org.apache.flink              \</span><br><span class="line">      -DarchetypeArtifactId=flink-quickstart-java      \</span><br><span class="line">      -DarchetypeVersion=1.9.0</span><br></pre></td></tr></table></figure>
<p>在执行的过程中它会提示你输入 groupId、artifactId、和 package 名，你按照要求输入就行，最后就可以成功创建一个项目。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uocicqoj21xw1aatf4.jpg" alt="undefined"></p>
<p>进入到目录你就可以看到已经创建了项目，里面结构如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> zhisheng@zhisheng  ~/IdeaProjects/github/Flink-WordCount  tree</span><br><span class="line">.</span><br><span class="line">├── pom.xml</span><br><span class="line">└── src</span><br><span class="line">    └── main</span><br><span class="line">        ├── java</span><br><span class="line">        │   └── com</span><br><span class="line">        │       └── zhisheng</span><br><span class="line">        │           ├── BatchJob.java</span><br><span class="line">        │           └── StreamingJob.java</span><br><span class="line">        └── resources</span><br><span class="line">            └── log4j.properties</span><br><span class="line"></span><br><span class="line">6 directories, 4 files</span><br></pre></td></tr></table></figure>
<p>该项目中包含了两个类 BatchJob 和 StreamingJob，另外还有一个 log4j.properties 配置文件，然后你就可以将该项目导入到 IDEA 了。</p>
<p>你可以在该目录下执行 <code>mvn clean package</code> 就可以编译该项目，编译成功后在 target 目录下会生成一个 Job 的 Jar 包，但是这个 Job 还不能执行，因为 StreamingJob 这个类中的 main 方法里面只是简单的创建了 StreamExecutionEnvironment 环境，然后就执行 execute 方法，这在 Flink 中是不算一个可执行的 Job 的，因此如果你提交到 Flink UI 上也是会报错的。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uokn3kbj227w0pawfm.jpg" alt="undefined"></p>
<p>运行报错：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uosbkccj22640mq3zx.jpg" alt="undefined"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Server Response Message:</span><br><span class="line">Internal server error.</span><br></pre></td></tr></table></figure>
<p>我们查看 Flink Job Manager 的日志可以看到：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uoz4xbmj226w12e0zj.jpg" alt="undefined"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-04-26 17:27:33,706 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler    - Unhandled exception.</span><br><span class="line">org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: No operators defined in streaming topology. Cannot execute.</span><br></pre></td></tr></table></figure>
<p>因为 execute 方法之前我们是需要补充我们 Job 的一些算子操作的，所以报错还是很正常的，本文下面将会提供完整代码。</p>
<h3 id="IDEA-创建项目"><a href="#IDEA-创建项目" class="headerlink" title="IDEA 创建项目"></a>IDEA 创建项目</h3><p>一般我们项目可能是由多个 Job 组成，并且代码也都是在同一个工程下面进行管理，上面那种适合单个 Job 执行，但如果多人合作的时候还是得在同一个工程下面进行项目的创建，每个 Flink Job 一个 module，下面我们将来讲解下如何利用 IDEA 创建 Flink 项目。</p>
<p>我们利用 IDEA 创建 Maven 项目，工程如下图这样，项目下面分很多模块，每个模块负责不同的业务</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0up8brt7j20p20k0dg6.jpg" alt="undefined"></p>
<p>接下来我们需要在父工程的 pom.xml 中加入如下属性（含编码、Flink 版本、JDK 版本、Scala 版本、Maven 编译版本）：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Flink 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--JDK 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Scala 2.11 版本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>然后加入依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Apache Flink dependencies --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Add logging framework, to produce console output when running in the IDE. --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- These dependencies are excluded from the application JAR by default. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>上面依赖中 flink-java 和 flink-streaming-java 是我们 Flink 必备的核心依赖，为什么设置 scope 为 provided 呢（默认是 compile）？</p>
<p>是因为 Flink 其实在自己的安装目录中 lib 文件夹里的 <code>lib/flink-dist_2.11-1.9.0.jar</code> 已经包含了这些必备的 Jar 了，所以我们在给自己的 Flink Job 添加依赖的时候最后打成的 Jar 包可不希望又将这些重复的依赖打进去。有两个好处：</p>
<ul>
<li>减小了我们打的 Flink Job Jar 包容量大小</li>
<li>不会因为打入不同版本的 Flink 核心依赖而导致类加载冲突等问题</li>
</ul>
<p>但是问题又来了，我们需要在 IDEA 中调试运行我们的 Job，如果将 scope 设置为 provided 的话，是会报错的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Error: A JNI error has occurred, please check your installation and try again</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/api/common/ExecutionConfig$GlobalJobParameters</span><br><span class="line">    at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class="line">    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class="line">    at java.lang.Class.privateGetMethodRecursive(Class.java:3048)</span><br><span class="line">    at java.lang.Class.getMethod0(Class.java:3018)</span><br><span class="line">    at java.lang.Class.getMethod(Class.java:1784)</span><br><span class="line">    at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)</span><br><span class="line">    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.flink.api.common.ExecutionConfig$GlobalJobParameters</span><br><span class="line">    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">    ... 7 more</span><br></pre></td></tr></table></figure>
<p>默认 scope 为 compile 的话，本地调试的话就不会出错了。</p>
<p>另外测试到底能够减小多少 Jar 包的大小呢？我这里先写了个 Job 测试。</p>
<p>当 scope 为 compile 时，编译后的 target 目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  ~/Flink-WordCount/target   master ●✚  ll</span><br><span class="line">total 94384</span><br><span class="line">-rw-r--r--  1 zhisheng  staff    45M  4 26 21:23 Flink-WordCount-1.0-SNAPSHOT.jar</span><br><span class="line">drwxr-xr-x  4 zhisheng  staff   128B  4 26 21:23 classes</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 generated-sources</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 maven-archiver</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:23 maven-status</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.2K  4 26 21:23 original-Flink-WordCount-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<p>当 scope 为 provided 时，编译后的 target 目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng ~/Flink-WordCount/target   master ●✚  ll</span><br><span class="line">total 32</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.5K  4 26 21:27 Flink-WordCount-1.0-SNAPSHOT.jar</span><br><span class="line">drwxr-xr-x  4 zhisheng  staff   128B  4 26 21:27 classes</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 generated-sources</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 maven-archiver</span><br><span class="line">drwxr-xr-x  3 zhisheng  staff    96B  4 26 21:27 maven-status</span><br><span class="line">-rw-r--r--  1 zhisheng  staff   7.2K  4 26 21:27 original-Flink-WordCount-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<p>可以发现：当 scope 为 provided 时 Jar 包才 7.5k，而为 compile 时 Jar 包就 45M 了，你要想想这才只是一个简单的 WordCount 程序呢，差别就这么大。当我们把 Flink Job 打成一个 fat Jar 时，上传到 UI 的时间就能够很明显的对比出来（Jar 包越小上传的时间越短），所以把 scope 设置为 provided 还是很有必要的。</p>
<p>有人就会想了，那这不是和上面有冲突了吗？假如我既想打出来的 Jar 包要小，又想能够在本地 IDEA 中进行运行和调试 Job ？这里我提供一种方法：在父工程中的 pom.xml 引入如下 profiles。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">profiles</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">profile</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>add-dependencies-for-IDEA<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">activation</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>idea.version<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">activation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">profile</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">profiles</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>当你在 IDEA 中运行 Job 的时候，它会给你引入 flink-java、flink-streaming-java，且 scope 设置为 compile，但是你是打成 Jar 包的时候它又不起作用。如果你加了这个 profile 还是报错的话，那么可能是 IDEA 中没有识别到，你可以在 IDEA 的中查看下面两个配置确定一下（配置其中一个即可以起作用）。</p>
<p>1、查看 Maven 中的该 profile 是否已经默认勾选上了，如果没有勾选上，则手动勾选一下才会起作用</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0upjwdhnj20yw080q2z.jpg" alt="undefined"></p>
<p>2、Include dependencies with “Provided” scope 是否勾选，如果未勾选，则手动勾选后才起作用</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0upq3ibrj21n8188dia.jpg" alt="undefined"></p>
<h3 id="流计算-WordCount-应用程序代码"><a href="#流计算-WordCount-应用程序代码" class="headerlink" title="流计算 WordCount 应用程序代码"></a>流计算 WordCount 应用程序代码</h3><p>回到正题，利用 IDEA 创建好 WordCount 应用后，我们开始编写代码。</p>
<p><strong>Main 类</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//创建流运行环境</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args));</span><br><span class="line">        env.fromElements(WORDS)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] splits = value.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split, <span class="number">1</span>));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(value1.f0, value1.f1 + value1.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print();</span><br><span class="line">        <span class="comment">//Streaming 程序必须加这个才能启动程序，否则不会有结果</span></span><br><span class="line">        env.execute(<span class="string">"zhisheng —— word count streaming demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] WORDS = <span class="keyword">new</span> String[]&#123;</span><br><span class="line">            <span class="string">"To be, or not to be,--that is the question:--"</span>,</span><br><span class="line">            <span class="string">"Whether 'tis nobler in the mind to suffer"</span></span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>pom.xml</strong> 文件中引入 build 插件并且要替换成你自己项目里面的 mainClass：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Java Compiler --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 使用 maven-shade 插件创建一个包含所有必要的依赖项的 fat Jar --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.apache.flink:force-shading<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!--注意：这里一定要换成你自己的 Job main 方法的启动类--&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.zhisheng.wordcount.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注意：上面这个 build 插件要记得加，否则打出来的 jar 包是不完整的，提交运行会报 ClassNotFoundException，该问题是初学者很容易遇到的问题，很多人咨询过笔者这个问题。</p>
<h3 id="WordCount-应用程序运行"><a href="#WordCount-应用程序运行" class="headerlink" title="WordCount 应用程序运行"></a>WordCount 应用程序运行</h3><h4 id="本地-IDE-运行"><a href="#本地-IDE-运行" class="headerlink" title="本地 IDE 运行"></a>本地 IDE 运行</h4><p>编译好 WordCount 程序后，我们在 IDEA 中右键 run main 方法就可以把 Job 运行起来，结果如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uq05u0wj21pc0vy74y.jpg" alt="undefined"></p>
<p>图中的就是将每个 word 和对应的个数一行一行打印出来，在本地 IDEA 中运行没有问题，我们接下来使用命令 <code>mvn clean package</code> 打包成一个 Jar (flink-learning-examples-1.0-SNAPSHOT.jar) 然后将其上传到 Flink UI 上运行一下看下效果。</p>
<h4 id="UI-运行-Job"><a href="#UI-运行-Job" class="headerlink" title="UI 运行 Job"></a>UI 运行 Job</h4><p>在 <code>http://localhost:8081/#/submit</code> 页面上传 flink-learning-examples-1.0-SNAPSHOT.jar 后，然后点击 Submit 后就可以运行了。</p>
<p>运行 Job 的 UI 如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uq6lppkj226o1awdhc.jpg" alt="undefined"></p>
<p>Job 的结果在 Task Manager 的 Stdout 中：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uqcmgspj224u1ewq50.jpg" alt="undefined"></p>
<h3 id="WordCount-应用程序代码分析"><a href="#WordCount-应用程序代码分析" class="headerlink" title="WordCount 应用程序代码分析"></a>WordCount 应用程序代码分析</h3><p>我们已经将 WordCount 程序代码写好了并且也在 IDEA 中和 Flink UI 上运行了 Job，并且程序运行的结果都是正常的。</p>
<p>那么我们来分析一下这个 WordCount 程序代码：</p>
<p>1、创建好 StreamExecutionEnvironment（流程序的运行环境）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure>
<p>2、给流程序的运行环境设置全局的配置（从参数 args 获取）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args));</span><br></pre></td></tr></table></figure>
<p>3、构建数据源，WORDS 是个字符串数组</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.fromElements(WORDS)</span><br></pre></td></tr></table></figure>
<p>4、将字符串进行分隔然后收集，组装后的数据格式是 (word、1)，1 代表 word 出现的次数为 1</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] splits = value.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">            <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>5、根据 word 关键字进行分组（0 代表对第一个字段分组，也就是对 word 进行分组）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyBy(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>6、对单个 word 进行计数操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>7、打印所有的数据流，格式是 (word，count)，count 代表 word 出现的次数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print()</span><br></pre></td></tr></table></figure>
<p>8、开始执行 Job</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.execute(<span class="string">"zhisheng —— word count streaming demo"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="小结与反思-5"><a href="#小结与反思-5" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节给大家介绍了 Maven 创建 Flink Job、IDEA 中创建 Flink 项目（详细描述了里面要注意的事情）、编写 WordCount 程序、IDEA 运行程序、在 Flink UI 运行程序、对 WordCount 程序每个步骤进行分析。</p>
<p>通过本小节，你接触了第一个 Flink 应用程序，也开启了 Flink 实战之旅。你有自己运行本节的代码去测试吗？动手测试的过程中有遇到什么问题吗？</p>
<p>本节涉及的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/wordcount" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/wordcount</a></p>
<h2 id="七、Flink-实时处理-Socket-数据"><a href="#七、Flink-实时处理-Socket-数据" class="headerlink" title="七、Flink 实时处理 Socket 数据"></a>七、Flink 实时处理 Socket 数据</h2><p>在 2.3 中讲解了 Flink 最简单的 WordCount 程序的创建、运行结果查看和代码分析，这篇文章继续带大家来看一个入门上手的程序：Flink 处理 Socket 数据。</p>
<h3 id="IDEA-创建项目-1"><a href="#IDEA-创建项目-1" class="headerlink" title="IDEA 创建项目"></a>IDEA 创建项目</h3><p>使用 IDEA 创建新的 module，结构如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">├── pom.xml</span><br><span class="line">└── src</span><br><span class="line">    ├── main</span><br><span class="line">    │   ├── java</span><br><span class="line">    │   │   └── com</span><br><span class="line">    │   │       └── zhisheng</span><br><span class="line">    │   │           └── socket</span><br><span class="line">    │   │               └── Main.java</span><br><span class="line">    │   └── resources</span><br><span class="line">    │       └── log4j.properties</span><br><span class="line">    └── test</span><br><span class="line">        └── java</span><br></pre></td></tr></table></figure>
<p>项目创建好了后，我们下一步开始编写 Flink Socket Job 的代码。</p>
<h3 id="Flink-Socket-应用程序代码"><a href="#Flink-Socket-应用程序代码" class="headerlink" title="Flink Socket 应用程序代码"></a>Flink Socket 应用程序代码</h3><p><strong>Main 类</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//参数检查</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String hostname = args[<span class="number">0</span>];</span><br><span class="line">        Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//获取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br><span class="line">        <span class="comment">//计数</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        sum.print();</span><br><span class="line">        env.execute(<span class="string">"Java WordCount from SocketText"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">            String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>pom.xml</strong> 添加 build：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>$&#123;java.version&#125;<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.apache.flink:force-shading<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!--注意：这里一定要换成你自己的 Job main 方法的启动类--&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.zhisheng.socket.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="Flink-Socket-应用程序运行"><a href="#Flink-Socket-应用程序运行" class="headerlink" title="Flink Socket 应用程序运行"></a>Flink Socket 应用程序运行</h3><h4 id="本地-IDE-运行-1"><a href="#本地-IDE-运行-1" class="headerlink" title="本地 IDE 运行"></a>本地 IDE 运行</h4><p>我们先在终端开启监听 9000 端口：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -l 9000</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ur4cqi4j21b60aeq36.jpg" alt="undefined"></p>
<p>然后右键运行 Main 类的 main 方法 (注意：需要传入运行参数 <code>127.0.0.1 9000</code>)：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urc5j0wj21ni14egno.jpg" alt="undefined"></p>
<p>运行结果如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urirdxgj21s20legma.jpg" alt="undefined"></p>
<p>我在终端一个个输入下面的字符串：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hello</span><br><span class="line">zhisheng</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">zhisheng</span><br><span class="line">zhisheng</span><br><span class="line">This is zhisheng‘s book</span><br></pre></td></tr></table></figure>
<p>然后在 IDEA 的运行结果会一个个输出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2&gt; (hello,1)</span><br><span class="line">2&gt; (zhisheng,1)</span><br><span class="line">2&gt; (hello,2)</span><br><span class="line">2&gt; (hello,3)</span><br><span class="line">2&gt; (zhisheng,2)</span><br><span class="line">2&gt; (zhisheng,3)</span><br><span class="line">3&gt; (s,1)</span><br><span class="line">1&gt; (this,1)</span><br><span class="line">4&gt; (is,1)</span><br><span class="line">2&gt; (zhisheng,4)</span><br><span class="line">3&gt; (book,1)</span><br></pre></td></tr></table></figure>
<p>在本地 IDEA 中运行没有问题，我们接下来使用命令 <code>mvn clean package</code> 打包成一个 Jar (flink-learning-examples-1.0-SNAPSHOT.jar) 然后将其上传到 Flink UI 上运行一下看下效果。</p>
<h4 id="UI-运行-Job-1"><a href="#UI-运行-Job-1" class="headerlink" title="UI 运行 Job"></a>UI 运行 Job</h4><p>依旧和上面那样开启监听本地端口 9200，然后在 <code>http://localhost:8081/#/submit</code> 页面上传 flink-learning-examples-1.0-SNAPSHOT.jar 后，接着在 Main Class 填写运行的主函数，Program Arguments 填写参数 <code>127.0.0.1 9000</code>，最后点击 Submit 后就可以运行了。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0urqumhzj22760mqgmr.jpg" alt="undefined"></p>
<p>UI 的运行详情如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ury0iq8j225w1au3zz.jpg" alt="undefined"></p>
<p>我在终端一个个输入下面的字符串：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  ~  nc -l 9000</span><br><span class="line">zhisheng</span><br><span class="line">zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">zhisheng</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">This is zhisheng&apos;s Book</span><br><span class="line">zhisheng</span><br></pre></td></tr></table></figure>
<p>查看 Task Manager 的 Stdout 可以查看到输出：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0us5qqsej227819wgnp.jpg" alt="undefined"></p>
<h3 id="Flink-Socket-应用程序代码分析"><a href="#Flink-Socket-应用程序代码分析" class="headerlink" title="Flink Socket 应用程序代码分析"></a>Flink Socket 应用程序代码分析</h3><p>1、参数检查，需要传入两个参数（hostname 和 port），符合条件就赋值给 hostname 和 port</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">    System.err.println(<span class="string">"USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">String hostname = args[<span class="number">0</span>];</span><br><span class="line">Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br></pre></td></tr></table></figure>
<p>2、创建好 StreamExecutionEnvironment（流程序的运行环境）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure>
<p>3、构建数据源，获取 Socket 数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br></pre></td></tr></table></figure>
<p>4、对 Socket 数据字符串分隔后收集在根据 word 分组后计数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将字符串进行分隔然后收集，组装后的数据格式是 (word、1)，1 代表 word 出现的次数为 1</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">        String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">            <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>5、打印所有的数据流，格式是 (word，count)，count 代表 word 出现的次数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum.print();</span><br></pre></td></tr></table></figure>
<p>6、开始执行 Job</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.execute(<span class="string">"Java WordCount from SocketText"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="Flink-中使用-Lambda-表达式"><a href="#Flink-中使用-Lambda-表达式" class="headerlink" title="Flink 中使用 Lambda 表达式"></a>Flink 中使用 Lambda 表达式</h3><p>因为 Lambda 表达式看起来简洁，所以有时候也是希望在这些 Flink 作业中也可以使用上它，虽然 Flink 中是支持 Lambda，但是个人感觉不太友好。比如上面的应用程序如果将 LineSplitter 该类之间用 Lambda 表达式完成的话则要像下面这样写：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">stream.flatMap((s, collector) -&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (String token : s.toLowerCase().split(<span class="string">"\\W+"</span>)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure>
<p>但是这样写完后，运行作业报错如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.flink.api.common.functions.InvalidTypesException: The return type of function &apos;main(LambdaMain.java:34)&apos; could not be determined automatically, due to type erasure. You can give type information hints by using the returns(...) method on the result of the transformation call, or by letting your function implement the &apos;ResultTypeQueryable&apos; interface.</span><br><span class="line">    at org.apache.flink.api.dag.Transformation.getOutputType(Transformation.java:417)</span><br><span class="line">    at org.apache.flink.streaming.api.datastream.DataStream.getType(DataStream.java:175)</span><br><span class="line">    at org.apache.flink.streaming.api.datastream.DataStream.keyBy(DataStream.java:318)</span><br><span class="line">    at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:41)</span><br><span class="line">Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of &apos;Collector&apos; are missing. In many cases lambda methods don&apos;t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the &apos;org.apache.flink.api.common.functions.FlatMapFunction&apos; interface. Otherwise the type has to be specified explicitly using type information.</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractionUtils.validateLambdaType(TypeExtractionUtils.java:350)</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractionUtils.extractTypeFromLambda(TypeExtractionUtils.java:176)</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:571)</span><br><span class="line">    at org.apache.flink.api.java.typeutils.TypeExtractor.getFlatMapReturnTypes(TypeExtractor.java:196)</span><br><span class="line">    at org.apache.flink.streaming.api.datastream.DataStream.flatMap(DataStream.java:611)</span><br><span class="line">    at com.zhisheng.examples.streaming.socket.LambdaMain.main(LambdaMain.java:34)</span><br></pre></td></tr></table></figure>
<p>根据上面的报错信息其实可以知道要怎么解决了，该错误是因为 Flink 在用户自定义的函数中会使用泛型来创建 serializer，当使用匿名函数时，类型信息会被保留。但 Lambda 表达式并不是匿名函数，所以 javac 编译的时候并不会把泛型保存到 class 文件里。</p>
<p>解决方法：使用 Flink 提供的 returns 方法来指定 flatMap 的返回类型</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用 TupleTypeInfo 来指定 Tuple 的参数类型</span></span><br><span class="line">.returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class))</span><br></pre></td></tr></table></figure>
<p>在 flatMap 后面加上上面这个 returns 就行了，但是如果算子多了的话，每个都去加一个 returns，其实会很痛苦的，所以通常使用匿名函数或者自定义函数居多。</p>
<h3 id="小结与反思-6"><a href="#小结与反思-6" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 的第二个应用程序 —— 读取 Socket 数据，希望通过两个简单的程序可以让你对 Flink 有个简单的认识，然后讲解了下 Flink 应用程序中使用 Lambda 表达式的问题。</p>
<p>本节涉及的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/socket" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/socket</a></p>
<h2 id="八、Flink多种时间语义对比"><a href="#八、Flink多种时间语义对比" class="headerlink" title="八、Flink多种时间语义对比"></a>八、Flink多种时间语义对比</h2><p>Flink 在流应用程序中支持不同的 <strong>Time</strong> 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。下面我们一起来看看这三个 Time。</p>
<h3 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h3><p>Processing Time 是指事件被处理时机器的系统时间。</p>
<p>如果我们 Flink Job 设置的时间策略是 Processing Time 的话，那么后面所有基于时间的操作（如时间窗口）都将会使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</p>
<p>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</p>
<p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p>
<h3 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h3><p>Event Time 是指事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。</p>
<p>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（事件产生的时间顺序）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</p>
<p>假设所有数据都已到达，Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，不管它们到达的顺序如何（是否按照事件产生的时间）。</p>
<h3 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h3><p>Ingestion Time 是事件进入 Flink 的时间。 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</p>
<p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，成本可能会高一点，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（只在进入 Flink 的时候分配一次），所以对事件的不同窗口操作将使用相同的时间戳（第一次分配的时间戳），而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p>
<p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序中不必指定如何生成水印。</p>
<p>在 Flink 中，Ingestion Time 与 Event Time 非常相似，唯一区别就是 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p>
<h3 id="三种-Time-对比结果"><a href="#三种-Time-对比结果" class="headerlink" title="三种 Time 对比结果"></a>三种 Time 对比结果</h3><p>一张图概括上面说的三种 Time：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uwc85x5j21li0u075w.jpg" alt="undefined"></p>
<ul>
<li>Processing Time：事件被处理时机器的系统时间</li>
<li>Event Time：事件自身的时间</li>
<li>Ingestion Time：事件进入 Flink 的时间</li>
</ul>
<p>一张图形象描述上面说的三种 Time：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uwqobe4j227414ogn1.jpg" alt="undefined"></p>
<h3 id="使用场景分析"><a href="#使用场景分析" class="headerlink" title="使用场景分析"></a>使用场景分析</h3><p>通过上面两个图相信大家已经对 Flink 中的这三个 Time 有所了解了，那么我们实际生产环境中通常该如何选择哪种 Time 呢？</p>
<p>一般来说在生产环境中将 Event Time 与 Processing Time 对比的比较多，这两个也是我们常用的策略，Ingestion Time 一般用的较少。</p>
<p>用 Processing Time 的场景大多是用户不关心事件时间，它只需要关心这个时间窗口要有数据进来，只要有数据进来了，我就可以对进来窗口中的数据进行一系列的计算操作，然后再将计算后的数据发往下游。</p>
<p>而用 Event Time 的场景一般是业务需求需要时间这个字段（比如购物时是要先有下单事件、再有支付事件；借贷事件的风控是需要依赖时间来做判断的；机器异常检测触发的告警也是要具体的异常事件的时间展示出来；商品广告及时精准推荐给用户依赖的就是用户在浏览商品的时间段/频率/时长等信息），只能根据事件时间来处理数据，而且还要从事件中获取到事件的时间。</p>
<p>但是使用事件时间的话，就可能有这样的情况：数据源采集的数据往消息队列中发送时可能因为网络抖动、服务可用性、消息队列的分区数据堆积的影响而导致数据到达的不一定及时，可能会出现数据出现一定的乱序、延迟几分钟等，庆幸的是 Flink 支持通过 WaterMark 机制来处理这种延迟的数据。关于 WaterMark 的机制我会在后面的文章讲解。</p>
<h3 id="如何设置-Time-策略？"><a href="#如何设置-Time-策略？" class="headerlink" title="如何设置 Time 策略？"></a>如何设置 Time 策略？</h3><p>在创建完流运行环境的时候，然后就可以通过 <code>env.setStreamTimeCharacteristic</code> 设置时间策略：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他两种:</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span></span><br></pre></td></tr></table></figure>
<h3 id="小结与反思-7"><a href="#小结与反思-7" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节介绍了 Flink 中的三种时间语义，相比较其他的流处理引擎来说支持的更多，你知道的流处理引擎支持哪些时间语义呢？</p>
<h2 id="九、Flink-Window-基础概念与实现原理"><a href="#九、Flink-Window-基础概念与实现原理" class="headerlink" title="九、Flink Window 基础概念与实现原理"></a>九、Flink Window 基础概念与实现原理</h2><p>目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语，例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” 。</p>
<p>对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。</p>
<p>在本节将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。</p>
<h3 id="什么是-Window？"><a href="#什么是-Window？" class="headerlink" title="什么是 Window？"></a>什么是 Window？</h3><p>下面我们结合一个现实的例子来说明。</p>
<p>就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？</p>
<p>假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0uzyaa4aj21mm08mmx1.jpg" alt="undefined"></p>
<p>可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v0en8wuj21m80hudfw.jpg" alt="undefined"></p>
<p>这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？</p>
<p>这个问题，就相当于一个定义了一个 Window（窗口），Window 的界限是 1 分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v0wadrcj21my0gmaa6.jpg" alt="undefined"></p>
<p>第一分钟的数量为 18，第二分钟是 28，第三分钟是 24……这样，1 个小时内会有 60 个 Window。</p>
<p>再考虑一种情况，每 30 秒统计一次过去 1 分钟的汽车数量之和：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v1b9o6jj21mu0o2wet.jpg" alt="undefined"></p>
<p>此时，Window 出现了重合。这样，1 个小时内会有 120 个 Window。</p>
<h3 id="Window-有什么作用？"><a href="#Window-有什么作用？" class="headerlink" title="Window 有什么作用？"></a>Window 有什么作用？</h3><p>通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。</p>
<h3 id="Flink-自带的-Window"><a href="#Flink-自带的-Window" class="headerlink" title="Flink 自带的 Window"></a>Flink 自带的 Window</h3><p>Flink 在 KeyedStream（DataStream 的继承类） 中提供了下面几种 Window：</p>
<ul>
<li>以时间驱动的 Time Window</li>
<li>以事件数量驱动的 Count Window</li>
<li>以会话间隔驱动的 Session Window</li>
</ul>
<p>提供上面三种 Window 机制后，由于某些特殊的需要，DataStream API 也提供了定制化的 Window 操作，供用户自定义 Window。</p>
<p>下面将先围绕上面说的三种 Window 来进行分析并教大家如何使用，然后对其原理分析，最后在解析其源码实现。</p>
<h3 id="Time-Window-使用及源码分析"><a href="#Time-Window-使用及源码分析" class="headerlink" title="Time Window 使用及源码分析"></a>Time Window 使用及源码分析</h3><p>正如命名那样，Time Window 根据时间来聚合流数据。例如：一分钟的时间窗口就只会收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于下一个算子。</p>
<p>在 Flink 中使用 Time Window 非常简单，输入一个时间参数，这个时间参数可以利用 Time 这个类来控制，如果事前没指定 TimeCharacteristic 类型的话，则默认使用的是 ProcessingTime，如果对 Flink 中的 Time 还不了解的话，可以看前一篇文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db6a06bf6a6211cb96164ff" target="_blank" rel="noopener">Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析</a> 如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>)) <span class="comment">//time Window 每分钟统计一次数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>时间窗口的数据窗口聚合流程如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v50qmevj21sc0w2wgo.jpg" alt="undefined"></p>
<p>在第一个窗口中（1 ～ 2 分钟）和为 7、第二个窗口中（2 ～ 3 分钟）和为 12、第三个窗口中（3 ～ 4 分钟）和为 7、第四个窗口中（4 ～ 5 分钟）和为 19。</p>
<p>该 timeWindow 方法在 KeyedStream 中对应的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//时间窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, TimeWindow&gt; <span class="title">timeWindow</span><span class="params">(Time size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingProcessingTimeWindows.of(size));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingEventTimeWindows.of(size));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外在 Time Window 中还支持滑动的时间窗口，比如定义了一个每 30s 滑动一次的 1 分钟时间窗口，它会每隔 30s 去统计过去一分钟窗口内的数据，同样使用也很简单，输入两个时间参数，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>)) <span class="comment">//sliding time Window 每隔 30s 统计过去一分钟的数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>滑动时间窗口的数据聚合流程如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0v9hcl5wj21sa0xq410.jpg" alt="undefined"></p>
<p>在该第一个时间窗口中（1 ～ 2 分钟）和为 7，第二个时间窗口中（1:30 ~ 2:30）和为 10，第三个时间窗口中（2 ~ 3 分钟）和为 12，第四个时间窗口中（2:30 ~ 3:30）和为 10，第五个时间窗口中（3 ~ 4 分钟）和为 7，第六个时间窗口中（3:30 ~ 4:30）和为 11，第七个时间窗口中（4 ~ 5 分钟）和为 19。</p>
<p>该 timeWindow 方法在 KeyedStream 中对应的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动时间窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, TimeWindow&gt; <span class="title">timeWindow</span><span class="params">(Time size, Time slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        <span class="keyword">return</span> window(SlidingProcessingTimeWindows.of(size, slide));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> window(SlidingEventTimeWindows.of(size, slide));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Count-Window-使用及源码分析"><a href="#Count-Window-使用及源码分析" class="headerlink" title="Count Window 使用及源码分析"></a>Count Window 使用及源码分析</h3><p>Apache Flink 还提供计数窗口功能，如果计数窗口的值设置的为 3 ，那么将会在窗口中收集 3 个事件，并在添加第 3 个元素时才会计算窗口中所有事件的值。</p>
<p>在 Flink 中使用 Count Window 非常简单，输入一个 long 类型的参数，这个参数代表窗口中事件的数量，使用如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .countWindow(<span class="number">3</span>) <span class="comment">//统计每 3 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>计数窗口的数据窗口聚合流程如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vazxmyfj21ek0ny769.jpg" alt="undefined"></p>
<p>该 countWindow 方法在 KeyedStream 中对应的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外在 Count Window 中还支持滑动的计数窗口，比如定义了一个每 3 个事件滑动一次的 4 个事件的计数窗口，它会每隔 3 个事件去统计过去 4 个事件计数窗口内的数据，使用也很简单，输入两个 long 类型的参数，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>) </span><br><span class="line">    .countWindow(<span class="number">4</span>, <span class="number">3</span>) <span class="comment">//每隔 3 个元素统计过去 4 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>滑动计数窗口的数据窗口聚合流程如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vbqi86ej225m0wmac7.jpg" alt="undefined"></p>
<p>该 countWindow 方法在 KeyedStream 中对应的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Session-Window-使用及源码分析"><a href="#Session-Window-使用及源码分析" class="headerlink" title="Session Window 使用及源码分析"></a>Session Window 使用及源码分析</h3><p>Apache Flink 还提供了会话窗口，是什么意思呢？使用该窗口的时候你可以传入一个时间参数（表示某种数据维持的会话持续时长），如果超过这个时间，就代表着超出会话时长。</p>
<p>在 Flink 中使用 Session Window 非常简单，你该使用 Flink KeyedStream 中的 window 方法，然后使用 ProcessingTimeSessionWindows.withGap()（不一定就是只使用这个），在该方法里面你需要做的是传入一个时间参数，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">5</span>)))<span class="comment">//表示如果 5s 内没出现数据则认为超出会话时长，然后计算这个窗口的和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>会话窗口的数据窗口聚合流程如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vcsdq1kj22gu0xaq5w.jpg" alt="undefined"></p>
<p>该 Window 方法在 KeyedStream 中对应的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供自定义 Window</span></span><br><span class="line"><span class="keyword">public</span> &lt;W extends Window&gt; <span class="function">WindowedStream&lt;T, KEY, W&gt; <span class="title">window</span><span class="params">(WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; assigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WindowedStream&lt;&gt;(<span class="keyword">this</span>, assigner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="如何自定义-Window？"><a href="#如何自定义-Window？" class="headerlink" title="如何自定义 Window？"></a>如何自定义 Window？</h3><p>当然除了上面几种自带的 Window 外，Apache Flink 还提供了用户可自定义的 Window，那么该如何操作呢？其实细心的同学可能已经发现了上面我写的每种 Window 的实现方式了，它们有 assigner、 evictor、trigger。如果你没发现的话，也不要紧，这里我们就来了解一下实现 Window 的机制，这样我们才能够更好的学会如何自定义 Window。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vdsn6xyj21nc12qq3x.jpg" alt="undefined"></p>
<h3 id="3-2-8-Window-源码定义"><a href="#3-2-8-Window-源码定义" class="headerlink" title="3.2.8 Window 源码定义"></a>3.2.8 Window 源码定义</h3><p>上面说了 Flink 中自带的 Window，主要利用了 KeyedStream 的 API 来实现，我们这里来看下 Window 的源码定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//获取属于此窗口的最大时间戳</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">maxTimestamp</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看源码可以看见 Window 这个抽象类有如下实现类：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vi0o960j20y20as3yg.jpg" alt="undefined"></p>
<p><strong>TimeWindow</strong> 源码定义如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//窗口开始时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> start;</span><br><span class="line">    <span class="comment">//窗口结束时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> end;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>GlobalWindow</strong> 源码定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> GlobalWindow INSTANCE = <span class="keyword">new</span> GlobalWindow();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">GlobalWindow</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">    <span class="comment">//对外提供 get() 方法返回 GlobalWindow 实例，并且是个全局单例</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> GlobalWindow <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Window-组件之-WindowAssigner-使用及源码分析"><a href="#Window-组件之-WindowAssigner-使用及源码分析" class="headerlink" title="Window 组件之 WindowAssigner 使用及源码分析"></a>Window 组件之 WindowAssigner 使用及源码分析</h3><p>到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。</p>
<p>窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。我们来看下 WindowAssigner 的代码的定义吧：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowAssigner</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//分配数据到窗口并返回窗口集合</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Collection&lt;W&gt; <span class="title">assignWindows</span><span class="params">(T element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看源码可以看见 WindowAssigner 这个抽象类有如下实现类：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vlvjsgdj22ro0p0wey.jpg" alt="undefined"></p>
<p>这些 WindowAssigner 实现类的作用介绍：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vpz366lj20iv08gn0z.jpg" alt="TIM截图20191218145154.png"></p>
<p>如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，比如我拿 TumblingEventTimeWindows 的源码来分析，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TumblingEventTimeWindows</span> <span class="keyword">extends</span> <span class="title">WindowAssigner</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> size;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">TumblingEventTimeWindows</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> offset)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (Math.abs(offset) &gt;= size) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"TumblingEventTimeWindows parameters must satisfy abs(offset) &lt; size"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.size = size;</span><br><span class="line">        <span class="keyword">this</span>.offset = offset;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 WindowAssigner 抽象类中的抽象方法 assignWindows</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title">assignWindows</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//实现该 TumblingEventTimeWindows 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他方法，对外提供静态方法，供其他类调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面你就会发现<strong>套路</strong>：</p>
<p>1、定义好实现类的属性</p>
<p>2、根据定义的属性添加构造方法</p>
<p>3、重写 WindowAssigner 中的 assignWindows 等方法</p>
<p>4、定义其他的方法供外部调用</p>
<h3 id="Window-组件之-Trigger-使用及源码分析"><a href="#Window-组件之-Trigger-使用及源码分析" class="headerlink" title="Window 组件之 Trigger 使用及源码分析"></a>Window 组件之 Trigger 使用及源码分析</h3><p>Trigger 表示触发器，每个窗口都拥有一个 Trigger（触发器），该 Trigger 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发，即清除（删除窗口并丢弃其内容），或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。</p>
<p>说了这么一大段，我们还是来看看 Trigger 的源码，定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Trigger</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//当有数据进入到 Window 运算符就会触发该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onElement</span><span class="params">(T element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的处理时间计时器触发时调用</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的事件时间计时器触发时调用该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当有数据流入 Window 运算符时就会触发 onElement 方法、当处理时间和事件时间生效时会触发 onProcessingTime 和 onEventTime 方法。每个触发动作的返回结果用 TriggerResult 定义。继续来看下 TriggerResult 的源码定义：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> TriggerResult &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//不做任何操作</span></span><br><span class="line">    CONTINUE(<span class="keyword">false</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理并移除窗口中的数据</span></span><br><span class="line">    FIRE_AND_PURGE(<span class="keyword">true</span>, <span class="keyword">true</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理窗口数据，窗口计算后不做清理</span></span><br><span class="line">    FIRE(<span class="keyword">true</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//清除窗口中的所有元素，并且在不计算窗口函数或不发出任何元素的情况下丢弃窗口</span></span><br><span class="line">    PURGE(<span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看源码可以看见 Trigger 这个抽象类有如下实现类：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vqk5hxnj22ua0is3ys.jpg" alt="undefined"></p>
<p>这些 Trigger 实现类的作用介绍：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vqqwlzej227017sdnt.jpg" alt="undefined"></p>
<p>如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，拿 CountTrigger 的源码来分析，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountTrigger</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Trigger</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReducingStateDescriptor&lt;Long&gt; stateDesc = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(<span class="string">"count"</span>, <span class="keyword">new</span> Sum(), LongSerializer.INSTANCE);</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountTrigger</span><span class="params">(<span class="keyword">long</span> maxCount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = maxCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写抽象类 Trigger 中的抽象方法 </span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//实现 CountTrigger 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>套路</strong>：</p>
<ol>
<li>定义好实现类的属性</li>
<li>根据定义的属性添加构造方法</li>
<li>重写 Trigger 中的 onElement、onEventTime、onProcessingTime 等方法</li>
<li>定义其他的方法供外部调用</li>
</ol>
<h3 id="Window-组件之-Evictor-使用及源码分析"><a href="#Window-组件之-Evictor-使用及源码分析" class="headerlink" title="Window 组件之 Evictor 使用及源码分析"></a>Window 组件之 Evictor 使用及源码分析</h3><p>Evictor 表示驱逐者，它可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素，然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。</p>
<p>我们来看看 Evictor 的源码定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Evictor</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//在窗口函数之前调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">    <span class="comment">//在窗口函数之后调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查看源码可以看见 Evictor 这个接口有如下实现类：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vrfwkxpj215m0f8q30.jpg" alt="undefined"></p>
<p>这些 Evictor 实现类的作用介绍：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vro7oi3j225y0ts0vk.jpg" alt="undefined"></p>
<p>如果你细看了上面三种中某个类的实现的话，你会发现一个规律，比如我就拿 CountEvictor 的源码来分析，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountEvictor</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Evictor</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> doEvictAfter;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count, <span class="keyword">boolean</span> doEvictAfter)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = doEvictAfter;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictBefore 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictAfter 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">evict</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//内部的关键实现方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他的方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>发现<strong>套路</strong>：</p>
<ol>
<li>定义好实现类的属性</li>
<li>根据定义的属性添加构造方法</li>
<li>重写 Evictor 中的 evictBefore 和 evictAfter 方法</li>
<li>定义关键的内部实现方法 evict，处理具体的逻辑</li>
<li>定义其他的方法供外部调用</li>
</ol>
<p>上面我们详细讲解了 Window 中的组件 WindowAssigner、Trigger、Evictor，然后继续回到问题：如何自定义 Window？</p>
<p>上文讲解了 Flink 自带的 Window（Time Window、Count Window、Session Window），然后还分析了他们的源码实现，通过这几个源码，我们可以发现，它最后调用的都有一个方法，那就是 Window 方法，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供自定义 Window</span></span><br><span class="line"><span class="keyword">public</span> &lt;W extends Window&gt; <span class="function">WindowedStream&lt;T, KEY, W&gt; <span class="title">window</span><span class="params">(WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; assigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WindowedStream&lt;&gt;(<span class="keyword">this</span>, assigner);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//构造一个 WindowedStream 实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">WindowedStream</span><span class="params">(KeyedStream&lt;T, K&gt; input,</span></span></span><br><span class="line"><span class="function"><span class="params">        WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; windowAssigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.input = input;</span><br><span class="line">    <span class="keyword">this</span>.windowAssigner = windowAssigner;</span><br><span class="line">    <span class="comment">//获取一个默认的 Trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = windowAssigner.getDefaultTrigger(input.getExecutionEnvironment());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到这个 Window 方法传入的参数是一个 WindowAssigner 对象（你可以利用 Flink 现有的 WindowAssigner，也可以根据上面的方法来自定义自己的 WindowAssigner），然后再通过构造一个 WindowedStream 实例（在构造实例的会传入 WindowAssigner 和获取默认的 Trigger）来创建一个 Window。</p>
<p>另外你可以看到滑动计数窗口，在调用 window 方法之后，还调用了 WindowedStream 的 evictor 和 trigger 方法，trigger 方法会覆盖掉你之前调用 Window 方法中默认的 trigger，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//trigger 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">trigger</span><span class="params">(Trigger&lt;? <span class="keyword">super</span> T, ? <span class="keyword">super</span> W&gt; trigger)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner &amp;&amp; !trigger.canMerge()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"A merging window assigner cannot be used with a trigger that does not support merging."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> BaseAlignedWindowAssigner) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Cannot use a "</span> + windowAssigner.getClass().getSimpleName() + <span class="string">" with a custom trigger."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//覆盖之前的 trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = trigger;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的各种窗口实现，你就会发现了：Evictor 是可选的，但是 WindowAssigner 和 Trigger 是必须会有的，这种创建 Window 的方法充分利用了 KeyedStream 和 WindowedStream 的 API，再加上现有的 WindowAssigner、Trigger、Evictor，你就可以创建 Window 了，另外你还可以自定义这三个窗口组件的实现类来满足你公司项目的需求。</p>
<h3 id="小结与反思-8"><a href="#小结与反思-8" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节从生活案例来分享关于 Window 方面的需求，进而开始介绍 Window 相关的知识，并把 Flink 中常使用的三种窗口都一一做了介绍，并告诉大家如何使用，还分析了其实现原理。最后还对 Window 的内部组件做了详细的分析，为自定义 Window 提供了方法。</p>
<p>不知道你看完本节后对 Window 还有什么疑问吗？你们是根据什么条件来选择使用哪种 Window 的？在使用的过程中有遇到什么问题吗？</p>
<h2 id="十、数据转换必须熟悉的算子（Operator）"><a href="#十、数据转换必须熟悉的算子（Operator）" class="headerlink" title="十、数据转换必须熟悉的算子（Operator）"></a>十、数据转换必须熟悉的算子（Operator）</h2><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0vsgous8j21ri0dcmxj.jpg" alt="undefined"></p>
<p>在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是上图这种模型，有数据源（source），有数据下游（sink），我们写的应用程序多是对数据源过来的数据做一系列操作，总结如下。</p>
<ol>
<li><strong>Source</strong>: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</li>
<li><strong>Transformation</strong>: 数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</li>
<li><strong>Sink</strong>: 接收器，Sink 是指 Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来。Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 Socket 、自定义的 Sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 Sink。</li>
</ol>
<p>那么本文将给大家介绍的就是 Flink 中的批和流程序常用的算子（Operator）。</p>
<h3 id="DataStream-Operator"><a href="#DataStream-Operator" class="headerlink" title="DataStream Operator"></a>DataStream Operator</h3><p>我们先来看看流程序中常用的算子。</p>
<h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><p>Map 算子的输入流是 DataStream，经过 Map 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成一个元素，举个例子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; map = employeeStream.map(<span class="keyword">new</span> MapFunction&lt;Employee, Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Employee <span class="title">map</span><span class="params">(Employee employee)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        employee.salary = employee.salary + <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> employee;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">map.print();</span><br></pre></td></tr></table></figure>
<p>新的一年给每个员工的工资加 5000。</p>
<h4 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h4><p>FlatMap 算子的输入流是 DataStream，经过 FlatMap 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成零个、一个或多个元素，举个例子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; flatMap = employeeStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Employee, Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Employee employee, Collector&lt;Employee&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (employee.salary &gt;= <span class="number">40000</span>) &#123;</span><br><span class="line">            out.collect(employee);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">flatMap.print();</span><br></pre></td></tr></table></figure>
<p>将工资大于 40000 的找出来。</p>
<h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0xjwko8fj21200aqaa3.jpg" alt="undefined"></p>
<p>对每个元素都进行判断，返回为 true 的元素，如果为 false 则丢弃数据，上面找出工资大于 40000 的员工其实也可以用 Filter 来做：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; filter = employeeStream.filter(<span class="keyword">new</span> FilterFunction&lt;Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Employee employee)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (employee.salary &gt;= <span class="number">40000</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">filter.print();</span><br></pre></td></tr></table></figure>
<h4 id="KeyBy"><a href="#KeyBy" class="headerlink" title="KeyBy"></a>KeyBy</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0xkegqp2j213s0ju3yu.jpg" alt="undefined"></p>
<p>KeyBy 在逻辑上是基于 key 对流进行分区，相同的 Key 会被分到一个分区（这里分区指的就是下游算子多个并行节点的其中一个）。在内部，它使用 hash 函数对流进行分区。它返回 KeyedDataStream 数据流。举个例子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream&lt;ProductEvent, Integer&gt; keyBy = productStream.keyBy(<span class="keyword">new</span> KeySelector&lt;ProductEvent, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(ProductEvent product)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> product.shopId;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">keyBy.print();</span><br></pre></td></tr></table></figure>
<p>根据商品的店铺 id 来进行分区。</p>
<h4 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h4><p>Reduce 返回单个的结果值，并且 reduce 操作每处理一个元素总是创建一个新值。常用的方法有 average、sum、min、max、count，使用 Reduce 方法都可实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Employee&gt; reduce = employeeStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Employee, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Employee employee)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> employee.shopId;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).reduce(<span class="keyword">new</span> ReduceFunction&lt;Employee&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Employee <span class="title">reduce</span><span class="params">(Employee employee1, Employee employee2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        employee1.salary = (employee1.salary + employee2.salary) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">return</span> employee1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">reduce.print();</span><br></pre></td></tr></table></figure>
<p>上面先将数据流进行 keyby 操作，因为执行 Reduce 操作只能是 KeyedStream，然后将员工的工资做了一个求平均值的操作。</p>
<h4 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h4><p>DataStream API 支持各种聚合，例如 min、max、sum 等。 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream.sum(<span class="number">0</span>) </span><br><span class="line">KeyedStream.sum(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.min(<span class="number">0</span>) </span><br><span class="line">KeyedStream.min(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.max(<span class="number">0</span>) </span><br><span class="line">KeyedStream.max(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.minBy(<span class="number">0</span>) </span><br><span class="line">KeyedStream.minBy(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.maxBy(<span class="number">0</span>) </span><br><span class="line">KeyedStream.maxBy(<span class="string">"key"</span>)</span><br></pre></td></tr></table></figure>
<p>max 和 maxBy 之间的区别在于 max 返回流中的最大值，但 maxBy 返回具有最大值的键， min 和 minBy 同理。</p>
<h4 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h4><p>Window 函数允许按时间或其他条件对现有 KeyedStream 进行分组。 以下是以 10 秒的时间窗口聚合：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.keyBy(<span class="number">0</span>).window(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure>
<p>有时候因为业务需求场景要求：聚合一分钟、一小时的数据做统计报表使用。</p>
<h4 id="WindowAll"><a href="#WindowAll" class="headerlink" title="WindowAll"></a>WindowAll</h4><p>WindowAll 将元素按照某种特性聚集在一起，该函数不支持并行操作，默认的并行度就是 1，所以如果使用这个算子的话需要注意一下性能问题，以下是使用例子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.keyBy(<span class="number">0</span>).windowAll(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)));</span><br></pre></td></tr></table></figure>
<h4 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0y9pd8g3j210u0heq35.jpg" alt="undefined"></p>
<p>Union 函数将两个或多个数据流结合在一起。 这样后面在使用的时候就只需使用一个数据流就行了。 如果我们将一个流与自身组合，那么组合后的数据流会有两份同样的数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.union(inputStream1, inputStream2, ...);</span><br></pre></td></tr></table></figure>
<h4 id="Window-Join"><a href="#Window-Join" class="headerlink" title="Window Join"></a>Window Join</h4><p>我们可以通过一些 key 将同一个 window 的两个数据流 join 起来。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputStream.join(inputStream1)</span><br><span class="line">           .where(<span class="number">0</span>).equalTo(<span class="number">1</span>)</span><br><span class="line">           .window(Time.seconds(<span class="number">5</span>))     </span><br><span class="line">           .apply (<span class="keyword">new</span> JoinFunction () &#123;...&#125;);</span><br></pre></td></tr></table></figure>
<p>以上示例是在 5 秒的窗口中连接两个流，其中第一个流的第一个属性的连接条件等于另一个流的第二个属性。</p>
<h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0ybr9m9lj212e0h4mxg.jpg" alt="undefined"></p>
<p>此功能根据条件将流拆分为两个或多个流。 当你获得混合流然后你可能希望单独处理每个数据流时，可以使用此方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split = inputStream.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;(); </span><br><span class="line">        <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            output.add(<span class="string">"even"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            output.add(<span class="string">"odd"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>上面就是将偶数数据流放在 even 中，将奇数数据流放在 odd 中。</p>
<h4 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h4><p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yg0tundj21gg0ggq3e.jpg" alt="undefined"></p>
<p>上面用 Split 算子将数据流拆分成两个数据流（奇数、偶数），接下来你可能想从拆分流中选择特定流，那么就得搭配使用 Select 算子（一般这两者都是搭配在一起使用的），</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split;</span><br><span class="line">DataStream&lt;Integer&gt; even = split.select(<span class="string">"even"</span>); </span><br><span class="line">DataStream&lt;Integer&gt; odd = split.select(<span class="string">"odd"</span>); </span><br><span class="line">DataStream&lt;Integer&gt; all = split.select(<span class="string">"even"</span>,<span class="string">"odd"</span>);</span><br></pre></td></tr></table></figure>
<p>我们就介绍这么些常用的算子了，当然肯定也会有遗漏，具体还得查看官网 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/</a> 的介绍。</p>
<h3 id="DataSet-Operator"><a href="#DataSet-Operator" class="headerlink" title="DataSet Operator"></a>DataSet Operator</h3><p>上面介绍了 DataStream 的常用算子，其实上面也有一些算子也是同样适合于 DataSet 的，比如 Map、FlatMap、Filter 等（相同的我就不再重复了）；也有一些算子是 DataSet API 独有的，比如 DataStream 中分区使用的是 KeyBy，但是 DataSet 中使用的是 GroupBy。</p>
<h4 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = </span><br><span class="line"><span class="comment">// 返回 DataSet 中前 5 的元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out1 = in.first(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回分组后每个组的前 2 元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out2 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .first(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回分组后每个组的前 3 元素（按照上升排序）</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out3 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .sortGroup(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                          .first(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>
<p>还有一些，感兴趣的可以查看官网 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/dataset_transformations.html。" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/dataset_transformations.html。</a></p>
<h3 id="流批统一的思路"><a href="#流批统一的思路" class="headerlink" title="流批统一的思路"></a>流批统一的思路</h3><p>一般公司里的业务场景需求肯定不止是只有批计算，也不只是有流计算的。一般这两种需求是都存在的。比如每天凌晨 00:00 去算昨天一天商品的售卖情况，然后出报表给运营或者老板去分析；另外的就是处理实时的数据。</p>
<p>但是这样就会有一个问题，需要让开发掌握两套 API。有些数据工程师的开发能力可能并不高，他们会更擅长写一些 SQL 去分析，所以要是掌握两套 API 的话，对他们来说成本可能会很大。要是 Flink 能够提供一种高级的 API，上层做好完全封装，让开发无感知底层到底运行的是 DataSet 还是 DataStream API，这样不管是开发还是数据工程师只需要学习一套高级的 API 就行。</p>
<p>Flink 社区包括阿里巴巴实时计算团队也在大力推广这块，那就是我们的 Flink Table API &amp; SQL，在 Flink 1.9 版本，开源版本的 Blink 大部分代码已经合进去了，期待阿里实时计算团队为社区带来更多的贡献。</p>
<p>对于开发人员来说，流批统一的引擎（Table API &amp; SQL）在执行之前会根据运行的环境翻译成 DataSet 或者 DataStream API。因为这两种 API 底层的实现有很大的区别，所以在统一流和批的过程中遇到了不少挑战。</p>
<ul>
<li>理论基础：动态表</li>
<li>架构改进（统一的 Operator 框架、统一的查询处理）</li>
<li>优化器的统一</li>
<li>基础数据结构的统一</li>
<li>物理实现的共享</li>
</ul>
<p>关于 Table API &amp; SQL，在进阶篇第五章中有讲解！</p>
<h3 id="小结与反思-9"><a href="#小结与反思-9" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节介绍了在开发 Flink 作业中数据转换常使用的算子（包含流作业和批作业），DataStream API 和 DataSet API 中部分算子名字是一致的，也有不同的地方，最后讲解了下 Flink 社区后面流批统一的思路。</p>
<p>你们公司使用 Flink 是流作业居多还是批作业居多？</p>
<h2 id="十一、如何使用-DataStream-API-来处理数据？"><a href="#十一、如何使用-DataStream-API-来处理数据？" class="headerlink" title="十一、如何使用 DataStream API 来处理数据？"></a>十一、如何使用 DataStream API 来处理数据？</h2><p>在 3.3 节中讲了数据转换常用的 Operators（算子），然后在 3.2 节中也讲了 Flink 中窗口的概念和原理，那么我们这篇文章再来细讲一下 Flink 中的各种 DataStream API。</p>
<p>我们先来看下源码里面的 DataStream 大概有哪些类呢？</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yi4jstvj215m17ejss.jpg" alt="undefined"></p>
<p>可以发现其实还是有很多的类，只有熟练掌握了这些 API，我们才能在做数据转换和计算的时候足够灵活的运用开来（知道何时该选用哪种 DataStream？选用哪个 Function？）。那么我们先从 DataStream 开始吧！</p>
<h3 id="DataStream-如何使用及分析"><a href="#DataStream-如何使用及分析" class="headerlink" title="DataStream 如何使用及分析"></a>DataStream 如何使用及分析</h3><p>首先我们来看下 DataStream 这个类的定义吧：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A DataStream represents a stream of elements of the same type. A DataStreamcan be transformed into another DataStream by applying a transformation as</span><br><span class="line"> DataStream#map or DataStream#filter&#125;</span><br></pre></td></tr></table></figure>
<p>大概意思是：DataStream 表示相同类型的元素组成的数据流，一个数据流可以通过 map/filter 等算子转换成另一个数据流。</p>
<p>然后 DataStream 的类结构图如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yppinynj21cy0fodfz.jpg" alt="undefined"></p>
<p>它的继承类有 KeyedStream、SingleOutputStreamOperator 和 SplitStream。这几个类本文后面都会一一给大家讲清楚。下面我们来看看 DataStream 这个类中的属性和方法吧。</p>
<p>它的属性就只有两个：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> StreamExecutionEnvironment environment;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> StreamTransformation&lt;T&gt; transformation;</span><br></pre></td></tr></table></figure>
<p>但是它的方法却有很多，并且我们平时写的 Flink Job 几乎离不开这些方法，这也注定了这个类的重要性，所以得好好看下这些方法该如何使用，以及是如何实现的。</p>
<h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>通过合并相同数据类型的数据流，然后创建一个新的数据流，union 方法代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> DataStream&lt;T&gt; <span class="title">union</span><span class="params">(DataStream&lt;T&gt;... streams)</span> </span>&#123;</span><br><span class="line">    List&lt;StreamTransformation&lt;T&gt;&gt; unionedTransforms = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    unionedTransforms.add(<span class="keyword">this</span>.transformation);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (DataStream&lt;T&gt; newStream : streams) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!getType().equals(newStream.getType())) &#123;   <span class="comment">//判断数据类型是否一致</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot union streams of different types: "</span> + getType() + <span class="string">" and "</span> + newStream.getType());</span><br><span class="line">        &#125;</span><br><span class="line">        unionedTransforms.add(newStream.getTransformation());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构建新的数据流</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DataStream&lt;&gt;(<span class="keyword">this</span>.environment, <span class="keyword">new</span> UnionTransformation&lt;&gt;(unionedTransforms));<span class="comment">//通过使用 UnionTransformation 将多个 StreamTransformation 合并起来</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么我们该如何去使用 union 呢（不止连接一个数据流，也可以连接多个数据流）？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//数据流 1 和 2</span></span><br><span class="line"><span class="keyword">final</span> DataStream&lt;Integer&gt; stream1 = env.addSource(...);</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;Integer&gt; stream2 = env.addSource(...);</span><br><span class="line"><span class="comment">//union</span></span><br><span class="line">stream1.union(stream2)</span><br></pre></td></tr></table></figure>
<h4 id="split"><a href="#split" class="headerlink" title="split"></a>split</h4><p>该方法可以将两个数据流进行拆分，拆分后的数据流变成了 SplitStream（在下文会详细介绍这个类的内部实现），该 split 方法通过传入一个 OutputSelector 参数进行数据选择，方法内部实现就是构造一个 SplitStream 对象然后返回：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SplitStream&lt;T&gt; <span class="title">split</span><span class="params">(OutputSelector&lt;T&gt; outputSelector)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> SplitStream&lt;&gt;(<span class="keyword">this</span>, clean(outputSelector));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后我们该如何使用这个方法呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">dataStream.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">8354166915727490130L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; s = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        <span class="keyword">if</span> (value &gt; <span class="number">4</span>) &#123;    <span class="comment">//大于 4 的数据放到 &gt; 这个 tag 里面去</span></span><br><span class="line">            s.add(<span class="string">"&gt;"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;    <span class="comment">//小于等于 4 的数据放到 &lt; 这个 tag 里面去</span></span><br><span class="line">            s.add(<span class="string">"&lt;"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>注意：该方法已经不推荐使用了！在 1.7 版本以后建议使用 Side Output 来实现分流操作。</p>
<h4 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h4><p>通过连接不同或相同数据类型的数据流，然后创建一个新的连接数据流，如果连接的数据流也是一个 DataStream 的话，那么连接后的数据流为 ConnectedStreams（会在下文介绍这个类的具体实现），它的具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">ConnectedStreams&lt;T, R&gt; <span class="title">connect</span><span class="params">(DataStream&lt;R&gt; dataStream)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ConnectedStreams&lt;&gt;(environment, <span class="keyword">this</span>, dataStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果连接的数据流是一个 BroadcastStream（广播数据流），那么连接后的数据流是一个 BroadcastConnectedStream（会在下文详细介绍该类的内部实现），它的具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">BroadcastConnectedStream&lt;T, R&gt; <span class="title">connect</span><span class="params">(BroadcastStream&lt;R&gt; broadcastStream)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BroadcastConnectedStream&lt;&gt;(</span><br><span class="line">            environment, <span class="keyword">this</span>, Preconditions.checkNotNull(broadcastStream), </span><br><span class="line">            broadcastStream.getBroadcastStateDescriptor());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、连接 DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、连接 BroadcastStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));</span><br><span class="line"><span class="keyword">final</span> BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor);</span><br><span class="line">BroadcastConnectedStream&lt;Tuple2&lt;Long, Long&gt;, String&gt; connect = src1.connect(broadcast);</span><br></pre></td></tr></table></figure>
<h4 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h4><p>keyBy 方法是用来将数据进行分组的，通过该方法可以将具有相同 key 的数据划分在一起组成新的数据流，该方法有四种（它们的参数各不一样）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、参数是 KeySelector 对象</span></span><br><span class="line"><span class="keyword">public</span> &lt;K&gt; <span class="function">KeyedStream&lt;T, K&gt; <span class="title">keyBy</span><span class="params">(KeySelector&lt;T, K&gt; key)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(key));<span class="comment">//构造 KeyedStream 对象</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、参数是 KeySelector 对象和 TypeInformation 对象</span></span><br><span class="line"><span class="keyword">public</span> &lt;K&gt; <span class="function">KeyedStream&lt;T, K&gt; <span class="title">keyBy</span><span class="params">(KeySelector&lt;T, K&gt; key, TypeInformation&lt;K&gt; keyType)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(key), keyType);<span class="comment">//构造 KeyedStream 对象</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、参数是 1 至多个字段（用 0、1、2... 表示）</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(<span class="keyword">int</span>... fields)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (getType() <span class="keyword">instanceof</span> BasicArrayTypeInfo || getType() <span class="keyword">instanceof</span> PrimitiveArrayTypeInfo) &#123;</span><br><span class="line">        <span class="keyword">return</span> keyBy(KeySelectorUtil.getSelectorForArray(fields, getType()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> keyBy(<span class="keyword">new</span> Keys.ExpressionKeys&lt;&gt;(fields, getType()));<span class="comment">//调用 private 的 keyBy 方法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4、参数是 1 至多个字符串</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(String... fields)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> keyBy(<span class="keyword">new</span> Keys.ExpressionKeys&lt;&gt;(fields, getType()));<span class="comment">//调用 private 的 keyBy 方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//真正调用的方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> KeyedStream&lt;T, Tuple&gt; <span class="title">keyBy</span><span class="params">(Keys&lt;T&gt; keys)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> KeyedStream&lt;&gt;(<span class="keyword">this</span>, clean(KeySelectorUtil.getSelectorForKeys(keys,</span><br><span class="line">            getType(), getExecutionConfig())));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如何使用呢：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Event&gt; dataStream = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"zhisheng01"</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"zhisheng02"</span>, <span class="number">2.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"zhisheng03"</span>, <span class="number">2.1</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"zhisheng04"</span>, <span class="number">3.0</span>),</span><br><span class="line">    <span class="keyword">new</span> SubEvent(<span class="number">4</span>, <span class="string">"zhisheng05"</span>, <span class="number">4.0</span>, <span class="number">1.0</span>),</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第1种</span></span><br><span class="line">dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.getId();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第2种</span></span><br><span class="line">dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.getId();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;, Types.STRING);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第3种</span></span><br><span class="line">dataStream.keyBy(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第4种</span></span><br><span class="line">dataStream.keyBy(<span class="string">"zhisheng01"</span>, <span class="string">"zhisheng02"</span>);</span><br></pre></td></tr></table></figure>
<h4 id="partitionCustom"><a href="#partitionCustom" class="headerlink" title="partitionCustom"></a>partitionCustom</h4><p>使用自定义分区器在指定的 key 字段上将 DataStream 分区，这个 partitionCustom 有 3 个不同参数的方法，分别要传入的参数有自定义分区 Partitioner 对象、位置、字符和 KeySelector。它们内部也都是调用了私有的 partitionCustom 方法。</p>
<h4 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h4><p>broadcast 是将数据流进行广播，然后让下游的每个并行 Task 中都可以获取到这份数据流，通常这些数据是一些配置，一般这些配置数据的数据量不能太大，否则资源消耗会比较大。这个 broadcast 方法也有两个，一个是无参数，它返回的数据是 DataStream；另一种的参数是 MapStateDescriptor，它返回的参数是 BroadcastStream（这个也会在下文详细介绍）。</p>
<p>使用方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、第一种</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; source = env.addSource(...).broadcast();</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、第二种</span></span><br><span class="line"><span class="keyword">final</span> MapStateDescriptor&lt;Long, String&gt; utterDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">"broadcast-state"</span>, BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO</span><br><span class="line">);</span><br><span class="line"><span class="keyword">final</span> DataStream&lt;String&gt; srcTwo = env.fromCollection(expected.values());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> BroadcastStream&lt;String&gt; broadcast = srcTwo.broadcast(utterDescriptor);</span><br></pre></td></tr></table></figure>
<h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>map 方法需要传入的参数是一个 MapFunction，当然传入 RichMapFunction 也是可以的，它返回的是 SingleOutputStreamOperator（这个类在会在下文详细介绍），该 map 方法里面的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span><span class="params">(MapFunction&lt;T, R&gt; mapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;R&gt; outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(),</span><br><span class="line">            Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//调用 transform 方法</span></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Map"</span>, outType, <span class="keyword">new</span> StreamMap&lt;&gt;(clean(mapper)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法平时使用的非常频繁，然后我们该如何使用这个方法呢：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataStream.map(<span class="keyword">new</span> MapFunction&lt;Integer, String&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>flatMap 方法需要传入一个 FlatMapFunction 参数，当然传入 RichFlatMapFunction 也是可以的，如果你的 Flink Job 里面有连续的 filter 和 map 算子在一起，可以考虑使用 flatMap 一个算子来完成两个算子的工作，它返回的是 SingleOutputStreamOperator，该 flatMap 方法里面的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">flatMap</span><span class="params">(FlatMapFunction&lt;T, R&gt; flatMapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;R&gt; outType = TypeExtractor.getFlatMapReturnTypes(clean(flatMapper),</span><br><span class="line">            getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//调用 transform 方法</span></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Flat Map"</span>, outType, <span class="keyword">new</span> StreamFlatMap&lt;&gt;(clean(flatMapper)));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法平时使用的非常频繁，使用方式如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Integer value, Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        out.collect(value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="process"><a href="#process" class="headerlink" title="process"></a>process</h4><p>在输入流上应用给定的 ProcessFunction，从而创建转换后的输出流，通过该方法返回的是 SingleOutputStreamOperator，具体代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">process</span><span class="params">(ProcessFunction&lt;T, R&gt; processFunction)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;R&gt; outType = TypeExtractor.getUnaryOperatorReturnType(</span><br><span class="line">        processFunction, ProcessFunction.class, <span class="number">0</span>, <span class="number">1</span>,</span><br><span class="line">        TypeExtractor.NO_INDEX, getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">//调用下面的 process 方法</span></span><br><span class="line">    <span class="keyword">return</span> process(processFunction, outType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ProcessFunction&lt;T, R&gt; processFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">        TypeInformation&lt;R&gt; outputType)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    ProcessOperator&lt;T, R&gt; operator = <span class="keyword">new</span> ProcessOperator&lt;&gt;(clean(processFunction));</span><br><span class="line">    <span class="comment">//调用 transform 方法</span></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Process"</span>, outputType, operator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Long&gt; data = env.generateSequence(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义的 ProcessFunction</span></span><br><span class="line">ProcessFunction&lt;Long, Integer&gt; processFunction = <span class="keyword">new</span> ProcessFunction&lt;Long, Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Long value, Context ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">            Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">            Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; processed = data.keyBy(<span class="keyword">new</span> IdentityKeySelector&lt;Long&gt;()).process(processFunction);</span><br></pre></td></tr></table></figure>
<h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p>filter 用来过滤数据的，它需要传入一个 FilterFunction，然后返回的数据也是 SingleOutputStreamOperator，该方法的实现是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">filter</span><span class="params">(FilterFunction&lt;T&gt; filter)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Filter"</span>, getType(), <span class="keyword">new</span> StreamFilter&lt;&gt;(clean(filter)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法平时使用非常多：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; filter1 = src</span><br><span class="line">    .filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"zhisheng"</span>.equals(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<p>上面这些方法是平时写代码时用的非常多的方法，我们这里讲解了它们的实现原理和使用方式，当然还有其他方法，比如 assignTimestampsAndWatermarks、join、shuffle、forward、addSink、rebalance、iterate、coGroup、project、timeWindowAll、countWindowAll、windowAll、print 等，这里由于篇幅的问题就不一一展开来讲了。</p>
<h3 id="SingleOutputStreamOperator-如何使用及分析"><a href="#SingleOutputStreamOperator-如何使用及分析" class="headerlink" title="SingleOutputStreamOperator 如何使用及分析"></a>SingleOutputStreamOperator 如何使用及分析</h3><p>SingleOutputStreamOperator 这个类继承自 DataStream，所以 DataStream 中有的方法在这里也都有，那么这里就讲解下额外的方法的作用，如下。</p>
<ul>
<li>name()：该方法可以设置当前数据流的名称，如果设置了该值，则可以在 Flink UI 上看到该值；uid() 方法可以为算子设置一个指定的 ID，该 ID 有个作用就是如果想从 savepoint 恢复 Job 时是可以根据这个算子的 ID 来恢复到它之前的运行状态；</li>
<li>setParallelism() ：该方法是为每个算子单独设置并行度的，这个设置优先于你通过 env 设置的全局并行度；</li>
<li>setMaxParallelism() ：该为算子设置最大的并行度；</li>
<li>setResources()：该方法有两个（参数不同），设置算子的资源，但是这两个方法对外还没开放（是私有的，暂时功能性还不全）；</li>
<li>forceNonParallel()：该方法强行将并行度和最大并行度都设置为 1；</li>
<li>setChainingStrategy()：该方法对给定的算子设置 ChainingStrategy；</li>
<li>disableChaining()：该这个方法设置后将禁止该算子与其他的算子 chain 在一起；</li>
<li>getSideOutput()：该方法通过给定的 OutputTag 参数从 side output 中来筛选出对应的数据流。</li>
</ul>
<h3 id="KeyedStream-如何使用及分析"><a href="#KeyedStream-如何使用及分析" class="headerlink" title="KeyedStream 如何使用及分析"></a>KeyedStream 如何使用及分析</h3><p>KeyedStream 是 DataStream 在根据 KeySelector 分区后的数据流，DataStream 中常用的方法在 KeyedStream 后也可以用（除了 shuffle、forward 和 keyBy 等分区方法），在该类中的属性分别是 KeySelector 和 TypeInformation。</p>
<p>DataStream 中的窗口方法只有 timeWindowAll、countWindowAll 和 windowAll 这三种全局窗口方法，但是在 KeyedStream 类中的种类就稍微多了些，新增了 timeWindow、countWindow 方法，并且是还支持滑动窗口。</p>
<p>除了窗口方法的新增外，还支持大量的聚合操作方法，比如 reduce、fold、sum、min、max、minBy、maxBy、aggregate 等方法（列举的这几个方法都支持多种参数的）。</p>
<p>最后就是它还有 asQueryableState() 方法，能够将 KeyedStream 发布为可查询的 ValueState 实例。</p>
<h3 id="SplitStream-如何使用及分析"><a href="#SplitStream-如何使用及分析" class="headerlink" title="SplitStream 如何使用及分析"></a>SplitStream 如何使用及分析</h3><p>SplitStream 这个类比较简单，它代表着数据分流后的数据流了，它有一个 select 方法可以选择分流后的哪种数据流了，通常它是结合 split 使用的，对于单次分流来说还挺方便的。但是它是一个被废弃的类（Flink 1.7 后被废弃的，可以看下笔者之前写的一篇文章 <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/" target="_blank" rel="noopener">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ），其实可以用 side output 来代替这种 split，后面文章中我们也会讲通过简单的案例来讲解一下该如何使用 side output 做数据分流操作。</p>
<p>因为这个类的源码比较少，我们可以看下这个类的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStream</span>&lt;<span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">DataStream</span>&lt;<span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">SplitStream</span><span class="params">(DataStream&lt;OUT&gt; dataStream, OutputSelector&lt;OUT&gt; outputSelector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(dataStream.getExecutionEnvironment(), <span class="keyword">new</span> SplitTransformation&lt;OUT&gt;(dataStream.getTransformation(), outputSelector));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//选择要输出哪种数据流</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataStream&lt;OUT&gt; <span class="title">select</span><span class="params">(String... outputNames)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> selectOutput(outputNames);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//上面那个 public 方法内部调用的就是这个方法，该方法是个 private 方法，对外隐藏了它是如何去找到特定的数据流。</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> DataStream&lt;OUT&gt; <span class="title">selectOutput</span><span class="params">(String[] outputNames)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String outName : outputNames) &#123;</span><br><span class="line">            <span class="keyword">if</span> (outName == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Selected names must not be null"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//构造了一个 SelectTransformation 对象</span></span><br><span class="line">        SelectTransformation&lt;OUT&gt; selectTransform = <span class="keyword">new</span> SelectTransformation&lt;OUT&gt;(<span class="keyword">this</span>.getTransformation(), Lists.newArrayList(outputNames));</span><br><span class="line">        <span class="comment">//构造了一个 DataStream 对象</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DataStream&lt;OUT&gt;(<span class="keyword">this</span>.getExecutionEnvironment(), selectTransform);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="WindowedStream-如何使用及分析"><a href="#WindowedStream-如何使用及分析" class="headerlink" title="WindowedStream 如何使用及分析"></a>WindowedStream 如何使用及分析</h3><p>虽然 WindowedStream 不是继承自 DataStream，并且我们在 3.1 节中也做了一定的讲解，但是当时没讲里面的 Function，所以在这里刚好一起做一个补充。</p>
<p>在 WindowedStream 类中定义的属性有 KeyedStream、WindowAssigner、Trigger、Evictor、allowedLateness 和 lateDataOutputTag。</p>
<ul>
<li>KeyedStream：代表着数据流，数据分组后再开 Window</li>
<li>WindowAssigner：Window 的组件之一</li>
<li>Trigger：Window 的组件之一</li>
<li>Evictor：Window 的组件之一（可选）</li>
<li>allowedLateness：用户指定的允许迟到时间长</li>
<li>lateDataOutputTag：数据延迟到达的 Side output，如果延迟数据没有设置任何标记，则会被丢弃</li>
</ul>
<p>在 3.1 节中我们讲了上面的三个窗口组件 WindowAssigner、Trigger、Evictor，并教大家该如何使用，那么在这篇文章我就不再重复，那么接下来就来分析下其他几个的使用方式和其实现原理。</p>
<p>先来看下 allowedLateness 这个它可以在窗口后指定允许迟到的时间长，使用如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>)</span><br><span class="line">    .timeWindow(Time.milliseconds(<span class="number">20</span>))</span><br><span class="line">    .allowedLateness(Time.milliseconds(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>lateDataOutputTag 这个它将延迟到达的数据发送到由给定 OutputTag 标识的 side output（侧输出），当水印经过窗口末尾（并加上了允许的延迟后），数据就被认为是延迟了。</p>
<p>对于 keyed windows 有五个不同参数的 reduce 方法可以使用，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1、参数为 ReduceFunction</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> reduce(function, <span class="keyword">new</span> PassThroughWindowFunction&lt;K, W, T&gt;());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2、参数为 ReduceFunction 和 WindowFunction</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> reduce(reduceFunction, function, resultType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3、参数为 ReduceFunction、WindowFunction 和 TypeInformation</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> input.transform(opName, resultType, operator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4、参数为 ReduceFunction 和 ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> reduce(reduceFunction, function, resultType);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//5、参数为 ReduceFunction、ProcessWindowFunction 和 TypeInformation</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function, TypeInformation&lt;R&gt; resultType)</span> </span>&#123;</span><br><span class="line">    ... </span><br><span class="line">    <span class="keyword">return</span> input.transform(opName, resultType, operator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除了 reduce 方法，还有六个不同参数的 fold 方法、aggregate 方法；两个不同参数的 apply 方法、process 方法（其中你会发现这两个 apply 方法和 process 方法内部其实都隐式的调用了一个私有的 apply 方法）；其实除了前面说的两个不同参数的 apply 方法外，还有四个其他的 apply 方法，这四个方法也是参数不同，但是其实最终的是利用了 transform 方法；还有的就是一些预定义的聚合方法比如 sum、min、minBy、max、maxBy，它们的方法参数的个数不一致，这些预聚合的方法内部调用的其实都是私有的 aggregate 方法，该方法允许你传入一个 AggregationFunction 参数。我们来看一个具体的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//max</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">max</span><span class="params">(String field)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//内部调用私有的的 aggregate 方法</span></span><br><span class="line">    <span class="keyword">return</span> aggregate(<span class="keyword">new</span> ComparableAggregator&lt;&gt;(field, input.getType(), AggregationFunction.AggregationType.MAX, <span class="keyword">false</span>, input.getExecutionConfig()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//私有的 aggregate 方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">aggregate</span><span class="params">(AggregationFunction&lt;T&gt; aggregator)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//继续调用的是 reduce 方法</span></span><br><span class="line">    <span class="keyword">return</span> reduce(aggregator);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//该 reduce 方法内部其实又是调用了其他多个参数的 reduce 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; function)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    function = input.getExecutionEnvironment().clean(function);</span><br><span class="line">    <span class="keyword">return</span> reduce(function, <span class="keyword">new</span> PassThroughWindowFunction&lt;K, W, T&gt;());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的方法调用过程，你会发现代码封装的很深，得需要你自己好好跟一下源码才可以了解更深些。</p>
<p>上面讲了这么多方法，你会发现 reduce 方法其实是用的蛮多的之一，那么就来看看该如何使用：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">0</span>)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span>  </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .print();</span><br></pre></td></tr></table></figure>
<h3 id="AllWindowedStream-如何使用及分析"><a href="#AllWindowedStream-如何使用及分析" class="headerlink" title="AllWindowedStream 如何使用及分析"></a>AllWindowedStream 如何使用及分析</h3><p>前面讲完了 WindowedStream，再来看看这个 AllWindowedStream 你会发现它的实现其实无太大区别，该类中的属性和方法都和前面 WindowedStream 是一样的，然后我们就不再做过多的介绍，直接来看看该如何使用呢？</p>
<p>AllWindowedStream 这种场景下是不需要让数据流做 keyBy 分组操作，直接就进行 windowAll 操作，然后在 windowAll 方法中传入 WindowAssigner 参数对象即可，然后返回的数据结果就是 AllWindowedStream 了，下面使用方式继续执行了 AllWindowedStream 中的 reduce 方法来返回数据：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dataStream.windowAll(SlidingEventTimeWindows.of(Time.of(<span class="number">1</span>, TimeUnit.SECONDS), Time.of(<span class="number">100</span>, TimeUnit.MILLISECONDS)))</span><br><span class="line">    .reduce(<span class="keyword">new</span> RichReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">6448847205314995812L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1,</span></span></span><br><span class="line"><span class="function"><span class="params">                Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h3 id="ConnectedStreams-如何使用及分析"><a href="#ConnectedStreams-如何使用及分析" class="headerlink" title="ConnectedStreams 如何使用及分析"></a>ConnectedStreams 如何使用及分析</h3><p>ConnectedStreams 这个类定义是表示（可能）两个不同数据类型的数据连接流，该场景如果对一个数据流进行操作会直接影响另一个数据流，因此可以通过流连接来共享状态。比较常见的一个例子就是一个数据流（随时间变化的规则数据流）通过连接其他的数据流，这样另一个数据流就可以利用这些连接的规则数据流。</p>
<p>ConnectedStreams 在概念上可以认为和 Union 数据流是一样的。</p>
<p>在 ConnectedStreams 类中有三个属性：environment、inputStream1 和 inputStream2，该类中的方法如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yvb90dej21em0rygnv.jpg" alt="undefined"></p>
<p>在 ConnectedStreams 中可以通过 getFirstInput 获取连接的第一个流、通过 getSecondInput 获取连接的第二个流，同时它还含有六个 keyBy 方法来将连接后的数据流进行分组，这六个 keyBy 方法的参数各有不同。另外它还含有 map、flatMap、process 方法来处理数据（其中 map 和 flatMap 方法的参数分别使用的是 CoMapFunction 和 CoFlatMapFunction），其实如果你细看其方法里面的实现就会发现都是调用的 transform 方法。</p>
<p>上面讲完了 ConnectedStreams 类的基础定义，接下来我们来看下该类如何使用呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));    <span class="comment">//流 1</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Long&gt;&gt; src2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>));    <span class="comment">//流 2</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connected = src1.connect(src2);    <span class="comment">//连接流 1 和流 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的六种 keyBy 方法</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup1 = connected.keyBy(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup2 = connected.keyBy(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>&#125;, <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>&#125;);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup3 = connected.keyBy(<span class="string">"f0"</span>, <span class="string">"f0"</span>);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup4 = connected.keyBy(<span class="keyword">new</span> String[]&#123;<span class="string">"f0"</span>&#125;, <span class="keyword">new</span> String[]&#123;<span class="string">"f0"</span>&#125;);</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(<span class="keyword">new</span> FirstSelector(), <span class="keyword">new</span> FirstSelector());</span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt; connectedGroup5 = connected.keyBy(<span class="keyword">new</span> FirstSelector(), <span class="keyword">new</span> FirstSelector(), Types.STRING);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 map 方法</span></span><br><span class="line">connected.map(<span class="keyword">new</span> CoMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Object&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map1</span><span class="params">(Tuple2&lt;Long, Long&gt; value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map2</span><span class="params">(Tuple2&lt;Long, Long&gt; value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 flatMap 方法</span></span><br><span class="line">connected.flatMap(<span class="keyword">new</span> CoFlatMapFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap1</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap2</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;).name(<span class="string">"testCoFlatMap"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用连接流的 process 方法</span></span><br><span class="line">connected.process(<span class="keyword">new</span> CoProcessFunction&lt;Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;, Tuple2&lt;Long, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.f0 &lt; <span class="number">3</span>) &#123;</span><br><span class="line">            out.collect(value);</span><br><span class="line">            ctx.output(sideOutputTag, <span class="string">"sideout1-"</span> + String.valueOf(value));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(Tuple2&lt;Long, Long&gt; value, Context ctx, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.f0 &gt;= <span class="number">3</span>) &#123;</span><br><span class="line">            out.collect(value);</span><br><span class="line">            ctx.output(sideOutputTag, <span class="string">"sideout2-"</span> + String.valueOf(value));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h3 id="BroadcastStream-如何使用及分析"><a href="#BroadcastStream-如何使用及分析" class="headerlink" title="BroadcastStream 如何使用及分析"></a>BroadcastStream 如何使用及分析</h3><p>BroadcastStream 这个类定义是表示 broadcast state（广播状态）组成的数据流。通常这个 BroadcastStream 数据流是通过调用 DataStream 中的 broadcast 方法才返回的，注意 BroadcastStream 后面不能使用算子去操作这些流，唯一可以做的就是使用 KeyedStream/DataStream 的 connect 方法去连接 BroadcastStream，连接之后的话就会返回一个 BroadcastConnectedStream 数据流。</p>
<p>在 BroadcastStream 中我们该如何使用呢？通常是在 DataStream 中使用 broadcast 方法，该方法需要传入一个 MapStateDescriptor 对象，可以看下该方法的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> BroadcastStream&lt;T&gt; <span class="title">broadcast</span><span class="params">(<span class="keyword">final</span> MapStateDescriptor&lt;?, ?&gt;... broadcastStateDescriptors)</span> </span>&#123;</span><br><span class="line">    Preconditions.checkNotNull(broadcastStateDescriptors);  <span class="comment">//检查是否为空</span></span><br><span class="line">    <span class="keyword">final</span> DataStream&lt;T&gt; broadcastStream = setConnectionType(<span class="keyword">new</span> BroadcastPartitioner&lt;&gt;());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BroadcastStream&lt;&gt;(environment, broadcastStream, broadcastStateDescriptors);  <span class="comment">//构建 BroadcastStream 对象，传入 env 环境、broadcastStream 和 broadcastStateDescriptors</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面方法传入的参数 broadcastStateDescriptors，我们可以像下面这样去定义一个 MapStateDescriptor 对象：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> MapStateDescriptor&lt;Long, String&gt; utterDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">    <span class="string">"broadcast-state"</span>, BasicTypeInfo.LONG_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<h3 id="BroadcastConnectedStream-如何使用及分析"><a href="#BroadcastConnectedStream-如何使用及分析" class="headerlink" title="BroadcastConnectedStream 如何使用及分析"></a>BroadcastConnectedStream 如何使用及分析</h3><p>BroadcastConnectedStream 这个类定义是表示 keyed 或者 non-keyed 数据流和 BroadcastStream 数据流进行连接后组成的数据流。比如在 DataStream 中执行 connect 方法就可以连接两个数据流了，那么在 DataStream 中 connect 方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">BroadcastConnectedStream&lt;T, R&gt; <span class="title">connect</span><span class="params">(BroadcastStream&lt;R&gt; broadcastStream)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BroadcastConnectedStream&lt;&gt;( <span class="comment">//构造 BroadcastConnectedStream 对象</span></span><br><span class="line">            environment,</span><br><span class="line">            <span class="keyword">this</span>,</span><br><span class="line">            Preconditions.checkNotNull(broadcastStream),</span><br><span class="line">            broadcastStream.getBroadcastStateDescriptor());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个 BroadcastConnectedStream 类中主要的方法有：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0yvm1xntj21hq0hujsj.jpg" alt="undefined"></p>
<p>从图中可以看到四个 process 方法和一个 transform 私有方法，其中四个 process 方法也是参数不同，最后实际调用的方法就是这个私有的 transform 方法。</p>
<h3 id="QueryableStateStream-如何使用及分析"><a href="#QueryableStateStream-如何使用及分析" class="headerlink" title="QueryableStateStream 如何使用及分析"></a>QueryableStateStream 如何使用及分析</h3><p>QueryableStateStream 该类代表着可查询的状态流。该类的定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryableStateStream</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//要查询的状态名称</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String queryableStateName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//状态的 Key 序列化器</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TypeSerializer&lt;K&gt; keySerializer;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//状态的 descriptor </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> StateDescriptor&lt;?, V&gt; stateDescriptor;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造器</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">QueryableStateStream</span><span class="params">(String queryableStateName, StateDescriptor&lt;?, V&gt; stateDescriptor, TypeSerializer&lt;K&gt; keySerializer)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//返回可以查询状态的名称</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getQueryableStateName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> queryableStateName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//返回 key 序列化器</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeSerializer&lt;K&gt; <span class="title">getKeySerializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> keySerializer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//返回状态的 descriptor </span></span><br><span class="line">    <span class="keyword">public</span> StateDescriptor&lt;?, V&gt; getStateDescriptor() &#123;</span><br><span class="line">        <span class="keyword">return</span> stateDescriptor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 KeyedStream 你可以通过 asQueryableState() 方法返回一个 QueryableStateStream 数据流，这个方法可以通过传入不同的参数来实现，主要的参数就是 queryableStateName 和 StateDescriptor（这个参数你可以传入 ValueStateDescriptor、FoldingStateDescriptor 和 ReducingStateDescriptor 三种）。</p>
<p>具体如何使用呢，我们来看个 demo：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ValueStateDescriptor&lt;Tuple2&lt;Integer, Long&gt;&gt; valueState = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">    <span class="string">"any"</span>, source.getType(),    <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">QueryableStateStream&lt;Integer, Tuple2&lt;Integer, Long&gt;&gt; queryableState =</span><br><span class="line">    source.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Integer, Long&gt;, Integer&gt;() &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">7480503339992214681L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Tuple2&lt;Integer, Long&gt; value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).asQueryableState(<span class="string">"zhisheng"</span>, valueState);</span><br></pre></td></tr></table></figure>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本节算是对 Flink DataStream 包下的所有常用的 Stream 做了个讲解，不仅从使用方式来介绍这些 Stream API 该如何使用，而且还给出了部分 demo，此外还剖析了部分 Stream 的代码结构及其内部部分方法的源码实现，从而能够让大家不仅仅是从表面上去使用这些 DataStream API，还能够对它们的实现原理有了解，这样就可以做到活学活用，并且还可以自己去做扩展。</p>
<h2 id="十二、Flink-WaterMark-详解及结合-WaterMark-处理延迟数据"><a href="#十二、Flink-WaterMark-详解及结合-WaterMark-处理延迟数据" class="headerlink" title="十二、Flink WaterMark 详解及结合 WaterMark 处理延迟数据"></a>十二、Flink WaterMark 详解及结合 WaterMark 处理延迟数据</h2><p>在 3.1 节中讲解了 Flink 中的三种 Time 和其对应的使用场景，然后在 3.2 节中深入的讲解了 Flink 中窗口的机制以及 Flink 中自带的 Window 的实现原理和使用方法。如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。</p>
<p>下图表示选择 Event Time 与 Process Time 的实际效果图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zf4f9lvj217e0om74r.jpg" alt="undefined"></p>
<p>在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实却仍然这么骨感，会因为各种各样的问题（网络的抖动、设备的故障、应用的异常等原因）从而导致如图中曲线一样，Process Time 总是会与 Event Time 有一些延迟。所谓乱序，其实是指 Flink 接收到的事件的先后顺序并不是严格的按照事件的 Event Time 顺序排列的。比如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zfc2xczj20s20hsaaj.jpg" alt="undefined"></p>
<p>然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如：</p>
<ul>
<li>错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</li>
<li>设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</li>
</ul>
<p>这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。庆幸的是 Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。</p>
<h3 id="Watermark-是什么？"><a href="#Watermark-是什么？" class="headerlink" title="Watermark 是什么？"></a>Watermark 是什么？</h3><p>举个例子：</p>
<p>统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。</p>
<p>Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制（下文会讲）去处理。</p>
<p>下面通过几个图来了解一下 Watermark 是如何工作的！</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zfu71ysj21e20p0dgv.jpg" alt="undefined"></p>
<p>上图中的数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，<code>W(4)</code> 和 <code>W(9)</code> 是水印。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqks7btj21ik0pqdgr.jpg" alt="undefined"></p>
<p>经过 Flink 的消费，数据 <code>1</code>、<code>3</code>、<code>2</code> 进入了第一个窗口，然后 <code>7</code> 会进入第二个窗口，接着 <code>3</code> 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 <code>4</code> 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqrrk6kj21bi0nkgmc.jpg" alt="undefined"></p>
<p>那么接着后面的数据 <code>5</code> 和 <code>6</code> 会进入到第二个窗口里面，数据 <code>9</code> 会进入在第三个窗口里面。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zqyn66nj21cm0lsq46.jpg" alt="undefined"></p>
<p>那么当遇到水印 <code>9</code> 时，发现水印比第二个窗口的结束时间 <code>8</code> 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去。</p>
<p>相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。</p>
<h3 id="Flink-中-Watermark-的设置"><a href="#Flink-中-Watermark-的设置" class="headerlink" title="Flink 中 Watermark 的设置"></a>Flink 中 Watermark 的设置</h3><p>在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPunctuatedWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPunctuatedWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPunctuatedWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPunctuatedWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以设置 Watermark 是有如下两种方式：</p>
<ul>
<li>AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。</li>
</ul>
<p>在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。</p>
<ul>
<li>AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</li>
</ul>
<p>在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。</p>
<p>下面再分别详细讲下这两种的实现方式。</p>
<h3 id="Punctuated-Watermark"><a href="#Punctuated-Watermark" class="headerlink" title="Punctuated Watermark"></a>Punctuated Watermark</h3><p>AssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。</p>
<p>那么该怎么利用这个来定义水印生成器呢？</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPunctuatedWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(Word lastElement, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> extractedTimestamp % <span class="number">3</span> == <span class="number">0</span> ? <span class="keyword">new</span> Watermark(extractedTimestamp) : <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。</p>
<h3 id="3-5-4-Periodic-Watermark"><a href="#3-5-4-Periodic-Watermark" class="headerlink" title="3.5.4 Periodic Watermark"></a>3.5.4 Periodic Watermark</h3><p>通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentTimestamp = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word word, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (word.getTimestamp() &gt; currentTimestamp) &#123;</span><br><span class="line">            <span class="keyword">this</span>.currentTimestamp = word.getTimestamp();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> currentTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> maxTimeLag = <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，该方法会返回当前时间戳与事件时间进行比较，如果事件的时间戳比 currentTimestamp 大的话，那么就将当前事件的时间戳赋值给 currentTimestamp。getCurrentWatermark() 方法是获取当前的水位线，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 <code>long maxTimeLag = 5000;</code> 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。</p>
<p>通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。</p>
<p>AssignerWithPeriodicWatermarks 这个接口有四个实现类，分别如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zrpl1ivj21xk0i674j.jpg" alt="undefined"></p>
<p>BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zrx0blbj21hs1gsaci.jpg" alt="undefined"></p>
<p>你可以像下面一样使用该类来分配时间和生成水印：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Time.seconds(10) 代表允许延迟的时间大小</span></span><br><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">    <span class="comment">//重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。</li>
<li>AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> elementPrevTimestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = extractAscendingTimestamp(element);</span><br><span class="line">    <span class="keyword">if</span> (newTimestamp &gt;= <span class="keyword">this</span>.currentTimestamp) &#123;</span><br><span class="line">        <span class="keyword">this</span>.currentTimestamp = ne∏wTimestamp;</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        violationHandler.handleViolation(newTimestamp, <span class="keyword">this</span>.currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 <code>System.currentTimeMillis()</code> 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。</li>
</ul>
<p><strong>注意</strong>：</p>
<p>1、使用这种方式周期性生成水印的话，你可以通过 <code>env.getConfig().setAutoWatermarkInterval(...);</code> 来设置生成水印的间隔（每隔 n 毫秒）。</p>
<p>2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;MyType&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="comment">/* condition */</span>) &#123;</span><br><span class="line">        MyType next = getNext();</span><br><span class="line">        ctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class="line">            ctx.emitWatermark(<span class="keyword">new</span> Watermark(next.getWatermarkTime()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="每个-Kafka-分区的时间戳"><a href="#每个-Kafka-分区的时间戳" class="headerlink" title="每个 Kafka 分区的时间戳"></a>每个 Kafka 分区的时间戳</h3><p>当以 Kafka 来作为数据源的时候，通常每个 Kafka 分区的数据时间戳是递增的（事件是有序的），但是当你作业设置多个并行度的时候，Flink 去消费 Kafka 数据流是并行的，那么并行的去消费 Kafka 分区的数据就会导致打乱原每个分区的数据时间戳的顺序。在这种情况下，你可以使用 Flink 中的 <code>Kafka-partition-aware</code> 特性来生成水印，使用该特性后，水印会在 Kafka 消费端生成，然后每个 Kafka 分区和每个分区上的水印最后的合并方式和水印在数据流 shuffle 过程中的合并方式一致。</p>
<p>如果事件时间戳严格按照每个 Kafka 分区升序，则可以使用前面提到的 AscendingTimestampExtractor 水印生成器来为每个分区生成水印。下面代码教大家如何使用 <code>per-Kafka-partition</code> 来生成水印。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FlinkKafkaConsumer011&lt;Event&gt; kafkaSource = <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">"zhisheng"</span>, schema, props);</span><br><span class="line">kafkaSource.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Event&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.eventTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; stream = env.addSource(kafkaSource);</span><br></pre></td></tr></table></figure>
<p>下图表示水印在 Kafka 分区后如何通过流数据流传播：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga0zs9dj5oj20r10a8t9f.jpg" alt="undefined"></p>
<p>其实在上文中已经提到的一点是在设置 Periodic Watermark 时，是允许提供一个参数，表示数据最大的延迟时间。其实这个值要结合自己的业务以及数据的情况来设置，如果该值设置的太小会导致数据因为网络或者其他的原因从而导致乱序或者延迟的数据太多，那么最后窗口触发的时候，可能窗口里面的数据量很少，那么这样计算的结果很可能误差会很大，对于有的场景（要求正确性比较高）是不太符合需求的。但是如果该值设置的太大，那么就会导致很多窗口一直在等待延迟的数据，从而一直不触发，这样首先就会导致数据的实时性降低，另外将这么多窗口的数据存在内存中，也会增加作业的内存消耗，从而可能会导致作业发生 OOM 的问题。</p>
<p>综上建议：</p>
<ul>
<li>合理设置允许数据最大延迟时间</li>
<li>不太依赖事件时间的场景就不要设置时间策略为 EventTime</li>
</ul>
<h3 id="延迟数据该如何处理-三种方法"><a href="#延迟数据该如何处理-三种方法" class="headerlink" title="延迟数据该如何处理(三种方法)"></a>延迟数据该如何处理(三种方法)</h3><h4 id="丢弃（默认）"><a href="#丢弃（默认）" class="headerlink" title="丢弃（默认）"></a>丢弃（默认）</h4><p>在 Flink 中，对这么延迟数据的默认处理方式是丢弃。</p>
<h4 id="allowedLateness-再次指定允许数据延迟的时间"><a href="#allowedLateness-再次指定允许数据延迟的时间" class="headerlink" title="allowedLateness 再次指定允许数据延迟的时间"></a>allowedLateness 再次指定允许数据延迟的时间</h4><p>allowedLateness 表示允许数据延迟的时间，这个方法是在 WindowedStream 中的，用来设置允许窗口数据延迟的时间，超过这个时间的元素就会被丢弃，这个的默认值是 0，该设置仅针对于以事件时间开的窗口，它的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">allowedLateness</span><span class="params">(Time lateness)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> millis = lateness.toMilliseconds();</span><br><span class="line">    checkArgument(millis &gt;= <span class="number">0</span>, <span class="string">"The allowed lateness cannot be negative."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.allowedLateness = millis;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之前有多个小伙伴问过我 Watermark 中允许的数据延迟和这个数据延迟的区别是啥？我的回复是该允许延迟的时间是在 Watermark 允许延迟的基础上增加的时间。那么具体该如何使用 allowedLateness 呢。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">    .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">    .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .allowedLateness(Time.milliseconds(<span class="number">2</span>))  <span class="comment">//表示允许再次延迟 2 毫秒</span></span><br><span class="line">    .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">        <span class="comment">//计算逻辑</span></span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h4 id="sideOutputLateData-收集迟到的数据"><a href="#sideOutputLateData-收集迟到的数据" class="headerlink" title="sideOutputLateData 收集迟到的数据"></a>sideOutputLateData 收集迟到的数据</h4><p>sideOutputLateData 这个方法同样是 WindowedStream 中的方法，该方法会将延迟的数据发送到给定 OutputTag 的 side output 中去，然后你可以通过 <code>SingleOutputStreamOperator.getSideOutput(OutputTag)</code> 来获取这些延迟的数据。具体的操作方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义 OutputTag</span></span><br><span class="line">OutputTag&lt;Integer&gt; lateDataTag = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"late"</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; windowOperator = dataStream</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">        .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">        .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">        .allowedLateness(Time.milliseconds(<span class="number">2</span>))</span><br><span class="line">        .sideOutputLateData(lateDataTag)    <span class="comment">//指定 OutputTag</span></span><br><span class="line">        .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="comment">//计算逻辑</span></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">windowOperator.addSink(resultSink);</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定的 OutputTag 从 Side Output 中获取到延迟的数据之后，你可以通过 addSink() 方法存储下来，这样可以方便你后面去排查哪些数据是延迟的。</span></span><br><span class="line">windowOperator.getSideOutput(lateDataTag)</span><br><span class="line">        .addSink(lateResultSink);</span><br></pre></td></tr></table></figure>
<h3 id="小结与反思-10"><a href="#小结与反思-10" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Watermark 的概念，并讲解了 Flink 中自带的 Watermark，然后还教大家如何设置 Watermark 以及如何自定义 Watermark，最后通过结合 Window 与 Watermark 去处理延迟数据，还讲解了三种常见的处理延迟数据的方法。</p>
<p>关于 Watermark 你有遇到什么问题吗？对于延迟数据你通常是怎么处理的？</p>
<p>本节相关的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/watermark" target="_blank" rel="noopener">Watermark</a></p>
<h2 id="十三、Flink-常用的-Source-和-Sink-Connectors-介绍"><a href="#十三、Flink-常用的-Source-和-Sink-Connectors-介绍" class="headerlink" title="十三、Flink 常用的 Source 和 Sink Connectors 介绍"></a>十三、Flink 常用的 Source 和 Sink Connectors 介绍</h2><p>通过前面我们可以知道 Flink Job 的大致结构就是 <code>Source ——&gt; Transformation ——&gt; Sink</code>。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1164fu1kj21ri0dcmxj.jpg" alt="undefined"></p>
<p>那么这个 Source 是什么意思呢？我们下面来看看。</p>
<h3 id="Data-Source-介绍"><a href="#Data-Source-介绍" class="headerlink" title="Data Source 介绍"></a>Data Source 介绍</h3><p>Data Source 是什么呢？就字面意思其实就可以知道：数据来源。</p>
<p>Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即处理实时的数据流（做计算操作），然后将处理后的数据实时下发，只要数据源源不断过来，Flink 就能够一直计算下去。</p>
<p>Flink 中你可以使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 来为你的程序添加数据来源。</p>
<p>Flink 已经提供了若干实现好了的 source function，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。</p>
<p>那么常用的 Data Source 有哪些呢？</p>
<h3 id="常用的-Data-Source"><a href="#常用的-Data-Source" class="headerlink" title="常用的 Data Source"></a>常用的 Data Source</h3><p>StreamExecutionEnvironment 中可以使用以下这些已实现的 stream source。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga116diy51j21ji16aads.jpg" alt="undefined"></p>
<p>总的来说可以分为下面几大类：</p>
<h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><ol>
<li>fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。</li>
<li>fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。</li>
<li>fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; input = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"barfoo"</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"start"</span>, <span class="number">2.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"foobar"</span>, <span class="number">3.0</span>),</span><br><span class="line">    ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<ol>
<li>fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。</li>
<li>generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。</li>
</ol>
<h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text = env.readTextFile(<span class="string">"file:///path/to/file"</span>);</span><br></pre></td></tr></table></figure>
<p>2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。</p>
<p>3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS<em>CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS</em>ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class="line">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">100</span>,</span><br><span class="line">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br></pre></td></tr></table></figure>
<p><strong>实现:</strong></p>
<p>在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。</p>
<p><strong>重要注意：</strong></p>
<p>如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。</p>
<p>如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。</p>
<h4 id="基于-Socket"><a href="#基于-Socket" class="headerlink" title="基于 Socket"></a>基于 Socket</h4><p>socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">        .socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>) <span class="comment">// 监听 localhost 的 9999 端口过来的数据</span></span><br><span class="line">        .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<h4 id="自定义"><a href="#自定义" class="headerlink" title="自定义"></a>自定义</h4><p>addSource - 添加一个新的 source function。例如，你可以用 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 从 Apache Kafka 读取数据。</p>
<p><strong>说说上面几种的特点</strong></p>
<ol>
<li>基于集合：有界数据集，更偏向于本地测试用</li>
<li>基于文件：适合监听文件修改并读取其内容</li>
<li>基于 Socket：监听主机的 host port，从 Socket 中获取数据</li>
<li>自定义 addSource：大多数的场景数据都是无界的，会源源不断过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;KafkaEvent&gt; input = env</span><br><span class="line">        .addSource(</span><br><span class="line">            <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.getRequired(<span class="string">"input-topic"</span>), <span class="comment">//从参数中获取传进来的 topic </span></span><br><span class="line">                <span class="keyword">new</span> KafkaEventSchema(),</span><br><span class="line">                parameterTool.getProperties())</span><br><span class="line">            .assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkExtractor()));</span><br></pre></td></tr></table></figure>
<p>Flink 目前支持如下面常见的 Source：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga117rgkt8j21kw0yeakr.jpg" alt="undefined"></p>
<p>如果你想自定义自己的 Source 呢？在后面 3.8 节会讲解。</p>
<h3 id="Data-Sink-介绍"><a href="#Data-Sink-介绍" class="headerlink" title="Data Sink 介绍"></a>Data Sink 介绍</h3><p>首先 Sink 的意思是：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1181fv12j20w20ri42r.jpg" alt="undefined"></p>
<p>大概可以猜到了吧！Data sink 有点把数据存储下来（落库）的意思。Flink 在拿到数据后做一系列的计算后，最后要将计算的结果往下游发送。比如将数据存储到 MySQL、ElasticSearch、Cassandra，或者继续发往 Kafka、 RabbitMQ 等消息队列，更或者直接调用其他的第三方应用服务（比如告警）。</p>
<h3 id="常用的-Data-Sink"><a href="#常用的-Data-Sink" class="headerlink" title="常用的 Data Sink"></a>常用的 Data Sink</h3><p>上面介绍了 Flink Data Source 有哪些，这里也看看 Flink Data Sink 支持的有哪些。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga118a05luj21kw0xgtk0.jpg" alt="undefined"></p>
<p>再看下源码有哪些呢？</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga118gxfh0j22tc0mijrw.jpg" alt="undefined"></p>
<p>可以看到有 Kafka、ElasticSearch、Socket、RabbitMQ、JDBC、Cassandra POJO、File、Print 等 Sink 的方式。</p>
<p>可能自带的这些 Sink 不支持你的业务场景，那么你也可以自定义符合自己公司业务需求的 Sink，同样在后面 3.8 节将教会大家。</p>
<h3 id="小结与反思-11"><a href="#小结与反思-11" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中常用的 Connector，包括 Source 和 Sink 的，其中每种都有很多 Connector，大家可以根据实际场景去使用合适的 Connector。</p>
<h2 id="十四、Flink-Connector-Kafka-使用和剖析"><a href="#十四、Flink-Connector-Kafka-使用和剖析" class="headerlink" title="十四、Flink Connector Kafka 使用和剖析"></a>十四、Flink Connector Kafka 使用和剖析</h2><p>在前面 3.6 节中介绍了 Flink 中的 Data Source 和 Data Sink，然后还讲诉了自带的一些 Source 和 Sink 的 Connector。本篇文章将讲解一下用的最多的 Connector —— Kafka，带大家利用 Kafka Connector 读取 Kafka 数据，做一些计算操作后然后又通过 Kafka Connector 写入到 kafka 消息队列去。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11hb4lnmj229g0l43z0.jpg" alt="undefined"></p>
<h3 id="准备环境和依赖"><a href="#准备环境和依赖" class="headerlink" title="准备环境和依赖"></a>准备环境和依赖</h3><h4 id="环境安装和启动"><a href="#环境安装和启动" class="headerlink" title="环境安装和启动"></a>环境安装和启动</h4><p>如果你已经安装好了 Flink 和 Kafka，那么接下来使用命令运行启动 Flink、Zookepeer、Kafka 就行了。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11hipsabj218s0kq40p.jpg" alt="undefined"></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11ho1mg2j21di090754.jpg" alt="undefined"></p>
<p>执行命令都启动好了后就可以添加依赖了。</p>
<h4 id="添加-maven-依赖"><a href="#添加-maven-依赖" class="headerlink" title="添加 maven 依赖"></a>添加 maven 依赖</h4><p>Flink 里面支持 Kafka 0.8.x 以上的版本，具体采用哪个版本的 Maven 依赖需要根据安装的 Kafka 版本来确定。因为之前我们安装的 Kafka 是 1.1.0 版本，所以这里我们选择的 Kafka Connector 为 <code>flink-connector-kafka-0.11_2.11</code> （支持 Kafka 0.11.x 版本及以上，该 Connector 支持 Kafka 事务消息传递，所以能保证 Exactly Once）。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Flink、Kafka、Flink Kafka Connector 三者对应的版本可以根据 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html" target="_blank" rel="noopener">官网</a> 的对比来选择。需要注意的是 <code>flink-connector-kafka_2.11</code> 这个版本支持的 Kafka 版本要大于 1.0.0，从 Flink 1.9 版本开始，它使用的是 Kafka 2.2.0 版本的客户端，虽然这些客户端会做向后兼容，但是建议还是按照官网约定的来规范使用 Connector 版本。另外你还要添加的依赖有：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--flink java--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--log--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--alibaba fastjson--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.51<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="测试数据发到-Kafka-Topic"><a href="#测试数据发到-Kafka-Topic" class="headerlink" title="测试数据发到 Kafka Topic"></a>测试数据发到 Kafka Topic</h3><p>实体类，Metric.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Metric</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, Object&gt; fields;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, String&gt; tags;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>往 kafka 中写数据工具类：KafkaUtils.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"metric"</span>;  <span class="comment">// kafka topic，Flink 程序中需要和这个统一 </span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//key 序列化</span></span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//value 序列化</span></span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        Metric metric = <span class="keyword">new</span> Metric();</span><br><span class="line">        metric.setTimestamp(System.currentTimeMillis());</span><br><span class="line">        metric.setName(<span class="string">"mem"</span>);</span><br><span class="line">        Map&lt;String, String&gt; tags = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;String, Object&gt; fields = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        tags.put(<span class="string">"cluster"</span>, <span class="string">"zhisheng"</span>);</span><br><span class="line">        tags.put(<span class="string">"host_ip"</span>, <span class="string">"101.147.022.106"</span>);</span><br><span class="line"></span><br><span class="line">        fields.put(<span class="string">"used_percent"</span>, <span class="number">90</span>d);</span><br><span class="line">        fields.put(<span class="string">"max"</span>, <span class="number">27244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"used"</span>, <span class="number">17244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"init"</span>, <span class="number">27244873</span>d);</span><br><span class="line"></span><br><span class="line">        metric.setTags(tags);</span><br><span class="line">        metric.setFields(fields);</span><br><span class="line"></span><br><span class="line">        ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(metric));</span><br><span class="line">        producer.send(record);</span><br><span class="line">        System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(metric));</span><br><span class="line"></span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">300</span>);</span><br><span class="line">            writeToKafka();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11id4k1dj22iy166q6l.jpg" alt="undefined"></p>
<p>如果出现如上图标记的，即代表能够不断往 kafka 发送数据的。</p>
<h3 id="Flink-如何消费-Kafka-数据？"><a href="#Flink-如何消费-Kafka-数据？" class="headerlink" title="Flink 如何消费 Kafka 数据？"></a>Flink 如何消费 Kafka 数据？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);  <span class="comment">//key 反序列化</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>); <span class="comment">//value 反序列化</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"metric"</span>,  <span class="comment">//kafka topic</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),  <span class="comment">// String 序列化</span></span><br><span class="line">                props)).setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        dataStreamSource.print(); <span class="comment">//把从 kafka 读取到的数据打印在控制台</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data source"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行起来：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11j9hshgj22gq1acte2.jpg" alt="undefined"></p>
<p>看到没程序，Flink 程序控制台能够源源不断地打印数据呢。</p>
<h4 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h4><p>使用 FlinkKafkaConsumer011 时传入了三个参数。</p>
<ul>
<li>Kafka topic：这个代表了 Flink 要消费的是 Kafka 哪个 Topic，如果你要同时消费多个 Topic 的话，那么你可以传入一个 Topic List 进去，另外也支持正则表达式匹配 Topic</li>
<li>序列化：上面代码我们使用的是 SimpleStringSchema</li>
<li>配置属性：将 Kafka 等的一些配置传入</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11jkx2dlj21po15y76t.jpg" alt="undefined"></p>
<p>前面演示了 Flink 如何消费 Kafak 数据，接下来演示如何把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。</p>
<h3 id="Flink-如何将计算后的数据发到-Kafka？"><a href="#Flink-如何将计算后的数据发到-Kafka？" class="headerlink" title="Flink 如何将计算后的数据发到 Kafka？"></a>Flink 如何将计算后的数据发到 Kafka？</h3><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//其他 Kafka 集群配置</span><br><span class="line">kafka.brokers=xxx:9092,xxx:9092,xxx:9092</span><br><span class="line">kafka.group.id=metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=xxx:2181</span><br><span class="line">metrics.topic=xxx</span><br><span class="line">stream.parallelism=5</span><br><span class="line">kafka.sink.brokers=localhost:9092</span><br><span class="line">kafka.sink.topic=metric-test</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure>
<p>目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11jv4exfj22660qm470.jpg" alt="undefined"></p>
<h4 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h4><p>Main.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        data.addSink(<span class="keyword">new</span> FlinkKafkaProducer011&lt;Metrics&gt;(</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.brokers"</span>),</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.topic"</span>),</span><br><span class="line">                <span class="keyword">new</span> MetricSchema()</span><br><span class="line">                )).name(<span class="string">"flink-connectors-kafka"</span>)</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"stream.sink.parallelism"</span>));</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h4><p>启动程序，查看运行结果，不段执行上面命令，查看是否有新的 topic 出来：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11k5j2xkj21i90u0alj.jpg" alt="undefined"></p>
<p>执行命令可以查看该 topic 的信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kelpl3j221a0lyguc.jpg" alt="undefined"></p>
<p>前面代码使用的 FlinkKafkaProducer011 只传了三个参数：brokerList、topicId、serializationSchema（序列化），其实是支持传入多个参数的。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kmeontj22ie14oad2.jpg" alt="undefined"></p>
<h3 id="FlinkKafkaConsumer-源码剖析"><a href="#FlinkKafkaConsumer-源码剖析" class="headerlink" title="FlinkKafkaConsumer 源码剖析"></a>FlinkKafkaConsumer 源码剖析</h3><p>FlinkKafkaConsumer 的继承关系如下图所示。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11kuirzfj21ha0lk74e.jpg" alt="undefined"></p>
<p>可以发现几个版本的 FlinkKafkaConsumer 都继承自 FlinkKafkaConsumerBase 抽象类，所以可知 FlinkKafkaConsumerBase 是最核心的类了。FlinkKafkaConsumerBase 实现了 CheckpointedFunction、CheckpointListener 接口，继承了 RichParallelSourceFunction 抽象类来读取 Kafka 数据。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11l1xtxpj21gw0cimx4.jpg" alt="undefined"></p>
<p>在 FlinkKafkaConsumerBase 中的 open 方法中做了大量的配置初始化工作，然后在 run 方法里面是由 AbstractFetcher 来获取数据的，在 AbstractFetcher 中有用 List&gt; 来存储着所有订阅分区的状态信息，包括了下面这些字段：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> KafkaTopicPartition partition;    <span class="comment">//分区</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> KPH kafkaPartitionHandle;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> offset;   <span class="comment">//消费到的 offset</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> committedOffset;  <span class="comment">//提交的 offset</span></span><br></pre></td></tr></table></figure>
<p>在 FlinkKafkaConsumerBase 中还有字段定义 Flink 自动发现 Kafka 主题和分区个数的时间，默认是不开启的（时间为 Long.MIN_VALUE），像如果传入的是正则表达式参数，那么动态的发现主题还是有意义的，如果配置的已经是固定的 Topic，那么完全就没有开启这个的必要，另外就是 Kafka 的分区个数的自动发现，像高峰流量的时期，如果 Kafka 的分区扩容了，但是在 Flink 这边没有配置这个参数那就会导致 Kafka 新分区中的数据不会被消费到，这个参数由 <code>flink.partition-discovery.interval-millis</code> 控制。</p>
<h3 id="FlinkKafkaProducer-源码剖析"><a href="#FlinkKafkaProducer-源码剖析" class="headerlink" title="FlinkKafkaProducer 源码剖析"></a>FlinkKafkaProducer 源码剖析</h3><p>FlinkKafkaProducer 这个有些特殊，不同版本的类结构有些不一样，如 FlinkKafkaProducer011 是继承的 TwoPhaseCommitSinkFunction 抽象类，而 FlinkKafkaProducer010 和 FlinkKafkaProducer09 是基于 FlinkKafkaProducerBase 类来实现的。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11la4kjqj217m0g0gln.jpg" alt="undefined"></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga11lhi3j5j21as0jq0su.jpg" alt="undefined"></p>
<p>在 Kafka 0.11.x 版本后支持了事务，这让 Flink 与 Kafka 的事务相结合从而实现端到端的 Exactly once 才有了可能，在 9.5 节中会详细讲解如何利用 TwoPhaseCommitSinkFunction 来实现 Exactly once 的。</p>
<p>数据 Sink 到下游的 Kafka，可你能会关心数据的分区策略，在 Flink 中自带了一种就是 FlinkFixedPartitioner，它使用的是 round-robin 策略进行下发到下游 Kafka Topic 的分区上的，当然也提供了 FlinkKafkaPartitioner 接口供你去实现自定义的分区策略。</p>
<h3 id="使用-Flink-connector-kafka-可能会遇到的问题"><a href="#使用-Flink-connector-kafka-可能会遇到的问题" class="headerlink" title="使用 Flink-connector-kafka 可能会遇到的问题"></a>使用 Flink-connector-kafka 可能会遇到的问题</h3><h4 id="如何消费多个-Kafka-Topic"><a href="#如何消费多个-Kafka-Topic" class="headerlink" title="如何消费多个 Kafka Topic"></a>如何消费多个 Kafka Topic</h4><p>通常可能会有很多类型的数据全部发到 Kafka，但是发送的数据却不是在同一个 Topic 里面，然后在 Flink 处消费的时候，又要去同时消费这些多个 Topic，在 Flink 中除了支持可以消费单个 Topic 的数据，还支持传入多个 Topic，另外还支持 Topic 的正则表达式（因为有时候可能会事先不确定到底会有多少个 Topic，所以使用正则来处理会比较好，只要在 Kafka 建立的 Topic 名是有规律的就行），如下几种构造器可以传入不同参数来创建 FlinkKafkaConsumer 对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//多个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//正则表达式 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="想要获取数据的元数据信息"><a href="#想要获取数据的元数据信息" class="headerlink" title="想要获取数据的元数据信息"></a>想要获取数据的元数据信息</h4><p>在消费 Kafka 数据的时候，有时候想获取到数据是从哪个 Topic、哪个分区里面过来的，这条数据的 offset 值是多少。这些元数据信息在有的场景真的需要，那么这种场景下该如何获取呢？其实在获取数据进行反序列化的时候使用 KafkaDeserializationSchema 就行。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KafkaDeserializationSchema</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span>, <span class="title">ResultTypeQueryable</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isEndOfStream</span><span class="params">(T nextElement)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">T <span class="title">deserialize</span><span class="params">(ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 KafkaDeserializationSchema 接口中的 deserialize 方法里面的 ConsumerRecord 类中是包含了数据的元数据信息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> checksum;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedKeySize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedValueSize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所在在使用 FlinkKafkaConsumer011 构造对象的的时候可以传入实现 KafkaDeserializationSchema 接口后的参数对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//多个 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(topics, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//正则表达式 Topic</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer011</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="多种数据类型"><a href="#多种数据类型" class="headerlink" title="多种数据类型"></a>多种数据类型</h4><p>因为在 Kafka 的数据的类型可能会有很多种类型，比如是纯 String、String 类型的 JSON、Avro、Protobuf。那么源数据类型不同，在消费 Kafka 的时候反序列化也是会有一定的不同，但最终还是依赖前面的 KafkaDeserializationSchema 或者 DeserializationSchema （反序列化的 Schema），数据经过处理后的结果再次发到 Kafka 数据类型也是会有多种，它依赖的是 SerializationSchema（序列化的 Schema）。</p>
<h4 id="序列化失败"><a href="#序列化失败" class="headerlink" title="序列化失败"></a>序列化失败</h4><p>因为数据是从 Kafka 过来的，难以避免的是 Kafka 中的数据可能会出现 null 或者不符合预期规范的数据，然后在反序列化的时候如果作业里面没有做异常处理的话，就会导致作业失败重启，这样情况可以在反序列化处做异常处理，保证作业的健壮性。</p>
<h4 id="Kafka-消费-Offset-的选择"><a href="#Kafka-消费-Offset-的选择" class="headerlink" title="Kafka 消费 Offset 的选择"></a>Kafka 消费 Offset 的选择</h4><p>因为在 Flink Kafka Consumer 中是支持配置如何确定从 Kafka 分区开始消费的起始位置的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FlinkKafkaConsumer011&lt;String&gt; myConsumer = <span class="keyword">new</span> FlinkKafkaConsumer0111&lt;&gt;(...);</span><br><span class="line">consumer.setStartFromEarliest();     <span class="comment">//从最早的数据开始消费</span></span><br><span class="line">consumer.setStartFromLatest();       <span class="comment">//从最新的数据开始消费</span></span><br><span class="line">consumer.setStartFromTimestamp(...); <span class="comment">//从根据指定的时间戳（ms）处开始消费</span></span><br><span class="line">consumer.setStartFromGroupOffsets(); <span class="comment">//默认从提交的 offset 开始消费</span></span><br></pre></td></tr></table></figure>
<p>另外还支持根据分区指定的 offset 去消费 Topic 数据，示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">specificStartOffsets.put(<span class="keyword">new</span> KafkaTopicPartition(<span class="string">"zhisheng"</span>, <span class="number">0</span>), <span class="number">23L</span>);</span><br><span class="line">specificStartOffsets.put(<span class="keyword">new</span> KafkaTopicPartition(<span class="string">"zhisheng"</span>, <span class="number">1</span>), <span class="number">31L</span>);</span><br><span class="line">specificStartOffsets.put(<span class="keyword">new</span> KafkaTopicPartition(<span class="string">"zhisheng"</span>, <span class="number">2</span>), <span class="number">43L</span>);</span><br><span class="line"></span><br><span class="line">myConsumer.setStartFromSpecificOffsets(specificStartOffsets);</span><br></pre></td></tr></table></figure>
<p>注意：这种情况下如果该分区中不存在指定的 Offset 了，则会使用默认的 setStartFromGroupOffsets 来消费分区中的数据。如果作业是从 Checkpoint 或者 Savepoint 还原的，那么上面这些配置无效，作业会根据状态中存储的 Offset 为准，然后开始消费。</p>
<p>上面这几种策略是支持可以配置的，需要在作业中指定，具体选择哪种是需要根据作业的业务需求来判断的。</p>
<h3 id="小结与反思-12"><a href="#小结与反思-12" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中最常使用的 Connector —— Kafka，该 Connector 不仅可以作为 Source，还可以作为 Sink。通过了完成的案例讲解从 Kafka 读取数据和写入数据到 Kafka，并分析了这两个的主要类的结构。最后讲解了使用该 Connector 可能会遇到的一些问题，该如何去解决这些问题。</p>
<p>你在公司使用该 Connector 的过程中有遇到什么问题吗？是怎么解决的呢？还有什么问题要补充？</p>
<p>本节涉及代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-kafka" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-kafka</a></p>
<h2 id="十五、如何自定义-Flink-Connectors（Source-和-Sink）？"><a href="#十五、如何自定义-Flink-Connectors（Source-和-Sink）？" class="headerlink" title="十五、如何自定义 Flink Connectors（Source 和 Sink）？"></a>十五、如何自定义 Flink Connectors（Source 和 Sink）？</h2><p>在前面文章 3.6 节中讲解了 Flink 中的 Data Source 和 Data Sink，然后介绍了 Flink 中自带的一些 Source 和 Sink 的 Connector，接着我们还有几篇实战会讲解了如何从 Kafka 处理数据写入到 Kafka、ElasticSearch 等，当然 Flink 还有一些其他的 Connector，我们这里就不一一介绍了，大家如果感兴趣的话可以去官网查看一下，如果对其代码实现比较感兴趣的话，也可以去看看其源码的实现。我们这篇文章来讲解一下如何自定义 Source 和 Sink Connector？这样我们后面再遇到什么样的需求都难不倒我们了。</p>
<h3 id="如何自定义-Source-Connector？"><a href="#如何自定义-Source-Connector？" class="headerlink" title="如何自定义 Source Connector？"></a>如何自定义 Source Connector？</h3><p>这里就演示一下如何自定义 Source 从 MySQL 中读取数据。</p>
<h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加 MySQL 依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="数据库建表"><a href="#数据库建表" class="headerlink" title="数据库建表"></a>数据库建表</h4><p>数据库建表如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`student`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">unsigned</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`password`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`age`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">5</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COLLATE</span>=utf8_bin;</span><br></pre></td></tr></table></figure>
<h4 id="数据库插入数据"><a href="#数据库插入数据" class="headerlink" title="数据库插入数据"></a>数据库插入数据</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`student`</span> <span class="keyword">VALUES</span> (<span class="string">'1'</span>, <span class="string">'zhisheng01'</span>, <span class="string">'123456'</span>, <span class="string">'18'</span>), (<span class="string">'2'</span>, <span class="string">'zhisheng02'</span>, <span class="string">'123'</span>, <span class="string">'17'</span>), (<span class="string">'3'</span>, <span class="string">'zhisheng03'</span>, <span class="string">'1234'</span>, <span class="string">'18'</span>), (<span class="string">'4'</span>, <span class="string">'zhisheng04'</span>, <span class="string">'12345'</span>, <span class="string">'16'</span>);</span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure>
<h4 id="新建实体类"><a href="#新建实体类" class="headerlink" title="新建实体类"></a>新建实体类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> String password;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="自定义-Source-类"><a href="#自定义-Source-类" class="headerlink" title="自定义 Source 类"></a>自定义 Source 类</h4><p>SourceFromMySQL 是自定义的 Source 类，该类继承 RichSourceFunction，实现里面的 open、close、run、cancel 方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceFromMySQL</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"select * from Student;"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 程序执行完毕就可以进行，关闭连接和释放资源的动作了</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123; <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DataStream 调用一次 run() 方法用来获取数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Student&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ResultSet resultSet = ps.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(</span><br><span class="line">                    resultSet.getInt(<span class="string">"id"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"name"</span>).trim(),</span><br><span class="line">                    resultSet.getString(<span class="string">"password"</span>).trim(),</span><br><span class="line">                    resultSet.getInt(<span class="string">"age"</span>));</span><br><span class="line">            ctx.collect(student);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">                con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"123456"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                System.out.println(<span class="string">"mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Flink-应用程序代码"><a href="#Flink-应用程序代码" class="headerlink" title="Flink 应用程序代码"></a>Flink 应用程序代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> SourceFromMySQL()).print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data sourc"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行 Flink 程序，控制台日志中可以看见打印的 student 信息。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1svkzhh7j21kw0zknbx.jpg" alt="undefined"></p>
<h3 id="RichSourceFunction-使用及源码分析"><a href="#RichSourceFunction-使用及源码分析" class="headerlink" title="RichSourceFunction 使用及源码分析"></a>RichSourceFunction 使用及源码分析</h3><p>从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，其实也是可以使用 SourceFunction 函数来自定义 Source。 RichSourceFunction 函数比 SourceFunction 多了 open 方法（可以用来初始化）和获取应用上下文的方法，那么来了解一下该类。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1svvcw3jj21h811ct9j.jpg" alt="undefined"></p>
<p>它是一个抽象类，继承自 AbstractRichFunction，实现了 SourceFunction 接口，其子类有三个，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sw3v5pzj22a011gtao.jpg" alt="undefined"></p>
<ul>
<li>MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。</li>
<li>MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。</li>
<li>ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。</li>
</ul>
<p>除了上面使用 RichSourceFunction 和 SourceFunction 来自定义 Source，还可以继承 RichParallelSourceFunction 抽象类或实现 ParallelSourceFunction 接口来实现自定义 Source 函数。</p>
<h3 id="如何自定义-Sink-Connector？"><a href="#如何自定义-Sink-Connector？" class="headerlink" title="如何自定义 Sink Connector？"></a>如何自定义 Sink Connector？</h3><p>下面将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去</p>
<h4 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h4><p>写了一个工具类往 Kafka 的 topic 中发送数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据，可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(student));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="SinkToMySQL"><a href="#SinkToMySQL" class="headerlink" title="SinkToMySQL"></a>SinkToMySQL</h4><p>该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法，在 invoke 方法中将数据插入到 MySQL 中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Student value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//组装数据，执行插入操作</span></span><br><span class="line">        ps.setInt(<span class="number">1</span>, value.getId());</span><br><span class="line">        ps.setString(<span class="number">2</span>, value.getName());</span><br><span class="line">        ps.setString(<span class="number">3</span>, value.getPassword());</span><br><span class="line">        ps.setInt(<span class="number">4</span>, value.getAge());</span><br><span class="line">        ps.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">            con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"root123456"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span>+ e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Flink-程序"><a href="#Flink-程序" class="headerlink" title="Flink 程序"></a>Flink 程序</h4><p>这里的 source 是从 Kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 Student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; JSON.parseObject(string, Student.class)); <span class="comment">//Fastjson 解析字符串成 student 对象</span></span><br><span class="line"></span><br><span class="line">        student.addSink(<span class="keyword">new</span> SinkToMySQL()); <span class="comment">//数据 sink 到 mysql</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add sink"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>运行 Flink 程序，然后再运行 KafkaUtils2.java 工具类，这样就可以了。</p>
<p>如果数据插入成功了，那么查看下我们的数据库：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sy3run1j21kw0zkti9.jpg" alt="undefined"></p>
<p>数据库中已经插入了 100 条我们从 Kafka 发送的数据了。证明我们的 SinkToMySQL 起作用了。</p>
<h3 id="RichSinkFunction-使用及源码分析"><a href="#RichSinkFunction-使用及源码分析" class="headerlink" title="RichSinkFunction 使用及源码分析"></a>RichSinkFunction 使用及源码分析</h3><p>通过上面的 demo 可以发现继承 RichSinkFunction 类，然后实现内部的 open、close、invoke 方法就可以实现自定义 Sink 了，RichSinkFunction 的类图如下。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1sypiutnj21ka10o0tl.jpg" alt="undefined"></p>
<p>该类继承了 AbstractRichFunction 抽象类，实现了 SinkFunction 接口，同样该类也是一个 Rich 函数，它比 SinkFunction 多了 open（可以初始化数据） 和 getRuntimeContext（可以获取上下文）方法，如果不需要这两个方法，同样也是可以实现 SinkFunction 接口来自定义 Sink 的。</p>
<h3 id="小结与反思-13"><a href="#小结与反思-13" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中该如何去自定义 Connector，包括 Source 和 Sink，每种也都有提供样例去教大家如何操作。</p>
<p>本节相关代码链接：</p>
<ul>
<li><a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sources" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sources</a></li>
<li><a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sinks" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-data-sinks</a></li>
</ul>
<h2 id="十六、如何使用-Flink-Connectors-——-ElasticSearch？"><a href="#十六、如何使用-Flink-Connectors-——-ElasticSearch？" class="headerlink" title="十六、如何使用 Flink Connectors —— ElasticSearch？"></a>十六、如何使用 Flink Connectors —— ElasticSearch？</h2><h3 id="准备环境和依赖-1"><a href="#准备环境和依赖-1" class="headerlink" title="准备环境和依赖"></a>准备环境和依赖</h3><h4 id="ElasticSearch-安装"><a href="#ElasticSearch-安装" class="headerlink" title="ElasticSearch 安装"></a>ElasticSearch 安装</h4><p>因为在 2.1 节中已经讲过 ElasticSearch 的安装，这里就不做过多的重复，需要注意的一点就是 Flink 的 ElasticSearch Connector 是区分版本号的。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tmjwgh3j213s05wmx9.jpg" alt="undefined"></p>
<p>所以添加依赖的时候要区分一下，根据你安装的 ElasticSearch 来选择不一样的版本依赖，另外就是不同版本的 ElasticSearch 还会导致下面的数据写入到 ElasticSearch 中出现一些不同，我们这里使用的版本是 ElasticSearch6，如果你使用的是其他的版本可以参考官网的实现。</p>
<h4 id="添加依赖-1"><a href="#添加依赖-1" class="headerlink" title="添加依赖"></a>添加依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>上面这依赖版本号请自己根据使用的版本对应改变下。</p>
<h3 id="Flink-写入数据到-ElasticSearch-应用程序"><a href="#Flink-写入数据到-ElasticSearch-应用程序" class="headerlink" title="Flink 写入数据到 ElasticSearch 应用程序"></a>Flink 写入数据到 ElasticSearch 应用程序</h3><h4 id="ESSinkUtil-工具类"><a href="#ESSinkUtil-工具类" class="headerlink" title="ESSinkUtil 工具类"></a>ESSinkUtil 工具类</h4><p>这个工具类是自己封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面章节还会再讲些其他的配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ESSinkUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * es sink</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts es hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bulkFlushMaxActions bulk flush size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism 并行数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data 数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> func</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSink</span><span class="params">(List&lt;HttpHost&gt; hosts, <span class="keyword">int</span> bulkFlushMaxActions, <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func)</span> </span>&#123;</span><br><span class="line">        ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(hosts, func);</span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions);</span><br><span class="line">        data.addSink(esSinkBuilder.build()).setParallelism(parallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析配置文件的 es hosts</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> MalformedURLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;HttpHost&gt; <span class="title">getEsAddresses</span><span class="params">(String hosts)</span> <span class="keyword">throws</span> MalformedURLException </span>&#123;</span><br><span class="line">        String[] hostList = hosts.split(<span class="string">","</span>);</span><br><span class="line">        List&lt;HttpHost&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String host : hostList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (host.startsWith(<span class="string">"http"</span>)) &#123;</span><br><span class="line">                URL url = <span class="keyword">new</span> URL(host);</span><br><span class="line">                addresses.add(<span class="keyword">new</span> HttpHost(url.getHost(), url.getPort()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String[] parts = host.split(<span class="string">":"</span>, <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span> (parts.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    addresses.add(<span class="keyword">new</span> HttpHost(parts[<span class="number">0</span>], Integer.parseInt(parts[<span class="number">1</span>])));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> MalformedURLException(<span class="string">"invalid elasticsearch hosts format"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> addresses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Main-启动类"><a href="#Main-启动类" class="headerlink" title="Main 启动类"></a>Main 启动类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sink2ES6Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取所有参数</span></span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        <span class="comment">//准备好环境</span></span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        <span class="comment">//从kafka读取数据</span></span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从配置文件中读取 es 的地址</span></span><br><span class="line">        List&lt;HttpHost&gt; esAddresses = ESSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS));</span><br><span class="line">        <span class="comment">//从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒</span></span><br><span class="line">        <span class="keyword">int</span> bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, <span class="number">40</span>);</span><br><span class="line">        <span class="comment">//从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积</span></span><br><span class="line">        <span class="keyword">int</span> sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自己再自带的 es sink 上一层封装了下</span></span><br><span class="line">        ESSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data,</span><br><span class="line">                (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123;</span><br><span class="line">                    requestIndexer.add(Requests.indexRequest()</span><br><span class="line">                            .index(ZHISHENG + <span class="string">"_"</span> + metric.getName())  <span class="comment">//es 索引名</span></span><br><span class="line">                            .type(ZHISHENG) <span class="comment">//es type</span></span><br><span class="line">                            .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); </span><br><span class="line">                &#125;);</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors es6"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="配置文件-1"><a href="#配置文件-1" class="headerlink" title="配置文件"></a>配置文件</h4><p>配置都支持集群模式填写，注意用 <code>,</code> 分隔！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng-metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng-metrics</span><br><span class="line">stream.parallelism=5</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">elasticsearch.hosts=localhost:9200</span><br><span class="line">elasticsearch.bulk.flush.max.actions=40</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure>
<h3 id="验证数据是否写入-ElasticSearch？"><a href="#验证数据是否写入-ElasticSearch？" class="headerlink" title="验证数据是否写入 ElasticSearch？"></a>验证数据是否写入 ElasticSearch？</h3><p>执行 Main 类的 main 方法，我们的程序是只打印 Flink 的日志，没有打印存入的日志（因为我们这里没有打日志）：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tmzgh4vj232e1jyjyg.jpg" alt="undefined"></p>
<p>所以看起来不知道我们的 Sink 是否有用，数据是否从 Kafka 读取出来后存入到 ES 了。你可以查看下本地起的 ES 终端或者服务器的 ES 日志就可以看到效果了。ES 日志如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1toe3bzdj21et0u0wqw.jpg" alt="undefined"></p>
<p>上图是我本地 Mac 电脑终端的 ES 日志，可以看到我们的索引了。如果还不放心，你也可以在你的电脑装个 Kibana，然后更加的直观查看下 ES 的索引情况（或者直接敲 ES 的命令）。我们用 Kibana 查看存入 ES 的索引如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1toma7ioj21en0u0agv.jpg" alt="undefined"></p>
<p>程序执行了一会，存入 ES 的数据量就很大了。</p>
<h3 id="如何保证在海量数据实时写入下-ElasticSearch-的稳定性？"><a href="#如何保证在海量数据实时写入下-ElasticSearch-的稳定性？" class="headerlink" title="如何保证在海量数据实时写入下 ElasticSearch 的稳定性？"></a>如何保证在海量数据实时写入下 ElasticSearch 的稳定性？</h3><p>上面代码已经可以实现你的大部分场景了，但是如果你的业务场景需要保证数据的完整性（不能出现丢数据的情况），那么就需要添加一些重试策略，因为在我们的生产环境中，很有可能会因为某些组件不稳定性导致各种问题，所以这里我们就要在数据存入失败的时候做重试操作，这个 Flink 自带的 es sink 就支持了，常用的失败重试配置有:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. bulk.flush.backoff.enable 用来表示是否开启重试机制</span><br><span class="line"></span><br><span class="line">2. bulk.flush.backoff.type 重试策略，有两种：EXPONENTIAL 指数型（表示多次重试之间的时间间隔按照指数方式进行增长）、CONSTANT 常数型（表示多次重试之间的时间间隔为固定常数）</span><br><span class="line"></span><br><span class="line">3. bulk.flush.backoff.delay 进行重试的时间间隔</span><br><span class="line"></span><br><span class="line">4. bulk.flush.backoff.retries 失败重试的次数</span><br><span class="line"></span><br><span class="line">5. bulk.flush.max.actions: 批量写入时的最大写入条数</span><br><span class="line"></span><br><span class="line">6. bulk.flush.max.size.mb: 批量写入时的最大数据量</span><br><span class="line"></span><br><span class="line">7. bulk.flush.interval.ms: 批量写入的时间间隔，配置后则会按照该时间间隔严格执行，无视上面的两个批量写入配置</span><br></pre></td></tr></table></figure>
<p>看下，就是如下这些配置了，如果你需要的话，可以在这个地方配置扩充。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tp0gu4uj21ec10utb6.jpg" alt="undefined"></p>
<h3 id="使用-Flink-connector-elasticsearch-可能会遇到的问题"><a href="#使用-Flink-connector-elasticsearch-可能会遇到的问题" class="headerlink" title="使用 Flink-connector-elasticsearch 可能会遇到的问题"></a>使用 Flink-connector-elasticsearch 可能会遇到的问题</h3><p>写入 ES 的时候会有这些情况会导致写入 ES 失败。</p>
<p>1、ES 集群队列满了，报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12:08:07.326 [I/O dispatcher 13] ERROR o.a.f.s.c.e.ElasticsearchSinkBase - Failed Elasticsearch item request: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@566c9379 on EsThreadPoolExecutor[name = node-1/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@f00b373[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 6277]]]]</span><br></pre></td></tr></table></figure>
<p>是这样的，我电脑安装的 ES 队列容量默认应该是 200，我没有修改过。我这里如果配置的 bulk flush size * 并发 Sink 数量 这个值如果大于这个 queue capacity ，那么就很容易导致出现这种因为 ES 队列满了而写入失败。</p>
<p>当然这里你也可以通过调大点 es 的队列。参考：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html</a></p>
<p>2、ES 集群某个节点挂了</p>
<p>这个就不用说了，肯定写入失败的。跟过源码可以发现 RestClient 类里的 performRequestAsync 方法一开始会随机的从集群中的某个节点进行写入数据，如果这台机器掉线，会进行重试在其他的机器上写入，那么当时写入的这台机器的请求就需要进行失败重试，否则就会把数据丢失！</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpbszamj22h628igqp.jpg" alt="undefined"></p>
<p>3、ES 集群某个节点的磁盘满了</p>
<p>这里说的磁盘满了，并不是磁盘真的就没有一点剩余空间的，是 ES 会在写入的时候检查磁盘的使用情况，在 85% 的时候会打印日志警告。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpnrm0zj21yh0k6qqj.jpg" alt="undefined"></p>
<p>这里我看了下源码如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tpvtgmwj21tm18gtcn.jpg" alt="undefined"></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tq2ra8kj21hu0o4abf.jpg" alt="undefined"></p>
<p>如果你想继续让 ES 写入的话就需要去重新配一下 ES 让它继续写入，或者你也可以清空些不必要的数据腾出磁盘空间来。</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.addSink(<span class="keyword">new</span> ElasticsearchSink&lt;&gt;(</span><br><span class="line">    config, transportAddresses,</span><br><span class="line">    <span class="keyword">new</span> ElasticsearchSinkFunction&lt;String&gt;() &#123;...&#125;,</span><br><span class="line">    <span class="keyword">new</span> ActionRequestFailureHandler() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(ActionRequest action,</span></span></span><br><span class="line"><span class="function"><span class="params">                Throwable failure,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> restStatusCode,</span></span></span><br><span class="line"><span class="function"><span class="params">                RequestIndexer indexer)</span> throw Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) &#123;</span><br><span class="line">                <span class="comment">//队列满了，重新添加用于索引的 document</span></span><br><span class="line">                indexer.add(action);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, ElasticsearchParseException.class)) &#123;</span><br><span class="line">                <span class="comment">// 对于有问题的 document，删除该请求，没有额外的错误处理逻辑</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">//对于抛出其他的异常错误，直接就当成 sink 失败，向外抛出异常，你也可以抛出自定义的异常</span></span><br><span class="line">                <span class="keyword">throw</span> failure;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure>
<p>如果仅仅只是想做失败重试，也可以直接使用官方提供的默认的 RetryRejectedExecutionFailureHandler ，该处理器会对 EsRejectedExecutionException 导致到失败写入做重试处理。如果你没有设置失败处理器（failure handler），那么就会使用默认的 NoOpFailureHandler 来简单处理所有的异常。</p>
<h3 id="小结与反思-14"><a href="#小结与反思-14" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了 Flink 中的 ElasticSearch Connector 的使用，通过一个案例教大家如何将读取到的 Kafka 数据写入到 ElasticSearch，最后讲解了 Flink 写入 ElasticSearch 的时候的各种配置和可能遇到的问题及其解决方法。</p>
<p>本节涉及的代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6</a></p>
<h2 id="十七、如何使用-Flink-Connectors-——-HBase？"><a href="#十七、如何使用-Flink-Connectors-——-HBase？" class="headerlink" title="十七、如何使用 Flink Connectors —— HBase？"></a>十七、如何使用 Flink Connectors —— HBase？</h2><h3 id="准备环境和依赖-2"><a href="#准备环境和依赖-2" class="headerlink" title="准备环境和依赖"></a>准备环境和依赖</h3><h4 id="HBase-安装"><a href="#HBase-安装" class="headerlink" title="HBase 安装"></a>HBase 安装</h4><p>如果是苹果系统，可以使用 HomeBrew 命令安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install hbase</span><br></pre></td></tr></table></figure>
<p>HBase 最终会安装在路径 <code>/usr/local/Cellar/hbase/</code> 下面，安装版本不同，文件名也不同。</p>
<h4 id="配置-HBase"><a href="#配置-HBase" class="headerlink" title="配置 HBase"></a>配置 HBase</h4><p>打开 <code>libexec/conf/hbase-env.sh</code> 修改里面的 JAVA_HOME：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line">export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home&quot;</span><br></pre></td></tr></table></figure>
<p>根据你自己的 JAVA_HOME 来配置这个变量。</p>
<p>打开 <code>libexec/conf/hbase-site.xml</code> 配置 HBase 文件存储目录:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/var/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HBase存储内建zookeeper文件的目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/var/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.dns.interface<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lo0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="运行-HBase"><a href="#运行-HBase" class="headerlink" title="运行 HBase"></a>运行 HBase</h4><p>执行启动的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/start-hbase.sh</span><br></pre></td></tr></table></figure>
<p>执行后打印出来的日志如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">starting master, logging to /usr/local/var/log/hbase/hbase-zhisheng-master-zhisheng.out</span><br></pre></td></tr></table></figure>
<h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><p>使用 jps 命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/Cellar/hbase/1.2.9/libexec  jps</span><br><span class="line">91302 HMaster</span><br><span class="line">62535 RemoteMavenServer</span><br><span class="line">1100</span><br><span class="line">91471 Jps</span><br></pre></td></tr></table></figure>
<p>出现 HMaster 说明安装运行成功。</p>
<h4 id="启动-HBase-Shell"><a href="#启动-HBase-Shell" class="headerlink" title="启动 HBase Shell"></a>启动 HBase Shell</h4><p>执行下面命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hbase shell</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tsa2sfwj22800a4abv.jpg" alt="undefined"></p>
<h4 id="停止-HBase"><a href="#停止-HBase" class="headerlink" title="停止 HBase"></a>停止 HBase</h4><p>执行下面的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/stop-hbase.sh</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1ttq07q5j21xk05sjrz.jpg" alt="undefined"></p>
<h4 id="HBase-常用命令"><a href="#HBase-常用命令" class="headerlink" title="HBase 常用命令"></a>HBase 常用命令</h4><p>HBase 中常用的命令有：list（列出已存在的表）、create（创建表）、put（写数据）、get（读数据）、scan（读数据，读全表）、describe（显示表详情）</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tu0f06kj216m0f4wfe.jpg" alt="undefined"></p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tu7vqa2j227y10cdke.jpg" alt="undefined"></p>
<h4 id="添加依赖-2"><a href="#添加依赖-2" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加 HBase 相关的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hbase_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Flink HBase Connector 中，HBase 不仅可以作为数据源，也还可以写入数据到 HBase 中去，我们先来看看如何从 HBase 中读取数据。</p>
<h3 id="Flink-使用-TableInputFormat-读取-HBase-批量数据"><a href="#Flink-使用-TableInputFormat-读取-HBase-批量数据" class="headerlink" title="Flink 使用 TableInputFormat 读取 HBase 批量数据"></a>Flink 使用 TableInputFormat 读取 HBase 批量数据</h3><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>先往 HBase 中插入五条数据如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">put &apos;zhisheng&apos;, &apos;first&apos;, &apos;info:bar&apos;, &apos;hello&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;second&apos;, &apos;info:bar&apos;, &apos;zhisheng001&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;third&apos;, &apos;info:bar&apos;, &apos;zhisheng002&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;four&apos;, &apos;info:bar&apos;, &apos;zhisheng003&apos;</span><br><span class="line">put &apos;zhisheng&apos;, &apos;five&apos;, &apos;info:bar&apos;, &apos;zhisheng004&apos;</span><br></pre></td></tr></table></figure>
<p>scan 整个 <code>zhisheng</code> 表的话，有五条数据：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tuhmykdj21qu0co75v.jpg" alt="undefined"></p>
<h4 id="Flink-Job-代码"><a href="#Flink-Job-代码" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 读取 HBase 数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReadMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//表名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HBASE_TABLE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line">    <span class="comment">// 列族</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] INFO = <span class="string">"info"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line">    <span class="comment">//列名</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] BAR = <span class="string">"bar"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.createInput(<span class="keyword">new</span> TableInputFormat&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> Tuple2&lt;String, String&gt; reuse = <span class="keyword">new</span> Tuple2&lt;String, String&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Scan <span class="title">getScanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">                scan.addColumn(INFO, BAR);</span><br><span class="line">                <span class="keyword">return</span> scan;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> String <span class="title">getTableName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> HBASE_TABLE_NAME;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">protected</span> Tuple2&lt;String, String&gt; <span class="title">mapResultToTuple</span><span class="params">(Result result)</span> </span>&#123;</span><br><span class="line">                String key = Bytes.toString(result.getRow());</span><br><span class="line">                String val = Bytes.toString(result.getValue(INFO, BAR));</span><br><span class="line">                reuse.setField(key, <span class="number">0</span>);</span><br><span class="line">                reuse.setField(val, <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">return</span> reuse;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple2&lt;String, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f1.startsWith(<span class="string">"zhisheng"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码中将 HBase 中的读取全部读取出来后然后过滤以 <code>zhisheng</code> 开头的 value 数据。读取结果：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tuqtg8pj224k172gud.jpg" alt="undefined"></p>
<p>可以看到输出的结果中已经将以 <code>zhisheng</code> 开头的四条数据都打印出来了。</p>
<h3 id="Flink-使用-TableOutputFormat-向-HBase-写入数据"><a href="#Flink-使用-TableOutputFormat-向-HBase-写入数据" class="headerlink" title="Flink 使用 TableOutputFormat 向 HBase 写入数据"></a>Flink 使用 TableOutputFormat 向 HBase 写入数据</h3><h4 id="添加依赖-3"><a href="#添加依赖-3" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>在 pom.xml 中添加依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hadoop-compatibility_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>要在 HBase 中提交创建 <code>zhisheng_sink</code> 表，并且 Column 为 <code>info_sink</code> （如果先运行程序的话是会报错说该表不存在的）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &apos;zhisheng_sink&apos;, &apos;info_sink&apos;</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tv440c3j227k0okn03.jpg" alt="undefined"></p>
<h4 id="Flink-Job-代码-1"><a href="#Flink-Job-代码-1" class="headerlink" title="Flink Job 代码"></a>Flink Job 代码</h4><p>接着写 Flink Job 的代码，这里我们将 WordCount 的结果 KV 数据写入到 HBase 中去，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 写入数据到 HBase</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseWriteMain</span> </span>&#123;</span><br><span class="line">    <span class="comment">//表名</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HBASE_TABLE_NAME = <span class="string">"zhisheng_sink"</span>;</span><br><span class="line">    <span class="comment">// 列族</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] INFO = <span class="string">"info_sink"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line">    <span class="comment">//列名</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] BAR = <span class="string">"bar_sink"</span>.getBytes(ConfigConstants.DEFAULT_CHARSET);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        Job job = Job.getInstance();</span><br><span class="line">        job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, HBASE_TABLE_NAME);</span><br><span class="line">        env.fromElements(WORDS)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] splits = value.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split, <span class="number">1</span>));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;Text, Mutation&gt;&gt;() &#123;</span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">transient</span> Tuple2&lt;Text, Mutation&gt; reuse;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        reuse = <span class="keyword">new</span> Tuple2&lt;Text, Mutation&gt;();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;Text, Mutation&gt; <span class="title">map</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        reuse.f0 = <span class="keyword">new</span> Text(value.f0);</span><br><span class="line">                        Put put = <span class="keyword">new</span> Put(value.f0.getBytes(ConfigConstants.DEFAULT_CHARSET));</span><br><span class="line">                        put.addColumn(INFO, BAR, Bytes.toBytes(value.f1.toString()));</span><br><span class="line">                        reuse.f1 = put;</span><br><span class="line">                        <span class="keyword">return</span> reuse;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).output(<span class="keyword">new</span> HadoopOutputFormat&lt;Text, Mutation&gt;(<span class="keyword">new</span> TableOutputFormat&lt;Text&gt;(), job));</span><br><span class="line">        env.execute(<span class="string">"Flink Connector HBase sink Example"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] WORDS = <span class="keyword">new</span> String[]&#123;</span><br><span class="line">            <span class="string">"To be, or not to be,--that is the question:--"</span>,</span><br><span class="line">            <span class="string">"The fair is be in that orisons"</span></span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行该 Job 的话，然后再用 HBase shell 命令去验证数据是否插入成功了：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tvgngwkj21v40fw41q.jpg" alt="undefined"></p>
<p>可以看见数据已经成功写入了 11 条，然后我们验证一下数据的条数是不是一样的呢？我们在上面的代码中将 map 和 output 算子给注释掉，然后用上 print 打印出来的话，结果如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(be,<span class="number">3</span>)</span><br><span class="line">(is,<span class="number">2</span>)</span><br><span class="line">(in,<span class="number">1</span>)</span><br><span class="line">(or,<span class="number">1</span>)</span><br><span class="line">(orisons,<span class="number">1</span>)</span><br><span class="line">(not,<span class="number">1</span>)</span><br><span class="line">(the,<span class="number">2</span>)</span><br><span class="line">(fair,<span class="number">1</span>)</span><br><span class="line">(question,<span class="number">1</span>)</span><br><span class="line">(that,<span class="number">2</span>)</span><br><span class="line">(to,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>统计的结果刚好也是 11 条数据，说明我们的写入过程中没有丢失数据。但是运行 Job 的话你会看到日志中报了一条这样的错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.IllegalArgumentException: Can not create a Path from a null string</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tvrecrwj22xc0p4n5q.jpg" alt="undefined"></p>
<p>这个问题是因为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Path partitionsPath = <span class="keyword">new</span> Path(conf.get(<span class="string">"mapred.output.dir"</span>), <span class="string">"partitions_"</span> + UUID.randomUUID());</span><br></pre></td></tr></table></figure>
<p>当配置项 mapred.output.dir 不存在时，conf.get() 将返回 null，从而导致上述异常。那么该如何解决这个问题呢？</p>
<blockquote>
<p>需要在代码中或配置文件中添加配置项 mapred.output.dir。</p>
</blockquote>
<p>比如在代码里加上这行代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.getConfiguration().set(<span class="string">"mapred.output.dir"</span>, <span class="string">"/tmp"</span>);</span><br></pre></td></tr></table></figure>
<p>再次运行这个 Job 你就不会发现报错了。</p>
<h3 id="Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据"><a href="#Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据" class="headerlink" title="Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据"></a>Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据</h3><p>从上面两个程序中你可以发现两个都是批程序（从 HBase 读取批量的数据、写入批量的数据进 HBase），下面跟着笔者来演示一个流程序。</p>
<h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><p>本来是打算演示从 Kafka 读取 String 类型的数据，但是为了好演示，我这里直接在代码里面造一些数据：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; dataStream = env.addSource(<span class="keyword">new</span> SourceFunction&lt;String&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            out.collect(String.valueOf(Math.floor(Math.random() * <span class="number">100</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>如果是读取 Kafka 数据请对应替换成：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">    parameterTool.get(METRICS_TOPIC),   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">    <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">    props));</span><br></pre></td></tr></table></figure>
<h4 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h4><p>获取到数据后需要将数据写入到 HBase，这里使用的实现 HBaseOutputFormat 接口，然后重写里面的 configure、open、writeRecord、close 方法，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseOutputFormat</span> <span class="keyword">implements</span> <span class="title">OutputFormat</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> org.apache.hadoop.conf.Configuration configuration;</span><br><span class="line">    <span class="keyword">private</span> Connection connection = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> String taskNumber = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> Table table = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> rowNumber = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Configuration parameters)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//设置配置信息</span></span><br><span class="line">        configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(HBASE_ZOOKEEPER_QUORUM, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_ZOOKEEPER_QUORUM));</span><br><span class="line">        configuration.set(HBASE_ZOOKEEPER_PROPERTY_CLIENTPORT, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_ZOOKEEPER_PROPERTY_CLIENTPORT));</span><br><span class="line">        configuration.set(HBASE_RPC_TIMEOUT, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_RPC_TIMEOUT));</span><br><span class="line">        configuration.set(HBASE_CLIENT_OPERATION_TIMEOUT, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_CLIENT_OPERATION_TIMEOUT));</span><br><span class="line">        configuration.set(HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(<span class="keyword">int</span> taskNumber, <span class="keyword">int</span> numTasks)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        TableName tableName = TableName.valueOf(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_TABLE_NAME));</span><br><span class="line">        Admin admin = connection.getAdmin();</span><br><span class="line">        <span class="keyword">if</span> (!admin.tableExists(tableName)) &#123; <span class="comment">//检查是否有该表，如果没有，创建</span></span><br><span class="line">            log.info(<span class="string">"==============不存在表 = &#123;&#125;"</span>, tableName);</span><br><span class="line">                admin.createTable(<span class="keyword">new</span> HTableDescriptor(TableName.valueOf(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_TABLE_NAME)))</span><br><span class="line">                        .addFamily(<span class="keyword">new</span> HColumnDescriptor(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_COLUMN_NAME))));</span><br><span class="line">        &#125;</span><br><span class="line">        table = connection.getTable(tableName);</span><br><span class="line">        <span class="keyword">this</span>.taskNumber = String.valueOf(taskNumber);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeRecord</span><span class="params">(String record)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Put put = <span class="keyword">new</span> Put(Bytes.toBytes(taskNumber + rowNumber));</span><br><span class="line">        put.addColumn(Bytes.toBytes(ExecutionEnvUtil.PARAMETER_TOOL.get(HBASE_COLUMN_NAME)), Bytes.toBytes(<span class="string">"zhisheng"</span>),</span><br><span class="line">                Bytes.toBytes(String.valueOf(rowNumber)));</span><br><span class="line">        rowNumber++;</span><br><span class="line">        table.put(put);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        table.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="配置文件-2"><a href="#配置文件-2" class="headerlink" title="配置文件"></a>配置文件</h4><p>配置文件中的一些配置如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng</span><br><span class="line">stream.parallelism=4</span><br><span class="line">stream.sink.parallelism=4</span><br><span class="line">stream.default.parallelism=4</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line"></span><br><span class="line"># HBase</span><br><span class="line">hbase.zookeeper.quorum=localhost:2181</span><br><span class="line">hbase.client.retries.number=1</span><br><span class="line">hbase.master.info.port=-1</span><br><span class="line">hbase.zookeeper.property.clientPort=2081</span><br><span class="line">hbase.rpc.timeout=30000</span><br><span class="line">hbase.client.operation.timeout=30000</span><br><span class="line">hbase.client.scanner.timeout.period=30000</span><br><span class="line"></span><br><span class="line"># HBase table name</span><br><span class="line">hbase.table.name=zhisheng_stream</span><br><span class="line">hbase.column.name=info_stream</span><br></pre></td></tr></table></figure>
<h3 id="项目运行及验证"><a href="#项目运行及验证" class="headerlink" title="项目运行及验证"></a>项目运行及验证</h3><p>运行项目后然后你再去用 HBase shell 命令查看你会发现该 <code>zhisheng_stream</code> 表之前没有建立，现在建立了，再通过 scan 命令查看的话，你会发现数据一直在更新，不断增加数据条数。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1tw2katwj2230114wmp.jpg" alt="undefined"></p>
<h3 id="小结与反思-15"><a href="#小结与反思-15" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节开始讲解了 HBase 相关的环境安装和基础命令，接着讲解了如何去读取 HBase 数据和写入数据到 HBase。</p>
<p>本节涉及的完整代码地址在：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-hbase" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-hbase</a></p>
<h2 id="十八、如何使用-Flink-Connectors-——-Redis？"><a href="#十八、如何使用-Flink-Connectors-——-Redis？" class="headerlink" title="十八、如何使用 Flink Connectors —— Redis？"></a>十八、如何使用 Flink Connectors —— Redis？</h2><p>在生产环境中，通常会将一些计算后的数据存储在 Redis 中，以供第三方的应用去 Redis 查找对应的数据，至于 Redis 的特性笔者不会在本节做过多的讲解。</p>
<h3 id="安装-Redis"><a href="#安装-Redis" class="headerlink" title="安装 Redis"></a>安装 Redis</h3><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><p>先在 <a href="https://redis.io/download" target="_blank" rel="noopener">https://redis.io/download</a> 下载到 Redis。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-5.0.4.tar.gz</span><br><span class="line">tar xzf redis-5.0.4.tar.gz</span><br><span class="line">cd redis-5.0.4</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<h4 id="通过-HomeBrew-安装"><a href="#通过-HomeBrew-安装" class="headerlink" title="通过 HomeBrew 安装"></a>通过 HomeBrew 安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install redis</span><br></pre></td></tr></table></figure>
<p>如果需要后台运行 Redis 服务，使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew services start redis</span><br></pre></td></tr></table></figure>
<p>要运行命令，可以直接到 /usr/local/bin 目录下，有：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br><span class="line">redis-cli</span><br></pre></td></tr></table></figure>
<p>两个命令，执行 <code>redis-server</code> 可以打开服务端：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u105kjbj21980em75o.jpg" alt="undefined"></p>
<p>然后另外开一个终端，运行 <code>redis-cli</code> 命令可以运行客户端：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u1cqotwj20r605tzk9.jpg" alt="undefined"></p>
<h3 id="准备商品数据发送至-Kafka"><a href="#准备商品数据发送至-Kafka" class="headerlink" title="准备商品数据发送至 Kafka"></a>准备商品数据发送至 Kafka</h3><p>这里我打算将从 Kafka 读取到所有到商品的信息，然后将商品信息中的 <strong>商品ID</strong> 和 <strong>商品价格</strong> 提取出来，然后写入到 Redis 中，供第三方服务根据商品 ID 查询到其对应的商品价格。</p>
<p>首先定义我们的商品类 （其中 id 和 price 字段是我们最后要提取的）为：</p>
<p>ProductEvent.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 商品</span></span><br><span class="line"><span class="comment"> * blog：http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> * 微信公众号：zhisheng</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@Builder</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类目 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long categoryId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 编码</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String code;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long shopId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 店铺 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String shopName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 Id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long brandId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 品牌 name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String brandName;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product name</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 图片地址</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String imageUrl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 状态（1(上架),-1(下架),-2(冻结),-3(删除)）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> status;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 类型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> type;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 标签</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;String&gt; tags;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Product 价格（以分为单位）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Long price;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后写个工具类不断的模拟商品数据发往 Kafka，工具类 <code>ProductUtil.java</code> ：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProductUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"zhisheng"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10000</span>; i++) &#123;</span><br><span class="line">            ProductEvent product = ProductEvent.builder().id((<span class="keyword">long</span>) i)  <span class="comment">//商品的 id</span></span><br><span class="line">                    .name(<span class="string">"product"</span> + i)    <span class="comment">//商品 name</span></span><br><span class="line">                    .price(random.nextLong() / <span class="number">10000000000000L</span>) <span class="comment">//商品价格（以分为单位）</span></span><br><span class="line">                    .code(<span class="string">"code"</span> + i).build();  <span class="comment">//商品编码</span></span><br><span class="line"></span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(product));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(product));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Flink-消费-Kafka-中商品数据"><a href="#Flink-消费-Kafka-中商品数据" class="headerlink" title="Flink 消费 Kafka 中商品数据"></a>Flink 消费 Kafka 中商品数据</h3><p>我们需要在 Flink 中消费 Kafka 数据，然后将商品中的两个数据（商品 id 和 price）取出来。先来看下这段 Flink Job 代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line">        Properties props = KafkaConfigUtil.buildKafkaProps(parameterTool);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; product = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.get(METRICS_TOPIC),   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props))</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, ProductEvent.class)) <span class="comment">//反序列化 JSON</span></span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ProductEvent, Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(ProductEvent value, Collector&lt;Tuple2&lt;String, String&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//收集商品 id 和 price 两个属性</span></span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(value.getId().toString(), value.getPrice().toString()));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        product.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink redis connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后 IDEA 中启动运行 Job，再运行上面的 ProductUtil 发送 Kafka 数据的工具类，注意：也得提前启动 Kafka。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u1psyytj225g0pstcr.jpg" alt="undefined"></p>
<p>上图左半部分是工具类发送数据到 Kafka 打印的日志，右半部分是 Job 执行的结果，可以看到它已经将商品的 id 和 price 数据获取到了。</p>
<p>那么接下来我们需要的就是将这种 <code>Tuple2</code> 格式的 KV 数据写入到 Redis 中去。要将数据写入到 Redis 的话是需要先添加依赖的。</p>
<h3 id="Redis-Connector-简介"><a href="#Redis-Connector-简介" class="headerlink" title="Redis Connector 简介"></a>Redis Connector 简介</h3><p>Redis Connector 提供用于向 Redis 发送数据的接口的类。接收器可以使用三种不同的方法与不同类型的 Redis 环境进行通信：</p>
<ul>
<li>单 Redis 服务器</li>
<li>Redis 集群</li>
<li>Redis Sentinel</li>
</ul>
<h3 id="添加依赖-4"><a href="#添加依赖-4" class="headerlink" title="添加依赖"></a>添加依赖</h3><p>需要添加 Flink Redis Sink 的 Connector，这个 Redis Connector 官方只有老的版本，后面也一直没有更新，所以可以看到网上有些文章都是添加老的版本的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>包括该部分的文档都是很早之前的啦，可以查看 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/streaming/connectors/redis.html。" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.1/apis/streaming/connectors/redis.html。</a></p>
<p>另外在 <a href="https://bahir.apache.org/docs/flink/current/flink-streaming-redis/" target="_blank" rel="noopener">https://bahir.apache.org/docs/flink/current/flink-streaming-redis/</a> 也看到一个 Flink Redis Connector 的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>两个依赖功能都是一样的，我们还是就用官方的那个 Maven 依赖来进行演示。</p>
<h3 id="Flink-写入数据到-Redis"><a href="#Flink-写入数据到-Redis" class="headerlink" title="Flink 写入数据到 Redis"></a>Flink 写入数据到 Redis</h3><p>像写入到 Redis，那么肯定要配置 Redis 服务的地址（不管是单机的还是集群）。</p>
<p><strong>单机的 Redis</strong> 你可以这样配置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder().setHost(<span class="string">"127.0.0.1"</span>).build();</span><br></pre></td></tr></table></figure>
<p>这个 FlinkJedisPoolConfig 源码中有四个属性：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String host;  <span class="comment">//hostname or IP</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> port;     <span class="comment">//端口，默认 6379</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> database; <span class="comment">//database index</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String password;  <span class="comment">//password</span></span><br></pre></td></tr></table></figure>
<p>另外你还可以通过 FlinkJedisPoolConfig 设置其他的的几个属性（因为 FlinkJedisPoolConfig 继承自 FlinkJedisConfigBase，这几个属性在 FlinkJedisConfigBase 抽象类的）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> maxTotal;   <span class="comment">//池可分配的对象最大数量，默认是 8</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> maxIdle;    <span class="comment">//池中空闲的对象最大数量，默认是 8</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> minIdle;    <span class="comment">//池中空闲的对象最小数量，默认是 0</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">int</span> connectionTimeout;  <span class="comment">//socket 或者连接超时时间，默认是 2000ms</span></span><br></pre></td></tr></table></figure>
<p><strong>Redis 集群</strong> 你可以这样配置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisClusterConfig config = <span class="keyword">new</span> FlinkJedisClusterConfig.Builder()</span><br><span class="line">                .setNodes(<span class="keyword">new</span> HashSet&lt;InetSocketAddress&gt;(</span><br><span class="line">                        Arrays.asList(<span class="keyword">new</span> InetSocketAddress(<span class="string">"redis1"</span>, <span class="number">6379</span>)))).build();</span><br></pre></td></tr></table></figure>
<p><strong>Redis Sentinels</strong> 你可以这样配置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FlinkJedisSentinelConfig sentinelConfig = <span class="keyword">new</span> FlinkJedisSentinelConfig.Builder()</span><br><span class="line">        .setMasterName(<span class="string">"master"</span>)</span><br><span class="line">        .setSentinels(<span class="keyword">new</span> HashSet&lt;&gt;(Arrays.asList(<span class="string">"sentinel1"</span>, <span class="string">"sentinel2"</span>)))</span><br><span class="line">        .setPassword(<span class="string">""</span>)</span><br><span class="line">        .setDatabase(<span class="number">1</span>).build();</span><br></pre></td></tr></table></figure>
<p>另外就是 Redis Sink 了，Redis Sink 核心类是 RedisMapper，它是一个接口，里面有三个方法，使用时我们需要重写这三个方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RedisMapper</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//设置使用 Redis 的数据结构类型，和 key 的名词，RedisCommandDescription 中有两个属性 RedisCommand、key</span></span><br><span class="line">    <span class="function">RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">//获取 key 值</span></span><br><span class="line">    <span class="function">String <span class="title">getKeyFromData</span><span class="params">(T var1)</span></span>;</span><br><span class="line">    <span class="comment">//获取 value 值</span></span><br><span class="line">    <span class="function">String <span class="title">getValueFromData</span><span class="params">(T var1)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面 RedisCommandDescription 中有两个属性 RedisCommand、key。RedisCommand 可以设置 Redis 的数据结果类型，下面是 Redis 数据结构的类型对应着的 Redis Command 的类型：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2170ijj214e0mcjsd.jpg" alt="undefined"></p>
<p>其对应的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> RedisCommand &#123;</span><br><span class="line">    LPUSH(RedisDataType.LIST),</span><br><span class="line">    RPUSH(RedisDataType.LIST),</span><br><span class="line">    SADD(RedisDataType.SET),</span><br><span class="line">    SET(RedisDataType.STRING),</span><br><span class="line">    PFADD(RedisDataType.HYPER_LOG_LOG),</span><br><span class="line">    PUBLISH(RedisDataType.PUBSUB),</span><br><span class="line">    ZADD(RedisDataType.SORTED_SET),</span><br><span class="line">    HSET(RedisDataType.HASH);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> RedisDataType redisDataType;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">RedisCommand</span><span class="params">(RedisDataType redisDataType)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.redisDataType = redisDataType;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisDataType <span class="title">getRedisDataType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.redisDataType;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们实现这个 RedisMapper 接口如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisSinkMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//指定 RedisCommand 的类型是 HSET，对应 Redis 中的数据结构是 HASH，另外设置 key = zhisheng</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.HSET, <span class="string">"zhisheng"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.f0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.f1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后在 Flink Job 中加入下面这行，将数据通过 RedisSinkMapper 写入到 Redis 中去：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">product.addSink(<span class="keyword">new</span> RedisSink&lt;Tuple2&lt;String, String&gt;&gt;(conf, <span class="keyword">new</span> RedisSinkMapper()));</span><br></pre></td></tr></table></figure>
<h3 id="验证写入结果"><a href="#验证写入结果" class="headerlink" title="验证写入结果"></a>验证写入结果</h3><p>运行 Job 的话，就是把数据已经插入进 Redis 了，那么如何验证我们的结果是否正确呢？</p>
<p>1、我们去终端 Cli 执行命令查看这个 zhisheng 的 key，然后查找某个商品 id (1 ~ 10000) 对应的商品价格，超过这个 id 则为 nil。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2a7aouj215o0pg75r.jpg" alt="undefined"></p>
<p>2、另外一种验证的方式就是通过 Java 代码来操作 Redis 查询数据了。</p>
<p>我们先引入 Redis 的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>连接 Redis 查询数据：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Jedis jedis = <span class="keyword">new</span> Jedis(<span class="string">"127.0.0.1"</span>);</span><br><span class="line">        System.out.println(<span class="string">"Server is running: "</span> + jedis.ping());</span><br><span class="line">        System.out.println(<span class="string">"result:"</span> + jedis.hgetAll(<span class="string">"zhisheng"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1u2jipebj224g12owj7.jpg" alt="undefined"></p>
<p>这一行把所有的数据都打印出来了，所以我们的数据确实成功地插入到 Redis 中去了。</p>
<h3 id="小结与反思-16"><a href="#小结与反思-16" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本文先讲解了 Redis 的安装，然后讲了 Flink 如何消费 Kafka 的数据并将数据写入到 Redis 中去。在实战的过程中还分析了 Flink Redis Connector 中的原理，只要我们懂得了这些原理，后面再去做这块的需求就难不倒大家了。</p>
<p>本节涉及的代码地址在：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-redis" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-redis</a></p>
<h2 id="十九、如何使用-Side-Output-来分流"><a href="#十九、如何使用-Side-Output-来分流" class="headerlink" title="十九、如何使用 Side Output 来分流?"></a>十九、如何使用 Side Output 来分流?</h2><p>通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分，那么在 Flink 中该怎么完成这需求呢，有如下这些方法。</p>
<h3 id="使用-Filter-分流"><a href="#使用-Filter-分流" class="headerlink" title="使用 Filter 分流"></a>使用 Filter 分流</h3><p>使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; <span class="string">"machine"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出机器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; <span class="string">"docker"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出容器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; <span class="string">"application"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));  <span class="comment">//过滤出应用的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; <span class="string">"middleware"</span>.equals(m.getTags().get(<span class="string">"type"</span>)));    <span class="comment">//过滤出中间件的数据</span></span><br></pre></td></tr></table></figure>
<h3 id="使用-Split-分流"><a href="#使用-Split-分流" class="headerlink" title="使用 Split 分流"></a>使用 Split 分流</h3><p>先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SplitStream&lt;MetricEvent&gt; splitData = data.split(<span class="keyword">new</span> OutputSelector&lt;MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(MetricEvent metricEvent)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                tags.add(<span class="string">"machine"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                tags.add(<span class="string">"docker"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                tags.add(<span class="string">"application"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                tags.add(<span class="string">"middleware"</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;MetricEvent&gt; machine = splitData.select(<span class="string">"machine"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = splitData.select(<span class="string">"docker"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = splitData.select(<span class="string">"application"</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = splitData.select(<span class="string">"middleware"</span>);</span><br></pre></td></tr></table></figure>
<p>上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/" target="_blank" rel="noopener">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ，当时排查这个问题还查到两个相关的 Flink Issue。</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-5031" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-5031</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-11084" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-11084</a></li>
</ul>
<p>这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。</p>
<h3 id="使用-Side-Output-分流"><a href="#使用-Side-Output-分流" class="headerlink" title="使用 Side Output 分流"></a>使用 Side Output 分流</h3><p>要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 output tag</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; machineTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"machine"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; dockerTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"docker"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; applicationTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"application"</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; middlewareTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">"middleware"</span>) &#123;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>定义好 OutputTag 后，可以使用下面几种函数来处理数据：</p>
<ul>
<li>ProcessFunction</li>
<li>KeyedProcessFunction</li>
<li>CoProcessFunction</li>
<li>ProcessWindowFunction</li>
<li>ProcessAllWindowFunction</li>
</ul>
<p>在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(<span class="keyword">new</span> ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">"type"</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"machine"</span>:</span><br><span class="line">                context.output(machineTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"docker"</span>:</span><br><span class="line">                context.output(dockerTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"application"</span>:</span><br><span class="line">                context.output(applicationTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"middleware"</span>:</span><br><span class="line">                context.output(middlewareTag, metricEvent);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                collector.collect(metricEvent);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag);</span><br></pre></td></tr></table></figure>
<p>这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。</p>
<h3 id="小结与反思-17"><a href="#小结与反思-17" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节讲了下 Flink 中将数据分流的三种方式，涉及的完整代码 GitHub 地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput" target="_blank" rel="noopener">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput</a></p>
<h2 id="二十、Flink-State-深度讲解"><a href="#二十、Flink-State-深度讲解" class="headerlink" title="二十、Flink State 深度讲解"></a>二十、Flink State 深度讲解</h2><p>在基础篇中的 1.2 节中介绍了 Flink 是一款有状态的流处理框架。那么大家可能有点疑问，这个状态是什么意思？拿 Flink 最简单的 Word Count 程序来说，它需要不断的对 word 出现的个数进行结果统计，那么后一个结果就需要利用前一个的结果然后再做 +1 的操作，这样前一个计算就需要将 word 出现的次数 count 进行存着（这个 count 那么就是一个状态）然后后面才可以进行累加。</p>
<h3 id="为什么需要-state？"><a href="#为什么需要-state？" class="headerlink" title="为什么需要 state？"></a>为什么需要 state？</h3><p>对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）：</p>
<ul>
<li>Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费</li>
<li>Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1vh26in0j21k20qg752.jpg" alt="undefined"></p>
<p>为了解决上面两种情况（数据重复消费或者数据没有消费）的发生，那么是不是就得需要个什么东西做个记录将这种数据消费状态，Flink state 就这样诞生了，state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态），当 Job 因为某种错误或者其他原因导致重启时，就能够从 checkpoint（定时将 state 做一个全局快照，在 Flink 中，为了能够让 Job 在运行的过程中保证容错性，才会对这些 state 做一个快照，在 4.3 节中会详细讲） 中的 state 数据进行恢复。</p>
<h3 id="State-的种类"><a href="#State-的种类" class="headerlink" title="State 的种类"></a>State 的种类</h3><p>在 Flink 中有两个基本的 state：Keyed state 和 Operator state，下面来分别介绍一下这两种 State。</p>
<h3 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h3><p>Keyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。你可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。逻辑上，Keyed State 总是对应着一个 二元组，在某种程度上，因为每个具体的 key 总是属于唯一一个具体的 parallel-operator-instance（并行操作实例），这种情况下，那么就可以简化认为是 。Keyed State 可以进一步组织成 Key Group，Key Group 是 Flink 重新分配 Keyed State 的最小单元，所以有多少个并行，就会有多少个 Key Group。在执行过程中，每个 keyed operator 的并行实例会处理来自不同 key 的不同 Key Group。</p>
<h3 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h3><p>对 Operator State 而言，每个 operator state 都对应着一个并行实例。Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。</p>
<p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p>
<p>在 Flink 源码中，在 flink-core module 下的 org.apache.flink.api.common.state 中可以看到 Flink 中所有和 State 相关的类。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w0k0t1xj216a1kyq4n.jpg" alt="undefined"></p>
<h3 id="Raw-and-Managed-State"><a href="#Raw-and-Managed-State" class="headerlink" title="Raw and Managed State"></a>Raw and Managed State</h3><p>Keyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。</p>
<p>原始状态是 Operator（算子）保存它们自己的数据结构中的 state，当 checkpoint 时，原始状态会以字节流的形式写入进 checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。</p>
<p>托管状态可以使用 Flink runtime 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink runtime 会对这些状态进行编码然后将它们写入到 checkpoint 中。</p>
<p>DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在实现 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。</p>
<p>注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。</p>
<h3 id="如何使用托管-Keyed-State"><a href="#如何使用托管-Keyed-State" class="headerlink" title="如何使用托管 Keyed State"></a>如何使用托管 Keyed State</h3><p>托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。</p>
<p>我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的：</p>
<ul>
<li>ValueState: 保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。</li>
<li>ListState: 保存一个值的列表，用 add(T) 或者 addAll(List) 来添加，用 Iterable get() 来获取。</li>
<li>ReducingState: 保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。</li>
<li>AggregatingState: 保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。</li>
<li>FoldingState: 与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。</li>
<li>MapState: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map) 添加，或者使用 get(UK) 获取。</li>
</ul>
<p>所有类型的状态都有一个 clear() 方法来清除当前的状态。</p>
<p>注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。</p>
<p>需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。</p>
<p>要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。</p>
<p>状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态：</p>
<ul>
<li>ValueState getState(ValueStateDescriptor)</li>
<li>ReducingState getReducingState(ReducingStateDescriptor)</li>
<li>ListState getListState(ListStateDescriptor)</li>
<li>AggregatingState getAggregatingState(AggregatingState)</li>
<li>FoldingState getFoldingState(FoldingStateDescriptor)</li>
<li>MapState getMapState(MapStateDescriptor)</li>
</ul>
<p>上面讲了这么多概念，那么来一个例子来看看如何使用状态：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowAverage</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//访问状态的 value 值</span></span><br><span class="line">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 count</span></span><br><span class="line">        currentSum.f0 += <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 sum</span></span><br><span class="line">        currentSum.f1 += input.f1;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        sum.update(currentSum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果 count 等于 2, 发出平均值并清除状态</span></span><br><span class="line">        <span class="keyword">if</span> (currentSum.f0 &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class="line">            sum.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">                        <span class="string">"average"</span>, <span class="comment">//状态名称</span></span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class="comment">//类型信息</span></span><br><span class="line">                        Tuple2.of(<span class="number">0L</span>, <span class="number">0L</span>)); <span class="comment">//状态的默认值</span></span><br><span class="line">        sum = getRuntimeContext().getState(descriptor);<span class="comment">//获取状态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">env.fromElements(Tuple2.of(<span class="number">1L</span>, <span class="number">3L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">5L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">7L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">4L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">2L</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> CountWindowAverage())</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果会打印出 (1,4) 和 (1,5)</span></span><br></pre></td></tr></table></figure>
<p>这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。</p>
<h3 id="State-TTL-存活时间"><a href="#State-TTL-存活时间" class="headerlink" title="State TTL(存活时间)"></a>State TTL(存活时间)</h3><h4 id="State-TTL-介绍"><a href="#State-TTL-介绍" class="headerlink" title="State TTL 介绍"></a>State TTL 介绍</h4><p>TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">"zhisheng"</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);    <span class="comment">//开启 ttl</span></span><br></pre></td></tr></table></figure>
<p>上面配置中有几个选项需要注意：</p>
<p>1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。</p>
<p>2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）：</p>
<ul>
<li>StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新</li>
<li>StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新</li>
</ul>
<p>3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired：</p>
<ul>
<li>StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值</li>
<li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回</li>
</ul>
<p>在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据.</p>
<p>另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。</p>
<p>注意：</p>
<ul>
<li>状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。</li>
<li>目前仅支持参考 processing time 的 TTL</li>
<li>使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。</li>
<li>TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。</li>
<li>只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。</li>
</ul>
<h4 id="清除过期-state"><a href="#清除过期-state" class="headerlink" title="清除过期 state"></a>清除过期 state</h4><p>默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。</p>
<p>注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。</p>
<p>此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupFullSnapshot()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>
<p>此配置不适用于 RocksDB 状态后端中的增量 checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。</p>
<p>除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupInBackground()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure>
<p>要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。</p>
<p>我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。</p>
<p>在该类中的属性有如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w14qx84j20pg0bw3ys.jpg" alt="undefined"></p>
<ul>
<li>DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig</li>
<li>UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选</li>
<li>StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired</li>
<li>TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选</li>
<li>Time：设置 TTL 的时间，这里有两个参数 unit 和 size</li>
<li>CleanupStrategies：TTL 清理策略，在该类中又有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL<em>STATE</em>SCAN<em>SNAPSHOT、INCREMENTAL</em>CLEANUP 和 ROCKSDB<em>COMPACTION</em>FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器）。</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w1dvbnrj21l40e4jrv.jpg" alt="undefined"></p>
<p>如果对 State TTL 还有不清楚的可以看看 Flink 源码 flink-runtime module 中的 state ttl 相关的实现：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w1nxwk9j215w0z8gmm.jpg" alt="undefined"></p>
<h3 id="如何使用托管-Operator-State"><a href="#如何使用托管-Operator-State" class="headerlink" title="如何使用托管 Operator State"></a>如何使用托管 Operator State</h3><p>为了使用托管的 Operator State，必须要有一个有状态的函数，这个函数可以实现 CheckpointedFunction 或者 ListCheckpointed 接口。</p>
<p>下面分别讲一下如何使用：</p>
<h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><p>如果是实现 CheckpointedFunction 接口的话，那么我们先来看下这个接口里面有什么方法呢：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//当请求 checkpoint 快照时，将调用此方法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//在分布式执行期间创建并行功能实例时，将调用此方法。 函数通常在此方法中设置其状态存储数据结构</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure>
<p>当有请求执行 checkpoint 的时候，snapshotState() 方法就会被调用，initializeState() 方法会在每次初始化用户定义的函数时或者从更早的 checkpoint 恢复的时候被调用，因此 initializeState() 不仅是不同类型的状态被初始化的地方，而且还是 state 恢复逻辑的地方。</p>
<p>目前，List 类型的托管状态是支持的，状态被期望是一个可序列化的对象的 List，彼此独立，这样便于重分配，换句话说，这些对象是可以重新分配的 non-keyed state 的最小粒度，根据状态的访问方法，定义了重新分配的方案：</p>
<ul>
<li>Even-split redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或者恢复的时候，这个状态元素列表会被按照并行度分为子列表，每个算子会得到一个子列表。这个子列表可能为空，或包含一个或多个元素。举个例子，如果使用并行性 1，算子的检查点状态包含元素 element1 和 element2，当将并行性增加到 2 时，element1 可能最终在算子实例 0 中，而 element2 将转到算子实例 1 中。</li>
<li>Union redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或恢复的时候，每个算子都会获得完整的状态元素列表。</li>
</ul>
<p>如下示例是一个有状态的 SinkFunction 使用 CheckpointedFunction 来发送到外部之前缓存数据，使用了Even-split策略。</p>
<p>下面是一个有状态的 SinkFunction 的示例，它使用 CheckpointedFunction 来缓存数据，然后再将这些数据发送到外部系统，使用了 Even-split 策略：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BufferingSink</span> <span class="keyword">implements</span> <span class="title">SinkFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;, <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threshold;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BufferingSink</span><span class="params">(<span class="keyword">int</span> threshold)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">        <span class="keyword">this</span>.bufferedElements = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context contex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        bufferedElements.add(value);</span><br><span class="line">        <span class="keyword">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class="line">                <span class="comment">//将数据发到外部系统</span></span><br><span class="line">            &#125;</span><br><span class="line">            bufferedElements.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        checkpointedState.clear();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123;</span><br><span class="line">            checkpointedState.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">            <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">"buffered-elements"</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class="line">                bufferedElements.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>initializeState 方法将 FunctionInitializationContext 作为参数，它用来初始化 non-keyed 状态。注意状态是如何初始化的，类似于 Keyed state，StateDescriptor 包含状态名称和有关状态值的类型的信息：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">    <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">"buffered-elements"</span>,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br></pre></td></tr></table></figure>
<h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><p>是一种受限的 CheckpointedFunction，只支持 List 风格的状态和 even-spit 的重分配策略。该接口里面的方法有：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w20ksxwj21160aoq2y.jpg" alt="undefined"></p>
<ul>
<li>snapshotState(): 获取函数的当前状态。状态必须返回此函数先前所有的调用结果。</li>
<li>restoreState(): 将函数或算子的状态恢复到先前 checkpoint 的状态。此方法在故障恢复后执行函数时调用。如果函数的特定并行实例无法恢复到任何状态，则状态列表可能为空。</li>
</ul>
<h3 id="Stateful-Source-Functions"><a href="#Stateful-Source-Functions" class="headerlink" title="Stateful Source Functions"></a>Stateful Source Functions</h3><p>与其他算子相比，有状态的 source 函数需要注意的地方更多，比如为了保证状态的更新和结果的输出原子性，用户必须在 source 的 context 上加锁。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Long</span>&gt; <span class="keyword">implements</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次语义的当前偏移量</span></span><br><span class="line">    <span class="keyword">private</span> Long offset = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//作业取消标志</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Object lock = ctx.getCheckpointLock();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            <span class="comment">//输出和状态更新是原子性的</span></span><br><span class="line">            <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">                ctx.collect(offset);</span><br><span class="line">                offset += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> checkpointTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(offset);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;Long&gt; state)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Long s : state)</span><br><span class="line">            offset = s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>或许有些算子想知道什么时候 checkpoint 全部做完了，可以参考使用 org.apache.flink.runtime.state.CheckpointListener 接口来实现，在该接口里面有 notifyCheckpointComplete 方法。</p>
<h3 id="Broadcast-State"><a href="#Broadcast-State" class="headerlink" title="Broadcast State"></a>Broadcast State</h3><h4 id="Broadcast-State-如何使用"><a href="#Broadcast-State-如何使用" class="headerlink" title="Broadcast State 如何使用"></a>Broadcast State 如何使用</h4><p>前面提到了两种 Operator state 支持的动态扩展方法：even-split redistribution 和 union redistribution。Broadcast State 是 Flink 支持的另一种扩展方式，它用来支持将某一个流的数据广播到下游所有的 Task 中，数据都会存储在下游 Task 内存中，接收到广播的数据流后就可以在操作中利用这些数据，一般我们会将一些规则数据进行这样广播下去，然后其他的 Task 也都能根据这些规则数据做配置，更常见的就是规则动态的更新，然后下游还能够动态的感知。</p>
<p>Broadcast state 的特点是：</p>
<ul>
<li>使用 Map 类型的数据结构</li>
<li>仅适用于同时具有广播流和非广播流作为数据输入的特定算子</li>
<li>可以具有多个不同名称的 Broadcast state</li>
</ul>
<p>那么我们该如何使用 Broadcast State 呢？下面通过一个例子来讲解一下，在这个例子中，我要广播的数据是监控告警的通知策略规则，然后下游拿到我这个告警通知策略去判断哪种类型的告警发到哪里去，该使用哪种方式来发，静默时间多长等。</p>
<p>第一个数据流是要处理的数据源，流中的对象具有告警或者恢复的事件，其中用一个 type 字段来标识哪个事件是告警，哪个事件是恢复，然后还有其他的字段标明是哪个集群的或者哪个项目的，简单代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;AlertEvent&gt; alertData = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">"alert"</span>,</span><br><span class="line">        <span class="keyword">new</span> AlertEventSchema(),</span><br><span class="line">        parameterTool.getProperties()));</span><br></pre></td></tr></table></figure>
<p>然后第二个数据流是要广播的数据流，它是告警通知策略数据（定时从 MySQL 中读取的规则表），简单代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Rule&gt; alarmdata = env.addSource(<span class="keyword">new</span> GetAlarmNotifyData());</span><br><span class="line"></span><br><span class="line"><span class="comment">// MapState 中保存 (RuleName, Rule) ，在描述类中指定 State name</span></span><br><span class="line">MapStateDescriptor&lt;String, Rule&gt; ruleStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">            <span class="string">"RulesBroadcastState"</span>,</span><br><span class="line">            BasicTypeInfo.STRING_TYPE_INFO,</span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Rule&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line"><span class="comment">// alarmdata 使用 MapStateDescriptor 作为参数广播，得到广播流</span></span><br><span class="line">BroadcastStream&lt;Rule&gt; ruleBroadcastStream = alarmdata.broadcast(ruleStateDescriptor);</span><br></pre></td></tr></table></figure>
<p>然后你要做的是将两个数据流进行连接，连接后再根据告警规则数据流的规则数据进行处理（这个告警的逻辑很复杂，我们这里就不再深入讲），伪代码大概如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alertData.connect(ruleBroadcastStream)</span><br><span class="line">    .process(</span><br><span class="line">        <span class="keyword">new</span> KeyedBroadcastProcessFunction&lt;AlertEvent, Rule&gt;() &#123;</span><br><span class="line">            <span class="comment">//根据告警规则的数据进行处理告警事件</span></span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//可能还有更多的操作</span></span><br></pre></td></tr></table></figure>
<p><code>alertData.connect(ruleBroadcastStream)</code> 该 connect 方法将两个流连接起来后返回一个 BroadcastConnectedStream 对象，如果对 BroadcastConnectedStream 不太清楚的可以回看下文章 <a href="https://gitbook.cn/gitchat/column/5dad4a20669f843a1a37cb4f/topic/5db6a754f6a6211cb9616526" target="_blank" rel="noopener">4如何使用 DataStream API 来处理数据？</a> 再次复习一下。BroadcastConnectedStream 调用 process() 方法执行处理逻辑，需要指定一个逻辑实现类作为参数，具体是哪种实现类取决于非广播流的类型：</p>
<ul>
<li>如果非广播流是 keyed stream，需要实现 KeyedBroadcastProcessFunction</li>
<li>如果非广播流是 non-keyed stream，需要实现 BroadcastProcessFunction</li>
</ul>
<p>那么该怎么获取这个 Broadcast state 呢，它需要通过上下文来获取:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctx.getBroadcastState(ruleStateDescriptor)</span><br></pre></td></tr></table></figure>
<h4 id="BroadcastProcessFunction-和-KeyedBroadcastProcessFunction"><a href="#BroadcastProcessFunction-和-KeyedBroadcastProcessFunction" class="headerlink" title="BroadcastProcessFunction 和 KeyedBroadcastProcessFunction"></a>BroadcastProcessFunction 和 KeyedBroadcastProcessFunction</h4><p>这两个抽象函数有两个相同的需要实现的接口:</p>
<ul>
<li>processBroadcastElement()：处理广播流中接收的数据元</li>
<li>processElement()：处理非广播流数据的方法</li>
</ul>
<p>用于处理非广播流是 non-keyed stream 的情况:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastProcessFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">BaseBroadcastProcessFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用于处理非广播流是 keyed stream 的情况</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedBroadcastProcessFunction</span>&lt;<span class="title">KS</span>, <span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到这两个接口提供的上下文对象有所不同。非广播方（processElement）使用 ReadOnlyContext，而广播方（processBroadcastElement）使用 Context。这两个上下文对象（简称 ctx）通用的方法接口有：</p>
<ul>
<li>访问 Broadcast state：ctx.getBroadcastState(MapStateDescriptor stateDescriptor)</li>
<li>查询数据元的时间戳：ctx.timestamp()</li>
<li>获取当前水印：ctx.currentWatermark()</li>
<li>获取当前处理时间：ctx.currentProcessingTime()</li>
<li>向旁侧输出（side-outputs）发送数据：ctx.output(OutputTag outputTag, X value)</li>
</ul>
<p>这两者不同之处在于对 Broadcast state 的访问限制：广播方对其具有读和写的权限（read-write），非广播方只有读的权限（read-only），为什么要这么设计呢，主要是为了保证 Broadcast state 在算子的所有并行实例中是相同的。由于 Flink 中没有跨任务的通信机制，在一个任务实例中的修改不能在并行任务间传递，而广播端在所有并行任务中都能看到相同的数据元，只对广播端提供可写的权限。同时要求在广播端的每个并行任务中，对接收数据的处理是相同的。如果忽略此规则会破坏 State 的一致性保证，从而导致不一致且难以诊断的结果。也就是说，processBroadcast() 的实现逻辑必须在所有并行实例中具有相同的确定性行为。</p>
<h4 id="使用-Broadcast-state-需要注意"><a href="#使用-Broadcast-state-需要注意" class="headerlink" title="使用 Broadcast state 需要注意"></a>使用 Broadcast state 需要注意</h4><p>前面介绍了 Broadcast state，并将 BroadcastProcessFunction 和 KeyedBroadcastProcessFunction 做了个对比，那么接下来强调一下使用 Broadcast state 时需要注意的事项：</p>
<ul>
<li>没有跨任务的通信，这就是为什么只有广播方可以修改 Broadcast state 的原因。</li>
<li>用户必须确保所有任务以相同的方式为每个传入的数据元更新 Broadcast state，否则可能导致结果不一致。</li>
<li>跨任务的 Broadcast state 中的事件顺序可能不同，虽然广播的元素可以保证所有元素都将转到所有下游任务，但元素到达的顺序可能不一致。因此，Broadcast state 更新不能依赖于传入事件的顺序。</li>
<li>所有任务都会把 Broadcast state 存入 checkpoint，虽然 checkpoint 发生时所有任务都具有相同的 Broadcast state。这是为了避免在恢复期间所有任务从同一文件中进行恢复（避免热点），然而代价是 state 在 checkpoint 时的大小成倍数（并行度数量）增加。</li>
<li>Flink 确保在恢复或改变并行度时不会有重复数据，也不会丢失数据。在具有相同或改小并行度后恢复的情况下，每个任务读取其状态 checkpoint。在并行度增大时，原先的每个任务都会读取自己的状态，新增的任务以循环方式读取前面任务的检查点。</li>
<li>不支持 RocksDB state backend，Broadcast state 在运行时保存在内存中。</li>
</ul>
<h3 id="Queryable-State"><a href="#Queryable-State" class="headerlink" title="Queryable State"></a>Queryable State</h3><p>Queryable State，顾名思义，就是可查询的状态。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w2cze3oj20pl0e1jrv.jpg" alt="undefined"></p>
<p>传统管理这些状态的方式是通过将计算后的状态结果存储在第三方 KV 存储中，然后由第三方应用去获取这些 KV 状态，但是在 Flink 种，现在有了 Queryable State，意味着允许用户对流的内部状态进行实时查询。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w2kwwogj20mn0bvjrw.jpg" alt="undefined"></p>
<p>那么就不再像其他流计算框架，需要将结果存储到其他外部存储系统才能够被查询到，这样我们就可以不再需要等待状态写入外部存储（这块可能是其他系统的主要瓶颈之一），甚至可以做到无需任何数据库就可以让用户直接查询到数据，这使得数据获取到的时间会更短，更及时，如果你有这块的需求（需要将某些状态数据进行展示，比如数字大屏），那么就强烈推荐使用 Queryable State。目前可查询的 state 主要针对可分区的 state，如 keyed state 等。</p>
<p>在 Flink 源码中，为此还专门有一个 module 来讲 Queryable State 呢！</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3663xoj216a08qt8v.jpg" alt="undefined"></p>
<p>那么我们该如何使用 Queryable State 呢？有如下两种方式 ：</p>
<ul>
<li>QueryableStateStream, 将 KeyedStream 转换为 QueryableStateStream，类似于 Sink，后续不能进行任何转换操作</li>
<li>StateDescriptor#setQueryable(String queryableStateName)，将 Keyed State 设置为可查询的 （不支持 Operator State）</li>
</ul>
<p>外部应用在查询 Flink 应用程序内部状态的时候要使用 QueryableStateClient, 提交异步查询请求来获取状态。如何使状态可查询呢，假如已经创建了一个状态可查询的 Job，并通过 JobClient 提交 Job，那么它在 Flink 内部的具体实现如下图（图片来自 <a href="http://vishnuviswanath.com/flink_queryable_state1.html" target="_blank" rel="noopener">Queryable States in ApacheFlink - How it works</a>）所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3h8lofj212w0x776i.jpg" alt="undefined"></p>
<p>上面讲解了让 State 可查询的原理，如果要在 Flink 集群中使用的话，首先得将 Flink 安装目录下 opt 里面的 <code>flink-queryable-state-runtime_2.11-1.9.0.jar</code> 复制到 lib 目录下，默认 lib 目录是不包含这个 jar 的。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w3pegglj21tg1aaqbx.jpg" alt="undefined"></p>
<p>然后你可以像下面这样操作让状态可查询：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reducing state</span></span><br><span class="line">ReducingStateDescriptor&lt;Tuple2&lt;Integer, Long&gt;&gt; reducingState = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">"zhisheng"</span>,</span><br><span class="line">        <span class="keyword">new</span> SumReduce(),</span><br><span class="line">        source.getType());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> String queryName = <span class="string">"zhisheng"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> QueryableStateStream&lt;Integer, Tuple2&lt;Integer, Long&gt;&gt; queryableState =</span><br><span class="line">        dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Integer, Long&gt;, Integer&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4126824763829132959L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Tuple2&lt;Integer, Long&gt; value)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).asQueryableState(queryName, reducingState);</span><br></pre></td></tr></table></figure>
<p>除了上面的 Reducing，你还可以使用 ValueState、FoldingState，还可以直接通过asQueryableState(queryName），注意不支持 ListState，调用 asQueryableState 方法后会返回 QueryableStateStream，接着无需再做其他操作。</p>
<p>那么用户如果定义了 Queryable State 的话，该怎么来查询对应的状态呢？下面来看看具体逻辑：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga1w44qz7bj212w0x7wgo.jpg" alt="undefined"></p>
<p>简单来说，当用户在 Job 中定义了 queryable state 之后，就可以在外部通过QueryableStateClient 来查询对应的状态实时值，你可以创建如下方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 Queryable State Client</span></span><br><span class="line">QueryableStateClient client = <span class="keyword">new</span> QueryableStateClient(host, port);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">QueryableStateClient</span><span class="params">(<span class="keyword">final</span> InetAddress remoteAddress, <span class="keyword">final</span> <span class="keyword">int</span> remotePort)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.client = <span class="keyword">new</span> Client&lt;&gt;(</span><br><span class="line">            <span class="string">"Queryable State Client"</span>, <span class="number">1</span>,</span><br><span class="line">            messageSerializer, <span class="keyword">new</span> DisabledKvStateRequestStats());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 QueryableStateClient 中有几个不同参数的 getKvState 方法，参数可有 JobID、queryableStateName、key、namespace、keyTypeInfo、namespaceTypeInfo、StateDescriptor，其实内部最后调用的是一个私有的 getKvState 方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;KvStateResponse&gt; <span class="title">getKvState</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobID jobId, <span class="keyword">final</span> String queryableStateName,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">int</span> keyHashCode, <span class="keyword">final</span> <span class="keyword">byte</span>[] serializedKeyAndNamespace)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//构造 KV state 查询的请求</span></span><br><span class="line">    KvStateRequest request = <span class="keyword">new</span> KvStateRequest(jobId, queryableStateName, keyHashCode, serializedKeyAndNamespace);</span><br><span class="line">    <span class="comment">//这个 client 是在构造 QueryableStateClient 中赋值的，这个 client 是 Client&lt;KvStateRequest, KvStateResponse&gt;，发送请求后会返回 CompletableFuture&lt;KvStateResponse&gt;</span></span><br><span class="line">    <span class="keyword">return</span> client.sendRequest(remoteAddress, request);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 Flink 源码中专门有一个 QueryableStateOptions 类来设置可查询状态相关的配置，有如下这些配置。</p>
<p>服务器端：</p>
<ul>
<li>queryable-state.proxy.ports：可查询状态代理的服务器端口范围的配置参数，默认是 9069</li>
<li>queryable-state.proxy.network-threads：客户端代理的网络线程数，默认是 0</li>
<li>queryable-state.proxy.query-threads：客户端代理的异步查询线程数，默认是 0</li>
<li>queryable-state.server.ports：可查询状态服务器的端口范围，默认是 9067</li>
<li>queryable-state.server.network-threads：KvState 服务器的网络线程数</li>
<li>queryable-state.server.query-threads：KvStateServerHandler 的异步查询线程数</li>
<li>queryable-state.enable：是否启用可查询状态代理和服务器</li>
</ul>
<p>客户端：</p>
<ul>
<li>queryable-state.client.network-threads：KvState 客户端的网络线程数</li>
</ul>
<p><strong>注意</strong>：</p>
<p>可查询状态的生命周期受限于 Job 的生命周期，例如，任务在启动时注册可查询状态，在清理的时候会注销它。在未来的版本中，可能会将其解耦，以便在任务完成后仍可以允许查询到任务的状态。</p>
<h3 id="小结与反思-18"><a href="#小结与反思-18" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节一开始讲解了 State 出现的原因，接着讲解了 Flink 中的 State 分类，然后对 Flink 中的每种 State 做了详细的讲解，希望可以好好消化这节的内容。你对本节的内容有什么不理解的地方吗？在使用 State 的过程中有遇到什么问题吗？</p>
<h2 id="二十一、如何选择-Flink-状态后端存储"><a href="#二十一、如何选择-Flink-状态后端存储" class="headerlink" title="二十一、如何选择 Flink 状态后端存储?"></a>二十一、如何选择 Flink 状态后端存储?</h2><h3 id="State-Backends"><a href="#State-Backends" class="headerlink" title="State Backends"></a>State Backends</h3><p>当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，刚好 Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外，在 Flink 安装路径下 conf 目录中的 flink-conf.yaml 配置文件中也有状态后端存储相关的配置，为此在 Flink 源码中还特有一个 CheckpointingOptions 类来控制 state 存储的相关配置，该类中有如下配置：</p>
<ul>
<li>state.backend: 用于存储和进行状态 checkpoint 的状态后端存储方式，无默认值</li>
<li>state.checkpoints.num-retained: 要保留的已完成 checkpoint 的最大数量，默认值为 1</li>
<li>state.backend.async: 状态后端是否使用异步快照方法，默认值为 true</li>
<li>state.backend.incremental: 状态后端是否创建增量检查点，默认值为 false</li>
<li>state.backend.local-recovery: 状态后端配置本地恢复，默认情况下，本地恢复被禁用</li>
<li>taskmanager.state.local.root-dirs: 定义存储本地恢复的基于文件的状态的目录</li>
<li>state.savepoints.dir: 存储 savepoints 的目录</li>
<li>state.checkpoints.dir: 存储 checkpoint 的数据文件和元数据</li>
<li>state.backend.fs.memory-threshold: 状态数据文件的最小大小，默认值是 1024</li>
</ul>
<p>虽然配置这么多，但是，Flink 还支持基于每个 Job 单独设置状态后端存储，方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> MemoryStateBackend());  <span class="comment">//设置堆内存存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));   //设置文件存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22tyfs6dj212w0dwaa2.jpg" alt="undefined"></p>
<p>上面三种方式取一种就好了。但是有三种方式，我们该如何去挑选用哪种去存储状态呢？下面讲讲这三种的特点以及该如何选择。</p>
<h3 id="如何使用-MemoryStateBackend-及剖析"><a href="#如何使用-MemoryStateBackend-及剖析" class="headerlink" title="如何使用 MemoryStateBackend 及剖析"></a>如何使用 MemoryStateBackend 及剖析</h3><p>如果 Job 没有配置指定状态后端存储的话，就会默认采取 MemoryStateBackend 策略。如果你细心的话，可以从你的 Job 中看到类似日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-04-28 00:16:41.892 [Sink: zhisheng (1/4)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask  - No state backend has been configured, using default (Memory / Job Manager) MemoryStateBackend (data in heap memory / checkpoints to Job Manager) (checkpoints: &apos;null&apos;, savepoints: &apos;null&apos;, asynchronous: TRUE, maxStateSize: 5242880)</span><br></pre></td></tr></table></figure>
<p>上面日志的意思就是说如果没有配置任何状态存储，使用默认的 MemoryStateBackend 策略，这种状态后端存储把数据以内部对象的形式保存在 Task Managers 的内存（JVM 堆）中，当应用程序触发 checkpoint 时，会将此时的状态进行快照然后存储在 Job Manager 的内存中。因为状态是存储在内存中的，所以这种情况会有点限制，比如：</p>
<ul>
<li>不太适合在生产环境中使用，仅用于本地测试的情况较多，主要适用于状态很小的 Job，因为它会将状态最终存储在 Job Manager 中，如果状态较大的话，那么会使得 Job Manager 的内存比较紧张，从而导致 Job Manager 会出现 OOM 等问题，然后造成连锁反应使所有的 Job 都挂掉，所以 Job 的状态与之前的 Checkpoint 的数据所占的内存要小于 JobManager 的内存。</li>
<li>每个单独的状态大小不能超过最大的 DEFAULT<em>MAX</em>STATE_SIZE(5MB)，可以通过构造 MemoryStateBackend 参数传入不同大小的 maxStateSize。</li>
<li>Job 的操作符状态和 keyed 状态加起来都不要超过 RPC 系统的默认配置 10 MB，虽然可以修改该配置，但是不建议去修改。</li>
</ul>
<p>另外就是 MemoryStateBackend 支持配置是否是异步快照还是同步快照，它有一个字段 asynchronousSnapshots 来表示，可选值有：</p>
<ul>
<li>TRUE（true 代表使用异步的快照，这样可以避免因快照而导致数据流处理出现阻塞等问题）</li>
<li>FALSE（同步）</li>
<li>UNDEFINED（默认值）</li>
</ul>
<p>在构造 MemoryStateBackend 的默认函数时是使用的 UNDEFINED，而不是异步：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//使用的是 UNDEFINED</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>网上有人说默认是异步的，这里给大家解释清楚一下，从上面的那条日志打印的确实也是表示异步，但是前提是你对 State 无任何操作，我跟了下源码，当你没有配置任何的 state 时，它是会在 StateBackendLoader 类中通过 MemoryStateBackendFactory 来创建的 state 的。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22u88wmij21k60z0jsl.jpg" alt="undefined"></p>
<p>继续跟进 MemoryStateBackendFactory 可以发现他这里创建了一个 MemoryStateBackend 实例并通过 configure 方法进行配置，大概流程代码是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//MemoryStateBackendFactory 类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">createFromConfig</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend().configure(config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//MemoryStateBackend 类中的 config 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> MemoryStateBackend <span class="title">configure</span><span class="params">(Configuration config, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MemoryStateBackend(<span class="keyword">this</span>, config, classLoader);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//私有的构造方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">MemoryStateBackend</span><span class="params">(MemoryStateBackend original, Configuration configuration, ClassLoader classLoader)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.asynchronousSnapshots = original.asynchronousSnapshots.resolveUndefined(</span><br><span class="line">            configuration.getBoolean(CheckpointingOptions.ASYNC_SNAPSHOTS));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据 CheckpointingOptions 类中的 ASYNC_SNAPSHOTS 参数进行设置的</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ConfigOption&lt;Boolean&gt; ASYNC_SNAPSHOTS = ConfigOptions</span><br><span class="line">        .key(<span class="string">"state.backend.async"</span>)</span><br><span class="line">        .defaultValue(<span class="keyword">true</span>) <span class="comment">//默认值就是 true，代表异步</span></span><br><span class="line">        .withDescription(...)</span><br></pre></td></tr></table></figure>
<p>可以发现最终是通过读取 <code>state.backend.async</code> 参数的默认值（true）来配置是否要异步的进行快照，但是如果你手动配置 MemoryStateBackend 的话，利用无参数的构造方法，那么就不是默认异步，如果想使用异步的话，需要利用下面这个构造函数（需要传入一个 boolean 值，true 代表异步，false 代表同步）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(<span class="keyword">boolean</span> asynchronousSnapshots)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.fromBoolean(asynchronousSnapshots));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果你再细看了这个 MemoryStateBackend 类的话，那么你可能会发现这个构造函数：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">MemoryStateBackend</span><span class="params">(@Nullable String checkpointPath, @Nullable String savepointPath)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(checkpointPath, savepointPath, DEFAULT_MAX_STATE_SIZE, TernaryBoolean.UNDEFINED);<span class="comment">//需要你传入 checkpointPath 和 savepointPath</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个也是用来创建一个 MemoryStateBackend 的，它需要传入的参数是两个路径（checkpointPath、savepointPath），其中 checkpointPath 是写入 checkpoint 元数据的路径，savepointPath 是写入 savepoint 的路径。</p>
<p>这个来看看 MemoryStateBackend 的继承关系图可以更明确的知道它是继承自 AbstractFileStateBackend，然后 AbstractFileStateBackend 这个抽象类就是为了能够将状态存储中的数据或者元数据进行文件存储的。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22uh2q3hj21120luweq.jpg" alt="undefined"></p>
<p>所以 FsStateBackend 和 MemoryStateBackend 都会继承该类。</p>
<h3 id="如何使用-FsStateBackend-及剖析"><a href="#如何使用-FsStateBackend-及剖析" class="headerlink" title="如何使用 FsStateBackend 及剖析"></a>如何使用 FsStateBackend 及剖析</h3><p>这种状态后端存储也是将工作状态存储在 Task Manager 中的内存（JVM 堆）中，但是 checkpoint 的时候，它和 MemoryStateBackend 不一样，它是将状态存储在文件（可以是本地文件，也可以是 HDFS）中，这个文件具体是哪种需要配置，比如：”hdfs://namenode:40010/flink/checkpoints” 或 “file://flink/checkpoints” (通常使用 HDFS 比较多，如果是使用本地文件，可能会造成 Job 恢复的时候找不到之前的 checkkpoint，因为 Job 重启后如果由调度器重新分配在不同的机器的 Task Manager 执行时就会导致这个问题，所以还是建议使用 HDFS 或者其他的分布式文件系统)。</p>
<p>同样 FsStateBackend 也是支持通过 asynchronousSnapshots 字段来控制是使用异步还是同步来进行 checkpoint 的，异步可以避免在状态 checkpoint 时阻塞数据流的处理，然后还有一点的就是在 FsStateBackend 有个参数 fileStateThreshold，如果状态大小比 MAX<em>FILE</em>STATE_THRESHOLD（1MB） 小的话，那么会将状态数据直接存储在 meta data 文件中，而不是存储在配置的文件中（避免出现很小的状态文件），如果该值为 “-1” 表示尚未配置，在这种情况下会使用默认值（1024，该默认值可以通过 <code>state.backend.fs.memory-threshold</code> 来配置）。</p>
<p>那么我们该什么时候使用 FsStateBackend 呢？</p>
<ul>
<li>如果你要处理大状态，长窗口等有状态的任务，那么 FsStateBackend 就比较适合</li>
<li>使用分布式文件系统，如 HDFS 等，这样 failover 时 Job 的状态可以恢复</li>
</ul>
<p>使用 FsStateBackend 需要注意的地方有什么呢？</p>
<ul>
<li>工作状态仍然是存储在 Task Manager 中的内存中，虽然在 Checkpoint 的时候会存在文件中，所以还是得注意这个状态要保证不超过 Task Manager 的内存</li>
</ul>
<h3 id="如何使用-RocksDBStateBackend-及剖析"><a href="#如何使用-RocksDBStateBackend-及剖析" class="headerlink" title="如何使用 RocksDBStateBackend 及剖析"></a>如何使用 RocksDBStateBackend 及剖析</h3><p>RocksDBStateBackend 和上面两种都有点不一样，RocksDB 是一种嵌入式的本地数据库，它会在本地文件系统中维护状态，KeyedStateBackend 等会直接写入本地 RocksDB 中，它还需要配置一个文件系统（一般是 HDFS），比如 <code>hdfs://namenode:40010/flink/checkpoints</code>，当触发 checkpoint 的时候，会把整个 RocksDB 数据库复制到配置的文件系统中去，当 failover 时从文件系统中将数据恢复到本地。</p>
<p>在 Flink 源码中，你也可以看见专门有一个 module 是 flink-statebackend-rocksdb 来放在 flink-state-backends 下面，在后面的版本中可能还会加上 flink-statebackend-heap-spillable 模块用来当作一种新的状态后端存储，感兴趣可以去官网的计划中查看。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22urjlpjj213y0jet9o.jpg" alt="undefined"></p>
<p>足以证明了官方其实也是推荐使用 RocksDB 来作为状态的后端存储，为什么呢：</p>
<ul>
<li>state 直接存放在 RocksDB 中，不需要存在内存中，这样就可以减少 Task Manager 的内存压力，如果是存内存的话大状态的情况下会导致 GC 次数比较多，同时还能在 checkpoint 时将状态持久化到远端的文件系统，那么就比较适合在生产环境中使用</li>
<li>RocksDB 本身支持 checkpoint 功能</li>
<li>RocksDBStateBackend 支持增量的 checkpoint，在 RocksDBStateBackend 中有一个字段 enableIncrementalCheckpointing 来确认是否开启增量的 checkpoint，默认是不开启的，在 CheckpointingOptions 类中有个 state.backend.incremental 参数来表示，增量 checkpoint 非常使用于超大状态的场景。</li>
</ul>
<p>讲了这么多 RocksDBStateBackend 的好处，那么该如何去使用呢，可以来看看 RocksDBStateBackend 这个类的相关属性以及构造函数。</p>
<p><strong>属性</strong>：</p>
<ul>
<li>checkpointStreamBackend：用于创建 checkpoint 流的状态后端</li>
<li>localRocksDbDirectories：RocksDB 目录的基本路径，默认是 Task Manager 的临时目录</li>
<li>enableIncrementalCheckpointing：是否增量 checkpoint</li>
<li>numberOfTransferingThreads：用于传输(下载和上传)状态的线程数量，默认为 1</li>
<li>enableTtlCompactionFilter：是否启用压缩过滤器来清除带有 TTL 的状态</li>
</ul>
<p><strong>构造函数</strong>：</p>
<ul>
<li>RocksDBStateBackend(String checkpointDataUri)：单参数，只传入一个路径</li>
<li>RocksDBStateBackend(String checkpointDataUri, boolean enableIncrementalCheckpointing)：两个参数，传入 checkpoint 数据目录路径和是否开启增量 checkpoint</li>
<li>RocksDBStateBackend(StateBackend checkpointStreamBackend)：传入一种 StateBackend</li>
<li>RocksDBStateBackend(StateBackend checkpointStreamBackend, TernaryBoolean enableIncrementalCheckpointing)：传入一种 StateBackend 和是否开启增量 checkpoint</li>
<li>RocksDBStateBackend(RocksDBStateBackend original, Configuration config, ClassLoader classLoader)：私有的构造方法，用于重新配置状态后端</li>
</ul>
<p>既然知道这么多构造函数了，那么使用就很简单了，根据你的场景考虑使用哪种构造函数创建 RocksDBStateBackend 对象就行了，然后通过 <code>env.setStateBackend()</code> 传入对象实例就行，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure>
<p>那么在使用 RocksDBStateBackend 时该注意什么呢：</p>
<ul>
<li>当使用 RocksDB 时，状态大小将受限于磁盘可用空间的大小</li>
<li>状态存储在 RocksDB 中，整个更新和获取状态的操作都是要通过序列化和反序列化才能完成的，跟状态直接存储在内存中，性能可能会略低些</li>
<li>如果你应用程序的状态很大，那么使用 RocksDB 无非是最佳的选择</li>
</ul>
<p>另外在 Flink 源码中有一个专门的 RocksDBOptions 来表示 RocksDB 相关的配置：</p>
<ul>
<li>state.backend.rocksdb.localdir：本地目录(在 Task Manager 上)，RocksDB 将其文件放在其中</li>
<li>state.backend.rocksdb.timer-service.factory：定时器服务实现，默认值是 HEAP</li>
<li>state.backend.rocksdb.checkpoint.transfer.thread.num：用于在后端传输(下载和上载)文件的线程数，默认是 1</li>
<li>state.backend.rocksdb.ttl.compaction.filter.enabled：是否启用压缩过滤器来清除带有 TTL 的状态，默认值是 false</li>
</ul>
<h3 id="如何选择状态后端存储？"><a href="#如何选择状态后端存储？" class="headerlink" title="如何选择状态后端存储？"></a>如何选择状态后端存储？</h3><p>通过上面三种 State Backends 的介绍，让大家了解了状态存储有哪些种类，然后对每种状态存储是该如何使用的、它们内部的实现、使用场景、需要注意什么都细讲了一遍，三种存储方式各有特点，可以满足不同场景的需求，通常来说，在开发程序之前，我们要先分析自己 Job 的场景和状态大小的预测，然后根据预测来进行选择何种状态存储，如果拿捏不定的话，建议先在测试环境进行测试，只有选择了正确的状态存储后端，这样才能够保证后面自己的 Job 在生产环境能够稳定的运行。</p>
<h3 id="小结与反思-19"><a href="#小结与反思-19" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节对 Flink 的 State 做了一个很详尽的讲解，不管是从使用方面，还从原理进行深度分析，涉及的有 State 的分类如 Keyed State、Operator State、Raw State、 Managed State、Broadcast State 等。还讲了如何让 State 进行可查询的配置，State 的过期，最后还讲了 State 的三种常见的后端存储方式，并分析了三者适合于哪种场景，同时也都对这几种方式的源码进行解读，目的就是让大家对 State 彻底的了解使用方式和原理实现。</p>
<p>下面一图来看看 State 在 Flink 中的整体结构：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga22v2xtrmj248s16mtc3.jpg" alt="undefined"></p>
<h2 id="二十二、Flink-Checkpoint-和-Savepoint-区别及其如何配置使用？"><a href="#二十二、Flink-Checkpoint-和-Savepoint-区别及其如何配置使用？" class="headerlink" title="二十二、Flink Checkpoint 和 Savepoint 区别及其如何配置使用？"></a>二十二、Flink Checkpoint 和 Savepoint 区别及其如何配置使用？</h2><p>Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。本节主要讲述在 Flink 中 Checkpoint 和 Savepoint 的使用方式及它们之间的区别。</p>
<h3 id="Checkpoint-介绍及使用"><a href="#Checkpoint-介绍及使用" class="headerlink" title="Checkpoint 介绍及使用"></a>Checkpoint 介绍及使用</h3><p>为了保障的容错，Flink 需要对状态进行快照。Flink 可以从 Checkpoint 中恢复流的状态和位置，从而使得应用程序发生故障后能够得到与无故障执行相同的语义。</p>
<p>Flink 的 Checkpoint 有以下先决条件：</p>
<ul>
<li>需要具有持久性且支持重放一定时间范围内数据的数据源。例如：Kafka、RabbitMQ 等。这里为什么要求支持重放一定时间范围内的数据呢？因为 Flink 的容错机制决定了，当 Flink 任务失败后会自动从最近一次成功的 Checkpoint 处恢复任务，此时可能需要把任务失败前消费的部分数据再消费一遍，所以必须要求数据源支持重放。假如一个Flink 任务消费 Kafka 并将数据写入到 MySQL 中，任务从 Kafka 读取到数据，还未将数据输出到 MySQL 时任务突然失败了，此时如果 Kafka 不支持重放，就会造成这部分数据永远丢失了。支持重放数据的数据源可以保障任务消费失败后，能够重新消费来保障任务不丢数据。</li>
<li>需要一个能保存状态的持久化存储介质，例如：HDFS、S3 等。当 Flink 任务失败后，自动从 Checkpoint 处恢复，但是如果 Checkpoint 时保存的状态信息快照全丢了，那就会影响 Flink 任务的正常恢复。就好比我们看书时经常使用书签来记录当前看到的页码，当下次看书时找到书签的位置继续阅读即可，但是如果书签三天两头经常丢，那我们就无法通过书签来恢复阅读。</li>
</ul>
<p>Flink 中 Checkpoint 是默认关闭的，对于需要保障 At Least Once 和 Exactly Once 语义的任务，强烈建议开启 Checkpoint，对于丢一小部分数据不敏感的任务，可以不开启 Checkpoint，例如：一些推荐相关的任务丢一小部分数据并不会影响推荐效果。下面来介绍 Checkpoint 具体如何使用。</p>
<p>首先调用 StreamExecutionEnvironment 的方法 enableCheckpointing(n) 来开启 Checkpoint，参数 n 以毫秒为单位表示 Checkpoint 的时间间隔。Checkpoint 配置相关的 Java 代码如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 开启 Checkpoint，每 1000毫秒进行一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 语义设置为 EXACTLY_ONCE</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckPoint 的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 同一时间，只允许 有 1 个 Checkpoint 在发生</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 两次 Checkpoint 之间的最小时间间隔为 500 毫秒</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当 Flink 任务取消时，保留外部保存的 CheckPoint 信息</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 作业最多允许 Checkpoint 失败 1 次（flink 1.9 开始支持）</span></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Checkpoint 失败后，整个 Flink 任务也会失败（flink 1.9 之前）</span></span><br><span class="line">env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(<span class="keyword">true</span>)</span><br></pre></td></tr></table></figure>
<p>以上 Checkpoint 相关的参数描述如下所示：</p>
<ul>
<li>Checkpoint 语义：EXACTLY<em>ONCE 或 AT</em>LEAST<em>ONCE，EXACTLY</em>ONCE 表示所有要消费的数据被恰好处理一次，即所有数据既不丢数据也不重复消费；AT<em>LEAST</em>ONCE 表示要消费的数据至少处理一次，可能会重复消费。</li>
<li>Checkpoint 超时时间：如果 Checkpoint 时间超过了设定的超时时间，则 Checkpoint 将会被终止。</li>
<li>同时进行的 Checkpoint 数量：默认情况下，当一个 Checkpoint 在进行时，JobManager 将不会触发下一个 Checkpoint，但 Flink 允许多个 Checkpoint 同时在发生。</li>
<li>两次 Checkpoint 之间的最小时间间隔：从上一次 Checkpoint 结束到下一次 Checkpoint 开始，中间的间隔时间。例如，env.enableCheckpointing(60000) 表示 1 分钟触发一次 Checkpoint，同时再设置两次 Checkpoint 之间的最小时间间隔为 30 秒，假如任务运行过程中一次 Checkpoint 就用了50s，那么等 Checkpoint 结束后，理论来讲再过 10s 就要开始下一次 Checkpoint 了，但是由于设置了最小时间间隔为30s，所以需要再过 30s 后，下次 Checkpoint 才开始。注：如果配置了该参数就决定了同时进行的 Checkpoint 数量只能为 1。</li>
<li>当任务被取消时，外部 Checkpoint 信息是否被清理：Checkpoint 在默认的情况下仅用于恢复运行失败的 Flink 任务，当任务手动取消时 Checkpoint 产生的状态信息并不保留。当然可以通过该配置来保留外部的 Checkpoint 状态信息，这些被保留的状态信息在作业手动取消时不会被清除，这样就可以使用该状态信息来恢复 Flink 任务，对于需要从状态恢复的任务强烈建议配置为外部 Checkpoint 状态信息不清理。可选择的配置项为：</li>
<li>ExternalizedCheckpointCleanup.RETAIN<em>ON</em>CANCELLATION：当作业手动取消时，保留作业的 Checkpoint 状态信息。注意，这种情况下，需要手动清除该作业保留的 Checkpoint 状态信息，否则这些状态信息将永远保留在外部的持久化存储中。</li>
<li>ExternalizedCheckpointCleanup.DELETE<em>ON</em>CANCELLATION：当作业取消时，Checkpoint 状态信息会被删除。仅当作业失败时，作业的 Checkpoint 才会被保留用于任务恢复。</li>
<li>任务失败，当有较新的 Savepoint 时，作业是否回退到 Checkpoint 进行恢复：默认情况下，当 Savepoint 比 Checkpoint 较新时，任务会从 Savepoint 处恢复。</li>
<li>作业可以容忍 Checkpoint 失败的次数：默认值为 0，表示不能接受 Checkpoint 失败。</li>
</ul>
<p>关于 Checkpoint 时，状态后端相关的配置请参阅本课 4.2 节。</p>
<h3 id="Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用"><a href="#Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用" class="headerlink" title="Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用"></a>Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用</h3><p>Savepoint 与 Checkpoint 类似，同样需要把状态信息存储到外部介质，当作业失败时，可以从外部存储中恢复。Savepoint 与 Checkpoint 的区别很多：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Checkpoint</th>
<th style="text-align:center">Savepoint</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">由 Flink 的 JobManager 定时自动触发并管理</td>
<td style="text-align:center">由用户手动触发并管理</td>
</tr>
<tr>
<td style="text-align:center">主要用于任务发生故障时，为任务提供给自动恢复机制</td>
<td style="text-align:center">主要用户升级 Flink 版本、修改任务的逻辑代码、调整算子的并行度，且必须手动恢复</td>
</tr>
<tr>
<td style="text-align:center">当使用 RocksDBStateBackend 时，支持增量方式对状态信息进行快照</td>
<td style="text-align:center">仅支持全量快照</td>
</tr>
<tr>
<td style="text-align:center">Flink 任务停止后，Checkpoint 的状态快照信息默认被清除</td>
<td style="text-align:center">一旦触发 Savepoint，状态信息就被持久化到外部存储，除非用户手动删除</td>
</tr>
<tr>
<td style="text-align:center">Checkpoint 设计目标：轻量级且尽可能快地恢复任务</td>
<td style="text-align:center">Savepoint 的生成和恢复成本会更高一些，Savepoint 更多地关注代码的可移植性和兼容任务的更改操作</td>
</tr>
</tbody>
</table>
<p>除了上述描述外，Checkpoint 和 Savepoint 在当前的实现上基本相同。</p>
<p>强烈建议在程序中给算子分配 Operator ID，以便来升级程序。主要通过 <code>uid(String)</code> 方法手动指定算子的 ID ，这些 ID 将用于恢复每个算子的状态。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.</span><br><span class="line">  <span class="comment">// Stateful source (e.g. Kafka) with ID</span></span><br><span class="line">  .addSource(<span class="keyword">new</span> StatefulSource())</span><br><span class="line">  .uid(<span class="string">"source-id"</span>) <span class="comment">// ID for the source operator</span></span><br><span class="line">  .shuffle()</span><br><span class="line">  <span class="comment">// Stateful mapper with ID</span></span><br><span class="line">  .map(<span class="keyword">new</span> StatefulMapper())</span><br><span class="line">  .uid(<span class="string">"mapper-id"</span>) <span class="comment">// ID for the mapper</span></span><br><span class="line">  <span class="comment">// Stateless printing sink</span></span><br><span class="line">  .print(); <span class="comment">// Auto-generated ID</span></span><br></pre></td></tr></table></figure>
<p>如果不为算子手动指定 ID，Flink 会为算子自动生成 ID。当 Flink 任务从 Savepoint 中恢复时，是按照 Operator ID 将快照信息与算子进行匹配的，只要这些 ID 不变，Flink 任务就可以从 Savepoint 中恢复。自动生成的 ID 取决于代码的结构，并且对代码更改比较敏感，因此强烈建议给程序中所有有状态的算子手动分配 Operator ID。如下左图所示，一个 Flink 任务包含了 算子 A 和 算子 B，代码中都未指定 Operator ID，所以 Flink 为 Task A 自动生成了 Operator ID 为 aaa，为 Task B 自动生成了 Operator ID 为 bbb，且 Savepoint 成功完成。但是在代码改动后，任务并不能从 Savepoint 中正常恢复，因为 Flink 为算子生成的 Operator ID 取决于代码结构，代码改动后可能会把算子 B 的 Operator ID 改变成 ccc，导致任务从 Savepoint 恢复时，SavePoint 中只有 Operator ID 为 aaa 和 bbb 的状态信息，算子 B 找不到 Operator ID 为 ccc 的状态信息，所以算子 B 不能正常恢复。这里如果在写代码时通过 <code>uid(String)</code> 手动指定了 Operator ID，就不会存在 上述问题了。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga234u9p5rj25eg1oyhaw.jpg" alt="undefined"></p>
<p>Savepoint 需要用户手动去触发，触发 Savepoint 的方式如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure>
<p>这将触发 ID 为 <code>:jobId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径，用户需要此路径来还原和删除 Savepoint 。</p>
<p>使用 YARN 触发 Savepoint 的方式如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory] -yid :yarnAppId</span><br></pre></td></tr></table></figure>
<p>这将触发 ID 为 <code>:jobId</code> 和 YARN 应用程序 ID <code>:yarnAppId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径。</p>
<p>使用 Savepoint 取消 Flink 任务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink cancel -s [:targetDirectory] :jobId</span><br></pre></td></tr></table></figure>
<p>这将自动触发 ID 为 <code>:jobid</code> 的作业进行 Savepoint，并在 Checkpoint 结束后取消该任务。此外，可以指定一个目标文件系统目录来存储 Savepoint 的状态信息，也可以在 flink 的 conf 目录下 flink-conf.yaml 中配置 state.savepoints.dir 参数来指定 Savepoint 的默认目录，触发 Savepoint 时，如果不指定目录则使用该默认目录。无论使用哪种方式配置，都需要保障配置的目录能被所有的 JobManager 和 TaskManager 访问。</p>
<h3 id="Checkpoint-流程"><a href="#Checkpoint-流程" class="headerlink" title="Checkpoint 流程"></a>Checkpoint 流程</h3><p>Flink 任务 Checkpoint 的详细流程如下所示：</p>
<p>\1. JobManager 端的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga2355t4kzj23i52i3du7.jpg" alt="undefined"></p>
<ol start="2">
<li>当 task 收到上游所有实例的 barrier 后，向自己的下游继续传递 barrier，然后自身同步进行快照，并将自己的状态异步写入到持久化存储中</li>
</ol>
<ul>
<li>如果是增量 Checkpoint，则只是把最新的一部分更新写入到外部持久化存储中</li>
<li>为了下游尽快进行 Checkpoint，所以 task 会先发送 barrier 到下游，自身再同步进行快照</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga235ip3bjj23op2j1qr6.jpg" alt="undefined"></p>
<blockquote>
<p>注：Task B 必须接收到上游 Task A 所有实例发送的 barrier 时，Task B 才能开始进行快照，这里有一个 barrier 对齐的概念，关于 barrier 对齐的详细介绍请参阅 9.5.1 节 Flink 内部如何保证 Exactly Once 中的 barrier 对齐部分</p>
</blockquote>
<ol start="3">
<li><p>当 task 将状态信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的CheckPointCoordinator，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除</p>
</li>
<li><p>如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束</p>
</li>
</ol>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga235uyrs8j23sr3937pe.jpg" alt="undefined"></p>
<p>如果对上述 Checkpoint 过程不理解，在后续 9.5 节 Flink 如何保障 Exactly Once 中会详细介绍 Flink 的 Checkpoint 过程以及为什么这么做。</p>
<h4 id="基于-RocksDB-的增量-Checkpoint-实现原理"><a href="#基于-RocksDB-的增量-Checkpoint-实现原理" class="headerlink" title="基于 RocksDB 的增量 Checkpoint 实现原理"></a>基于 RocksDB 的增量 Checkpoint 实现原理</h4><p>当使用 RocksDBStateBackend 时，增量 Checkpoint 是如何实现的呢？RocksDB 是一个基于 LSM 实现的 KV 数据库。LSM 全称 Log Structured Merge Trees，LSM 树本质是将大量的磁盘随机写操作转换成磁盘的批量写操作来极大地提升磁盘数据写入效率。一般 LSM Tree 实现上都会有一个基于内存的 MemTable 介质，所有的增删改操作都是写入到 MemTable 中，当 MemTable 足够大以后，将 MemTable 中的数据 flush 到磁盘中生成不可变且内部有序的 ssTable（Sorted String Table）文件，全量数据保存在磁盘的多个 ssTable 文件中。HBase 也是基于 LSM Tree 实现的，HBase 磁盘上的 HFile 就相当于这里的 ssTable 文件，每次生成的 HFile 都是不可变的而且内部有序的文件。基于 ssTable 不可变的特性，才实现了增量 Checkpoint，具体流程如下所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga2364ybmqj24lp1jdan6.jpg" alt="undefined"></p>
<p>第一次 Checkpoint 时生成的状态快照信息包含了两个 sstable 文件：sstable1 和 sstable2 及 Checkpoint1 的元数据文件 MANIFEST-chk1，所以第一次 Checkpoint 时需要将 sstable1、sstable2 和 MANIFEST-chk1 上传到外部持久化存储中。第二次 Checkpoint 时生成的快照信息为 sstable1、sstable2、sstable3 及元数据文件 MANIFEST-chk2，由于 sstable 文件的不可变特性，所以状态快照信息的 sstable1、sstable2 这两个文件并没有发生变化，sstable1、sstable2 这两个文件不需要重复上传到外部持久化存储中，因此第二次 Checkpoint 时，只需要将 sstable3 和 MANIFEST-chk2 文件上传到外部持久化存储中即可。这里只将新增的文件上传到外部持久化存储，也就是所谓的增量 Checkpoint。</p>
<p>基于 LSM Tree 实现的数据库为了提高查询效率，都需要定期对磁盘上多个 sstable 文件进行合并操作，合并时会将删除的、过期的以及旧版本的数据进行清理，从而降低 sstable 文件的总大小。图中可以看到第三次 Checkpoint 时生成的快照信息为sstable3、sstable4、sstable5 及元数据文件 MANIFEST-chk3， 其中新增了 sstable4 文件且 sstable1 和 sstable2 文件合并成 sstable5 文件，因此第三次 Checkpoint 时只需要向外部持久化存储上传 sstable4、sstable5 及元数据文件 MANIFEST-chk3。</p>
<p>基于 RocksDB 的增量 Checkpoint 从本质上来讲每次 Checkpoint 时只将本次 Checkpoint 新增的快照信息上传到外部的持久化存储中，依靠的是 LSM Tree 中 sstable 文件不可变的特性。对 LSM Tree 感兴趣的同学可以深入研究 RocksDB 或 HBase 相关原理及实现。</p>
<h3 id="状态如何从-Checkpoint-恢复"><a href="#状态如何从-Checkpoint-恢复" class="headerlink" title="状态如何从 Checkpoint 恢复"></a>状态如何从 Checkpoint 恢复</h3><p>在 Checkpoint 和 Savepoint 的比较过程中，知道了相比 Savepoint 而言，Checkpoint 的成本更低一些，但有些场景 Checkpoint 并不能完全满足我们的需求。所以在使用过程中，如果我们的需求能使用 Checkpoint 来解决优先使用 Checkpoint。当 Flink 任务中的一些依赖组件需要升级重启时，例如 hdfs、Kafka、yarn 升级或者 Flink 任务的 Sink 端对应的 MySQL、Redis 由于某些原因需要重启时，Flink 任务在这段时间也需要重启。但是由于 Flink 任务的代码并没有修改，所以 Flink 任务启动时可以从 Checkpoint 处恢复任务，此时必须配置取消 Flink 任务时保留外部存储的 Checkpoint 状态信息。从 Checkpoint 处恢复任务的命令如下所示，checkpointMetaDataPath 表示 Checkpoint 的目录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure>
<p>如果 flink on yarn 模式，启动命令如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure>
<p>问题来了，Flink 自动维护 Checkpoint，所以用户在这里并拿不到任务取消之前最后一次 Checkpoint 的目录。那怎么办呢？如下图所示，在任务取消之前，Flink 任务的 WebUI 中可以看到 Checkpoint 的目录，可以在取消任务之前将此目录保存起来，恢复时就可以从该目录恢复任务。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga236iyh3zj21p00vwn3t.jpg" alt="undefined"></p>
<p>上述方法最大缺陷就是用户的人力成本太高了，假如需要重启 100 个任务，难道需要用户手动维护 100 个任务的 Checkpoint 目录吗？可以做一个简单后台项目，用于管理和发布 Flink 任务，这里讲述一种通过 rest api 来获取 Checkpoint 目录的方式。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga236rp2l1j21de0j4mz0.jpg" alt="undefined"></p>
<p>如上图所示是 Flink JobManager 的 overview 页面，只需要将端口号后面的路径和参数按照以下替换即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node107.bigdata.dmp.local.com:35524/jobs/a1c70b36d19b3a9fc2713ba98cfc4a4f/metrics?get=lastCheckpointExternalPath</span><br></pre></td></tr></table></figure>
<p>调用以上接口，即可返回 a1c70b36d19b3a9fc2713ba98cfc4a4f 对应的 job 最后一次 Checkpoint 的目录，返回格式如下所示。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"id"</span>: <span class="string">"lastCheckpointExternalPath"</span>,</span><br><span class="line">    <span class="attr">"value"</span>: <span class="string">"hdfs:/user/flink/checkpoints/a1c70b36d19b3a9fc2713ba98cfc4a4f/chk-18"</span></span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>通过这种方式可以方便地维护所有 Flink 任务的 Checkpoint 目录，当然也可以通过 Metrics 的 Reporter 将 Checkpoint 目录保存到外部存储介质中，当任务需要从 Checkpoint 处恢复时，则从外部存储中读取到相应的 Checkpoint 目录。</p>
<p>当设置取消 Flink 任务保留外部的 Checkpoint 状态信息时，可能会带来的负面影响是：长期运行下去，hdfs 上将会保留很多废弃的且不再会使用的 Checkpoint 目录，所以如果开启了此配置，需要制定策略，定期清理那些不再会使用到的 Checkpoint 目录。</p>
<h3 id="状态如何从-Savepoint-恢复"><a href="#状态如何从-Savepoint-恢复" class="headerlink" title="状态如何从 Savepoint 恢复"></a>状态如何从 Savepoint 恢复</h3><p>如下所示，从 Savepoint 恢复任务的命令与 Checkpoint 恢复命令类似，savepointPath 表示 Savepoint 保存的目录，Savepoint 的各种触发方式都会返回 Savepoint 目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure>
<p>如果 flink on yarn 模式，启动命令如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure>
<p>默认情况下，恢复操作将尝试将 Savepoint 的所有状态映射到要还原的程序。如果删除了算子，则可以通过 <code>--allowNonRestoredState</code>（short：<code>-n</code>）选项跳过那些无法映射到新程序的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath -n xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure>
<p>如果从 Savepoint 恢复时，在任务中添加一个需要状态的新算子，会发生什么？向任务添加新算子时，它将在没有任何状态的情况下进行初始化，Savepoint 中包含每个有状态算子的状态，无状态算子根本不是 Savepoint 的一部分，新算子的行为类似于无状态算子。</p>
<p>如果在任务中对算子进行重新排序，会发生什么？如果给这些算子分配了 ID，它们将像往常一样恢复。如果没有分配 ID ，则有状态算子自动生成的 ID 很可能在重新排序后发生更改，这将导致无法从之前的 Savepoint 中恢复。</p>
<p>Savepoint 目录里的状态快照信息，目前不支持移动位置，由于技术原因元数据文件中使用绝对路径来保存数据。如果因为某种原因必须要移动 Savepoint 文件，那么有两种方案来实现：</p>
<ul>
<li>使用编辑器修改 Savepoint 的元数据文件信息，将旧路径改为新路径</li>
<li>可以使用 <code>SavepointV2Serializer</code> 类以编程方式读取、操作和重写元数据文件的新路径</li>
</ul>
<p>长期使用 Savepoint 同样要注意清理那些废弃 Savepoint 目录的问题。</p>
<h3 id="小结与反思-20"><a href="#小结与反思-20" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节主要介绍了 Checkpoint、Savepoint、Checkpoint 与 Savepoint 之间的区别以及 Checkpoint 和 Savepoint 具体如何使用并从 Checkpoint 和 Savepoint 中恢复任务。在 Checkpoint 过程中有一个同步做快照的过程，同步在快照期间 Flink 不会处理数据，为什么这里不能处理数据呢？如果做快照的同时处理数据会有什么影响呢？</p>
<h2 id="二十三、Flink-Table-amp-SQL-概念与通用-API"><a href="#二十三、Flink-Table-amp-SQL-概念与通用-API" class="headerlink" title="二十三、Flink Table &amp; SQL 概念与通用 API"></a>二十三、Flink Table &amp; SQL 概念与通用 API</h2><p>前面的内容都是讲解 DataStream 和 DataSet API 相关的，在 1.2.5 节中讲解 Flink API 时提及到 Flink 的高级 API——Table API&amp;SQL，本节将开始 Table&amp;SQL 之旅。</p>
<h3 id="新增-Blink-SQL-查询处理器"><a href="#新增-Blink-SQL-查询处理器" class="headerlink" title="新增 Blink SQL 查询处理器"></a>新增 Blink SQL 查询处理器</h3><p>在 Flink 1.9 版本中，合进了阿里巴巴开源的 Blink 版本中的大量代码，其中最重要的贡献就是 Blink SQL 了。在 Blink 捐献给 Apache Flink 之后，社区就致力于为 Table API&amp;SQL 集成 Blink 的查询优化器和 runtime。先来看下 1.8 版本的 Flink Table 项目结构如下图：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25c4l92ij21ig0pudi6.jpg" alt="undefined"></p>
<p>1.9 版本的 Flink Table 项目结构图如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25drrn61j21iu0wm0w4.jpg" alt="undefined"></p>
<p>可以发现新增了 flink-sql-parser、flink-table-planner-blink、flink-table-runtime-blink、flink-table-uber-blink 模块，对 Flink Table 模块的重构详细内容可以参考 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions" target="_blank" rel="noopener">FLIP-32</a>。这样对于 Java 和 Scala API 模块、优化器以及 runtime 模块来说，分层更清楚，接口更明确。</p>
<p>另外 flink-table-planner-blink 模块中实现了新的优化器接口，所以现在有两个插件化的查询处理器来执行 Table API&amp;SQL：1.9 以前的 Flink 处理器和新的基于 Blink 的处理器。基于 Blink 的查询处理器提供了更好的 SQL 覆盖率、支持更广泛的查询优化、改进了代码生成机制、通过调优算子的实现来提升批处理查询的性能。除此之外，基于 Blink 的查询处理器还提供了更强大的流处理能力，包括了社区一些非常期待的新功能（如维表 Join、TopN、去重）和聚合场景缓解数据倾斜的优化，以及内置更多常用的函数，具体可以查看 flink-table-runtime-blink 代码。目前整个模块的结构如下：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25e8yjstj217e0luaak.jpg" alt="undefined"></p>
<p>注意：两个查询处理器之间的语义和功能大部分是一致的，但未完全对齐，因为基于 Blink 的查询处理器还在优化中，所以在 1.9 版本中默认查询处理器还是 1.9 之前的版本。如果你想使用 Blink 处理器的话，可以在创建 TableEnvironment 时通过 EnvironmentSettings 配置启用。被选择的处理器必须要在正在执行的 Java 进程的类路径中。对于集群设置，默认两个查询处理器都会自动地加载到类路径中。如果要在 IDE 中运行一个查询，需要在项目中添加 planner 依赖。</p>
<h3 id="为什么选择-Table-API-amp-SQL？"><a href="#为什么选择-Table-API-amp-SQL？" class="headerlink" title="为什么选择 Table API&amp;SQL？"></a>为什么选择 Table API&amp;SQL？</h3><p>在 1.2 节中介绍了 Flink 的 API 是包含了 Table API&amp;SQL，在 1.3 节中也介绍了在 Flink 1.9 中阿里开源的 Blink 分支中的很强大的 SQL 功能合并进 Flink 主分支，另外通过阿里 Blink 相关的介绍，可以知道阿里在 SQL 功能这块是做了很多的工作。从前面章节的内容可以发现 Flink 的 DataStream/DataSet API 的功能已经很全并且很强大了，常见复杂的数据处理问题也都可以处理，那么社区为啥还在一直推广 Table API&amp;SQL 呢？</p>
<p>其实通过观察其它的大数据组件，就不会好奇了，比如 Spark、Storm、Beam、Hive 、KSQL（面向 Kafka 的 SQL 引擎）、Elasticsearch、Phoenix（使用 SQL 进行 HBase 数据的查询）等，可以发现 SQL 已经成为各个大数据组件必不可少的数据查询语言，那么 Flink 作为一个大数据实时处理引擎，笔者对其支持 SQL 查询流数据也不足为奇了，但是还是来稍微介绍一下 Table API&amp;SQL。</p>
<p>Table API&amp;SQL 是一种关系型 API，用户可以像操作数据库一样直接操作流数据，而不再需要通过 DataStream API 来写很多代码完成计算需求，更不用手动去调优你写的代码，另外 SQL 最大的优势在于它是一门学习成本很低的语言，普及率很高，用户基数大，和其他的编程语言相比，它的入门相对简单。</p>
<p>除了上面的原因，还有一个原因是：可以借助 Table API&amp;SQL 统一流处理和批处理，因为在 DataStream/DataSet API 中，用户开发流作业和批作业需要去了解两种不同的 API，这对于公司有些开发能力不高的数据分析师来说，学习成本有点高，他们其实更擅长写 SQL 来分析。Table API&amp;SQL 做到了批与流上的查询具有同样的语法语义，因此不用改代码就能同时在批和流上执行。</p>
<p>总结来说，为什么选择 Table API&amp;SQL：</p>
<ul>
<li>声明式语言表达业务逻辑</li>
<li>无需代码编程——易于上手</li>
<li>查询能够被有效的优化</li>
<li>查询可以高效的执行</li>
</ul>
<h3 id="Flink-Table-项目模块"><a href="#Flink-Table-项目模块" class="headerlink" title="Flink Table 项目模块"></a>Flink Table 项目模块</h3><p>在上文中提及到 Flink Table 在 1.8 和 1.9 的区别，这里还是要再讲解一下这几个依赖，因为只有了解清楚了之后，我们在后面开发的时候才能够清楚挑选哪种依赖。它有如下几个模块：</p>
<ul>
<li>flink-table-common：table 中的公共模块，可以用于通过自定义 function，format 等来扩展 Table 生态系统</li>
<li>flink-table-api-java：支持使用 Java 语言，纯 Table＆SQL API</li>
<li>flink-table-api-scala：支持使用 Scala 语言，纯 Table＆SQL API</li>
<li>flink-table-api-java-bridge：支持使用 Java 语言，包含 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li>
<li>flink-table-api-scala-bridge：支持使用 Scala 语言，带有 DataStream/DataSet API 的 Table＆SQL API（推荐使用）</li>
<li>flink-sql-parser：SQL 语句解析层，主要依赖 calcite</li>
<li>flink-table-planner：Table 程序的 planner 和 runtime</li>
<li>flink-table-uber：将上诉模块打成一个 fat jar，在 lib 目录下</li>
<li>flink-table-planner-blink：Blink 的 Table 程序的 planner（阿里开源的版本）</li>
<li>flink-table-runtime-blink：Blink 的 Table 程序的 runtime（阿里开源的版本）</li>
<li>flink-table-uber-blink：将 Blink 版本的 planner 和 runtime 与前面模块（除 flink-table-planner 模块）打成一个 fat jar，在 lib 目录下</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga25eljzv0j21gg0cmdhq.jpg" alt="undefined"></p>
<ul>
<li>flink-sql-client：SQL 客户端</li>
</ul>
<h3 id="两种-planner-之间的区别"><a href="#两种-planner-之间的区别" class="headerlink" title="两种 planner 之间的区别"></a>两种 planner 之间的区别</h3><p>上面讲了两种不同的 planner 之间包含的模块有点区别，但是具体有什么区别如下所示：</p>
<ul>
<li>Blink planner 将批处理作业视为流的一种特殊情况。因此不支持 Table 和 DataSet 之间的转换，批处理作业会转换成 DataStream 程序，而不会转换成 DataSet 程序，流作业还是转换成 DataStream 程序。</li>
<li>Blink planner 不支持 BatchTableSource，而是使用有界的（bounded） StreamTableSource 代替它。</li>
<li>Blink planner 仅支持全新的 Catalog，不支持已经废弃的 ExternalCatalog。</li>
<li>以前的 planner 中 FilterableTableSource 的实现与现在的 Blink planner 有冲突，在以前的 planner 中是叠加 PlannerExpressions（在未来的版本中会移除），而在 Blink planner 中是 Expressions。</li>
<li>基于字符串的 KV 键值配置选项仅可以在 Blink planner 中使用。</li>
<li>PlannerConfig 的实现（CalciteConfig）在两种 planner 中不同。</li>
<li>Blink planner 会将多个 sink 优化在同一个 DAG 中（只在 TableEnvironment 中支持，StreamTableEnvironment 中不支持），而以前的 planner 是每个 sink 都有一个 DAG 中，相互独立的。</li>
<li>以前的 planner 不支持 catalog 统计，而 Blink planner 支持。</li>
</ul>
<p>在了解到了两种 planner 的区别后，接下来开始 Flink Table API&amp;SQL 之旅。</p>
<h3 id="添加项目依赖"><a href="#添加项目依赖" class="headerlink" title="添加项目依赖"></a>添加项目依赖</h3><p>因为在 Flink 1.9 版本中有两个 planner，所以得根据你使用的 planner 来选择对应的依赖，假设你选择的是最新的 Blink 版本，那么添加下面的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>如果是以前的 planner，则使用下面这个依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>如果要自定义 format 格式或者自定义 function，则需要添加 flink-table-common 依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="创建一个-TableEnvironment"><a href="#创建一个-TableEnvironment" class="headerlink" title="创建一个 TableEnvironment"></a>创建一个 TableEnvironment</h3><p>TableEnvironment 是 Table API 和 SQL 的统称，它负责的内容有：</p>
<ul>
<li>在内部的 catalog 注册 Table</li>
<li>注册一个外部的 catalog</li>
<li>执行 SQL 查询</li>
<li>注册用户自定义的 function</li>
<li>将 DataStream 或者 DataSet 转换成 Table</li>
<li>保持对 ExecutionEnvironment 和 StreamExecutionEnvironment 的引用</li>
</ul>
<p>Table 总是会绑定在一个指定的 TableEnvironment，不能在同一个查询中组合不同 TableEnvironment 的 Table，比如 join 或 union 操作。你可以使用下面的几种静态方法创建 TableEnvironment。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 StreamTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, EnvironmentSettings.newInstance().build());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, EnvironmentSettings settings)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, settings, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** <span class="doctag">@deprecated</span> */</span></span><br><span class="line"><span class="meta">@Deprecated</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamTableEnvironment <span class="title">create</span><span class="params">(StreamExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StreamTableEnvironmentImpl.create(executionEnvironment, EnvironmentSettings.newInstance().build(), tableConfig);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 BatchTableEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create(executionEnvironment, <span class="keyword">new</span> TableConfig());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> BatchTableEnvironment <span class="title">create</span><span class="params">(ExecutionEnvironment executionEnvironment, TableConfig tableConfig)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>你需要根据你的程序来使用对应的 TableEnvironment，是 BatchTableEnvironment 还是 StreamTableEnvironment。默认两个 planner 都是在 Flink 的安装目录下 lib 文件夹中存在的，所以应该在你的程序中指定使用哪种 planner。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Flink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line">ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Streaming query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line">StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">//或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Batch query</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure>
<p>如果在 lib 目录下只存在一个 planner，则可以使用 useAnyPlanner 来创建指定的 EnvironmentSettings。</p>
<h3 id="Table-API-amp-SQL-应用程序的结构"><a href="#Table-API-amp-SQL-应用程序的结构" class="headerlink" title="Table API&amp;SQL 应用程序的结构"></a>Table API&amp;SQL 应用程序的结构</h3><p>批处理和流处理的 Table API&amp;SQL 作业都有相同的模式，它们的代码结构如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据前面内容创建一个 TableEnvironment，指定是批作业还是流作业</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"></span><br><span class="line"><span class="comment">//用下面的其中一种方式注册一个 Table</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"table1"</span>, ...)          </span><br><span class="line">tableEnv.registerTableSource(<span class="string">"table2"</span>, ...); </span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">"extCat"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册一个 TableSink</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">"outputTable"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//根据一个 Table API 查询创建一个 Table</span></span><br><span class="line">Table tapiResult = tableEnv.scan(<span class="string">"table1"</span>).select(...);</span><br><span class="line"><span class="comment">//根据一个 SQL 查询创建一个 Table</span></span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM table2 ... "</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 Table API 或者 SQL 的结果发送给 TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//运行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="Catalog-中注册-Table"><a href="#Catalog-中注册-Table" class="headerlink" title="Catalog 中注册 Table"></a>Catalog 中注册 Table</h3><p>Table 有两种类型，输入表和输出表，可以在 Table API&amp;SQL 查询中引用输入表并提供输入数据，输出表可以用于将 Table API&amp;SQL 的查询结果发送到外部系统。输出表可以通过 TableSink 来注册，输入表可以从各种数据源进行注册：</p>
<ul>
<li>已经存在的 Table 对象，通过是 Table API 或 SQL 查询的结果</li>
<li>连接了外部系统的 TableSource，比如文件、数据库、MQ</li>
<li>从 DataStream 或 DataSet 程序中返回的 DataStream 和 DataSet</li>
</ul>
<h4 id="注册-Table"><a href="#注册-Table" class="headerlink" title="注册 Table"></a>注册 Table</h4><p>在 TableEnvironment 中可以像下面这样注册一个 Table：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; <span class="comment">// see "Create a TableEnvironment" section</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//projTable 是一个简单查询的结果</span></span><br><span class="line">Table projTable = tableEnv.scan(<span class="string">"X"</span>).select(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 projTable 表注册为 projectedTable 表</span></span><br><span class="line">tableEnv.registerTable(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure>
<h4 id="注册-TableSource"><a href="#注册-TableSource" class="headerlink" title="注册 TableSource"></a>注册 TableSource</h4><p>TableSource 让你可以访问存储系统（数据库 MySQL、HBase 等）、编码文件（CSV、Parquet、Avro 等）或 MQ（Kafka、RabbitMQ） 中的数据。Flink 为常用组件都提供了 TableSource，另外还提供自定义 TableSource。在 TableEnvironment 中可以像下面这样注册 TableSource：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 TableSource</span></span><br><span class="line">TableSource csvSource = <span class="keyword">new</span> CsvTableSource(<span class="string">"/Users/zhisheng/file"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 csvSource 注册为表</span></span><br><span class="line">tableEnv.registerTableSource(<span class="string">"CsvTable"</span>, csvSource);</span><br></pre></td></tr></table></figure>
<p>注意：用于 Blink planner 的 TableEnvironment 只能接受 StreamTableSource、LookupableTableSource 和 InputFormatTableSource，用于 Blink planner 批处理的 StreamTableSource 必须是有界的。</p>
<h4 id="注册-TableSink"><a href="#注册-TableSink" class="headerlink" title="注册 TableSink"></a>注册 TableSink</h4><p>TableSink 可以将 Table API&amp;SQL 查询的结果发送到外部的存储系统去，比如数据库、KV 存储、文件（CSV、Parquet 等）或 MQ 等。Flink 为常用等数据存储系统和文件格式都提供了 TableSink，另外还支持自定义 TableSink。在 TableEnvironment 中可以像下面这样注册 TableSink：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建 TableSink</span></span><br><span class="line">TableSink csvSink = <span class="keyword">new</span> CsvTableSink(<span class="string">"/Users/zhisheng/file"</span>, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义属性名和类型</span></span><br><span class="line">String[] fieldNames = &#123;<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>&#125;;</span><br><span class="line">TypeInformation[] fieldTypes = &#123;Types.INT, Types.STRING, Types.LONG&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 csvSink 注册为表 CsvSinkTable</span></span><br><span class="line">tableEnv.registerTableSink(<span class="string">"CsvSinkTable"</span>, fieldNames, fieldTypes, csvSink);</span><br></pre></td></tr></table></figure>
<h3 id="注册外部的-Catalog"><a href="#注册外部的-Catalog" class="headerlink" title="注册外部的 Catalog"></a>注册外部的 Catalog</h3><p>外部的 Catalog 可以提供外部的数据库和表的信息，例如它们的名称、schema、统计信息以及如何访问存储在外部数据库、表、文件中的数据。可以通过实现 ExternalCatalog 接口来创建外部的 Catalog，并像下面这样注册外部的 Catalog：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建外部的 catalog</span></span><br><span class="line">ExternalCatalog catalog = <span class="keyword">new</span> InMemoryExternalCatalog();</span><br><span class="line"><span class="comment">//注册 ExternalCatalog</span></span><br><span class="line">tableEnv.registerExternalCatalog(<span class="string">"InMemCatalog"</span>, catalog);<span class="comment">//该方法已经标记过期，可以使用 Catalog</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用下面这种</span></span><br><span class="line">Catalog catalog = <span class="keyword">new</span> GenericInMemoryCatalog(<span class="string">"zhisheng"</span>);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">"InMemCatalog"</span>, catalog);</span><br></pre></td></tr></table></figure>
<p>在注册后，ExternalCatalog 中的表数据信息可以通过 Table API&amp;SQL 查询获取到。Flink 提供了 Catalog 的一种实现类 GenericInMemoryCatalog 用于样例和测试。</p>
<h3 id="查询-Table"><a href="#查询-Table" class="headerlink" title="查询 Table"></a>查询 Table</h3><h4 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h4><p>先来演示使用 Table API 来完成一个简单聚合查询：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册 Orders 表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//查询注册的 Orders 表</span></span><br><span class="line">Table orders = tableEnv.scan(<span class="string">"Orders"</span>);</span><br><span class="line"><span class="comment">//计算来自中国的顾客的收入</span></span><br><span class="line">Table revenue = orders</span><br><span class="line">  .filter(<span class="string">"cCountry === 'China'"</span>)</span><br><span class="line">  .groupBy(<span class="string">"cID, cName"</span>)</span><br><span class="line">  .select(<span class="string">"cID, cName, revenue.sum AS revSum"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//转换或者提交该结果表</span></span><br><span class="line"><span class="comment">//运行该查询语句</span></span><br></pre></td></tr></table></figure>
<p>你可以使用 Java 或者 Scala 语言来利用 Table API 开发，而 SQL 却不是这样的。</p>
<h4 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h4><p>上面使用 Table API 的聚合查询样例使用 SQL 来完成就如下面这样：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册 Orders 表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//计算来自中国的顾客的收入</span></span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line"><span class="comment">//转换或者提交该结果表</span></span><br><span class="line"><span class="comment">//运行该查询语句</span></span><br></pre></td></tr></table></figure>
<p>Flink 的 SQL 是基于实现 SQL 标准的 Apache Calcite，SQL 的查询语句就是全部为字符串，上面这条 SQL 就说明了该如何指定查询并返回结果表，下面演示如何更新。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.sqlUpdate(</span><br><span class="line">    <span class="string">"INSERT INTO RevenueFrance "</span> +</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br></pre></td></tr></table></figure>
<h4 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API&amp;SQL"></a>Table API&amp;SQL</h4><p>Table API 和 SQL 之间可以相互结合，因为它们最后都是返回的 Table 对象，比如你可以在 SQL 查询返回的对象上定义 Table API 的查询，也可以在 Table API 查询结果返回的对象上定义 SQL 查询。</p>
<h3 id="提交-Table"><a href="#提交-Table" class="headerlink" title="提交 Table"></a>提交 Table</h3><p>在前面讲解了注册 TableSink，那么将表的结果提交就是将 Table 写入 TableSink，批处理的 Table 只能写入到 BatchTableSink，而流处理的 Table 可以写入进 AppendStreamTableSink、RetractStreamTableSink、UpsertStreamTableSink。使用 Table.insertInto(String tableName) 方法就可以将 Table 写入进已注册的 TableSink，它会根据名字去 catalog 中查找，并对比两者的 schema 是否相同。</p>
<h3 id="翻译并执行查询"><a href="#翻译并执行查询" class="headerlink" title="翻译并执行查询"></a>翻译并执行查询</h3><p>对于两种不同的 planner，翻译和执行查询的行为是不同的。</p>
<ul>
<li>之前的 planner：根据 Table API&amp;SQL 查询的输入是流还是批，然后先优化执行计划，接着对应转换成 DataStream 和 DataSet 程序，当 Table.insertInto() 和 TableEnvironment.sqlUpdate() 方法被调用、Table 转换成 DataStream 或 DataSet 时就会开始将 Table API 和 SQL 进行翻译，一旦翻译翻译完成后，也是和普通作业一样要执行 execute 方法后才开始运行。</li>
<li>Blink planner：不管 Table API 的输入是批还是流，都会转换成 DataStream 程序，对于 TableEnvironment 和 StreamTableEnvironment 的查询翻译是不一样的，对于 TableEnvironment，是在 TableEnvironment.execute() 调用的时候就会翻译 Table API&amp;SQL，因为 TableEnvironment 会将多个 Sink 优化在同一个 DAG 中，而 StreamTableEnvironment 和之前的 planner 是类似的。</li>
</ul>
<h3 id="小结与反思-21"><a href="#小结与反思-21" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节介绍了 Flink 新的 planner，然后详细地和之前的 planner 做了对比，然后对 Table API&amp;SQL 中的概念做了介绍，还通过样例去介绍了它们的通用 API。</p>
<h1 id="二十四、Flink-Table-API-amp-SQL-功能"><a href="#二十四、Flink-Table-API-amp-SQL-功能" class="headerlink" title="二十四、Flink Table API &amp; SQL 功能"></a>二十四、Flink Table API &amp; SQL 功能</h1><p>在 5.1 节中对 Flink Table API &amp; SQL 的概述和常见 API 都做了介绍，这篇文章先来看下其与 DataStream 和 DataSet API 的集成。</p>
<h3 id="Flink-Table-和-SQL-与-DataStream-和-DataSet-集成"><a href="#Flink-Table-和-SQL-与-DataStream-和-DataSet-集成" class="headerlink" title="Flink Table 和 SQL 与 DataStream 和 DataSet 集成"></a>Flink Table 和 SQL 与 DataStream 和 DataSet 集成</h3><p>两个 planner 都可以与 DataStream API 集成，只有以前的 planner 才可以集成 DataSet API，所以下面讨论 DataSet API 都是和以前的 planner 有关。</p>
<p>Table API &amp; SQL 查询与 DataStream 和 DataSet 程序集成是非常简单的，比如可以通过 Table API 或者 SQL 查询外部表数据，进行一些预处理后，然后使用 DataStream 或 DataSet API 继续处理一些复杂的计算，另外也可以将 DataStream 或 DataSet 处理后的数据利用 Table API 或者 SQL 写入到外部表去。总而言之，它们之间互相转换或者集成比较容易。</p>
<h4 id="Scala-的隐式转换"><a href="#Scala-的隐式转换" class="headerlink" title="Scala 的隐式转换"></a>Scala 的隐式转换</h4><p>Scala Table API 提供了 DataSet、DataStream 和 Table 类的隐式转换，可以通过导入 org.apache.flink.table.api.scala._ 或者 org.apache.flink.api.scala._ 包来启用这些转换。</p>
<h4 id="将-DataStream-或-DataSet-注册为-Table"><a href="#将-DataStream-或-DataSet-注册为-Table" class="headerlink" title="将 DataStream 或 DataSet 注册为 Table"></a>将 DataStream 或 DataSet 注册为 Table</h4><p>DataStream 或者 DataSet 可以注册为 Table，结果表的 schema 取决于已经注册的 DataStream 和 DataSet 的数据类型。你可以像下面这种方式转换：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable 表</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable"</span>, stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 注册为 myTable2 表（表中的字段为 myLong、myString）</span></span><br><span class="line">tableEnv.registerDataStream(<span class="string">"myTable2"</span>, stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure>
<h4 id="将-DataStream-或-DataSet-转换为-Table"><a href="#将-DataStream-或-DataSet-转换为-Table" class="headerlink" title="将 DataStream 或 DataSet 转换为 Table"></a>将 DataStream 或 DataSet 转换为 Table</h4><p>除了可以将 DataStream 或 DataSet 注册为 Table，还可以将它们转换为 Table，转换之后再去使用 Table API 查询就比较方便了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table1 = tableEnv.fromDataStream(stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将 DataStream 转换成 Table</span></span><br><span class="line">Table table2 = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure>
<h4 id="将-Table-转换成-DataStream-或-DataSet"><a href="#将-Table-转换成-DataStream-或-DataSet" class="headerlink" title="将 Table 转换成 DataStream 或 DataSet"></a>将 Table 转换成 DataStream 或 DataSet</h4><p>Table 可以转换为 DataStream 或 DataSet，这样就可以在 Table API 或 SQL 查询的结果上运行自定义的 DataStream 或 DataSet 程序。当将一个 Table 转换成 DataStream 或 DataSet 时，需要指定结果 DataStream 或 DataSet 的数据类型，最方便的数据类型是 Row，下面几个数据类型表示不同的功能：</p>
<ul>
<li>Row：字段按位置映射，任意数量的字段，支持 null 值，没有类型安全访问。</li>
<li>POJO：字段按名称映射，POJO 属性必须按照 Table 中的属性来命名，任意数量的字段，支持 null 值，类型安全访问。</li>
<li>Case Class：字段按位置映射，不支持 null 值，类型安全访问。</li>
<li>Tuple：按位置映射字段，限制为 22（Scala）或 25（Java）字段，不支持 null 值，类型安全访问。</li>
<li>原子类型：Table 必须具有单个字段，不支持 null 值，类型安全访问。</li>
</ul>
<h5 id="将-Table-转换成-DataStream"><a href="#将-Table-转换成-DataStream" class="headerlink" title="将 Table 转换成 DataStream"></a>将 Table 转换成 DataStream</h5><p>流查询的结果表会动态更新，即每个新的记录到达输入流时结果就会发生变化。所以在将 Table 转换成 DataStream 就需要对表的更新进行编码，有两种将 Table 转换为 DataStream 的模式：</p>
<ul>
<li>追加模式（Append Mode）：这种模式只能在动态表仅通过 INSERT 更改修改时才能使用，即仅追加，之前发出的结果不会更新。</li>
<li>撤回模式（Retract Mode）：任何时刻都可以使用此模式，它使用一个 boolean 标志来编码 INSERT 和 DELETE 的更改。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定类，将表转换为一个 append DataStream</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 append DataStream</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为一个 Retract DataStream Row</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure>
<h5 id="将-Table-转换成-DataSet"><a href="#将-Table-转换成-DataSet" class="headerlink" title="将 Table 转换成 DataSet"></a>将 Table 转换成 DataSet</h5><p>将 Table 转换成 DataSet 的样例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">//有两个字段(name、age) 的 Table</span></span><br><span class="line">Table table = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定一个类将表转换为一个 Row DataSet</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将表转换为 Tuple2&lt;String, Integer&gt; 的 DataSet</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(Types.STRING(), Types.INT());</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType);</span><br></pre></td></tr></table></figure>
<h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>Flink 使用 Calcite 来优化和翻译查询，以前的 planner 不会去优化 join 的顺序，而是按照查询中定义的顺序去执行。通过提供一个 CalciteConfig 对象来调整在不同阶段应用的优化规则集，这个可以通过调用 CalciteConfig.createBuilder() 获得的 builder 来创建，并且可以通过调用tableEnv.getConfig.setCalciteConfig(calciteConfig) 来提供给 TableEnvironment。而在 Blink planner 中扩展了 Calcite 来执行复杂的查询优化，这包括一系列基于规则和成本的优化，比如：</p>
<ul>
<li>基于 Calcite 的子查询去相关性</li>
<li>Project pruning</li>
<li>Partition pruning</li>
<li>Filter push-down</li>
<li>删除子计划中的重复数据以避免重复计算</li>
<li>重写特殊的子查询，包括两部分：<ul>
<li>将 IN 和 EXISTS 转换为 left semi-joins</li>
<li>将 NOT IN 和 NOT EXISTS 转换为 left anti-join</li>
</ul>
</li>
<li>重排序可选的 join<ul>
<li>通过启用 table.optimizer.join-reorder-enabled</li>
</ul>
</li>
</ul>
<p>注意：IN/EXISTS/NOT IN/NOT EXISTS 目前只支持子查询重写中的连接条件。</p>
<h4 id="解释-Table"><a href="#解释-Table" class="headerlink" title="解释 Table"></a>解释 Table</h4><p>Table API 提供了一种机制来解释计算 Table 的逻辑和优化查询计划。你可以通过 TableEnvironment.explain(table) 或者 TableEnvironment.explain() 方法来完成。explain(table) 会返回给定计划的 Table，explain() 会返回多路 Sink 计划的结果（主要用于 Blink planner）。它返回一个描述三个计划的字符串：</p>
<ul>
<li>关系查询的抽象语法树，即未优化的逻辑查询计划</li>
<li>优化的逻辑查询计划</li>
<li>实际执行计划</li>
</ul>
<p>以下代码演示了一个 Table 示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line"></span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, <span class="string">"count, word"</span>);</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, <span class="string">"count, word"</span>);</span><br><span class="line">Table table = table1.where(<span class="string">"LIKE(word, 'F%')"</span>).unionAll(table2);</span><br><span class="line"></span><br><span class="line">System.out.println(tEnv.explain(table));</span><br></pre></td></tr></table></figure>
<p>通过 explain(table) 方法返回的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">== Abstract Syntax Tree ==</span><br><span class="line">LogicalUnion(all=[true])</span><br><span class="line">  LogicalFilter(condition=[LIKE($1, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">DataStreamUnion(all=[true], union all=[count, word])</span><br><span class="line">  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE&apos;F%&apos;)])</span><br><span class="line">    DataStreamScan(id=[1], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[2], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== Physical Execution Plan ==</span><br><span class="line">Stage 1 : Data Source</span><br><span class="line">    content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 2 : Data Source</span><br><span class="line">    content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">    Stage 3 : Operator</span><br><span class="line">        content : from: (count, word)</span><br><span class="line">        ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">        Stage 4 : Operator</span><br><span class="line">            content : where: (LIKE(word, _UTF-16LE&apos;F%&apos;)), select: (count, word)</span><br><span class="line">            ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">            Stage 5 : Operator</span><br><span class="line">                content : from: (count, word)</span><br><span class="line">                ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>在 Flink 1.9 之前，Flink 的 Table API&amp;SQL 的数据类型与 Flink 中的 TypeInformation 紧密相关。TypeInformation 在 DataStream 和 DataSet API 中使用，另外它还可以描述在分布式中序列化和反序列化基于 JVM 对象所需的所有信息。从 1.9 版本之后，Table API&amp;SQL 会引入一种新类型来作为 API 稳定性和标准的长期解决方案。在以前的 planner 和 Blink planner 的数据类型有点不一致，具体差别可以参考官网。</p>
<h3 id="时间属性"><a href="#时间属性" class="headerlink" title="时间属性"></a>时间属性</h3><p>在 3.1 节中介绍过 Flink 的多种时间语义，常用的比如 Event time 和 Processing time，那么在 Table API&amp;SQL 中怎么去定义时间语义呢？</p>
<h4 id="Processing-Time-1"><a href="#Processing-Time-1" class="headerlink" title="Processing Time"></a>Processing Time</h4><p>因为处理时间是额外的数据字段，在原始的事件中是不存在该字段的，那么在将数据流转换成 Table 的时候就需要将这个 Processing time 当作 Table 的一个字段，以供后面需要，比如定义窗口。你可以像下面这样定义：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将附加的逻辑字段声明为 Processing time 属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"Username, Data, UserActionTime.proctime"</span>);</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"UserActionTime"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure>
<p>如果是直接使用 TableSource 的话，那么需要实现 DefinedProctimeAttribute 接口，然后去重写 getProctimeAttribute 方法，返回的字符串表示 Processing time 在 Table 中的字段名。</p>
<h4 id="Event-time"><a href="#Event-time" class="headerlink" title="Event time"></a>Event time</h4><p>Event time 是在采集上来的事件中就有的，将数据流转换成 Table 的时候需要像下面这样定义：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一种方法：</span></span><br><span class="line"><span class="comment">//提取流数据时间戳并分配水印</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"><span class="comment">//将附加的逻辑字段声明为 Event time 属性，和 Processing time 不同的是这里使用 rowtime</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"Username, Data, UserActionTime.rowtime"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第二种方法：</span></span><br><span class="line"><span class="comment">//从第一个字段提取时间戳，并分配水印</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"UserActionTime.rowtime, Username, Data"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用方式：</span></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"UserActionTime"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure>
<p>使用 TableSource 的话则需要实现 DefinedRowtimeAttributes 接口，重写 getRowtimeAttributeDescriptors 方法，该方法返回一个 RowtimeAttributeDescriptor 列表，其用于描述时间属性的最终名称、时间提取器以及该属性关联的水印策略。</p>
<h3 id="SQL-Connector"><a href="#SQL-Connector" class="headerlink" title="SQL Connector"></a>SQL Connector</h3><p>在第三部分中介绍了大量的 Flink Connectors 的使用，但是那些都是通过 DataStream API 是去使用，放在 Table API&amp;SQL 中其实不再适合，其实 Flink Table API&amp;SQL 是可以直接连接到外部系统的，然后读取和写入批处理表和流处理表。TableSource 提供从外部系统（数据库、MQ、文件系统等）读取数据，TableSink 将结果存储到数据库中。这里讲解一下该如何去定义 TableSource 和 TableSink 并将它们注册。在官网，它提供了如下这些 Connectors 和 Formats 的下载。</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga3besz7gfj21b00uuq4h.jpg" alt="undefined"></p>
<p>从 Flink 1.6 开始，不仅可以使用编程的方式指定 Connector，还可以使用声明式去定义。下面举个例子（读取 Kafka 中 Avro 格式的数据）来讲解这两种区别。</p>
<h4 id="使用代码"><a href="#使用代码" class="headerlink" title="使用代码"></a>使用代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  <span class="comment">//声明要连接的外部系统</span></span><br><span class="line">  .connect(</span><br><span class="line">    <span class="keyword">new</span> Kafka()</span><br><span class="line">      .version(<span class="string">"0.10"</span>)</span><br><span class="line">      .topic(<span class="string">"zhisheng_user"</span>)</span><br><span class="line">      .startFromEarliest()</span><br><span class="line">      .property(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>)</span><br><span class="line">      .property(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">  )</span><br><span class="line">  <span class="comment">//定义数据格式</span></span><br><span class="line">  .withFormat(</span><br><span class="line">    <span class="keyword">new</span> Avro()</span><br><span class="line">      .avroSchema(</span><br><span class="line">        <span class="string">"&#123;"</span> +</span><br><span class="line">        <span class="string">"  \"namespace\": \"com.zhisheng\","</span> +</span><br><span class="line">        <span class="string">"  \"type\": \"record\","</span> +</span><br><span class="line">        <span class="string">"  \"name\": \"UserMessage\","</span> +</span><br><span class="line">        <span class="string">"    \"fields\": ["</span> +</span><br><span class="line">        <span class="string">"      &#123;\"name\": \"timestamp\", \"type\": \"string\"&#125;,"</span> +</span><br><span class="line">        <span class="string">"      &#123;\"name\": \"user\", \"type\": \"long\"&#125;,"</span> +</span><br><span class="line">        <span class="string">"      &#123;\"name\": \"message\", \"type\": [\"string\", \"null\"]&#125;"</span> +</span><br><span class="line">        <span class="string">"    ]"</span> +</span><br><span class="line">        <span class="string">"&#125;"</span></span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line">  <span class="comment">//定义 Table schema</span></span><br><span class="line">  .withSchema(</span><br><span class="line">    <span class="keyword">new</span> Schema()</span><br><span class="line">      .field(<span class="string">"rowtime"</span>, Types.SQL_TIMESTAMP)</span><br><span class="line">        .rowtime(<span class="keyword">new</span> Rowtime()</span><br><span class="line">          .timestampsFromField(<span class="string">"timestamp"</span>)</span><br><span class="line">          .watermarksPeriodicBounded(<span class="number">60000</span>)</span><br><span class="line">        )</span><br><span class="line">      .field(<span class="string">"user"</span>, Types.LONG)</span><br><span class="line">      .field(<span class="string">"message"</span>, Types.STRING)</span><br><span class="line">  )</span><br><span class="line">  .inAppendMode()  <span class="comment">//指定流表的 update-mode</span></span><br><span class="line">  .registerTableSource(<span class="string">"zhisheng"</span>);    <span class="comment">//注册表的名字</span></span><br></pre></td></tr></table></figure>
<h4 id="使用-YAML-文件"><a href="#使用-YAML-文件" class="headerlink" title="使用 YAML 文件"></a>使用 YAML 文件</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tables:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">zhisheng</span>      <span class="comment">#表的名字</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">source</span>           <span class="comment">#定义是 source，还是 sink，或者 both</span></span><br><span class="line"><span class="attr">    update-mode:</span> <span class="string">append</span>    <span class="comment">#指定流表的 update-mode</span></span><br><span class="line">    <span class="comment">#定义要连接的系统</span></span><br><span class="line"><span class="attr">    connector:</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">kafka</span></span><br><span class="line"><span class="attr">      version:</span> <span class="string">"0.10"</span></span><br><span class="line"><span class="attr">      topic:</span> <span class="string">zhisheng_user</span></span><br><span class="line"><span class="attr">      startup-mode:</span> <span class="string">earliest-offset</span></span><br><span class="line"><span class="attr">      properties:</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">zookeeper.connect</span></span><br><span class="line"><span class="attr">          value:</span> <span class="attr">localhost:2181</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">bootstrap.servers</span></span><br><span class="line"><span class="attr">          value:</span> <span class="attr">localhost:9092</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#定义格式</span></span><br><span class="line"><span class="attr">    format:</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">avro</span></span><br><span class="line"><span class="attr">      avro-schema:</span> <span class="string">&gt;</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">          "namespace": "com.zhisheng",</span></span><br><span class="line"><span class="string">          "type": "record",</span></span><br><span class="line"><span class="string">          "name": "UserMessage",</span></span><br><span class="line"><span class="string">            "fields": [</span></span><br><span class="line"><span class="string">              &#123;"name": "ts", "type": "string"&#125;,</span></span><br><span class="line"><span class="string">              &#123;"name": "user", "type": "long"&#125;,</span></span><br><span class="line"><span class="string">              &#123;"name": "message", "type": ["string", "null"]&#125;</span></span><br><span class="line"><span class="string">            ]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    #定义 table schema</span></span><br><span class="line"><span class="string"></span><span class="attr">    schema:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">rowtime</span></span><br><span class="line"><span class="attr">        type:</span> <span class="string">TIMESTAMP</span></span><br><span class="line"><span class="attr">        rowtime:</span></span><br><span class="line"><span class="attr">          timestamps:</span></span><br><span class="line"><span class="attr">            type:</span> <span class="string">from-field</span></span><br><span class="line"><span class="attr">            from:</span> <span class="string">ts</span></span><br><span class="line"><span class="attr">          watermarks:</span></span><br><span class="line"><span class="attr">            type:</span> <span class="string">periodic-bounded</span></span><br><span class="line"><span class="attr">            delay:</span> <span class="string">"60000"</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">user</span></span><br><span class="line"><span class="attr">        type:</span> <span class="string">BIGINT</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">message</span></span><br><span class="line"><span class="attr">        type:</span> <span class="string">VARCHAR</span></span><br></pre></td></tr></table></figure>
<h4 id="使用-DDL"><a href="#使用-DDL" class="headerlink" title="使用 DDL"></a>使用 DDL</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE zhisheng (</span><br><span class="line">  `user` BIGINT,</span><br><span class="line">  message VARCHAR,</span><br><span class="line">  ts VARCHAR</span><br><span class="line">) WITH (</span><br><span class="line">  &apos;connector.type&apos; = &apos;kafka&apos;,</span><br><span class="line">  &apos;connector.version&apos; = &apos;0.10&apos;,</span><br><span class="line">  &apos;connector.topic&apos; = &apos;zhisheng_user&apos;,</span><br><span class="line">  &apos;connector.startup-mode&apos; = &apos;earliest-offset&apos;,</span><br><span class="line">  &apos;connector.properties.0.key&apos; = &apos;zookeeper.connect&apos;,</span><br><span class="line">  &apos;connector.properties.0.value&apos; = &apos;localhost:2181&apos;,</span><br><span class="line">  &apos;connector.properties.1.key&apos; = &apos;bootstrap.servers&apos;,</span><br><span class="line">  &apos;connector.properties.1.value&apos; = &apos;localhost:9092&apos;,</span><br><span class="line">  &apos;update-mode&apos; = &apos;append&apos;,</span><br><span class="line">  &apos;format.type&apos; = &apos;avro&apos;,</span><br><span class="line">  &apos;format.avro-schema&apos; = &apos;&#123;</span><br><span class="line">                            &quot;namespace&quot;: &quot;com.zhisheng&quot;,</span><br><span class="line">                            &quot;type&quot;: &quot;record&quot;,</span><br><span class="line">                            &quot;name&quot;: &quot;UserMessage&quot;,</span><br><span class="line">                            &quot;fields&quot;: [</span><br><span class="line">                                &#123;&quot;name&quot;: &quot;ts&quot;, &quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">                                &#123;&quot;name&quot;: &quot;user&quot;, &quot;type&quot;: &quot;long&quot;&#125;,</span><br><span class="line">                                &#123;&quot;name&quot;: &quot;message&quot;, &quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]&#125;</span><br><span class="line">                            ]</span><br><span class="line">                         &#125;&apos;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>上面演示了 Kafka Connector 和 avro 数据格式化在 Table API&amp;SQL 中的使用方式，在官网中还有文件系统和 Elasticsearch Connector、CSV 和 JSON 等的使用说明。</p>
<h3 id="SQL-Client"><a href="#SQL-Client" class="headerlink" title="SQL Client"></a>SQL Client</h3><p>虽然 Flink Table API&amp;SQL 让使用 SQL 去查询流数据有了可能，但是这些查询语句通常要嵌入在 Java 或者 Scala 程序中，最后在提交到集群运行之前还要通过构建工具打包，这就导致 Table API&amp;SQL 的限制性很大，所以 SQL Client 就起到这么个作用，让用户不再编写任何 Java 或者 Scala 代码，直接编写 SQL 就可以去调试运行，并且可以通过其他命令行实时查看运行的结果，但是该功能目前还比较弱。</p>
<p>在启动 Flink 后可以通过运行 <code>./bin/sql-client.sh embedded</code> 命令来启动 SQL Client CLI，如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/bec9bff2gy1ga3bf4ctclj21qc27uthw.jpg" alt="undefined"></p>
<p>你可以运行下面的命令就可以知道名字和其出现的次数的结果。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">name</span>, <span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt <span class="keyword">FROM</span> (<span class="keyword">VALUES</span> (<span class="string">'Bob'</span>), (<span class="string">'Alice'</span>), (<span class="string">'Greg'</span>), (<span class="string">'Bob'</span>)) <span class="keyword">AS</span> NameTable(<span class="keyword">name</span>) <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure>
<p>另外它还支持传入 YAML 文件，你可以在 YAML 文件中如前面内容一样定义的 Kafka Connector 等信息，关于 SQL Client 的更多功能可以查阅官网。</p>
<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>Hive 是建立在 Hadoop 上的数据仓库基础构架，它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。</p>
<p>Flink 在 1.9 版本中提供了与 Hive 的双重集成。首先是利用 Hive 的 Metastore 存储 Flink 特定元数据，另一个是 Flink 支持读取和写入 Hive 表。支持的 Hive 2.3.4 和 1.2.1 版本，如果你要使用的话，注意它们的依赖是有点不一样。</p>
<p>你可以通过 Java、Scala、YAML 连接 Hive，比如使用 Java 代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">String name            = <span class="string">"myhive"</span>;</span><br><span class="line">String defaultDatabase = <span class="string">"mydatabase"</span>;</span><br><span class="line">String hiveConfDir     = <span class="string">"/opt/hive-conf"</span>;</span><br><span class="line">String version         = <span class="string">"2.3.4"</span>; <span class="comment">//或者 1.2.1</span></span><br><span class="line"></span><br><span class="line">HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir, version);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">"myhive"</span>, hive);</span><br></pre></td></tr></table></figure>
<h3 id="小结与反思-22"><a href="#小结与反思-22" class="headerlink" title="小结与反思"></a>小结与反思</h3><p>本节继续介绍了 Flink Table API&amp;SQL 中的部分 API，然后讲解了 Flink 之前的 planner 和 Blink planner 在某些特性上面的区别，还讲解了 SQL Connector，最后介绍了 SQL Client 和 Hive。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Flink/" rel="tag"># Flink</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/11/07/Pulsar与Kafka对比/" rel="next" title="Pulsar与Kafka对比">
                <i class="fa fa-chevron-left"></i> Pulsar与Kafka对比
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/11/12/Kylin/" rel="prev" title="Apache Kylin">
                Apache Kylin <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80Mzk4NC8yMDUyMA=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Fly Hugh">
            
              <p class="site-author-name" itemprop="name">Fly Hugh</p>
              <p class="site-description motion-element" itemprop="description">TRUST THE PROCESS</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">37</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/FlyMeToTheMars" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yourname@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/3200892914" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/Fly__HoBo" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#大数据实时计算引擎-Flink-实战与性能优化"><span class="nav-number">1.</span> <span class="nav-text">大数据实时计算引擎 Flink 实战与性能优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、公司到底需不需要引入实时计算引擎？"><span class="nav-number">1.1.</span> <span class="nav-text">一、公司到底需不需要引入实时计算引擎？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实时计算需求"><span class="nav-number">1.1.1.</span> <span class="nav-text">实时计算需求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据实时采集"><span class="nav-number">1.1.2.</span> <span class="nav-text">数据实时采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据实时计算"><span class="nav-number">1.1.3.</span> <span class="nav-text">数据实时计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据实时下发"><span class="nav-number">1.1.4.</span> <span class="nav-text">数据实时下发</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实时计算场景"><span class="nav-number">1.1.5.</span> <span class="nav-text">实时计算场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#离线计算-vs-实时计算"><span class="nav-number">1.1.6.</span> <span class="nav-text">离线计算 vs 实时计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#流处理与批处理"><span class="nav-number">1.1.6.1.</span> <span class="nav-text">流处理与批处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#离线计算的特点"><span class="nav-number">1.1.6.2.</span> <span class="nav-text">离线计算的特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实时计算的特点"><span class="nav-number">1.1.6.3.</span> <span class="nav-text">实时计算的特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#流式数据的特点"><span class="nav-number">1.1.6.4.</span> <span class="nav-text">流式数据的特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实时计算的优势"><span class="nav-number">1.1.6.5.</span> <span class="nav-text">实时计算的优势</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实时计算面临的挑战"><span class="nav-number">1.1.7.</span> <span class="nav-text">实时计算面临的挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思"><span class="nav-number">1.1.8.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、彻底了解大数据实时计算框架-Flink"><span class="nav-number">1.2.</span> <span class="nav-text">二、彻底了解大数据实时计算框架 Flink</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集类型"><span class="nav-number">1.2.0.1.</span> <span class="nav-text">数据集类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据运算模型"><span class="nav-number">1.2.0.2.</span> <span class="nav-text">数据运算模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-是什么？"><span class="nav-number">1.2.1.</span> <span class="nav-text">Flink 是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-整体架构"><span class="nav-number">1.2.2.</span> <span class="nav-text">Flink 整体架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-支持多种方式部署"><span class="nav-number">1.2.3.</span> <span class="nav-text">Flink 支持多种方式部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-分布式运行"><span class="nav-number">1.2.4.</span> <span class="nav-text">Flink 分布式运行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-程序与数据流结构"><span class="nav-number">1.2.5.</span> <span class="nav-text">Flink 程序与数据流结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-支持丰富的-Connector"><span class="nav-number">1.2.6.</span> <span class="nav-text">Flink 支持丰富的 Connector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-提供事件时间-amp-处理时间语义"><span class="nav-number">1.2.7.</span> <span class="nav-text">Flink 提供事件时间&amp;处理时间语义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-提供灵活的窗口机制"><span class="nav-number">1.2.8.</span> <span class="nav-text">Flink 提供灵活的窗口机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-并行的执行任务"><span class="nav-number">1.2.9.</span> <span class="nav-text">Flink 并行的执行任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-支持状态存储和容错"><span class="nav-number">1.2.10.</span> <span class="nav-text">Flink 支持状态存储和容错</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-实现了自己的内存管理机制"><span class="nav-number">1.2.11.</span> <span class="nav-text">Flink 实现了自己的内存管理机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-支持多种扩展库"><span class="nav-number">1.2.12.</span> <span class="nav-text">Flink 支持多种扩展库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-1"><span class="nav-number">1.2.13.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、大数据框架-Flink、Blink、Spark-Streaming、Structured-Streaming和-Storm-的区别。"><span class="nav-number">1.3.</span> <span class="nav-text">三、大数据框架 Flink、Blink、Spark Streaming、Structured Streaming和 Storm 的区别。</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink"><span class="nav-number">1.3.1.</span> <span class="nav-text">Flink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Blink"><span class="nav-number">1.3.2.</span> <span class="nav-text">Blink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-3-Spark"><span class="nav-number">1.3.3.</span> <span class="nav-text">1.3.3 Spark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">1.3.4.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Structured-Streaming"><span class="nav-number">1.3.5.</span> <span class="nav-text">Structured Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-VS-Spark"><span class="nav-number">1.3.6.</span> <span class="nav-text">Flink VS Spark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Storm"><span class="nav-number">1.3.7.</span> <span class="nav-text">Storm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Storm-核心组件"><span class="nav-number">1.3.7.1.</span> <span class="nav-text">Storm 核心组件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Storm-核心概念"><span class="nav-number">1.3.7.2.</span> <span class="nav-text">Storm 核心概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Storm-数据处理流程图"><span class="nav-number">1.3.7.3.</span> <span class="nav-text">Storm 数据处理流程图</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-VS-Storm"><span class="nav-number">1.3.8.</span> <span class="nav-text">Flink VS Storm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#全部对比结果"><span class="nav-number">1.3.9.</span> <span class="nav-text">全部对比结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-2"><span class="nav-number">1.3.10.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、Flink-环境准备"><span class="nav-number">1.4.</span> <span class="nav-text">四、Flink 环境准备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#JDK-安装与配置"><span class="nav-number">1.4.1.</span> <span class="nav-text">JDK 安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Maven-安装与配置"><span class="nav-number">1.4.2.</span> <span class="nav-text">Maven 安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IDE-安装与配置"><span class="nav-number">1.4.3.</span> <span class="nav-text">IDE 安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MySQL-安装与配置"><span class="nav-number">1.4.4.</span> <span class="nav-text">MySQL 安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-安装与配置"><span class="nav-number">1.4.5.</span> <span class="nav-text">Kafka 安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ElasticSearch-安装与配置"><span class="nav-number">1.4.6.</span> <span class="nav-text">ElasticSearch 安装与配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-3"><span class="nav-number">1.4.7.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五、Flink环境搭建"><span class="nav-number">1.5.</span> <span class="nav-text">五、Flink环境搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-下载与安装"><span class="nav-number">1.5.1.</span> <span class="nav-text">Flink 下载与安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Mac-amp-Linux-安装"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">Mac &amp; Linux 安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Windows-安装"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">Windows 安装</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-启动与运行"><span class="nav-number">1.5.2.</span> <span class="nav-text">Flink 启动与运行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-目录配置文件解读"><span class="nav-number">1.5.3.</span> <span class="nav-text">Flink 目录配置文件解读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-源码下载"><span class="nav-number">1.5.4.</span> <span class="nav-text">Flink 源码下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-源码编译"><span class="nav-number">1.5.5.</span> <span class="nav-text">Flink 源码编译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-源码导入到-IDE"><span class="nav-number">1.5.6.</span> <span class="nav-text">Flink 源码导入到 IDE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-4"><span class="nav-number">1.5.7.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#六、FlinkWordCount"><span class="nav-number">1.6.</span> <span class="nav-text">六、FlinkWordCount</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Maven-创建项目"><span class="nav-number">1.6.1.</span> <span class="nav-text">Maven 创建项目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IDEA-创建项目"><span class="nav-number">1.6.2.</span> <span class="nav-text">IDEA 创建项目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#流计算-WordCount-应用程序代码"><span class="nav-number">1.6.3.</span> <span class="nav-text">流计算 WordCount 应用程序代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WordCount-应用程序运行"><span class="nav-number">1.6.4.</span> <span class="nav-text">WordCount 应用程序运行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#本地-IDE-运行"><span class="nav-number">1.6.4.1.</span> <span class="nav-text">本地 IDE 运行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#UI-运行-Job"><span class="nav-number">1.6.4.2.</span> <span class="nav-text">UI 运行 Job</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WordCount-应用程序代码分析"><span class="nav-number">1.6.5.</span> <span class="nav-text">WordCount 应用程序代码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-5"><span class="nav-number">1.6.6.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#七、Flink-实时处理-Socket-数据"><span class="nav-number">1.7.</span> <span class="nav-text">七、Flink 实时处理 Socket 数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IDEA-创建项目-1"><span class="nav-number">1.7.1.</span> <span class="nav-text">IDEA 创建项目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-Socket-应用程序代码"><span class="nav-number">1.7.2.</span> <span class="nav-text">Flink Socket 应用程序代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-Socket-应用程序运行"><span class="nav-number">1.7.3.</span> <span class="nav-text">Flink Socket 应用程序运行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#本地-IDE-运行-1"><span class="nav-number">1.7.3.1.</span> <span class="nav-text">本地 IDE 运行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#UI-运行-Job-1"><span class="nav-number">1.7.3.2.</span> <span class="nav-text">UI 运行 Job</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-Socket-应用程序代码分析"><span class="nav-number">1.7.4.</span> <span class="nav-text">Flink Socket 应用程序代码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-中使用-Lambda-表达式"><span class="nav-number">1.7.5.</span> <span class="nav-text">Flink 中使用 Lambda 表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-6"><span class="nav-number">1.7.6.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#八、Flink多种时间语义对比"><span class="nav-number">1.8.</span> <span class="nav-text">八、Flink多种时间语义对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Processing-Time"><span class="nav-number">1.8.1.</span> <span class="nav-text">Processing Time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Event-Time"><span class="nav-number">1.8.2.</span> <span class="nav-text">Event Time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ingestion-Time"><span class="nav-number">1.8.3.</span> <span class="nav-text">Ingestion Time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三种-Time-对比结果"><span class="nav-number">1.8.4.</span> <span class="nav-text">三种 Time 对比结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用场景分析"><span class="nav-number">1.8.5.</span> <span class="nav-text">使用场景分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何设置-Time-策略？"><span class="nav-number">1.8.6.</span> <span class="nav-text">如何设置 Time 策略？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-7"><span class="nav-number">1.8.7.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#九、Flink-Window-基础概念与实现原理"><span class="nav-number">1.9.</span> <span class="nav-text">九、Flink Window 基础概念与实现原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是-Window？"><span class="nav-number">1.9.1.</span> <span class="nav-text">什么是 Window？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Window-有什么作用？"><span class="nav-number">1.9.2.</span> <span class="nav-text">Window 有什么作用？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-自带的-Window"><span class="nav-number">1.9.3.</span> <span class="nav-text">Flink 自带的 Window</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Time-Window-使用及源码分析"><span class="nav-number">1.9.4.</span> <span class="nav-text">Time Window 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Count-Window-使用及源码分析"><span class="nav-number">1.9.5.</span> <span class="nav-text">Count Window 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Session-Window-使用及源码分析"><span class="nav-number">1.9.6.</span> <span class="nav-text">Session Window 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何自定义-Window？"><span class="nav-number">1.9.7.</span> <span class="nav-text">如何自定义 Window？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-8-Window-源码定义"><span class="nav-number">1.9.8.</span> <span class="nav-text">3.2.8 Window 源码定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Window-组件之-WindowAssigner-使用及源码分析"><span class="nav-number">1.9.9.</span> <span class="nav-text">Window 组件之 WindowAssigner 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Window-组件之-Trigger-使用及源码分析"><span class="nav-number">1.9.10.</span> <span class="nav-text">Window 组件之 Trigger 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Window-组件之-Evictor-使用及源码分析"><span class="nav-number">1.9.11.</span> <span class="nav-text">Window 组件之 Evictor 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-8"><span class="nav-number">1.9.12.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十、数据转换必须熟悉的算子（Operator）"><span class="nav-number">1.10.</span> <span class="nav-text">十、数据转换必须熟悉的算子（Operator）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DataStream-Operator"><span class="nav-number">1.10.1.</span> <span class="nav-text">DataStream Operator</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Map"><span class="nav-number">1.10.1.1.</span> <span class="nav-text">Map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FlatMap"><span class="nav-number">1.10.1.2.</span> <span class="nav-text">FlatMap</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Filter"><span class="nav-number">1.10.1.3.</span> <span class="nav-text">Filter</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KeyBy"><span class="nav-number">1.10.1.4.</span> <span class="nav-text">KeyBy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reduce"><span class="nav-number">1.10.1.5.</span> <span class="nav-text">Reduce</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Aggregations"><span class="nav-number">1.10.1.6.</span> <span class="nav-text">Aggregations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Window"><span class="nav-number">1.10.1.7.</span> <span class="nav-text">Window</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#WindowAll"><span class="nav-number">1.10.1.8.</span> <span class="nav-text">WindowAll</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Union"><span class="nav-number">1.10.1.9.</span> <span class="nav-text">Union</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Window-Join"><span class="nav-number">1.10.1.10.</span> <span class="nav-text">Window Join</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Split"><span class="nav-number">1.10.1.11.</span> <span class="nav-text">Split</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Select"><span class="nav-number">1.10.1.12.</span> <span class="nav-text">Select</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DataSet-Operator"><span class="nav-number">1.10.2.</span> <span class="nav-text">DataSet Operator</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#First-n"><span class="nav-number">1.10.2.1.</span> <span class="nav-text">First-n</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#流批统一的思路"><span class="nav-number">1.10.3.</span> <span class="nav-text">流批统一的思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-9"><span class="nav-number">1.10.4.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十一、如何使用-DataStream-API-来处理数据？"><span class="nav-number">1.11.</span> <span class="nav-text">十一、如何使用 DataStream API 来处理数据？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DataStream-如何使用及分析"><span class="nav-number">1.11.1.</span> <span class="nav-text">DataStream 如何使用及分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#union"><span class="nav-number">1.11.1.1.</span> <span class="nav-text">union</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#split"><span class="nav-number">1.11.1.2.</span> <span class="nav-text">split</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#connect"><span class="nav-number">1.11.1.3.</span> <span class="nav-text">connect</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#keyBy"><span class="nav-number">1.11.1.4.</span> <span class="nav-text">keyBy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#partitionCustom"><span class="nav-number">1.11.1.5.</span> <span class="nav-text">partitionCustom</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#broadcast"><span class="nav-number">1.11.1.6.</span> <span class="nav-text">broadcast</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map"><span class="nav-number">1.11.1.7.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#flatMap"><span class="nav-number">1.11.1.8.</span> <span class="nav-text">flatMap</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#process"><span class="nav-number">1.11.1.9.</span> <span class="nav-text">process</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#filter"><span class="nav-number">1.11.1.10.</span> <span class="nav-text">filter</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SingleOutputStreamOperator-如何使用及分析"><span class="nav-number">1.11.2.</span> <span class="nav-text">SingleOutputStreamOperator 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KeyedStream-如何使用及分析"><span class="nav-number">1.11.3.</span> <span class="nav-text">KeyedStream 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SplitStream-如何使用及分析"><span class="nav-number">1.11.4.</span> <span class="nav-text">SplitStream 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WindowedStream-如何使用及分析"><span class="nav-number">1.11.5.</span> <span class="nav-text">WindowedStream 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AllWindowedStream-如何使用及分析"><span class="nav-number">1.11.6.</span> <span class="nav-text">AllWindowedStream 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ConnectedStreams-如何使用及分析"><span class="nav-number">1.11.7.</span> <span class="nav-text">ConnectedStreams 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BroadcastStream-如何使用及分析"><span class="nav-number">1.11.8.</span> <span class="nav-text">BroadcastStream 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BroadcastConnectedStream-如何使用及分析"><span class="nav-number">1.11.9.</span> <span class="nav-text">BroadcastConnectedStream 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QueryableStateStream-如何使用及分析"><span class="nav-number">1.11.10.</span> <span class="nav-text">QueryableStateStream 如何使用及分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结"><span class="nav-number">1.11.11.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十二、Flink-WaterMark-详解及结合-WaterMark-处理延迟数据"><span class="nav-number">1.12.</span> <span class="nav-text">十二、Flink WaterMark 详解及结合 WaterMark 处理延迟数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Watermark-是什么？"><span class="nav-number">1.12.1.</span> <span class="nav-text">Watermark 是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-中-Watermark-的设置"><span class="nav-number">1.12.2.</span> <span class="nav-text">Flink 中 Watermark 的设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Punctuated-Watermark"><span class="nav-number">1.12.3.</span> <span class="nav-text">Punctuated Watermark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-4-Periodic-Watermark"><span class="nav-number">1.12.4.</span> <span class="nav-text">3.5.4 Periodic Watermark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#每个-Kafka-分区的时间戳"><span class="nav-number">1.12.5.</span> <span class="nav-text">每个 Kafka 分区的时间戳</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#延迟数据该如何处理-三种方法"><span class="nav-number">1.12.6.</span> <span class="nav-text">延迟数据该如何处理(三种方法)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#丢弃（默认）"><span class="nav-number">1.12.6.1.</span> <span class="nav-text">丢弃（默认）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#allowedLateness-再次指定允许数据延迟的时间"><span class="nav-number">1.12.6.2.</span> <span class="nav-text">allowedLateness 再次指定允许数据延迟的时间</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sideOutputLateData-收集迟到的数据"><span class="nav-number">1.12.6.3.</span> <span class="nav-text">sideOutputLateData 收集迟到的数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-10"><span class="nav-number">1.12.7.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十三、Flink-常用的-Source-和-Sink-Connectors-介绍"><span class="nav-number">1.13.</span> <span class="nav-text">十三、Flink 常用的 Source 和 Sink Connectors 介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Source-介绍"><span class="nav-number">1.13.1.</span> <span class="nav-text">Data Source 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的-Data-Source"><span class="nav-number">1.13.2.</span> <span class="nav-text">常用的 Data Source</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于集合"><span class="nav-number">1.13.2.1.</span> <span class="nav-text">基于集合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于文件"><span class="nav-number">1.13.2.2.</span> <span class="nav-text">基于文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于-Socket"><span class="nav-number">1.13.2.3.</span> <span class="nav-text">基于 Socket</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自定义"><span class="nav-number">1.13.2.4.</span> <span class="nav-text">自定义</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Sink-介绍"><span class="nav-number">1.13.3.</span> <span class="nav-text">Data Sink 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的-Data-Sink"><span class="nav-number">1.13.4.</span> <span class="nav-text">常用的 Data Sink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-11"><span class="nav-number">1.13.5.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十四、Flink-Connector-Kafka-使用和剖析"><span class="nav-number">1.14.</span> <span class="nav-text">十四、Flink Connector Kafka 使用和剖析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#准备环境和依赖"><span class="nav-number">1.14.1.</span> <span class="nav-text">准备环境和依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#环境安装和启动"><span class="nav-number">1.14.1.1.</span> <span class="nav-text">环境安装和启动</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#添加-maven-依赖"><span class="nav-number">1.14.1.2.</span> <span class="nav-text">添加 maven 依赖</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试数据发到-Kafka-Topic"><span class="nav-number">1.14.2.</span> <span class="nav-text">测试数据发到 Kafka Topic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-如何消费-Kafka-数据？"><span class="nav-number">1.14.3.</span> <span class="nav-text">Flink 如何消费 Kafka 数据？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#代码分析"><span class="nav-number">1.14.3.1.</span> <span class="nav-text">代码分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-如何将计算后的数据发到-Kafka？"><span class="nav-number">1.14.4.</span> <span class="nav-text">Flink 如何将计算后的数据发到 Kafka？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#配置文件"><span class="nav-number">1.14.4.1.</span> <span class="nav-text">配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#程序代码"><span class="nav-number">1.14.4.2.</span> <span class="nav-text">程序代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#运行结果"><span class="nav-number">1.14.4.3.</span> <span class="nav-text">运行结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FlinkKafkaConsumer-源码剖析"><span class="nav-number">1.14.5.</span> <span class="nav-text">FlinkKafkaConsumer 源码剖析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FlinkKafkaProducer-源码剖析"><span class="nav-number">1.14.6.</span> <span class="nav-text">FlinkKafkaProducer 源码剖析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-Flink-connector-kafka-可能会遇到的问题"><span class="nav-number">1.14.7.</span> <span class="nav-text">使用 Flink-connector-kafka 可能会遇到的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#如何消费多个-Kafka-Topic"><span class="nav-number">1.14.7.1.</span> <span class="nav-text">如何消费多个 Kafka Topic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#想要获取数据的元数据信息"><span class="nav-number">1.14.7.2.</span> <span class="nav-text">想要获取数据的元数据信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多种数据类型"><span class="nav-number">1.14.7.3.</span> <span class="nav-text">多种数据类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#序列化失败"><span class="nav-number">1.14.7.4.</span> <span class="nav-text">序列化失败</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-消费-Offset-的选择"><span class="nav-number">1.14.7.5.</span> <span class="nav-text">Kafka 消费 Offset 的选择</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-12"><span class="nav-number">1.14.8.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十五、如何自定义-Flink-Connectors（Source-和-Sink）？"><span class="nav-number">1.15.</span> <span class="nav-text">十五、如何自定义 Flink Connectors（Source 和 Sink）？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#如何自定义-Source-Connector？"><span class="nav-number">1.15.1.</span> <span class="nav-text">如何自定义 Source Connector？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#添加依赖"><span class="nav-number">1.15.1.1.</span> <span class="nav-text">添加依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据库建表"><span class="nav-number">1.15.1.2.</span> <span class="nav-text">数据库建表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据库插入数据"><span class="nav-number">1.15.1.3.</span> <span class="nav-text">数据库插入数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新建实体类"><span class="nav-number">1.15.1.4.</span> <span class="nav-text">新建实体类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自定义-Source-类"><span class="nav-number">1.15.1.5.</span> <span class="nav-text">自定义 Source 类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flink-应用程序代码"><span class="nav-number">1.15.1.6.</span> <span class="nav-text">Flink 应用程序代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RichSourceFunction-使用及源码分析"><span class="nav-number">1.15.2.</span> <span class="nav-text">RichSourceFunction 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何自定义-Sink-Connector？"><span class="nav-number">1.15.3.</span> <span class="nav-text">如何自定义 Sink Connector？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#工具类"><span class="nav-number">1.15.3.1.</span> <span class="nav-text">工具类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SinkToMySQL"><span class="nav-number">1.15.3.2.</span> <span class="nav-text">SinkToMySQL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flink-程序"><span class="nav-number">1.15.3.3.</span> <span class="nav-text">Flink 程序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结果"><span class="nav-number">1.15.3.4.</span> <span class="nav-text">结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RichSinkFunction-使用及源码分析"><span class="nav-number">1.15.4.</span> <span class="nav-text">RichSinkFunction 使用及源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-13"><span class="nav-number">1.15.5.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十六、如何使用-Flink-Connectors-——-ElasticSearch？"><span class="nav-number">1.16.</span> <span class="nav-text">十六、如何使用 Flink Connectors —— ElasticSearch？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#准备环境和依赖-1"><span class="nav-number">1.16.1.</span> <span class="nav-text">准备环境和依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ElasticSearch-安装"><span class="nav-number">1.16.1.1.</span> <span class="nav-text">ElasticSearch 安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#添加依赖-1"><span class="nav-number">1.16.1.2.</span> <span class="nav-text">添加依赖</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-写入数据到-ElasticSearch-应用程序"><span class="nav-number">1.16.2.</span> <span class="nav-text">Flink 写入数据到 ElasticSearch 应用程序</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ESSinkUtil-工具类"><span class="nav-number">1.16.2.1.</span> <span class="nav-text">ESSinkUtil 工具类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Main-启动类"><span class="nav-number">1.16.2.2.</span> <span class="nav-text">Main 启动类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置文件-1"><span class="nav-number">1.16.2.3.</span> <span class="nav-text">配置文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#验证数据是否写入-ElasticSearch？"><span class="nav-number">1.16.3.</span> <span class="nav-text">验证数据是否写入 ElasticSearch？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何保证在海量数据实时写入下-ElasticSearch-的稳定性？"><span class="nav-number">1.16.4.</span> <span class="nav-text">如何保证在海量数据实时写入下 ElasticSearch 的稳定性？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-Flink-connector-elasticsearch-可能会遇到的问题"><span class="nav-number">1.16.5.</span> <span class="nav-text">使用 Flink-connector-elasticsearch 可能会遇到的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#解决方法"><span class="nav-number">1.16.5.1.</span> <span class="nav-text">解决方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-14"><span class="nav-number">1.16.6.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十七、如何使用-Flink-Connectors-——-HBase？"><span class="nav-number">1.17.</span> <span class="nav-text">十七、如何使用 Flink Connectors —— HBase？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#准备环境和依赖-2"><span class="nav-number">1.17.1.</span> <span class="nav-text">准备环境和依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#HBase-安装"><span class="nav-number">1.17.1.1.</span> <span class="nav-text">HBase 安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置-HBase"><span class="nav-number">1.17.1.2.</span> <span class="nav-text">配置 HBase</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#运行-HBase"><span class="nav-number">1.17.1.3.</span> <span class="nav-text">运行 HBase</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#验证是否安装成功"><span class="nav-number">1.17.1.4.</span> <span class="nav-text">验证是否安装成功</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动-HBase-Shell"><span class="nav-number">1.17.1.5.</span> <span class="nav-text">启动 HBase Shell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#停止-HBase"><span class="nav-number">1.17.1.6.</span> <span class="nav-text">停止 HBase</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HBase-常用命令"><span class="nav-number">1.17.1.7.</span> <span class="nav-text">HBase 常用命令</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#添加依赖-2"><span class="nav-number">1.17.1.8.</span> <span class="nav-text">添加依赖</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-使用-TableInputFormat-读取-HBase-批量数据"><span class="nav-number">1.17.2.</span> <span class="nav-text">Flink 使用 TableInputFormat 读取 HBase 批量数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#准备数据"><span class="nav-number">1.17.2.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flink-Job-代码"><span class="nav-number">1.17.2.2.</span> <span class="nav-text">Flink Job 代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-使用-TableOutputFormat-向-HBase-写入数据"><span class="nav-number">1.17.3.</span> <span class="nav-text">Flink 使用 TableOutputFormat 向 HBase 写入数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#添加依赖-3"><span class="nav-number">1.17.3.1.</span> <span class="nav-text">添加依赖</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flink-Job-代码-1"><span class="nav-number">1.17.3.2.</span> <span class="nav-text">Flink Job 代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-使用-HBaseOutputFormat-向-HBase-实时写入数据"><span class="nav-number">1.17.4.</span> <span class="nav-text">Flink 使用 HBaseOutputFormat 向 HBase 实时写入数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#读取数据"><span class="nav-number">1.17.4.1.</span> <span class="nav-text">读取数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#写入数据"><span class="nav-number">1.17.4.2.</span> <span class="nav-text">写入数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置文件-2"><span class="nav-number">1.17.4.3.</span> <span class="nav-text">配置文件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#项目运行及验证"><span class="nav-number">1.17.5.</span> <span class="nav-text">项目运行及验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-15"><span class="nav-number">1.17.6.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十八、如何使用-Flink-Connectors-——-Redis？"><span class="nav-number">1.18.</span> <span class="nav-text">十八、如何使用 Flink Connectors —— Redis？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-Redis"><span class="nav-number">1.18.1.</span> <span class="nav-text">安装 Redis</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#下载安装"><span class="nav-number">1.18.1.1.</span> <span class="nav-text">下载安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#通过-HomeBrew-安装"><span class="nav-number">1.18.1.2.</span> <span class="nav-text">通过 HomeBrew 安装</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#准备商品数据发送至-Kafka"><span class="nav-number">1.18.2.</span> <span class="nav-text">准备商品数据发送至 Kafka</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-消费-Kafka-中商品数据"><span class="nav-number">1.18.3.</span> <span class="nav-text">Flink 消费 Kafka 中商品数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Redis-Connector-简介"><span class="nav-number">1.18.4.</span> <span class="nav-text">Redis Connector 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#添加依赖-4"><span class="nav-number">1.18.5.</span> <span class="nav-text">添加依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-写入数据到-Redis"><span class="nav-number">1.18.6.</span> <span class="nav-text">Flink 写入数据到 Redis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#验证写入结果"><span class="nav-number">1.18.7.</span> <span class="nav-text">验证写入结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-16"><span class="nav-number">1.18.8.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#十九、如何使用-Side-Output-来分流"><span class="nav-number">1.19.</span> <span class="nav-text">十九、如何使用 Side Output 来分流?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-Filter-分流"><span class="nav-number">1.19.1.</span> <span class="nav-text">使用 Filter 分流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-Split-分流"><span class="nav-number">1.19.2.</span> <span class="nav-text">使用 Split 分流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-Side-Output-分流"><span class="nav-number">1.19.3.</span> <span class="nav-text">使用 Side Output 分流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-17"><span class="nav-number">1.19.4.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二十、Flink-State-深度讲解"><span class="nav-number">1.20.</span> <span class="nav-text">二十、Flink State 深度讲解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么需要-state？"><span class="nav-number">1.20.1.</span> <span class="nav-text">为什么需要 state？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#State-的种类"><span class="nav-number">1.20.2.</span> <span class="nav-text">State 的种类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Keyed-State"><span class="nav-number">1.20.3.</span> <span class="nav-text">Keyed State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Operator-State"><span class="nav-number">1.20.4.</span> <span class="nav-text">Operator State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raw-and-Managed-State"><span class="nav-number">1.20.5.</span> <span class="nav-text">Raw and Managed State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何使用托管-Keyed-State"><span class="nav-number">1.20.6.</span> <span class="nav-text">如何使用托管 Keyed State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#State-TTL-存活时间"><span class="nav-number">1.20.7.</span> <span class="nav-text">State TTL(存活时间)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#State-TTL-介绍"><span class="nav-number">1.20.7.1.</span> <span class="nav-text">State TTL 介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#清除过期-state"><span class="nav-number">1.20.7.2.</span> <span class="nav-text">清除过期 state</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何使用托管-Operator-State"><span class="nav-number">1.20.8.</span> <span class="nav-text">如何使用托管 Operator State</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CheckpointedFunction"><span class="nav-number">1.20.8.1.</span> <span class="nav-text">CheckpointedFunction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ListCheckpointed"><span class="nav-number">1.20.8.2.</span> <span class="nav-text">ListCheckpointed</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stateful-Source-Functions"><span class="nav-number">1.20.9.</span> <span class="nav-text">Stateful Source Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Broadcast-State"><span class="nav-number">1.20.10.</span> <span class="nav-text">Broadcast State</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Broadcast-State-如何使用"><span class="nav-number">1.20.10.1.</span> <span class="nav-text">Broadcast State 如何使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BroadcastProcessFunction-和-KeyedBroadcastProcessFunction"><span class="nav-number">1.20.10.2.</span> <span class="nav-text">BroadcastProcessFunction 和 KeyedBroadcastProcessFunction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-Broadcast-state-需要注意"><span class="nav-number">1.20.10.3.</span> <span class="nav-text">使用 Broadcast state 需要注意</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Queryable-State"><span class="nav-number">1.20.11.</span> <span class="nav-text">Queryable State</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-18"><span class="nav-number">1.20.12.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二十一、如何选择-Flink-状态后端存储"><span class="nav-number">1.21.</span> <span class="nav-text">二十一、如何选择 Flink 状态后端存储?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#State-Backends"><span class="nav-number">1.21.1.</span> <span class="nav-text">State Backends</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何使用-MemoryStateBackend-及剖析"><span class="nav-number">1.21.2.</span> <span class="nav-text">如何使用 MemoryStateBackend 及剖析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何使用-FsStateBackend-及剖析"><span class="nav-number">1.21.3.</span> <span class="nav-text">如何使用 FsStateBackend 及剖析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何使用-RocksDBStateBackend-及剖析"><span class="nav-number">1.21.4.</span> <span class="nav-text">如何使用 RocksDBStateBackend 及剖析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何选择状态后端存储？"><span class="nav-number">1.21.5.</span> <span class="nav-text">如何选择状态后端存储？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-19"><span class="nav-number">1.21.6.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二十二、Flink-Checkpoint-和-Savepoint-区别及其如何配置使用？"><span class="nav-number">1.22.</span> <span class="nav-text">二十二、Flink Checkpoint 和 Savepoint 区别及其如何配置使用？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Checkpoint-介绍及使用"><span class="nav-number">1.22.1.</span> <span class="nav-text">Checkpoint 介绍及使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用"><span class="nav-number">1.22.2.</span> <span class="nav-text">Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Checkpoint-流程"><span class="nav-number">1.22.3.</span> <span class="nav-text">Checkpoint 流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于-RocksDB-的增量-Checkpoint-实现原理"><span class="nav-number">1.22.3.1.</span> <span class="nav-text">基于 RocksDB 的增量 Checkpoint 实现原理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#状态如何从-Checkpoint-恢复"><span class="nav-number">1.22.4.</span> <span class="nav-text">状态如何从 Checkpoint 恢复</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#状态如何从-Savepoint-恢复"><span class="nav-number">1.22.5.</span> <span class="nav-text">状态如何从 Savepoint 恢复</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-20"><span class="nav-number">1.22.6.</span> <span class="nav-text">小结与反思</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二十三、Flink-Table-amp-SQL-概念与通用-API"><span class="nav-number">1.23.</span> <span class="nav-text">二十三、Flink Table &amp; SQL 概念与通用 API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#新增-Blink-SQL-查询处理器"><span class="nav-number">1.23.1.</span> <span class="nav-text">新增 Blink SQL 查询处理器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么选择-Table-API-amp-SQL？"><span class="nav-number">1.23.2.</span> <span class="nav-text">为什么选择 Table API&amp;SQL？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-Table-项目模块"><span class="nav-number">1.23.3.</span> <span class="nav-text">Flink Table 项目模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#两种-planner-之间的区别"><span class="nav-number">1.23.4.</span> <span class="nav-text">两种 planner 之间的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#添加项目依赖"><span class="nav-number">1.23.5.</span> <span class="nav-text">添加项目依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建一个-TableEnvironment"><span class="nav-number">1.23.6.</span> <span class="nav-text">创建一个 TableEnvironment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Table-API-amp-SQL-应用程序的结构"><span class="nav-number">1.23.7.</span> <span class="nav-text">Table API&amp;SQL 应用程序的结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Catalog-中注册-Table"><span class="nav-number">1.23.8.</span> <span class="nav-text">Catalog 中注册 Table</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#注册-Table"><span class="nav-number">1.23.8.1.</span> <span class="nav-text">注册 Table</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#注册-TableSource"><span class="nav-number">1.23.8.2.</span> <span class="nav-text">注册 TableSource</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#注册-TableSink"><span class="nav-number">1.23.8.3.</span> <span class="nav-text">注册 TableSink</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#注册外部的-Catalog"><span class="nav-number">1.23.9.</span> <span class="nav-text">注册外部的 Catalog</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查询-Table"><span class="nav-number">1.23.10.</span> <span class="nav-text">查询 Table</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Table-API"><span class="nav-number">1.23.10.1.</span> <span class="nav-text">Table API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SQL"><span class="nav-number">1.23.10.2.</span> <span class="nav-text">SQL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Table-API-amp-SQL"><span class="nav-number">1.23.10.3.</span> <span class="nav-text">Table API&amp;SQL</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提交-Table"><span class="nav-number">1.23.11.</span> <span class="nav-text">提交 Table</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#翻译并执行查询"><span class="nav-number">1.23.12.</span> <span class="nav-text">翻译并执行查询</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-21"><span class="nav-number">1.23.13.</span> <span class="nav-text">小结与反思</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#二十四、Flink-Table-API-amp-SQL-功能"><span class="nav-number">2.</span> <span class="nav-text">二十四、Flink Table API &amp; SQL 功能</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Flink-Table-和-SQL-与-DataStream-和-DataSet-集成"><span class="nav-number">2.0.1.</span> <span class="nav-text">Flink Table 和 SQL 与 DataStream 和 DataSet 集成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Scala-的隐式转换"><span class="nav-number">2.0.1.1.</span> <span class="nav-text">Scala 的隐式转换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#将-DataStream-或-DataSet-注册为-Table"><span class="nav-number">2.0.1.2.</span> <span class="nav-text">将 DataStream 或 DataSet 注册为 Table</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#将-DataStream-或-DataSet-转换为-Table"><span class="nav-number">2.0.1.3.</span> <span class="nav-text">将 DataStream 或 DataSet 转换为 Table</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#将-Table-转换成-DataStream-或-DataSet"><span class="nav-number">2.0.1.4.</span> <span class="nav-text">将 Table 转换成 DataStream 或 DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#将-Table-转换成-DataStream"><span class="nav-number">2.0.1.4.1.</span> <span class="nav-text">将 Table 转换成 DataStream</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#将-Table-转换成-DataSet"><span class="nav-number">2.0.1.4.2.</span> <span class="nav-text">将 Table 转换成 DataSet</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查询优化"><span class="nav-number">2.0.2.</span> <span class="nav-text">查询优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#解释-Table"><span class="nav-number">2.0.2.1.</span> <span class="nav-text">解释 Table</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据类型"><span class="nav-number">2.0.3.</span> <span class="nav-text">数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#时间属性"><span class="nav-number">2.0.4.</span> <span class="nav-text">时间属性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Processing-Time-1"><span class="nav-number">2.0.4.1.</span> <span class="nav-text">Processing Time</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Event-time"><span class="nav-number">2.0.4.2.</span> <span class="nav-text">Event time</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL-Connector"><span class="nav-number">2.0.5.</span> <span class="nav-text">SQL Connector</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#使用代码"><span class="nav-number">2.0.5.1.</span> <span class="nav-text">使用代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-YAML-文件"><span class="nav-number">2.0.5.2.</span> <span class="nav-text">使用 YAML 文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-DDL"><span class="nav-number">2.0.5.3.</span> <span class="nav-text">使用 DDL</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL-Client"><span class="nav-number">2.0.6.</span> <span class="nav-text">SQL Client</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive"><span class="nav-number">2.0.7.</span> <span class="nav-text">Hive</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结与反思-22"><span class="nav-number">2.0.8.</span> <span class="nav-text">小结与反思</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fly Hugh</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">194.7k</span>
  
</div>


<!-- 
  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>

-->



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

       <!-- 页面点击小红心 -->
    <script type="text/javascript" src="/js/src/src/clicklove.js"></script>

    
  </div>



  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("72l8G3xFxrAReJk0PBs8jCeC-gzGzoHsz", "r0TXzhifsg8y5LTKokFte6fz");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  





</body>


</html>

